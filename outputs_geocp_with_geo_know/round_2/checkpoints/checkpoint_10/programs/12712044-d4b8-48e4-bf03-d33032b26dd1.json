{"id": "12712044-d4b8-48e4-bf03-d33032b26dd1", "code": "# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float, regularization: float = 1e-6, adaptive: bool = True) -> NDArray:\n    \"\"\"\n    Kernel smoothing function with optional adaptive bandwidth and regularization.\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: base distance decay parameter\n    :param regularization: small constant to avoid overfitting to spatial clusters\n    :param adaptive: if True, adapt bandwidth per test point based on local calibration density\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n\n    # Adaptive bandwidth: for each test point, set bandwidth proportional to the distance to the k-th nearest calibration point\n    if adaptive:\n        k = min(20, z_calib.shape[0])\n        sort_dist = np.sort(distances, axis=1)\n        local_bandwidths = np.maximum(sort_dist[:, k-1], bandwidth * 0.5)  # never below base bandwidth * 0.5\n        # prevent zero bandwidth\n        local_bandwidths = np.maximum(local_bandwidths, 1e-6)\n        # Reshape for broadcasting\n        weights = gaussian_kernel(distances / local_bandwidths[:, None])\n    else:\n        weights = gaussian_kernel(distances / bandwidth)\n    # Regularization to mitigate excessive influence of close points (spatial autocorrelation)\n    weights += regularization\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1, \n                 kernel_regularization: float = 1e-6, adaptive_kernel: bool = True):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n        self.kernel_regularization = kernel_regularization\n        self.adaptive_kernel = adaptive_kernel\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        # Use standard conformal quantile\n        q_level = 1 - self.miscoverage_level\n        # Use adaptive kernel smoothing with regularization\n        weights = kernel_smoothing(coord_test, self.coord_calib, self.bandwidth, \n                                  regularization=self.kernel_regularization, adaptive=self.adaptive_kernel)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1,\n            kernel_regularization=1e-4, adaptive_kernel=True\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n", "language": "python", "parent_id": "12bd8670-9535-4369-afcf-1a0ad2e160a1", "generation": 1, "timestamp": 1758610058.727318, "iteration_found": 4, "metrics": {"interval_score": 53.29993077610273, "coverage": 0.9333333333333333, "avg_interval_length": 18.585946515472425, "combined_score": -53.29993077610273}, "complexity": 0.0, "diversity": 0.0, "metadata": {"island": 1}, "prompts": null, "artifacts_json": null, "artifact_dir": null}