{"id": "b81196ed-b61c-45de-b01d-1663e397129a", "code": "# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float, lmbda: float = 0.0) -> NDArray:\n    \"\"\"\n    Kernel smoothing function with optional Tikhonov regularization.\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: distance decay parameter (can be scalar or array for adaptive)\n    :param lmbda: regularization parameter to control spatial autocorrelation\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    # Allow for adaptive bandwidth: if bandwidth is array, each row is for one test point\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n    # Adaptive bandwidth: bandwidth can be 1D (length = n_test)\n    if np.ndim(bandwidth) == 1:\n        bw = bandwidth.reshape(-1, 1)\n        weights = gaussian_kernel(distances / bw)\n    else:\n        weights = gaussian_kernel(distances / bandwidth)\n    # Regularization to avoid overfitting to local clusters (spatial autocorrelation)\n    if lmbda > 0:\n        weights = weights + lmbda\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    # Avoid division by zero\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1, lmbda: float = 0.01, adaptive_bandwidth: bool = True, k_neighbors: int = 30):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n        self.lmbda = lmbda\n        self.adaptive_bandwidth = adaptive_bandwidth\n        self.k_neighbors = k_neighbors\n\n    def _compute_adaptive_bandwidth(self, z_test, z_calib):\n        \"\"\"\n        Compute adaptive bandwidth for each test point based on k-nearest spatial neighbors.\n        This method supports multiscale bandwidths by evaluating several k values and selecting the best via cross-validation.\n        \"\"\"\n        from sklearn.neighbors import NearestNeighbors\n        # Dynamically determine range of k for multiscale bandwidths based on spatial density\n        max_k = min(50, len(z_calib) // 10)\n        k_choices = np.linspace(10, max_k, num=5, dtype=int)\n        best_bandwidth = None\n        best_score = np.inf\n        # Calculate neighbors once and slice for each k\n        nbrs = NearestNeighbors(n_neighbors=max_k, algorithm='auto').fit(z_calib)\n        distances, _ = nbrs.kneighbors(z_calib)\n        for k in k_choices:\n            bw_cand = np.median(distances[:, :k], axis=1)\n            bw_cand = np.median(distances, axis=1)\n            # Compute weights for a held-out fold in calibration set\n            # To keep complexity low, use a hold-out split (not full k-fold)\n            split = int(0.8 * len(z_calib))\n            idx_train, idx_val = np.arange(split), np.arange(split, len(z_calib))\n            z_train, z_val = z_calib[idx_train], z_calib[idx_val]\n            bw_val = bw_cand[idx_val]\n            # Adjust regularization parameter based on spatial autocorrelation\n            spatial_autocorrelation = np.mean(np.std(z_calib, axis=0))\n            adjusted_lmbda = self.lmbda * spatial_autocorrelation\n            weights = kernel_smoothing(z_val, z_train, bw_val, adjusted_lmbda)\n            # For this fold, use weighted quantile and interval score\n            y_calib_pred_fold = self.predict_f(self.x_calib[idx_train])\n            nc_scores_fold = np.abs(y_calib_pred_fold - self.y_calib[idx_train])\n            q_level = 1 - self.miscoverage_level\n            geo_uncertainty_val = weighted_quantile(nc_scores_fold, weights, q_level)\n            y_val_pred = self.predict_f(self.x_calib[idx_val])\n            upper = y_val_pred + geo_uncertainty_val\n            lower = y_val_pred - geo_uncertainty_val\n            # Use mean interval length as proxy for scoring bandwidths\n            mean_length = np.mean(upper - lower)\n            if mean_length < best_score:\n                best_score = mean_length\n                best_bandwidth = np.median(distances, axis=1)\n        # Now compute for test points using best k\n        # Use precomputed distances for test points\n        distances_test, _ = nbrs.kneighbors(z_test, n_neighbors=max_k)\n        best_k = k_choices[np.argmin([k for k in k_choices])]\n        bw = np.median(distances_test[:, :best_k], axis=1)\n        bw = np.clip(bw, 1e-3, np.inf)\n        return bw\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        \"\"\"\n        Enhanced geo_conformalize with bandwidth selection by cross-validation and spatial autocorrelation regularization.\n        \"\"\"\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        q_level = 1 - self.miscoverage_level\n        # Improved: bandwidth selected by cross-validation/multiscale for each test point\n        if self.adaptive_bandwidth:\n            bandwidth = self._compute_adaptive_bandwidth(coord_test, self.coord_calib)\n        else:\n            bandwidth = self.bandwidth\n        # Adaptive kernel: use weights based on local calibration point density\n        weights = kernel_smoothing(coord_test, self.coord_calib, bandwidth, self.lmbda)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        # Use adaptive bandwidth and regularization in GeoConformalRegressor\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1,\n            lmbda=0.01, adaptive_bandwidth=True, k_neighbors=30\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n", "language": "python", "parent_id": "c0e210c9-41fb-4473-ab89-5c9753358f88", "generation": 1, "timestamp": 1758611107.3103812, "iteration_found": 8, "metrics": {"interval_score": 51.27697052374268, "coverage": 0.9233333333333333, "avg_interval_length": 16.882828777903228, "combined_score": -51.27697052374268}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 8 lines with 10 lines\nChange 2: Replace weights = kernel_smoothing(z_val, z_train, bw_val, self.lmbda) with 4 lines\nChange 3: Replace 3 lines with 4 lines", "parent_metrics": {"interval_score": 51.27697052374268, "coverage": 0.9233333333333333, "avg_interval_length": 16.882828777903228, "combined_score": -51.27697052374268}, "island": 0}, "prompts": {"diff_user": {"system": "You are tasked with evolving the GeoConformal Prediction algorithm aiming to minimize interval scores through enhancements in model structure and parameters. Focus on refining the kernel smoothing methodology by integrating advanced bandwidth selection techniques such as k-fold cross-validation and multiscale bandwidths that utilize geographic variations. Introduce regularization parameters to effectively manage spatial autocorrelation, enhancing overall model performance. Propose modifications to optimization operators, including dynamic bandwidth adjustments based on spatial data distributions, as well as adaptive kernel methods reflecting variations in spatial density. The evolutionary search should prioritize minimizing both average interval length and proximity to user-defined coverage while ensuring computational efficiency. \n\nConstraints: Ensure new parameters or operators do not significantly increase algorithmic complexity. Maintain compatibility with existing data structures for seamless integration with current training and calibration datasets. Address spatial autocorrelation issues explicitly.\n\nExpected outputs should include a refined GeoConformalRegressor class featuring an enhanced geo_conformalize method with detailed evaluations of resulting interval scores across multiple iterations. Optimize the model based on validation metrics, avoiding dependency on the training set while effectively leveraging both local and spatial effects during the modeling process.", "user": "# Current Program Information\n- Fitness: -51.2770\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at -51.2770\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: interval_score: 51.2770, coverage: 0.9233, avg_interval_length: 16.8828, combined_score: -51.2770\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: -51.2770)\n```python\n# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float, lmbda: float = 0.0) -> NDArray:\n    \"\"\"\n    Kernel smoothing function with optional Tikhonov regularization.\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: distance decay parameter (can be scalar or array for adaptive)\n    :param lmbda: regularization parameter to control spatial autocorrelation\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    # Allow for adaptive bandwidth: if bandwidth is array, each row is for one test point\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n    # Adaptive bandwidth: bandwidth can be 1D (length = n_test)\n    if np.ndim(bandwidth) == 1:\n        bw = bandwidth.reshape(-1, 1)\n        weights = gaussian_kernel(distances / bw)\n    else:\n        weights = gaussian_kernel(distances / bandwidth)\n    # Regularization to avoid overfitting to local clusters (spatial autocorrelation)\n    if lmbda > 0:\n        weights = weights + lmbda\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    # Avoid division by zero\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1, lmbda: float = 0.01, adaptive_bandwidth: bool = True, k_neighbors: int = 30):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n        self.lmbda = lmbda\n        self.adaptive_bandwidth = adaptive_bandwidth\n        self.k_neighbors = k_neighbors\n\n    def _compute_adaptive_bandwidth(self, z_test, z_calib):\n        \"\"\"\n        Compute adaptive bandwidth for each test point based on k-nearest spatial neighbors.\n        This method supports multiscale bandwidths by evaluating several k values and selecting the best via cross-validation.\n        \"\"\"\n        from sklearn.neighbors import NearestNeighbors\n        # Use a range of k for multiscale bandwidths\n        k_choices = [10, 20, 30, 50]\n        best_bandwidth = None\n        best_score = np.inf\n        # Cross-validation based bandwidth search on calibration set\n        for k in k_choices:\n            nbrs = NearestNeighbors(n_neighbors=min(k, len(z_calib)), algorithm='auto').fit(z_calib)\n            distances, _ = nbrs.kneighbors(z_calib)\n            bw_cand = np.median(distances, axis=1)\n            # Compute weights for a held-out fold in calibration set\n            # To keep complexity low, use a hold-out split (not full k-fold)\n            split = int(0.8 * len(z_calib))\n            idx_train, idx_val = np.arange(split), np.arange(split, len(z_calib))\n            z_train, z_val = z_calib[idx_train], z_calib[idx_val]\n            bw_val = bw_cand[idx_val]\n            weights = kernel_smoothing(z_val, z_train, bw_val, self.lmbda)\n            # For this fold, use weighted quantile and interval score\n            y_calib_pred_fold = self.predict_f(self.x_calib[idx_train])\n            nc_scores_fold = np.abs(y_calib_pred_fold - self.y_calib[idx_train])\n            q_level = 1 - self.miscoverage_level\n            geo_uncertainty_val = weighted_quantile(nc_scores_fold, weights, q_level)\n            y_val_pred = self.predict_f(self.x_calib[idx_val])\n            upper = y_val_pred + geo_uncertainty_val\n            lower = y_val_pred - geo_uncertainty_val\n            # Use mean interval length as proxy for scoring bandwidths\n            mean_length = np.mean(upper - lower)\n            if mean_length < best_score:\n                best_score = mean_length\n                best_bandwidth = np.median(distances, axis=1)\n        # Now compute for test points using best k\n        nbrs = NearestNeighbors(n_neighbors=min(k_choices[np.argmin([k for k in k_choices])], len(z_calib)), algorithm='auto').fit(z_calib)\n        distances, _ = nbrs.kneighbors(z_test)\n        bw = np.median(distances, axis=1)\n        bw = np.clip(bw, 1e-3, np.inf)\n        return bw\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        \"\"\"\n        Enhanced geo_conformalize with bandwidth selection by cross-validation and spatial autocorrelation regularization.\n        \"\"\"\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        q_level = 1 - self.miscoverage_level\n        # Improved: bandwidth selected by cross-validation/multiscale for each test point\n        if self.adaptive_bandwidth:\n            bandwidth = self._compute_adaptive_bandwidth(coord_test, self.coord_calib)\n        else:\n            bandwidth = self.bandwidth\n        # Adaptive kernel: use weights based on local calibration point density\n        weights = kernel_smoothing(coord_test, self.coord_calib, bandwidth, self.lmbda)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        # Use adaptive bandwidth and regularization in GeoConformalRegressor\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1,\n            lmbda=0.01, adaptive_bandwidth=True, k_neighbors=30\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n\n```\nKey features: Performs well on interval_score (51.2770), Performs well on coverage (0.9233), Performs well on avg_interval_length (16.8828), Performs well on combined_score (-51.2770)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float, lmbda: float = 0.0) -> NDArray:\n    \"\"\"\n    Kernel smoothing function with optional Tikhonov regularization.\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: distance decay parameter (can be scalar or array for adaptive)\n    :param lmbda: regularization parameter to control spatial autocorrelation\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    # Allow for adaptive bandwidth: if bandwidth is array, each row is for one test point\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n    # Adaptive bandwidth: bandwidth can be 1D (length = n_test)\n    if np.ndim(bandwidth) == 1:\n        bw = bandwidth.reshape(-1, 1)\n        weights = gaussian_kernel(distances / bw)\n    else:\n        weights = gaussian_kernel(distances / bandwidth)\n    # Regularization to avoid overfitting to local clusters (spatial autocorrelation)\n    if lmbda > 0:\n        weights = weights + lmbda\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    # Avoid division by zero\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1, lmbda: float = 0.01, adaptive_bandwidth: bool = True, k_neighbors: int = 30):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n        self.lmbda = lmbda\n        self.adaptive_bandwidth = adaptive_bandwidth\n        self.k_neighbors = k_neighbors\n\n    def _compute_adaptive_bandwidth(self, z_test, z_calib):\n        \"\"\"\n        Compute adaptive bandwidth for each test point based on k-nearest spatial neighbors.\n        This method supports multiscale bandwidths by evaluating several k values and selecting the best via cross-validation.\n        \"\"\"\n        from sklearn.neighbors import NearestNeighbors\n        # Use a range of k for multiscale bandwidths\n        k_choices = [10, 20, 30, 50]\n        best_bandwidth = None\n        best_score = np.inf\n        # Cross-validation based bandwidth search on calibration set\n        for k in k_choices:\n            nbrs = NearestNeighbors(n_neighbors=min(k, len(z_calib)), algorithm='auto').fit(z_calib)\n            distances, _ = nbrs.kneighbors(z_calib)\n            bw_cand = np.median(distances, axis=1)\n            # Compute weights for a held-out fold in calibration set\n            # To keep complexity low, use a hold-out split (not full k-fold)\n            split = int(0.8 * len(z_calib))\n            idx_train, idx_val = np.arange(split), np.arange(split, len(z_calib))\n            z_train, z_val = z_calib[idx_train], z_calib[idx_val]\n            bw_val = bw_cand[idx_val]\n            weights = kernel_smoothing(z_val, z_train, bw_val, self.lmbda)\n            # For this fold, use weighted quantile and interval score\n            y_calib_pred_fold = self.predict_f(self.x_calib[idx_train])\n            nc_scores_fold = np.abs(y_calib_pred_fold - self.y_calib[idx_train])\n            q_level = 1 - self.miscoverage_level\n            geo_uncertainty_val = weighted_quantile(nc_scores_fold, weights, q_level)\n            y_val_pred = self.predict_f(self.x_calib[idx_val])\n            upper = y_val_pred + geo_uncertainty_val\n            lower = y_val_pred - geo_uncertainty_val\n            # Use mean interval length as proxy for scoring bandwidths\n            mean_length = np.mean(upper - lower)\n            if mean_length < best_score:\n                best_score = mean_length\n                best_bandwidth = np.median(distances, axis=1)\n        # Now compute for test points using best k\n        nbrs = NearestNeighbors(n_neighbors=min(k_choices[np.argmin([k for k in k_choices])], len(z_calib)), algorithm='auto').fit(z_calib)\n        distances, _ = nbrs.kneighbors(z_test)\n        bw = np.median(distances, axis=1)\n        bw = np.clip(bw, 1e-3, np.inf)\n        return bw\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        \"\"\"\n        Enhanced geo_conformalize with bandwidth selection by cross-validation and spatial autocorrelation regularization.\n        \"\"\"\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        q_level = 1 - self.miscoverage_level\n        # Improved: bandwidth selected by cross-validation/multiscale for each test point\n        if self.adaptive_bandwidth:\n            bandwidth = self._compute_adaptive_bandwidth(coord_test, self.coord_calib)\n        else:\n            bandwidth = self.bandwidth\n        # Adaptive kernel: use weights based on local calibration point density\n        weights = kernel_smoothing(coord_test, self.coord_calib, bandwidth, self.lmbda)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        # Use adaptive bandwidth and regularization in GeoConformalRegressor\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1,\n            lmbda=0.01, adaptive_bandwidth=True, k_neighbors=30\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: score, complexity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["To improve the GeoConformal Prediction algorithm, we will make targeted enhancements focusing on bandwidth selection, regularization, and computational efficiency. These improvements aim to optimize the interval scores and maintain computational efficiency.\n\n### 1. Implementing Dynamic Bandwidth Adjustments\nThe current implementation uses a fixed set of k-values for cross-validation. We can enhance this by allowing the k-values to be dynamically adjusted based on the spatial distribution of data. This can be achieved by making the k-values depend on the density of calibration points, which might help in better capturing local spatial patterns.\n\n### 2. Optimizing Regularization Parameter\nThe regularization parameter `lmbda` is currently fixed. Introducing a dynamic mechanism to adjust `lmbda` based on spatial autocorrelation metrics could improve handling of spatial autocorrelation, leading to better interval scores.\n\n### 3. Improving Computational Efficiency\nInstead of recalculating the nearest neighbors for each k-value, we can calculate it once and slice the results, which reduces redundant computations and improves efficiency.\n\nLet's implement these changes:\n\n```python\n<<<<<<< SEARCH\n        # Use a range of k for multiscale bandwidths\n        k_choices = [10, 20, 30, 50]\n        best_bandwidth = None\n        best_score = np.inf\n        # Cross-validation based bandwidth search on calibration set\n        for k in k_choices:\n            nbrs = NearestNeighbors(n_neighbors=min(k, len(z_calib)), algorithm='auto').fit(z_calib)\n            distances, _ = nbrs.kneighbors(z_calib)\n=======\n        # Dynamically determine range of k for multiscale bandwidths based on spatial density\n        max_k = min(50, len(z_calib) // 10)\n        k_choices = np.linspace(10, max_k, num=5, dtype=int)\n        best_bandwidth = None\n        best_score = np.inf\n        # Calculate neighbors once and slice for each k\n        nbrs = NearestNeighbors(n_neighbors=max_k, algorithm='auto').fit(z_calib)\n        distances, _ = nbrs.kneighbors(z_calib)\n        for k in k_choices:\n            bw_cand = np.median(distances[:, :k], axis=1)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n            weights = kernel_smoothing(z_val, z_train, bw_val, self.lmbda)\n=======\n            # Adjust regularization parameter based on spatial autocorrelation\n            spatial_autocorrelation = np.mean(np.std(z_calib, axis=0))\n            adjusted_lmbda = self.lmbda * spatial_autocorrelation\n            weights = kernel_smoothing(z_val, z_train, bw_val, adjusted_lmbda)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        nbrs = NearestNeighbors(n_neighbors=min(k_choices[np.argmin([k for k in k_choices])], len(z_calib)), algorithm='auto').fit(z_calib)\n        distances, _ = nbrs.kneighbors(z_test)\n        bw = np.median(distances, axis=1)\n=======\n        # Use precomputed distances for test points\n        distances_test, _ = nbrs.kneighbors(z_test, n_neighbors=max_k)\n        best_k = k_choices[np.argmin([k for k in k_choices])]\n        bw = np.median(distances_test[:, :best_k], axis=1)\n>>>>>>> REPLACE\n```\n\n### Explanation\n1. **Dynamic k-values:** By determining the maximum k-value based on the size of the calibration dataset, we adapt the bandwidth selection process to the spatial data characteristics, potentially leading to better interval scores.\n\n2. **Adaptive Regularization:** By adjusting `lmbda` based on the spatial autocorrelation, we dynamically control the influence of spatial patterns on the model, which can improve handling of spatial dependencies.\n\n3. **Efficiency Improvement:** Precomputing nearest neighbors and slicing the distances array for different k-values reduces redundant calculations, enhancing computational efficiency.\n\nThese changes should help in minimizing interval scores while maintaining the model's adaptability to spatial data characteristics."]}}, "artifacts_json": null, "artifact_dir": null}