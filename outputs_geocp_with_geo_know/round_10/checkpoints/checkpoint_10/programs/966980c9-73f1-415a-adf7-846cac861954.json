{"id": "966980c9-73f1-415a-adf7-846cac861954", "code": "# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float, lmbda: float = 0.0) -> NDArray:\n    \"\"\"\n    Kernel smoothing function with optional Tikhonov regularization.\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: distance decay parameter (can be scalar or array for adaptive)\n    :param lmbda: regularization parameter to control spatial autocorrelation\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    # Allow for adaptive bandwidth: if bandwidth is array, each row is for one test point\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n    # Adaptive bandwidth: bandwidth can be 1D (length = n_test)\n    if np.ndim(bandwidth) == 1:\n        bw = bandwidth.reshape(-1, 1)\n        weights = gaussian_kernel(distances / bw)\n    else:\n        weights = gaussian_kernel(distances / bandwidth)\n    # Regularization to avoid overfitting to local clusters (spatial autocorrelation)\n    if lmbda > 0:\n        spatial_density_factor = np.mean(distances) / np.std(distances)\n        weights = weights + lmbda * spatial_density_factor\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    # Avoid division by zero\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1, lmbda: float = 0.01, adaptive_bandwidth: bool = True, k_neighbors: int = 30):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n        self.lmbda = lmbda\n        self.adaptive_bandwidth = adaptive_bandwidth\n        self.k_neighbors = k_neighbors\n\n    def _compute_adaptive_bandwidth(self, z_test, z_calib):\n        \"\"\"\n        Compute adaptive bandwidth for each test point based on k-nearest spatial neighbors.\n        This method supports multiscale bandwidths by evaluating several k values and selecting the best via cross-validation.\n        \"\"\"\n        from sklearn.neighbors import NearestNeighbors\n        # Use a range of k for multiscale bandwidths\n        k_choices = [10, 20, 30, 50]\n        best_bandwidth = None\n        best_score = np.inf\n        # Cross-validation based bandwidth search on calibration set\n        for k in k_choices:\n            nbrs = NearestNeighbors(n_neighbors=min(k, len(z_calib)), algorithm='auto').fit(z_calib)\n            distances, _ = nbrs.kneighbors(z_calib)\n            bw_cand = np.median(distances, axis=1)\n            # Compute weights for a held-out fold in calibration set\n            from sklearn.model_selection import KFold\n            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n            mean_length = 0\n            for train_index, val_index in kf.split(z_calib):\n                z_train, z_val = z_calib[train_index], z_calib[val_index]\n                bw_val = bw_cand[val_index]\n                weights = kernel_smoothing(z_val, z_train, bw_val, self.lmbda)\n                y_calib_pred_fold = self.predict_f(self.x_calib[train_index])\n                nc_scores_fold = np.abs(y_calib_pred_fold - self.y_calib[train_index])\n                q_level = 1 - self.miscoverage_level\n                geo_uncertainty_val = weighted_quantile(nc_scores_fold, weights, q_level)\n                y_val_pred = self.predict_f(self.x_calib[val_index])\n                upper = y_val_pred + geo_uncertainty_val\n                lower = y_val_pred - geo_uncertainty_val\n                mean_length += np.mean(upper - lower)\n            mean_length /= kf.get_n_splits()\n            if mean_length < best_score:\n                best_score = mean_length\n                best_bandwidth = np.median(distances, axis=1)\n        # Now compute for test points using best k\n        nbrs = NearestNeighbors(n_neighbors=min(k_choices[np.argmin([k for k in k_choices])], len(z_calib)), algorithm='auto').fit(z_calib)\n        distances, _ = nbrs.kneighbors(z_test)\n        bw = np.median(distances, axis=1)\n        bw = np.clip(bw, 1e-3, np.inf)\n        return bw\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        \"\"\"\n        Enhanced geo_conformalize with bandwidth selection by cross-validation and spatial autocorrelation regularization.\n        \"\"\"\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        q_level = 1 - self.miscoverage_level\n        # Improved: bandwidth selected by cross-validation/multiscale for each test point\n        if self.adaptive_bandwidth:\n            bandwidth = self._compute_adaptive_bandwidth(coord_test, self.coord_calib)\n        else:\n            bandwidth = self.bandwidth\n        # Adaptive kernel: use weights based on local calibration point density\n        weights = kernel_smoothing(coord_test, self.coord_calib, bandwidth, self.lmbda)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        # Use adaptive bandwidth and regularization in GeoConformalRegressor\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1,\n            lmbda=0.01, adaptive_bandwidth=True, k_neighbors=30\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n", "language": "python", "parent_id": "f1dc6210-4f80-45f7-b860-8f4a8f8911fb", "generation": 1, "timestamp": 1758612054.340859, "iteration_found": 10, "metrics": {"interval_score": 52.09919665393061, "coverage": 0.93, "avg_interval_length": 17.306056563659645, "combined_score": -52.09919665393061}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 16 lines with 16 lines\nChange 2: Replace 2 lines with 3 lines", "parent_metrics": {"interval_score": 51.27697052374268, "coverage": 0.9233333333333333, "avg_interval_length": 16.882828777903228, "combined_score": -51.27697052374268}, "island": 0}, "prompts": {"diff_user": {"system": "You are tasked with evolving the GeoConformal Prediction algorithm to optimize interval scores. Enhance the kernel smoothing approach by integrating dynamic k-fold cross-validation for optimal bandwidth selection and incorporating multiscale bandwidths to address geographic heterogeneity. Introduce regularization parameters to effectively manage spatial autocorrelation. Suggest modifications to optimization operators, including adaptive kernel methods that adjust bandwidth based on spatial data distributions and density variations. Implement a mechanism to evaluate calibration point density for robustness across different geographic contexts. The evolutionary search should aim to minimize average interval length and proximity to user-defined coverage goals while maintaining computational efficiency. Constraints include avoiding significant increases in algorithmic complexity and ensuring compatibility with existing data structures. Expected outputs should include an updated GeoConformalRegressor class featuring an enhanced geo_conformalize method, providing comprehensive evaluations of interval scores over multiple iterations. The model must be optimized using validation metrics that leverage local and spatial effects, independent of the training set.", "user": "# Current Program Information\n- Fitness: -51.2770\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at -51.2770\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: interval_score: 51.2770, coverage: 0.9233, avg_interval_length: 16.8828, combined_score: -51.2770\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: -51.2770)\n```python\n# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float, lmbda: float = 0.0) -> NDArray:\n    \"\"\"\n    Kernel smoothing function with optional Tikhonov regularization.\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: distance decay parameter (can be scalar or array for adaptive)\n    :param lmbda: regularization parameter to control spatial autocorrelation\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    # Allow for adaptive bandwidth: if bandwidth is array, each row is for one test point\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n    # Adaptive bandwidth: bandwidth can be 1D (length = n_test)\n    if np.ndim(bandwidth) == 1:\n        bw = bandwidth.reshape(-1, 1)\n        weights = gaussian_kernel(distances / bw)\n    else:\n        weights = gaussian_kernel(distances / bandwidth)\n    # Regularization to avoid overfitting to local clusters (spatial autocorrelation)\n    if lmbda > 0:\n        weights = weights + lmbda\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    # Avoid division by zero\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1, lmbda: float = 0.01, adaptive_bandwidth: bool = True, k_neighbors: int = 30):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n        self.lmbda = lmbda\n        self.adaptive_bandwidth = adaptive_bandwidth\n        self.k_neighbors = k_neighbors\n\n    def _compute_adaptive_bandwidth(self, z_test, z_calib):\n        \"\"\"\n        Compute adaptive bandwidth for each test point based on k-nearest spatial neighbors.\n        This method supports multiscale bandwidths by evaluating several k values and selecting the best via cross-validation.\n        \"\"\"\n        from sklearn.neighbors import NearestNeighbors\n        # Use a range of k for multiscale bandwidths\n        k_choices = [10, 20, 30, 50]\n        best_bandwidth = None\n        best_score = np.inf\n        # Cross-validation based bandwidth search on calibration set\n        for k in k_choices:\n            nbrs = NearestNeighbors(n_neighbors=min(k, len(z_calib)), algorithm='auto').fit(z_calib)\n            distances, _ = nbrs.kneighbors(z_calib)\n            bw_cand = np.median(distances, axis=1)\n            # Compute weights for a held-out fold in calibration set\n            # To keep complexity low, use a hold-out split (not full k-fold)\n            split = int(0.8 * len(z_calib))\n            idx_train, idx_val = np.arange(split), np.arange(split, len(z_calib))\n            z_train, z_val = z_calib[idx_train], z_calib[idx_val]\n            bw_val = bw_cand[idx_val]\n            weights = kernel_smoothing(z_val, z_train, bw_val, self.lmbda)\n            # For this fold, use weighted quantile and interval score\n            y_calib_pred_fold = self.predict_f(self.x_calib[idx_train])\n            nc_scores_fold = np.abs(y_calib_pred_fold - self.y_calib[idx_train])\n            q_level = 1 - self.miscoverage_level\n            geo_uncertainty_val = weighted_quantile(nc_scores_fold, weights, q_level)\n            y_val_pred = self.predict_f(self.x_calib[idx_val])\n            upper = y_val_pred + geo_uncertainty_val\n            lower = y_val_pred - geo_uncertainty_val\n            # Use mean interval length as proxy for scoring bandwidths\n            mean_length = np.mean(upper - lower)\n            if mean_length < best_score:\n                best_score = mean_length\n                best_bandwidth = np.median(distances, axis=1)\n        # Now compute for test points using best k\n        nbrs = NearestNeighbors(n_neighbors=min(k_choices[np.argmin([k for k in k_choices])], len(z_calib)), algorithm='auto').fit(z_calib)\n        distances, _ = nbrs.kneighbors(z_test)\n        bw = np.median(distances, axis=1)\n        bw = np.clip(bw, 1e-3, np.inf)\n        return bw\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        \"\"\"\n        Enhanced geo_conformalize with bandwidth selection by cross-validation and spatial autocorrelation regularization.\n        \"\"\"\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        q_level = 1 - self.miscoverage_level\n        # Improved: bandwidth selected by cross-validation/multiscale for each test point\n        if self.adaptive_bandwidth:\n            bandwidth = self._compute_adaptive_bandwidth(coord_test, self.coord_calib)\n        else:\n            bandwidth = self.bandwidth\n        # Adaptive kernel: use weights based on local calibration point density\n        weights = kernel_smoothing(coord_test, self.coord_calib, bandwidth, self.lmbda)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        # Use adaptive bandwidth and regularization in GeoConformalRegressor\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1,\n            lmbda=0.01, adaptive_bandwidth=True, k_neighbors=30\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n\n```\nKey features: Performs well on interval_score (51.2770), Performs well on coverage (0.9233), Performs well on avg_interval_length (16.8828), Performs well on combined_score (-51.2770)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float, lmbda: float = 0.0) -> NDArray:\n    \"\"\"\n    Kernel smoothing function with optional Tikhonov regularization.\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: distance decay parameter (can be scalar or array for adaptive)\n    :param lmbda: regularization parameter to control spatial autocorrelation\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    # Allow for adaptive bandwidth: if bandwidth is array, each row is for one test point\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n    # Adaptive bandwidth: bandwidth can be 1D (length = n_test)\n    if np.ndim(bandwidth) == 1:\n        bw = bandwidth.reshape(-1, 1)\n        weights = gaussian_kernel(distances / bw)\n    else:\n        weights = gaussian_kernel(distances / bandwidth)\n    # Regularization to avoid overfitting to local clusters (spatial autocorrelation)\n    if lmbda > 0:\n        weights = weights + lmbda\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    # Avoid division by zero\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1, lmbda: float = 0.01, adaptive_bandwidth: bool = True, k_neighbors: int = 30):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n        self.lmbda = lmbda\n        self.adaptive_bandwidth = adaptive_bandwidth\n        self.k_neighbors = k_neighbors\n\n    def _compute_adaptive_bandwidth(self, z_test, z_calib):\n        \"\"\"\n        Compute adaptive bandwidth for each test point based on k-nearest spatial neighbors.\n        This method supports multiscale bandwidths by evaluating several k values and selecting the best via cross-validation.\n        \"\"\"\n        from sklearn.neighbors import NearestNeighbors\n        # Use a range of k for multiscale bandwidths\n        k_choices = [10, 20, 30, 50]\n        best_bandwidth = None\n        best_score = np.inf\n        # Cross-validation based bandwidth search on calibration set\n        for k in k_choices:\n            nbrs = NearestNeighbors(n_neighbors=min(k, len(z_calib)), algorithm='auto').fit(z_calib)\n            distances, _ = nbrs.kneighbors(z_calib)\n            bw_cand = np.median(distances, axis=1)\n            # Compute weights for a held-out fold in calibration set\n            # To keep complexity low, use a hold-out split (not full k-fold)\n            split = int(0.8 * len(z_calib))\n            idx_train, idx_val = np.arange(split), np.arange(split, len(z_calib))\n            z_train, z_val = z_calib[idx_train], z_calib[idx_val]\n            bw_val = bw_cand[idx_val]\n            weights = kernel_smoothing(z_val, z_train, bw_val, self.lmbda)\n            # For this fold, use weighted quantile and interval score\n            y_calib_pred_fold = self.predict_f(self.x_calib[idx_train])\n            nc_scores_fold = np.abs(y_calib_pred_fold - self.y_calib[idx_train])\n            q_level = 1 - self.miscoverage_level\n            geo_uncertainty_val = weighted_quantile(nc_scores_fold, weights, q_level)\n            y_val_pred = self.predict_f(self.x_calib[idx_val])\n            upper = y_val_pred + geo_uncertainty_val\n            lower = y_val_pred - geo_uncertainty_val\n            # Use mean interval length as proxy for scoring bandwidths\n            mean_length = np.mean(upper - lower)\n            if mean_length < best_score:\n                best_score = mean_length\n                best_bandwidth = np.median(distances, axis=1)\n        # Now compute for test points using best k\n        nbrs = NearestNeighbors(n_neighbors=min(k_choices[np.argmin([k for k in k_choices])], len(z_calib)), algorithm='auto').fit(z_calib)\n        distances, _ = nbrs.kneighbors(z_test)\n        bw = np.median(distances, axis=1)\n        bw = np.clip(bw, 1e-3, np.inf)\n        return bw\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        \"\"\"\n        Enhanced geo_conformalize with bandwidth selection by cross-validation and spatial autocorrelation regularization.\n        \"\"\"\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        q_level = 1 - self.miscoverage_level\n        # Improved: bandwidth selected by cross-validation/multiscale for each test point\n        if self.adaptive_bandwidth:\n            bandwidth = self._compute_adaptive_bandwidth(coord_test, self.coord_calib)\n        else:\n            bandwidth = self.bandwidth\n        # Adaptive kernel: use weights based on local calibration point density\n        weights = kernel_smoothing(coord_test, self.coord_calib, bandwidth, self.lmbda)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        # Use adaptive bandwidth and regularization in GeoConformalRegressor\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1,\n            lmbda=0.01, adaptive_bandwidth=True, k_neighbors=30\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: score, complexity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["To enhance the GeoConformal Prediction algorithm, we will introduce several modifications. We aim to optimize the interval scores by incorporating dynamic k-fold cross-validation for bandwidth selection, introducing multiscale bandwidths, and adding regularization parameters for spatial autocorrelation management. Additionally, we'll make the kernel methods adaptive to adjust bandwidth based on spatial data distributions and density variations. These changes will help minimize average interval length and ensure coverage goals are met while maintaining computational efficiency.\n\nHere are the targeted improvements:\n\n1. **Dynamic k-Fold Cross-Validation for Bandwidth Selection**: We will replace the static split used for cross-validation with a dynamic k-fold cross-validation approach. This will ensure more robust bandwidth selection by leveraging multiple folds.\n\n2. **Multiscale Bandwidths**: We'll implement multiscale bandwidths to address geographic heterogeneity more effectively. This involves using different bandwidths for different regions based on local data density.\n\n3. **Adaptive Kernel Methods**: Introduce adaptive kernel methods that adjust bandwidth based on spatial data distributions and density variations, improving prediction accuracy.\n\n4. **Regularization for Spatial Autocorrelation**: We'll refine the regularization approach to manage spatial autocorrelation more effectively, ensuring the model does not overfit to local clusters.\n\nLet's proceed with the specific code changes:\n\n### Changes\n\n#### Change 1: Implement k-Fold Cross-Validation for Bandwidth Selection\nReplace the static hold-out split with k-fold cross-validation in the `_compute_adaptive_bandwidth` method.\n\n```python\n<<<<<<< SEARCH\n            # To keep complexity low, use a hold-out split (not full k-fold)\n            split = int(0.8 * len(z_calib))\n            idx_train, idx_val = np.arange(split), np.arange(split, len(z_calib))\n            z_train, z_val = z_calib[idx_train], z_calib[idx_val]\n            bw_val = bw_cand[idx_val]\n            weights = kernel_smoothing(z_val, z_train, bw_val, self.lmbda)\n            # For this fold, use weighted quantile and interval score\n            y_calib_pred_fold = self.predict_f(self.x_calib[idx_train])\n            nc_scores_fold = np.abs(y_calib_pred_fold - self.y_calib[idx_train])\n            q_level = 1 - self.miscoverage_level\n            geo_uncertainty_val = weighted_quantile(nc_scores_fold, weights, q_level)\n            y_val_pred = self.predict_f(self.x_calib[idx_val])\n            upper = y_val_pred + geo_uncertainty_val\n            lower = y_val_pred - geo_uncertainty_val\n            # Use mean interval length as proxy for scoring bandwidths\n            mean_length = np.mean(upper - lower)\n=======\n            from sklearn.model_selection import KFold\n            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n            mean_length = 0\n            for train_index, val_index in kf.split(z_calib):\n                z_train, z_val = z_calib[train_index], z_calib[val_index]\n                bw_val = bw_cand[val_index]\n                weights = kernel_smoothing(z_val, z_train, bw_val, self.lmbda)\n                y_calib_pred_fold = self.predict_f(self.x_calib[train_index])\n                nc_scores_fold = np.abs(y_calib_pred_fold - self.y_calib[train_index])\n                q_level = 1 - self.miscoverage_level\n                geo_uncertainty_val = weighted_quantile(nc_scores_fold, weights, q_level)\n                y_val_pred = self.predict_f(self.x_calib[val_index])\n                upper = y_val_pred + geo_uncertainty_val\n                lower = y_val_pred - geo_uncertainty_val\n                mean_length += np.mean(upper - lower)\n            mean_length /= kf.get_n_splits()\n>>>>>>> REPLACE\n```\n\n#### Change 2: Enhance Regularization for Spatial Autocorrelation\nAdjust the regularization to be more dynamic and based on spatial density.\n\n```python\n<<<<<<< SEARCH\n    if lmbda > 0:\n        weights = weights + lmbda\n=======\n    if lmbda > 0:\n        spatial_density_factor = np.mean(distances) / np.std(distances)\n        weights = weights + lmbda * spatial_density_factor\n>>>>>>> REPLACE\n```\n\nThese changes will make the GeoConformal Prediction algorithm more adaptive and robust across different geographic contexts while maintaining computational efficiency. The use of k-fold cross-validation for bandwidth selection ensures that we are leveraging the full potential of the data, and the improved regularization will help manage spatial autocorrelation more effectively."]}}, "artifacts_json": null, "artifact_dir": null}