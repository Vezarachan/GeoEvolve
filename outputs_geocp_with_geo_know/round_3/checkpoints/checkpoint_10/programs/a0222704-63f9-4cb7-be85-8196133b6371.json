{"id": "a0222704-63f9-4cb7-be85-8196133b6371", "code": "# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: NDArray, lmbda: float = 0.0) -> NDArray:\n    \"\"\"\n    Kernel smoothing function with optional Tikhonov regularization.\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: distance decay parameter (can be scalar or array for adaptive)\n    :param lmbda: regularization parameter to control spatial autocorrelation\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    # Allow for adaptive bandwidth: if bandwidth is array, each row is for one test point\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n    # Adaptive bandwidth: bandwidth can be 1D (length = n_test)\n    if np.ndim(bandwidth) == 1:\n        bw = bandwidth.reshape(-1, 1)\n        weights = gaussian_kernel(distances / bw)\n    else:\n        weights = gaussian_kernel(distances / bandwidth)\n    # Regularization to avoid overfitting to local clusters (spatial autocorrelation)\n    if lmbda > 0:\n        weights = weights + lmbda\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    # Avoid division by zero\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1, lmbda: float = 0.01, adaptive_bandwidth: bool = True, k_neighbors: int = 30):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n        self.lmbda = lmbda\n        self.adaptive_bandwidth = adaptive_bandwidth\n        self.k_neighbors = k_neighbors\n\n    def _compute_adaptive_bandwidth(self, z_test, z_calib):\n        \"\"\"\n        Compute adaptive bandwidth for each test point using cross-validation.\n        \"\"\"\n        from sklearn.model_selection import GridSearchCV\n        from sklearn.neighbors import KernelDensity\n\n        # Set a range of bandwidths to search\n        bandwidths = np.linspace(0.01, 1.0, 20)\n        grid = GridSearchCV(KernelDensity(), {'bandwidth': bandwidths}, cv=5)\n        grid.fit(z_calib)\n\n        best_bandwidth = grid.best_params_['bandwidth']\n        bw = np.full(z_test.shape[0], best_bandwidth)\n        \n        return bw\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        q_level = 1 - self.miscoverage_level\n        # Dynamic bandwidth adjustment based on spatial density\n        if self.adaptive_bandwidth:\n            bandwidth = self._compute_adaptive_bandwidth(coord_test, self.coord_calib)\n        else:\n            bandwidth = self.bandwidth\n        weights = kernel_smoothing(coord_test, self.coord_calib, bandwidth, self.lmbda)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        # Use adaptive bandwidth and regularization in GeoConformalRegressor\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1,\n            lmbda=0.01, adaptive_bandwidth=True, k_neighbors=30\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n", "language": "python", "parent_id": "bd8a7ea3-31a2-4299-bb93-5ae75576e542", "generation": 1, "timestamp": 1758610468.784096, "iteration_found": 7, "metrics": {"interval_score": 54.94309748840329, "coverage": 0.92, "avg_interval_length": 18.307894556681294, "combined_score": -54.94309748840329}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: 'def kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float, lmbda: float = 0.0) -> NDArray:' to 'def kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: NDArray, lmbda: float = 0.0) -> NDArray:'\nChange 2: Replace 12 lines with 16 lines\nChange 3: Replace weights = kernel_smoothing(coord_test, self.coord_calib, bandwidth, self.lmbda) with 4 lines", "parent_metrics": {"interval_score": 52.220420481079096, "coverage": 0.9233333333333333, "avg_interval_length": 17.58038754766844, "combined_score": -52.220420481079096}, "island": 0}, "prompts": {"diff_user": {"system": "You are tasked with evolving the GeoConformal Prediction algorithm to minimize interval scores through enhancements in model design and parameters. Focus on improving the kernel smoothing method by integrating advanced bandwidth selection techniques such as cross-validation and multiscale bandwidths derived from geography. Introduce regularization parameters to manage spatial autocorrelation, enhancing model performance. Propose modifications to optimization operators, including dynamic bandwidth adjustment based on geospatial data distribution and adaptive kernel methods that reflect spatial density variations. Your evolutionary search should prioritize minimizing both average interval length and distance to user-defined coverage, while ensuring computational efficiency. Optimize the model based on validation metrics instead of the training set. \n\nExpected outputs include a refined GeoConformalRegressor class with an enhanced geo_conformalize method and an evaluation of resulting interval scores over multiple iterations. Address spatial autocorrelation issues explicitly, ensure that new parameters or operators do not significantly increase complexity, and maintain compatibility with existing data structures. The algorithm is required to operate within existing training and calibration datasets, effectively leveraging both local and spatial effects in the modeling process.", "user": "# Current Program Information\n- Fitness: -52.2204\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at -52.2204\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: interval_score: 52.2204, coverage: 0.9233, avg_interval_length: 17.5804, combined_score: -52.2204\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: -52.2204)\n```python\n# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float, lmbda: float = 0.0) -> NDArray:\n    \"\"\"\n    Kernel smoothing function with optional Tikhonov regularization.\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: distance decay parameter (can be scalar or array for adaptive)\n    :param lmbda: regularization parameter to control spatial autocorrelation\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    # Allow for adaptive bandwidth: if bandwidth is array, each row is for one test point\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n    # Adaptive bandwidth: bandwidth can be 1D (length = n_test)\n    if np.ndim(bandwidth) == 1:\n        bw = bandwidth.reshape(-1, 1)\n        weights = gaussian_kernel(distances / bw)\n    else:\n        weights = gaussian_kernel(distances / bandwidth)\n    # Regularization to avoid overfitting to local clusters (spatial autocorrelation)\n    if lmbda > 0:\n        weights = weights + lmbda\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    # Avoid division by zero\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1, lmbda: float = 0.01, adaptive_bandwidth: bool = True, k_neighbors: int = 30):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n        self.lmbda = lmbda\n        self.adaptive_bandwidth = adaptive_bandwidth\n        self.k_neighbors = k_neighbors\n\n    def _compute_adaptive_bandwidth(self, z_test, z_calib):\n        \"\"\"\n        Compute adaptive bandwidth for each test point based on k-nearest spatial neighbors.\n        \"\"\"\n        from sklearn.neighbors import NearestNeighbors\n        nbrs = NearestNeighbors(n_neighbors=min(self.k_neighbors, len(z_calib)), algorithm='auto').fit(z_calib)\n        distances, _ = nbrs.kneighbors(z_test)\n        # Use median distance to k-th neighbor as bandwidth for each test point\n        bw = np.median(distances, axis=1)\n        # Avoid bandwidth too small\n        bw = np.clip(bw, 1e-3, np.inf)\n        return bw\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        q_level = 1 - self.miscoverage_level\n        # Dynamic bandwidth adjustment based on spatial density\n        if self.adaptive_bandwidth:\n            bandwidth = self._compute_adaptive_bandwidth(coord_test, self.coord_calib)\n        else:\n            bandwidth = self.bandwidth\n        weights = kernel_smoothing(coord_test, self.coord_calib, bandwidth, self.lmbda)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        # Use adaptive bandwidth and regularization in GeoConformalRegressor\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1,\n            lmbda=0.01, adaptive_bandwidth=True, k_neighbors=30\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n\n```\nKey features: Performs well on interval_score (52.2204), Performs well on coverage (0.9233), Performs well on avg_interval_length (17.5804), Performs well on combined_score (-52.2204)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float, lmbda: float = 0.0) -> NDArray:\n    \"\"\"\n    Kernel smoothing function with optional Tikhonov regularization.\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: distance decay parameter (can be scalar or array for adaptive)\n    :param lmbda: regularization parameter to control spatial autocorrelation\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    # Allow for adaptive bandwidth: if bandwidth is array, each row is for one test point\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n    # Adaptive bandwidth: bandwidth can be 1D (length = n_test)\n    if np.ndim(bandwidth) == 1:\n        bw = bandwidth.reshape(-1, 1)\n        weights = gaussian_kernel(distances / bw)\n    else:\n        weights = gaussian_kernel(distances / bandwidth)\n    # Regularization to avoid overfitting to local clusters (spatial autocorrelation)\n    if lmbda > 0:\n        weights = weights + lmbda\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    # Avoid division by zero\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1, lmbda: float = 0.01, adaptive_bandwidth: bool = True, k_neighbors: int = 30):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n        self.lmbda = lmbda\n        self.adaptive_bandwidth = adaptive_bandwidth\n        self.k_neighbors = k_neighbors\n\n    def _compute_adaptive_bandwidth(self, z_test, z_calib):\n        \"\"\"\n        Compute adaptive bandwidth for each test point based on k-nearest spatial neighbors.\n        \"\"\"\n        from sklearn.neighbors import NearestNeighbors\n        nbrs = NearestNeighbors(n_neighbors=min(self.k_neighbors, len(z_calib)), algorithm='auto').fit(z_calib)\n        distances, _ = nbrs.kneighbors(z_test)\n        # Use median distance to k-th neighbor as bandwidth for each test point\n        bw = np.median(distances, axis=1)\n        # Avoid bandwidth too small\n        bw = np.clip(bw, 1e-3, np.inf)\n        return bw\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        q_level = 1 - self.miscoverage_level\n        # Dynamic bandwidth adjustment based on spatial density\n        if self.adaptive_bandwidth:\n            bandwidth = self._compute_adaptive_bandwidth(coord_test, self.coord_calib)\n        else:\n            bandwidth = self.bandwidth\n        weights = kernel_smoothing(coord_test, self.coord_calib, bandwidth, self.lmbda)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        # Use adaptive bandwidth and regularization in GeoConformalRegressor\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1,\n            lmbda=0.01, adaptive_bandwidth=True, k_neighbors=30\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: score, complexity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["To improve the fitness score of the GeoConformal Prediction algorithm, we can implement a few targeted enhancements. These improvements will focus on optimizing the kernel smoothing method, improving bandwidth selection, and addressing spatial autocorrelation more effectively. The goal is to minimize the interval scores while maintaining computational efficiency.\n\n### Suggested Improvements\n\n1. **Implement Cross-Validation for Bandwidth Selection:**\n   Using a fixed bandwidth or even a median distance for adaptive bandwidth may not be optimal. Cross-validation can be employed to select the best bandwidth value that minimizes the interval score over a validation set.\n\n2. **Enhance Regularization:**\n   We can improve the management of spatial autocorrelation by introducing a more dynamic regularization parameter that adapts based on local data density.\n\n3. **Modify Kernel Smoothing Method:**\n   Introduce an adaptive kernel that adjusts based on the distribution of spatial data, potentially using a multiscale approach for different regions.\n\n4. **Optimize Evaluation:**\n   Ensure that evaluation metrics are based on validation datasets rather than training datasets to better generalize the model's performance.\n\nHere's how these changes can be implemented:\n\n```python\n<<<<<<< SEARCH\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float, lmbda: float = 0.0) -> NDArray:\n=======\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: NDArray, lmbda: float = 0.0) -> NDArray:\n>>>>>>> REPLACE\n```\n\n### Explanation:\n\n- **Adaptive Bandwidth as NDArray:** The change from `float` to `NDArray` for `bandwidth` allows more flexibility, supporting different bandwidths for different test points, potentially derived from a cross-validation process.\n\n```python\n<<<<<<< SEARCH\n    def _compute_adaptive_bandwidth(self, z_test, z_calib):\n        \"\"\"\n        Compute adaptive bandwidth for each test point based on k-nearest spatial neighbors.\n        \"\"\"\n        from sklearn.neighbors import NearestNeighbors\n        nbrs = NearestNeighbors(n_neighbors=min(self.k_neighbors, len(z_calib)), algorithm='auto').fit(z_calib)\n        distances, _ = nbrs.kneighbors(z_test)\n        # Use median distance to k-th neighbor as bandwidth for each test point\n        bw = np.median(distances, axis=1)\n        # Avoid bandwidth too small\n        bw = np.clip(bw, 1e-3, np.inf)\n        return bw\n=======\n    def _compute_adaptive_bandwidth(self, z_test, z_calib):\n        \"\"\"\n        Compute adaptive bandwidth for each test point using cross-validation.\n        \"\"\"\n        from sklearn.model_selection import GridSearchCV\n        from sklearn.neighbors import KernelDensity\n\n        # Set a range of bandwidths to search\n        bandwidths = np.linspace(0.01, 1.0, 20)\n        grid = GridSearchCV(KernelDensity(), {'bandwidth': bandwidths}, cv=5)\n        grid.fit(z_calib)\n\n        best_bandwidth = grid.best_params_['bandwidth']\n        bw = np.full(z_test.shape[0], best_bandwidth)\n        \n        return bw\n>>>>>>> REPLACE\n```\n\n### Explanation:\n\n- **Cross-Validation for Bandwidth:** This change introduces a cross-validation process to find the optimal bandwidth, ensuring the bandwidth is not just statically or heuristically chosen but is instead empirically optimized for the calibration data.\n\n```python\n<<<<<<< SEARCH\n    weights = kernel_smoothing(coord_test, self.coord_calib, bandwidth, self.lmbda)\n=======\n    # Adjust regularization dynamically based on local density\n    local_density = np.sum(weights, axis=1)\n    dynamic_lmbda = self.lmbda / np.maximum(local_density, 1e-5)\n    weights = kernel_smoothing(coord_test, self.coord_calib, bandwidth, dynamic_lmbda)\n>>>>>>> REPLACE\n```\n\n### Explanation:\n\n- **Dynamic Regularization:** The regularization parameter (`lmbda`) is adjusted dynamically based on the local density of calibration points, which can help manage spatial autocorrelation more effectively by reducing overfitting in dense areas.\n\nThese improvements aim to optimize the model's predictive interval, ensuring it accounts for spatial variability and autocorrelation without compromising computational efficiency. The cross-validation for bandwidth selection and dynamic regularization are particularly promising for enhancing model performance across diverse geographic datasets."]}}, "artifacts_json": null, "artifact_dir": null}