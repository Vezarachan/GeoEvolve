{"id": "d006a57e-c48e-4d53-9c7b-6c9fc58040f5", "code": "# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float) -> NDArray:\n    \"\"\"\n    Kernel smoothing function\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: distance decay parameter\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n    weights = gaussian_kernel(distances / bandwidth)\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    # Avoid division by zero\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    sorted_indices = np.argsort(scores)\n    sorted_scores = scores[sorted_indices]\n    sorted_weights = weights[sorted_indices]\n    cumulative_weights = np.cumsum(sorted_weights) - 0.5 * sorted_weights\n    cutoff = q * cumulative_weights[-1]\n    return sorted_scores[cumulative_weights >= cutoff][0]\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        N = nonconformity_scores.shape[0]\n        # Use standard conformal quantile\n        q_level = 1 - self.miscoverage_level\n        weights = kernel_smoothing(coord_test, self.coord_calib, self.bandwidth)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    coord_std = np.std(loc_calib.values, axis=0)\n    dynamic_bandwidth = 0.15 * np.mean(coord_std)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=dynamic_bandwidth, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n", "language": "python", "parent_id": "c483bea8-5e1c-4ed4-8290-8a8209d3fc6f", "generation": 2, "timestamp": 1758609866.846739, "iteration_found": 10, "metrics": {"interval_score": 0.0, "coverage": 0.0, "avg_interval_length": 0.0, "combined_score": -1000, "error": "operands could not be broadcast together with shapes (90000,) (300,300) "}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 22 lines with 14 lines\nChange 2: Replace 2 lines with 4 lines", "parent_metrics": {"interval_score": 54.88071256341551, "coverage": 0.9366666666666666, "avg_interval_length": 19.4521057169291, "combined_score": -54.88071256341551}, "island": 1}, "prompts": {"diff_user": {"system": "You are an expert in GIS and an experienced coder. Your task is to improve the GeoConformal Prediction algorithm in Python. The target is to minimize interval_score. The best algorithm should have smallest values for both average interval (avg_interval_length) and distance to user-defined coverage (dist_to_coverage). You should not to optimize the model fitted on the training set.", "user": "# Current Program Information\n- Fitness: -54.8807\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness unchanged at -54.8807\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: interval_score: 54.8807, coverage: 0.9367, avg_interval_length: 19.4521, combined_score: -54.8807\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: -54.8807)\n```python\n# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float) -> NDArray:\n    \"\"\"\n    Kernel smoothing function\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: distance decay parameter\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n    weights = gaussian_kernel(distances / bandwidth)\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    # Avoid division by zero\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        N = nonconformity_scores.shape[0]\n        # Use standard conformal quantile\n        q_level = 1 - self.miscoverage_level\n        weights = kernel_smoothing(coord_test, self.coord_calib, self.bandwidth)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n\n```\nKey features: Performs well on interval_score (54.8807), Performs well on coverage (0.9367), Performs well on avg_interval_length (19.4521), Performs well on combined_score (-54.8807)\n\n\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport os.path\nfrom typing import Callable, Union\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom numpy.typing import NDArray\n\n\ndef gaussian_kernel(d: NDArray) -> NDArray:\n    \"\"\"\n    Gaussian distance decay function\n    :param d: distances from test samples to calibration samples\n    :return: list of weights for calibration samples\n    \"\"\"\n    return np.exp(-0.5 * d ** 2)\n\n\ndef kernel_smoothing(z_test: NDArray, z_calib: NDArray, bandwidth: float) -> NDArray:\n    \"\"\"\n    Kernel smoothing function\n    :param z_test: the coordinates of test samples\n    :param z_calib: the coordinates of calibration samples\n    :param bandwidth: distance decay parameter\n    :return: list of normalized weights for calibration samples\n    \"\"\"\n    z_test_norm = np.sum(z_test ** 2, axis=1).reshape(-1, 1)\n    z_calib_norm = np.sum(z_calib ** 2, axis=1).reshape(1, -1)\n    distances = np.sqrt(z_test_norm + z_calib_norm - 2 * np.dot(z_test, z_calib.T))\n    weights = gaussian_kernel(distances / bandwidth)\n    weights_sum = np.sum(weights, axis=1, keepdims=True)\n    # Avoid division by zero\n    weights_sum[weights_sum == 0] = 1\n    weights = weights / weights_sum\n    return weights\n\n\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n\n\nclass GeoConformalBase:\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        self.predict_f = predict_f\n        self.bandwidth = bandwidth\n        self.x_calib = x_calib\n        self.y_calib = y_calib\n        self.coord_calib = coord_calib\n        self.miscoverage_level = miscoverage_level\n\n    def geo_conformalized(self, x_test: NDArray, y_test: NDArray, coord_test: NDArray):\n        raise NotImplementedError\n\n\nclass GeoConformalRegressor(GeoConformalBase):\n    \"\"\"\n    Parameters\n    ----------\n    predict_f: spatial prediction function (regression or interpolation)\n    \"\"\"\n\n    def __init__(self, predict_f: Callable, x_calib: NDArray, y_calib: NDArray, coord_calib: NDArray,\n                 bandwidth: Union[float, int], miscoverage_level: float = 0.1):\n        super().__init__(predict_f, x_calib, y_calib, coord_calib, bandwidth, miscoverage_level)\n\n    def geo_conformalize(self,\n                         x_test: NDArray,\n                         y_test: NDArray,\n                         coord_test: NDArray):\n        y_calib_pred = self.predict_f(self.x_calib)\n        nonconformity_scores = np.abs(y_calib_pred - self.y_calib)\n        N = nonconformity_scores.shape[0]\n        # Use standard conformal quantile\n        q_level = 1 - self.miscoverage_level\n        weights = kernel_smoothing(coord_test, self.coord_calib, self.bandwidth)\n        geo_uncertainty = weighted_quantile(nonconformity_scores, weights, q_level)\n        y_test_pred = self.predict_f(x_test)\n        upper_bound = y_test_pred + geo_uncertainty\n        lower_bound = y_test_pred - geo_uncertainty\n        return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\n# EVOLVE-BLOCK-END\n\ndef run_geocp():\n    train = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/train.csv')\n    calib = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/calib.csv')\n    test = pd.read_csv(f'/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data/test.csv')\n    variables = ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X',\n                 'UTM_Y']\n    X_train, y_train = train[variables], train['price']\n    X_calib, y_calib, loc_calib = calib[variables], calib['price'], calib[['lat', 'lon']]\n    X_test, y_test, loc_test = test[variables], test['price'], test[['lat', 'lon']]\n    model = xgb.XGBRegressor(n_estimators=500, max_depth=3, min_child_weight=1.0, colsample_bytree=1.0).fit(\n        X_train.values, y_train.values)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n    geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(X_test.values, y_test.values,\n                                                                                         loc_test.values)\n    return geo_uncertainty, upper_bound, lower_bound, y_test\n\n\ndef interval_score(y_true, lower_bound, upper_bound, alpha=0.1, epsilon=1e-6):\n    width = np.maximum(upper_bound - lower_bound, epsilon)\n    below = (lower_bound - y_true) * (y_true < lower_bound)\n    above = (y_true - upper_bound) * (y_true > upper_bound)\n    score = width + (2 / alpha) * (below + above)\n    score = np.where(np.isnan(score), 0.0, score)\n    return np.mean(score)\n\n\ndef run_k_times(k=5):\n    from sklearn.model_selection import train_test_split\n    base_path = '/Users/louxiayin/PycharmProjects/GeoEvolve/examples/geocp/data'\n    data = pd.read_csv(os.path.join(base_path, 'seattle_sample_3k.csv'))\n    data = gpd.GeoDataFrame(data, crs=\"EPSG:32610\", geometry=gpd.points_from_xy(x=data.UTM_X, y=data.UTM_Y))\n    data = data.to_crs(4326)\n    data['lon'] = data['geometry'].get_coordinates()['x']\n    data['lat'] = data['geometry'].get_coordinates()['y']\n    data['price'] = np.power(10, data['log_price']) / 10000\n    X = data[\n        ['bathrooms', 'sqft_living', 'sqft_lot', 'grade', 'condition', 'waterfront', 'view', 'age', 'UTM_X', 'UTM_Y']]\n    y = data['price']\n    loc = data[['lat', 'lon']]\n    scores = np.zeros(k)\n    for i in range(k):\n        X_train, X_temp, y_train, y_temp, _, loc_temp = train_test_split(X, y, loc, train_size=0.8, random_state=42)\n        X_calib, X_test, y_calib, y_test, loc_calib, loc_test = train_test_split(X_temp, y_temp, loc_temp,\n                                                                                 train_size=0.5, random_state=42)\n        model = xgb.XGBRegressor(n_estimators=600, max_depth=5, min_child_weight=0.8, colsample_bytree=0.7,\n                                 subsample=0.8,\n                                 learning_rate=0.05, random_state=42).fit(\n            X_train.values, y_train.values)\n        geocp_regresser = GeoConformalRegressor(\n            predict_f=model.predict,\n            x_calib=X_calib.values, y_calib=y_calib.values,\n            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1\n        )\n        geo_uncertainty, upper_bound, lower_bound, y_test = geocp_regresser.geo_conformalize(\n            X_test.values, y_test.values, loc_test.values\n        )\n        scores[i] = interval_score(y_test, lower_bound, upper_bound, alpha=0.1)\n    return np.mean(scores)\n\n\nif __name__ == \"__main__\":\n    # geo_uncertainty, upper_bound, lower_bound, y_test = run_geocp()\n    # print(geo_uncertainty, upper_bound, lower_bound, y_test)\n    mean_interval_score = run_k_times(k=20)\n    print(f'Initial Code')\n    print(f'Mean Interval Score: {mean_interval_score}')\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: score, complexity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["To improve the GeoConformal Prediction algorithm, we should focus on optimizing the calculations and ensuring the interval scores are minimized while maintaining or improving the other performance metrics. Here are some targeted changes:\n\n1. **Improve Weighted Quantile Calculation:**\n   The current weighted quantile calculation can be optimized by using numpy's efficient functions. We can eliminate the need for reshaping and repeating arrays, which can be computationally expensive.\n\n2. **Adjust Bandwidth Dynamically:**\n   Instead of a fixed bandwidth, we can introduce a dynamic bandwidth adjustment to better capture the variability in the data, which might lead to more accurate interval predictions.\n\nHere's how to implement these improvements:\n\n### Change 1: Optimize Weighted Quantile Calculation\n\nThe current implementation reshapes and repeats arrays unnecessarily. We can optimize this by directly working with arrays without reshaping and using numpy's built-in functions.\n\n```python\n<<<<<<< SEARCH\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    if weights.ndim == 1:\n        weights = weights.reshape(1, -1)\n\n    N_test, N_calib = weights.shape\n    scores = scores.reshape(1, -1)\n    scores_repeated = np.repeat(scores, N_test, axis=0)\n    sorted_idx = np.argsort(scores_repeated, axis=1)\n    scores_sorted = np.take_along_axis(scores_repeated, sorted_idx, axis=1)\n    weights_sorted = np.take_along_axis(weights, sorted_idx, axis=1)\n\n    cumulative_weights = np.cumsum(weights_sorted, axis=1)\n    idx = np.argmax(cumulative_weights >= q, axis=1)\n    quantiles = scores_sorted[np.arange(N_test), idx]\n    return quantiles\n=======\ndef weighted_quantile(scores: NDArray, weights: NDArray, q: float):\n    \"\"\"\n    Calculate weighted quantile\n    :param scores: nonconformity scores\n    :param q: quantile level\n    :param weights: geographic weights (normalized)\n    :return: weighted quantile at (1-alpha) miscoverage level\n    \"\"\"\n    sorted_indices = np.argsort(scores)\n    sorted_scores = scores[sorted_indices]\n    sorted_weights = weights[sorted_indices]\n    cumulative_weights = np.cumsum(sorted_weights) - 0.5 * sorted_weights\n    cutoff = q * cumulative_weights[-1]\n    return sorted_scores[cumulative_weights >= cutoff][0]\n>>>>>>> REPLACE\n```\n\n### Change 2: Dynamic Bandwidth Adjustment\n\nIntroduce a mechanism to adjust the bandwidth based on the variability of the calibration data. A simple approach is to scale the bandwidth by a factor derived from the standard deviation of the calibration coordinates.\n\n```python\n<<<<<<< SEARCH\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=0.15, miscoverage_level=0.1)\n=======\n    coord_std = np.std(loc_calib.values, axis=0)\n    dynamic_bandwidth = 0.15 * np.mean(coord_std)\n    geocp_regresser = GeoConformalRegressor(predict_f=model.predict, x_calib=X_calib.values, y_calib=y_calib.values,\n                                            coord_calib=loc_calib.values, bandwidth=dynamic_bandwidth, miscoverage_level=0.1)\n>>>>>>> REPLACE\n```\n\n### Explanation\n\n1. **Weighted Quantile Optimization:** By sorting the scores and weights once and directly computing the cumulative weights, we reduce computational overhead and improve performance.\n\n2. **Dynamic Bandwidth:** By adjusting the bandwidth based on the spread of the calibration data, the model can adapt better to different spatial distributions, potentially improving the accuracy of the interval predictions.\n\nThese changes are intended to refine the existing algorithm, focusing on computational efficiency and adaptability, which should contribute to better performance metrics such as interval score and coverage."]}}, "artifacts_json": null, "artifact_dir": null}