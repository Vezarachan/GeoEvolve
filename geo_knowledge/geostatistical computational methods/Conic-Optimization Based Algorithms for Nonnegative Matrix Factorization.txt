Conic-Optimization Based Algorithms for
Nonnegative Matrix Factorization
Valentin Leplata, Yurii Nesterovb, Nicolas Gillisc, Fran¸cois Glineurb
aCenter for Artiﬁcial Intelligence Technology, Skoltech, Bolshoy Boulevard 30, bld. 1,
Moscow, Russia 121205; bCORE and ICTEAM/Mathematical Engineering (INMA),
UCLouvain, Avenue Georges Lemaˆıtre 4, B-1348 Louvain-la-Neuve, Belgium ; cDepartment
of Mathematics and Operational Research, Facult´e Polytechnique, Universit´e de Mons, Rue
de Houdain 9, 7000 Mons, Belgium
ARTICLE HISTORY
Compiled January 25, 2023
ABSTRACT
Nonnegative matrix factorization is the following problem: given a nonnegative input
matrix V and a factorization rank K, compute two nonnegative matrices, W with
K columns and H with K rows, such that WH approximates V as well as possible.
In this paper, we propose two new approaches for computing high-quality NMF
solutions using conic optimization. These approaches rely on the same two steps.
First, we reformulate NMF as minimizing a concave function over a product of
convex cones–one approach is based on the exponential cone, and the other on the
second-order cone. Then, we solve these reformulations iteratively: at each step,
we minimize exactly, over the feasible set, a majorization of the objective functions
obtained via linearization at the current iterate. Hence these subproblems are convex
conic programs and can be solved eﬃciently using dedicated algorithms. We prove
that our approaches reach a stationary point with an accuracy decreasing as Op 1
i q,
where i denotes the iteration number. To the best of our knowledge, our analysis is
the ﬁrst to provide a convergence rate to stationary points for NMF. Furthermore,
in the particular cases of rank-one factorizations (that is, K “ 1), we show that one
of our formulations can be expressed as a convex optimization problem implying
that optimal rank-one approximations can be computed eﬃciently. Finally, we show
on several numerical examples that our approaches are able to frequently compute
exact NMFs (that is, with V “ WH), and compete favorably with the state of the
art.
KEYWORDS
nonnegative matrix factorization, nonnegative rank, exponential cone,
second-order cone, concave minimization, conic optimization, Frank-Wolfe gap,
convergence to stationary points
VL acknowledges the support by the European Research Council (ERC Advanced Grant no 788368) and the
support by Ministry of Science and Higher Education grant No. 075-10-2021-068. Email: V.Leplat@skoltech.ru
NG acknowledges the support by the Fonds de la Recherche Scientiﬁque - FNRS and the Fonds Weten-
schappelijk Onderzoek - Vlanderen (FWO) under EOS Project no O005318F-RG47, by the European Research
Council (ERC Starting Grant no 679515), and by the Francqui Foundation. Email: nicolas.gillis@umons.ac.be
YN and FG acknowledge support from the European Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation program (grant agreement No. 788368). FG also acknowledges support
from EOS project no O005318F-RG47. Emails: {yurii.nesterov,francois.glineur}@uclouvain.be.
arXiv:2105.13646v3  [math.OC]  23 Jan 2023

1. Introduction
Nonnegative matrix factorization (NMF) is the problem of approximating a given
nonnegative matrix, V P RFˆN
`
, as the product of two smaller nonnegative matri-
ces, W P RFˆK
`
and H P RKˆN
`
, where K is a given parameter known as the fac-
torization rank. One aims at ﬁnding the best approximation, that is, the one that
minimizes the discrepancy between V and the product WH, often measured by the
Frobenius norm of their diﬀerence, }V ´ WH}F . Despite the fact that NMF is NP-
hard in general [30] (see also [27]), it has been used successfully in many domains
such as probability, geoscience, medical imaging, computational geometry, combina-
torial optimization, analytical chemistry, and machine learning; see [11, 13] and the
references therein. Many local optimization schemes have been developed to compute
NMFs. They aim to identify local minima or stationary points of optimization prob-
lems that minimize the discrepancy between V and the approximation WH. Most of
these iterative algorithms rely on a two-block coordinate descent scheme that consists
in (approximatively) optimizing alternatively over W with H ﬁxed, and vice-versa;
see [5, 13] and the references therein. In this paper, we are interested in computing
high-quality local minima for the NMF optimization problems without relying on the
block coordinate descent (BCD) framework. We will perform the optimization over
W and H jointly. Moreover, our focus is on ﬁnding exact NMFs, that is, computing
nonnegative factors W and H such that V “ WH, although our approaches can be
used to ﬁnd approximate NMFs as well.
The minimum factorization rank K for which an exact NMF exists is called the
nonnegative rank of V and is denoted rank`pV q, we have
rank`pV q “ min
!
K P N
ˇˇˇ there exist W P RFˆK
`
and H P RKˆN
`
such that V “ WH
)
.
The computation of the nonnegative rank is NP-hard [30], and is a research topic on
its own; see [13, Chapter 3][8, 9] and the references therein for recent progress on this
question.
1.1. Computational complexity
Solving exact NMF can be used to compute the nonnegative rank, by ﬁnding the
smallest K such that an exact NMF exists. Cohen and Rothblum [7] give a super-
exponential time algorithm for this problem. Vavasis [30] proved that checking whether
rankpV q “ rank`pV q, where rankpV q “ K is part of the input, is NP-hard. Since
determining the nonnnegative rank is a generalization of exact NMF, the results in [30]
imply that computing an exact NMF is also NP-hard. Similarly, the standard NMF
problem using any norm is a generalization of exact NMF, and therefore any hardness
result that applies to exact NMF also applies to most approximated NMF models [30].
Hence, unless P=NP, no algorithm can solve exact NMF using a number of arithmetic
operations bounded by a polynomial in K and in the size of V ; see also [27] that gives
a diﬀerent proof using algebraic arguments.
More recently, Arora et al. [2] showed that no algorithm to solve this problem can
run in time pFNqopKq unless1 3-SAT can be solved in time 2opnq on instances with n
13-SAT, or 3-satisﬁability, is an instrumental problem in computational complexity to prove NP-completeness
results. 3-SAT is the problem of deciding whether a set of disjunctions containing 3 Boolean variables or their
negation can be satisﬁed.
2

variables. However, in practice, K is small and it makes senses to wonder what is the
complexity if K is assumed to be a ﬁxed constant. In that case, they showed that exact
NMF can be solved in polynomial time in F and N, namely in time OppFNqc2KK2q
for some constant c, which Moitra [24] later improved to OppFNqcK2q.
The argument is based on quantiﬁer elimination theory, using the seminal result by
Basu, Pollack and Roy [3]. Unfortunately, this approach cannot be used in practice,
even for small size matrices, because of its high computational cost: although the term
OppFNqcK2q is a polynomial in F and N for K ﬁxed, it grows extremely fast (and
the hidden constants are usually very large). Let us illustrate with a 4-by-4 matrix
with K “ 3, we have a complexity of order 169 « 7 1010 and for a 5-by-5 matrix with
K “ 4, the complexity raises up to 2516 « 2 1022. Therefore developing an eﬀective
computational technique for exact NMF of small matrices is an important direction of
research. Some heuristics have been recently developed that allow solving exact NMF
for matrices up to a few dozen rows and columns [29].
1.2. Contribution and outline of the paper
In this paper, we introduce two formulations for computing an NMF using conic opti-
mization. They rely on the same two steps. First, in Section 2, we reformulate NMF as
minimizing a concave function over a product of convex cones; one approach is based on
the exponential cone and leads to under-approximations, and the other on the second-
order cone and leads to over-approximations. For the latter formulation, in the case of
a rank-one factorization, we show that it can be cast as a convex optimization prob-
lem, leading to an eﬃcient computation of the optimal rank-one over-approximation.
Then, in Section 3, we solve these reformulations iteratively: at each step, we mini-
mize exactly over the feasible set a majorization of the objective functions obtained
via linearization at the current iterate. Hence these subproblems are convex conic pro-
grams and can be solved eﬃciently using dedicated algorithms. In Section 4, we show
that our optimization scheme relying on successive linearizations is a special case of
the Frank-Wolfe (FW) algorithm. By using an appropriate measure of stationarity,
namely the FW gap, we show in Theorem 4.1 that the minimal FW gap generated by
our algorithm converges as Op1
i q, where i is the iteration index. Finally, in Section 5,
we use our approaches to compute exact NMFs, and show that they compete favor-
ably with the state of the art when applied to several classes of nonnegative matrices;
namely, randomly generated, inﬁnitesimally rigid and slack matrices.
Remark 1 (Focus on exact NMF). Our two NMF formulations can be used to com-
pute approximate factorizations (namely, under- and over-approximations). However,
in this paper, we focus on exact NMF for the numerical experiments. The reason is
twofold:
(1) Exact NMF problems allow us to guarantee whether a globally optimal solution
is reached, and hence compare algorithms in terms of global optimality.
(2) Our current algorithms rely on interior-point methods that do not scale well.
Therefore, they are signiﬁcantly slower, at this point, than state-of-the-art al-
gorithms to compute approximate NMFs on large data sets (such as images or
documents). Making our approach scalable is a topic of further research. More-
over, because of the under/over-approximations, the error obtained with the
proposed algorithms would be larger, and the comparison would not be fair.
3

Here is a simple example, let
V “
ˆ
0
1
1
1
˙
, VNMF “
ˆ
0.45
0.72
0.72
1.17
˙
, VU “
ˆ
0
1
0
1
˙
, VO “
ˆ
1
1
1
1
˙
.
The best rank-one NMF of V is VNMF (with two digits of accuracy) with Frobe-
nius error }V ´ VNMF}F “ 0.62, while a best rank-one under-approximation
(resp. over-approximation) of V is VU (resp. VO) with Frobenius error 1. (Note
however that under-/over-approximations can have some useful properties in
practice [16, 28].)
2. NMF formulations based on conic optimization
In this section we propose two new formulations for NMF, where the feasible set
is represented using the exponential cone (Section 2.1) and the second-order cone
(Section 2.2).
2.1. NMF formulation via exponential cones
Given a non-negative matrix V P RFˆN
`
and a positive integer K ! minpF, Nq, we
want to compute an NMF. Our ﬁrst proposed formulation is the following:
max
WPRF ˆK,HPRKˆN
Fÿ
f“1
N
ÿ
n“1
˜ K
ÿ
k“1
WfkHkn
¸
subject to
K
ÿ
k“1
WfkHkn ď Vfn for f P F, n P N,
Wfk ě 0, Hkn ě 0 for f P F, k P K, n P N.
(1)
where F “ t1, ..., Fu, N “ t1, ..., Nu and K “ t1, ..., Ku. Any feasible solution pW, Hq
of (1) provides an under-approximation of V , because of the elementwise constraint
WH ď V . The objective function of (1) maximizes the sum of the entries of WH.
Therefore, if V admits an exact NMF of size K, that is, rank`pV q ď K, any optimal
solution pW ˚, H˚q of (1) must satisfy W ˚H˚ “ V , and hence will provide an exact
NMF of V . Note that this problem is nonconvex because of the bilinear terms appearing
in the objective and the constraint WH ď V .
Let us now reformulate (1) using exponential cones. In order to deal with nonnega-
tivity constraints on the entries of W and H, we use the following change of variables:
Wfk “ GpUfkq “ eUfk and Hkn “ GpTknq “ eTkn, where U P RFˆK and T P RKˆN,
with f “ 1, . . . , F, n “ 1, . . . , N and k “ 1, . . . , K and Gptq “ et. By applying a
logarithm on top of this change of variables to the objective function, and on both
sides of the inequality constraints WH ď V , (1) can be nearly equivalently rewritten
4

as follows, the diﬀerence being that zero elements in W and H are now excluded:
max
UPRF ˆK,TPRKˆN
log
¨
˝ ÿ
f,n,k
eUfk`Tkn
˛
‚
subject to
log
˜ K
ÿ
k“1
eUfk`Tkn
¸
ď log pVfnq for f P F, n P N,
(2)
which corresponds to the maximization of a convex function (logarithm of the sums
of exponentials) over a convex set, each constraint being convex for the same reason.
We rewrite the convex feasible set of (2) with explicit conic constraints as follows:
K
ÿ
k“1
tfkn ď Vfn for f P F, n P N,
ptfkn, 1, Ufk ` Tknq P Kexp for f P F, k P K, n P N,
(3)
where Kexp Ă R3 denotes the (primal) exponential cone deﬁned as:
Kexp “
!
px1, x2, x3q P R3|x1 ě x2e
x3
x2 , x2 ą 0
)
Y tpx1, 0, x3q |x1 ě 0, x3 ď 0u .
(4)
Note
that
the
exponential
cone
is
closed
and
includes
the
subset
tpx1, 0, x3q |x1 ě 0, x3 ď 0u, therefore the scenarios for which the entries Vfn are
equal to zero can be handled by exponential conic constraints, which was not
possible with formulation (2) since the log function is not deﬁned at zero. Hence the
optimization problem (1) can be written completely equivalently as
max
UPRF ˆK,TPRKˆN,tPRF ˆKˆN
log
¨
˝ ÿ
f,n,k
eUfk`Tkn
˛
‚
subject to
K
ÿ
k“1
tfkn ď Vfn for f P F, n P N,
ptfkn, 1, Ufk ` Tknq P Kexp for f P F, k P K, n P N.
(5)
This leads to F ˆ N inequality constraints and the introduction of F ˆ K ˆ N ex-
ponential cones. In Section 3, we propose an algorithm to tackle (5) using successive
linearizations of the objective function.
5

2.2. NMF formulation via rotated second-order cones
Our second proposed NMF formulation is the following:
min
WPRF ˆK,HPRKˆN
Fÿ
f“1
N
ÿ
n“1
˜ K
ÿ
k“1
WfkHkn
¸
subject to
K
ÿ
k“1
WfkHkn ě Vfn for f P F, n P N,
Wfk, Hkn ě 0 for f P F, k P K, n P N.
(6)
Any feasible solution pW, Hq of (6) provides an over-approximation of V , because of
the constraint WH ě V . The objective function of (1) minimizes the sum of the
entries of WH. Therefore, if rank`pV q ď K, any optimal solution pW ˚, H˚q of (1)
must satisfy W ˚H˚ “ V , and hence will provide an exact NMF of V . Again the
problem is nonconvex due to the bilinear terms.
Let us use the following change of variables: we let Wfk “ GpUfkq “ aUfk and
Hkn “ GpTknq “ ?Tkn where U P RFˆK
`
and T P RKˆN
`
, with f “ 1, . . . , F, n “
1, . . . , N and k “ 1, . . . , K, this time with Gptq “
?
t. Thus the optimization problem
(6) can be equivalently rewritten as:
min
UPRF ˆK
`
,TPRKˆN
`
Fÿ
f“1
N
ÿ
n“1
˜ K
ÿ
k“1
a
Ufk
a
Tkn
¸
subject to
K
ÿ
k“1
a
Ufk
a
Tkn ě Vfn for f P F, n P N,
(7)
which minimizes a concave function over a convex set. Indeed, the function ?xy is
concave.
This set can be written with conic constraints as follows:
K
ÿ
k“1
tfkn ě Vfn, for f P F, n P N,
ˆ
Ufk, 1
2Tkn, tfkn
˙
P Q3
r for f P F, k P K, n P N,
(8)
where Q3
r denotes the 3-dimensional rotated second-order cone deﬁned as:
Q3
r “
␣
px1, x2, x3q P R3 | 2x1x2 ě x2
3, x1 ě 0, x2 ě 0
(
.
6

Thus, the optimization problem (7) becomes
min
UPRF ˆK
`
,TPRKˆN
`
,tPRF ˆKˆN
Fÿ
f“1
N
ÿ
n“1
˜ K
ÿ
k“1
a
Ufk
a
Tkn
¸
subject to
K
ÿ
k“1
tfkn ě Vfn for f P F, n P N,
ˆ
Ufk, 1
2Tkn, tfkn
˙
P Q3
r for f P F, k P K, n P N,
(9)
which leads to F ˆ N inequality constraints and the introduction of of F ˆ K ˆ N
rotated quadratic cones. In section 3, we present an algorithm to tackle (5) and (9).
2.2.1. Rank-one Nonnegative Matrix Over-approximation
In this section, we show that our over-approximation formulation (6) can be expressed
as a convex optimization problem in the case of a rank-one factorization (that is,
K “ 1). Hence we will be able to compute an optimal rank-one nonnegative matrix
over-approximation (NMO). For K “ 1, (6) becomes:
min
wPRF ,hPRN
Fÿ
f“1
N
ÿ
n“1
wfhn
subject to
wfhn ě Vfn for f P F, n P N,
wf, hn ě 0 for f P F, n P N.
(10)
Any feasible solution pw, hq of (10) provides a rank-one NMO of V , because of the
constraints. The objective function of (10) minimizes the sum of the entries of whJ,
which is equal to xwhJ, eeJy “ xw, ey xh, ey, where e denotes the all-one factor of
appropriate dimension. Since any solution whJ can be rescaled as pλwqphJ{λq for any
λ ą 0, we can assume without loss of generality (w.l.o.g.) that xw, ey “ 1, and hence
(10) can be equivalently written as follows:
min
wPRF ,hPRN
xh, ey
subject to
xw, ey “ 1,
wfhn ě Vfn for f P F, n P N,
wf, hn ě 0 for f P F, n P N.
(11)
Then, by letting each hn take the minimal value allowed by the constraints, that is,
hn “ maxfPFVfn{wf for each n P N, and replacing wf by its inverse, uf “ 1{wf for
each f P F, (11) becomes:
min
uPRF
ÿ
n
maxf pufVfnq
subject to
ÿ
f
1{uf ď 1, uf ě 0 for f P F.
(12)
The feasible set of (12) can be formulated by using various conic constraints:
7

‚ a semi-deﬁnite programming formulation: introduce variables yf such that
ufyf ě 1, ř
f yf ď 1 to obtain
ˆ
uf
1
1
yf
˙
P S2
` for f P F,
ÿ
f
yf ď 1,
where S2
` denotes the set of positive semi-deﬁnite matrices of dimension 2.
‚ a power-cone formulation: for p ă 0 the function gpxq “ xp is convex for x ą 0
and the inequality z ě xp is equivalent to z1{p1´pqx´p{p1´pq ě 1 ðñ pz, x, 1q P
P 1{p1´pq where P α “ tpx, z, aq | zαx1´α ě au is a power cone. In our case, by
introducing yf ě u´1
f , we obtain
pyf, uf, 1q P P 1{2 for f P F,
ÿ
f
yf ď 1.
‚ a (rotated) quadratic formulation: introducing variables yf such that yf ě 1{uf
for uf ě 0, this can be formulated as follows: puf, yf,
?
2q P Q3
r where Q3
r denotes
the set of rotated quadratic cones of dimension 3. We then have :
puf, yf,
?
2q P Q3
r for f P F,
ÿ
f
yf ď 1.
In this paper, we consider the (rotated) quadratic formulation which is the easiest to
implement in the MOSEK software [25].
Further, the objective function of (12) is a sum of convex piece-wise linear functions.
Hence by posing tn ě maxf pufVfnq, (12) can equivalently be formulated as follows:
min
tPRN,u,yPRF
ÿ
n
tn
subject to
ÿ
f
yf ď 1,
puf, yf,
?
2q P Q3
r for f P F,
tn ě ufVfn for f P F, n P N,
(13)
which involves 2F ` N variables and Fp1 ` Nq ` 1 constraints. This problem can be
solved to optimality and eﬃciently with an interior-point method (IPM), as available
for example in MOSEK [25].
For the formulation based on exponential cones, [12] showed that the rank-one
underapproximation for positive input matrices can be expressed as the dual of an
optimal transportation problem, and hence can also be solved optimally and eﬃciently
with polynomial-time methods [26].
8

3. A successive linearization algorithm
In this section, we present an iterative algorithm to tackle problems (5) and (9). Both
problems can be written as the minimization of a concave function Φ over a convex
set denoted by Q. Note that Q designates either the feasible set of (5) or the feasible
set of (9). We perform this minimization by solving a sequence of simpler problems in
which the objective function is replaced by its linearization constructed at the current
solution pU, Tq. Let us denote Zpiq “ pUpiq, T piqq the ith iterate of our algorithm. At
each iteration i, we update Z as follows:
Zpiq P argmin
ZPQ
ΦpZpi´1qq ` x∇ΦpZpi´1qq, Z ´ Zpi´1qy
P argmin
ZPQ
x∇ΦpZpi´1qq, Zy,
(14)
where Φ is the objective function of (5) or (9). Since the objective of (14) is linear
in Z, the subproblems become convex. Moreover they are particular structured conic
optimization problems. In this paper, we use the MOSEK software [25] to solve each
successive problem (14) with an IPM. Algorithm 1 summarizes our proposed method
to tackle (5) and (9).
To initialize U and T, we chose to randomly initialize W and H (using the uniform
distribution in the interval r0, 1s for each entry of W and H) and apply the two changes
of variables, Gp.q, to compute the initializations for U and T.
In this paper, we use a tolerance for the relative error equal to 10´6, that is,
we assume that an exact NMF pW, Hq is found for an input matrix V as soon as
}V ´WH}F
}V }F
ď 10´6, as done in [29].
The main algorithm integrates a procedure that automatically updates the opti-
mization problems in the case subsets of entries of the solution tend to zero. Indeed,
due to numerical limitations of the solver, the required level of accuracy cannot be
reached in some numerical tests even if the solution is close to convergence. This
procedure is referred to as Sparsity Patterns Integration (SPI) and is detailed in Ap-
pendix B. Note that the update of the optimization problem is computationally costly,
in particular the update of the matrix of coeﬃcients deﬁning the constraints. Hence,
SPI is triggered twice; at 80% and 95% of the maximum number of iterations. This
practical choice has been motivated by numerical experiments that showed that two
activations in the ﬁnal iterations are suﬃcient to reach the tolerance error when the
current solution is close enough to a high-accuracy local optimum. However, for expo-
nential cones, in some numerical tests, the relative error can be stuck in the interval
r10´4, 10´5s. In this context only, a ﬁnal reﬁnement step further improves the output
of the main algorithm using the state-of-the-art accelerated HALS algorithm, an exact
BCD method for NMF, from [14] to go below the 10´6 tolerance.
In Section 4 , we discuss the convergence guarantees for Algorithm 1.
4. Convergence results
Let us focus on the following optimization problem
min
ZPQ
Φ pZq ,
(15)
9

Algorithm 1 Successive Conic Convex Approximation for Exact NMF
Require: Input matrix V
P RFˆN
`
, the factorization rank K, number of itera-
tions maxiter, choose the formulation (5) (exponential cones) or (9) (second-order
cones).
Ensure: pW, Hq
ě
0 such that V
«
WH, and V
ď
WH for (5) (under-
approximation) or V ě WH for (9) (over-approximation).
1: % Block 1: Initialization
2: pW p0q, Hp0qq ÐÝ positive random initializationpF, K, Nq.
3: pUp0q, T p0qq ÐÝ G´1pW p0q, Hp0qq where G is the change of variables
4: Zp0q ÐÝ pUp0q, T p0qq
5: % Block 2: iterative update of Z
6: for i “ 1, 2, . . . , maxiter do
7:
Zpiq ÐÝ argmin
ZPQ
x∇ΦpZpi´1qq, Zy with IPMs available in MOSEK [25]
8: end for
9: pW, Hq ÐÝ GpZpiqq
where Φ is a concave continuously diﬀerentiable function over the domain Q which is
assumed to be convex and compact. Let us ﬁrst describe the convergence of the se-
quence of objective function values tΦpZpiqqu obtained with Algorithm 1. Since ΦpZq
is concave, its linearization around the current iterate Zpiq provides an upper approx-
imation, that is,
ΦpZq ď ΦpZpiqq ` x∇ΦpZpiqq, Z ´ Zpiqy for all Z P Q.
(16)
This upper bound is tight at the current iterate and is exactly minimized over the
feasible set Q at each iteration. Hence Algorithm 1 is a majorization-minimization
algorithm. This implies that ΦpZq is nonincreasing under the updates of Algorithm 1
and since ΦpZq is bounded below on Q, by construction, the sequence of objective
function values tΦpZpiqqu converges.
We now focus on the convergence analysis of the sequence of iterates tZpiqu gen-
erated by Algorithm 1, in particular convergence to a stationary point. To achieve
this goal, we ﬁrst recall some basics about the Frank-Wolfe (FW) algorithm. The FW
algorithm [10] is a popular ﬁrst-order method to solve (15) that relies on the abil-
ity to compute eﬃciently the so-called Linear Minimization Oracle (LMO), that is,
LMOpDq :“ argmin
ZPQ
xD, Zy where D denotes some search direction. The FW algo-
rithm with adaptive step size is given in Algorithm 2. A step of this algorithm can
be brieﬂy summarized as follows: at a current iterate Zpiq, the algorithm considers
the ﬁrst-order model of the objective function (its linearization), and moves towards
a minimizer of this linear function, computed on the same domain Q.
10

Algorithm 2 Frank-Wolfe algorithm
1: Zp0q P Q, number of iterations I.
2: for i “ 1, 2, . . . , I do
3:
Compute V piq :“ argmin
ZPQ
x∇ΦpZpi´1qq, Zy
4:
Choose 0 ă τ piq ď 1. (A standard choice in the literature is τ piq :“
2
i`1.)
5:
Update Zpiq :“ p1 ´ τ piqqZpi´1q ` τ piqV piq
6: end for
Algorithm 1 is a particular case of Algorithm 2 for which τ piq “ 1 for all i. In the
last decade, FW-based methods have regained interest in many ﬁelds, mainly driven
by their good scalability and the crucial property that Algorithm 2 maintains its iter-
ates as a convex combination of a few extreme points. This results in the generation
of sparse and low-rank solutions since at most one additional extreme point of the
set Q is added to the convex combination at each iteration. More details and insights
about the later observations can be found in [6, 17]. FW algorithms have been recently
studied in terms of convergence guarantees for the minimization of various classes of
functions over convex sets, such as convex functions with Lipschitz continous gradi-
ent [18], and non-convex diﬀerentiable functions [22]. However, we are not aware of any
convergence rates proven for Algorithm 2 when solving (15) assuming only concavity
of the objective. To derive rates in the concave setting, we need to deﬁne a measure
of stationarity for our iterates. In this paper, we consider the so-called FW gap of Φ
at Zpiq deﬁned as follows:
µpiq :“ max
ZPQx∇ΦpZpiqq, Zpiq ´ Zy.
(17)
This quantity is standard in the analysis of FW algorithms, see [18, 22] and the
references therein. A point Zpiq is a stationary point for the constrained optimization
problem (15) if and only if µpiq “ 0. Moreover, the FW gap
‚ provides a lower bound on the accuracy: 0 ď µpiq ď ΦpZpiqq ´ Φ˚ for all i, where
Φ˚ :“ minZPQ ΦpZq,
‚ is aﬃne invariant, that is, it is invariant with respect to an aﬃne transformation
of the domain Q in problem (15) [18], and
‚ is not tied to any speciﬁc choice of norms, unlike criteria such as
››∇ΦpZpiqq
››.
Let us provide a convergence rate for the FW algorithm (Algorithm 2).
Theorem 4.1. Consider the problem (15) where Φ is a continuously diﬀerentiable
concave function over the compact convex domain Q. Let us denote Zpiq the sequence
of iterates generated by the FW algorithm (Algorithm 2) applied on (15). Assume there
exists a constant ˜τ ą 0 such that ˜τ ď τ piq ď 1 for all i. Then the minimal FW gap,
deﬁned as ˜µpiq :“ min0ďjďi µpjq, satisﬁes, for all i “ 1, 2, . . . ,
˜µpiq ď 1
˜τ
ΦpZp0qq ´ Φ˚
i ` 1
,
(18)
where ΦpZp0qq ´ Φ˚ is the (global) accuracy of the initial iterate.
11

Proof. Using (16), any points Zpiq and Zpi`1q generated by Algorithm 2 satisfy
ΦpZpi`1qq ď ΦpZpiqq ` x∇ΦpZpiqq, Zpi`1q ´ Zpiqy.
(19)
Let us substitute Zpi`1q by its construction in Algorithm 2 (line 5) in (19) to obtain
ΦpZpi`1qq ď ΦpZpiqq ` x∇ΦpZpiqq, p1 ´ τ piqqZpiq ` τ piqV piq ´ Zpiqy
“ ΦpZpiqq ´ τ piqx∇ΦpZpiqq, Zpiq ´ V piqy.
(20)
Let us show that x∇ΦpZpiqq, Zpiq ´ V piqy is equal to µpiq deﬁned in Equation (17):
µpiq “ max
ZPQx∇ΦpZpiqq, Zpiq ´ Zy
“ x∇ΦpZpiqq, Zpiqy ` max
ZPQx∇ΦpZpiqq, ´Zy
“ x∇ΦpZpiqq, Zpiqy ´ min
ZPQx∇ΦpZpiqq, Zy
“ x∇ΦpZpiqq, Zpiqy ´ x∇ΦpZpiqq, V piqy “ x∇ΦpZpiqq, Zpiq ´ V piqy,
(21)
where the fourth equality holds be deﬁnition of V piq from Algorithm 2 (line 3). Equa-
tion (20) becomes:
ΦpZpi`1qq ď ΦpZpiqq ´ τ piqµpiq.
(22)
By recursively applying (22) for the iterates generated by Algorithm 2, we obtain
ΦpZpi`1qq ď ΦpZp0qq ´
iÿ
j“0
τ pjqµpjq.
(23)
Let deﬁne the quantities ˜µpiq :“ min
0ďjďiµpjq, the minimal FW gap encountered along
iterates, and ˜τ such that ˜τ ď τ piq ď 1 for all i ě 0, so that inequality (23) implies
ΦpZpi`1qq ď ΦpZp0qq ´ pi ` 1q˜τ ˜µpiq ðñ ˜µpiq ď 1
˜τ
ΦpZp0qq ´ ΦpZpi`1qq
i ` 1
.
(24)
Finally, using the fact that ΦpZp0qq´ΦpZpi`1qq ď ΦpZp0qq´Φ˚ where Φ˚ :“ min
ZPQΦpZq,
inequality (24) becomes
˜µpiq ď 1
˜τ
ΦpZp0qq ´ Φ˚
i ` 1
,
(25)
which concludes the proof.
Theorem 4.1 shows that it takes at most Op1
ϵq iterations to ﬁnd an approximate
stationary point with gap smaller than ϵ. Note that Theorem 4.1 requires mini τ piq ą 0
and, for example, the standard choice τ piq “
2
i`1 does not satisfy this assumption.
12

4.1. Compactness assumption and convergence of Algorithm 1
By looking at the convergence rate given by (18), it is tempting to take ˜τ as large as
possible. However, since the set Q is convex, the maximum allowed value for ˜τ is 1
to ensure that the iterates Zpiq remain feasible. This setting leads to a convergence
rate of Op1{iq, given the assumptions of Theorem 4.1 are satisﬁed, and corresponds
to Algorithm 1. In Theorem 4.1, we need the set Q to be compact. Let us discuss this
assumption in the context of Algorithm 1 that relies on our two formulations:
(1) For the formulation using exponential cones (5), a variable Wfk (resp. Hkn) equal
to zero will correspond to Ufk (resp. Tkn) going to ´8, which is not bounded. As
recommended by Mosek, we use additional artiﬁcial component-wise lower and
upper bounds for U and T, namely, -35 and 10. Therefore, our implementation
actually solves a component-wise bounded version of (5). However, since e´35 «
6 ¨ 10´16 and e10 « 2 ¨ 104, numerically, these constraints do not exclude good
approximations of V in practice, as long as the entries in V belong to a reasonable
range which can be assumed w.l.o.g. by a proper preprocessing of V , e.g., V Ð
V
maxf,n Vfn so that Vfn P r0, 1s for all f, n; see, e.g., the discussion in [15, page 66].
(2) For the formulation using second-order cones (9), we can assume compactness
w.l.o.g. In fact, for simplicity, let us consider the formulation (6) in variables
pW, Hq, since the change of variables is the component-wise square root, and
keeps the feasible set compact. We can w.l.o.g. add a set of normalization con-
straints on the rows of H, such as ř
n Hkn “ }Hk:}1 “ 1 for all k which leads to
a compact set for H, since we can use the degree of freedom in scaling between
the columns of W and rows of H. It remains to show that W can be assumed
to be in a compact set. Let us show that the level sets of the objective func-
tion are compact, which will give the result. The objective function of (6) is the
component-wise ℓ1 norm, }WH}1. Then, let pW 1, H1q be an arbitrary feasible so-
lution of (6) (such a solution can be easily constructed), and add the constraint
}WH}1 ď }W 1H1}1 “ f1 to formulation (6), which does not modify its optimal
solution set. We have for all k
}W:k}1 “ }W:k}1}Hk:}1 “ }W:kHk:}1 ď }WH}1 ď f1,
which shows that W in that modiﬁed formulation also belongs to a compact set.
The second equality and ﬁrst inequality follow from nonnegativity of W and H.
under these modiﬁcations that make the feasible set Q compact, we have the fol-
lowing corollary.
Corollary 4.2. Both variants (5) and (9) of Algorithm 1 generate a sequence of
iterates Zpiq whose FW gap converges according to ˜µpiq ď ΦpZp0qq´Φ˚
i`1
.
Remark 2 (Diﬀerence-of-convex-functions optimization). Algorithm 1 can also be
interpreted as a special case of a diﬀerence-of-convex algorithm that minimizes the
diﬀerence between two convex functions, namely f1pZq ´ f2pZq [23]. In our case, we
would have f1pZq “ 0. For such problems, a convergence rate to stationary points of
order Op1{iq has also been derived when optimizing over a convex compact feasible
set, but using a diﬀerent measure of stationary [1].
13

Remark 3. Using a proper normalization (see Section 4.1), the original NMF problem
can be tackled directly by the FW algorithm (Algorithm 2), as the linear minimization
oracle can be computed in closed form. Hence one could use existing results on FW to
derive a convergence rate. However, the FW algorithm applied to smooth nonconvex
problems leads to a worse rate of Op1{
?
iq [22].
5. Numerical experiments
In this section, Algorithm 1, using both formulations (5) and (9), is tested for the
computation of exact NMF for particular classes of matrices usually considered in the
literature: (1) 10-by-10 matrices randomly generated with nonnegative rank 5 (each
matrix is generated by multiplying two random rank-5 nonnegative matrices), (2) four
6-by-6 inﬁnitesimally rigid factorizations with nonnegative rank 5 [20], denoted Vinfi for
i “ 1, 2, 3, 4, and (3) four 5-by-5 slack matrices corresponding to nested hexagons, de-
noted Va“x, with nonnegative ranks 3, 4, 5, 5 depending on a parameter x “ 2, 3, 4, `8,
respectively. These matrices are described in more details in the Appendix A.
In order to make Algorithm 1 practically more eﬀective, we incorporate two im-
provements in the context of Exact NMF:
(1) Sparsity patterns integration: in NMF, entries of W and H are expected to be
equal to zero at optimality. Hence, when some entries of W or H are suﬃciently
close to zero, ﬁxing them to zero for all remaining iterations reduces the number
of variables and hence accelerates the subsequent iterations of Algorithm 1; see
Appendix B for the details.
(2) Final reﬁnement: once a solution is generated by Algorithm 1 for the formulation
based on exponential cones, it can typically be slightly improved by applying a
standard NMF algorithm (we use a few iterations of A-HALS [14]). This is due
to the bound constraints on W and H; see Section 4.1.
For each of the matrices, we run our Algorithm 1 for 750 iterations for nested
hexagons and random matrices and 3000 iterations for inﬁnitesimally rigid matrices,
and compare with the state-of-the-art algorithms from [29] with Multi-Start 1 heuristic
”ms1” and the Rank-by-rank heuristic ”rbr”. For each method, we run 100 initializa-
tions with SPARSE10, as recommended in [29], with target precision 10´6. Note that
a diﬀerent random matrix is generated each time for the experiments on random ma-
trices, following the procedure described in the Appendix A. All tests are preformed
using Matlab R2021b on a laptop Intel CORE i7-11800H CPU @2.3GHz 16GB RAM.
The code is available from https://bit.ly/3FqMqhD.
Table 1 reports the number of successes over 100 attempts to compute the exact
NMF of the input matrices, where the success is deﬁned as obtaining a solution where
}V ´WH}F
}V }F
is below the target precision, namely 10´6.
We observe the following:
‚ All algorithms ﬁnd exact NMFs in all runs for random matrices. It is well-known
that factorizing randomly generated matrices is typically easier [29]. This shows
that Algorithm 1 with both formulations (5) and (9) is also able to compute
exact NMFs in this scenario, which is reassuring.
‚ Looking at the nonrandom matrices, Algorithm 1 with both formulations (5)
and (9), and “ms1” from [29] are the only algorithms able to compute an exact
14

Table 1.
Comparison of the two variants of Algorithm 1 for (5) and (9) with the algorithm from [29] with
the ”ms1” and ”rbr” heuristics. Each run is performed with 100 initializations to compute the factorizations
of matrices described in Appendix A. In bold, we indicate the algorithm that found the most exact NMFs.
Algorithm 1
Algorithm 1
Algorithm from [29]
Algorithm from [29]
for (5)
for (9)
with “ms1”
with “rbr”
Matrices
/100
/100
/100
/100
Random matrices
100
100
100
100
Inf. Rig. Fac.
Vinf1
5
7
6
0
Vinf2
16
38
34
97
Vinf3
14
33
14
90
Vinf4
16
20
15
0
Nested hexagons
Va“2
100
100
100
100
Va“3
100
100
100
100
Va“4
35
69
36
100
VaÑ`8
17
42
20
100
NMF for at least some of the 100 initializations. Moreover, among these three
algorithms, Algorithm 1 with (9) found most frequently an exact NMF.
‚ For some matrices (namely, Va“4 and VaÑ`8), “rbr” from [29] is able to compute
exact NMF for all initializations, which is not the case of the other algorithms.
However, “rbr” is not able to compute exact NMFs for Vinf1 and Vinf2.
In summary, Algorithm 1 competes favorably with the algorithms proposed in [29],
and appears to be more robust in the sense that it computes exact NMFs in all
the tested cases. In addition, the second-order cone formulation, Algorithm 1 with (9),
performs slightly better than with the exponential cone formulation, Algorithm 1 with
(5).
Computational time In terms of computational time, Algorithm 1 performs simi-
larly to algorithm from [29] for the considered matrices, but it does not scale as well.
The main reason is that it relies on interior-point methods, while [29] relies on ﬁrst-
order methods (more precisely, exact BCD). For example, for the inﬁnitesimally rigid
matrices, Algorithm 1 and [29] take between 2 and 16 seconds. We would report slower
performance for Algorithm 1 compared to [29] for larger matrices. Hence a possible
direction of research would be to use faster methods to tackle the conic optimization
problems.
Numerical validation of the rate of convergence As a simple empirical vali-
dation of the rate of convergence proposed in Section 4, we report in Figure 1 the
evolution of the minimum FW gap computed along iterations by Algorithm 1 (for (5)
and (9)) for one tested matrix, namely Vinf2. Similar behaviours were observed for all
the tested input matrices: additional ﬁgures are given in Appendix C. Note that we
also integrated a variant of Algorithm 1 for which τ piq :“
2
i`1 (a standard choice in
the literature for FW algorithms). In Figure 1, we observe that the behaviour of the
minimal FW gap is in line with the theoretical results from Section 4. Furthermore,
the choice ˜τ “ τ piq :“ 1 leads to faster decrease of the minimal FW gap encountered
along iterations, as expected.
Impact of the initialization We also investigated the impact of using an im-
proved initialization for our Algorithm 1 with (9), based on a rank-one NMO slightly
15

100
200
300
400
500
600
700
800
900
10-12
10-10
10-8
10-6
10-4
10-2
(a) Algorithm 1 for (5)
100
200
300
400
500
600
700
800
900
10-6
10-4
10-2
100
102
(b) Algorithm 1 for (9)
Figure 1.
Evolution of the minimum FW gap computed along iterations by Algorithm 1 for (5) (left) and
(9) (right) for the matrix Vinf2.
perturbed with nonnegative random uniform values. More precisely, we set Zp0q “
ZK“1 ` dR }ZK“1}F
}R}F
where ZK“1 is the rank-one NMO computed with the methodol-
ogy detailed in Section 2.2.1, R is a matrix of appropriate size of uniformly distributed
random numbers in the interval p0, 1q, and d is a parameter to be deﬁned by the user.
In our numerical experiments, we chose values for d within the interval r0.01, 0.05s. We
ﬁnd that Algorithm 1 with (9) using that dedicated initialization found respectively
74, 65 and 52 successes for matrices VaÑ`8, Vinf2 and Vinf4, respectively, a marked
improvement over the default random initialization (where it had 42, 38, 20 successes,
respectively). For the other tested matrices, it did not change the result signiﬁcantly
(only a few additional successes). Therefore a possible direction of research would be
the design of more advanced strategies for the initialization of pU, Tq.
6. Conclusion
In this paper, we introduced two formulations for computing exact NMFs, namely (1)
and (6) that are under- and upper-approximation formulations for NMF, respectively.
For each formulation, we used a particular change of variables that enabled the use
of two convex cones, namely the exponential and second-order cones, to reformulate
these problems as the minimization of a concave function over a convex feasible set.
In order to solve the two optimization problems, we proposed Algorithm 1 that relies
on the resolution of successive linear approximations of the objective functions and
the use of interior-point methods. We also showed that our optimization scheme rely-
ing on successive linearizations is a special case of the Frank-Wolfe (FW) algorithm.
Using an appropriate measure of stationarity, namely the FW gap, we showed in The-
orem 4.1 that the minimal FW gap generated by our algorithm converges as Op1
i q,
where i is the iteration index. We believe this type of global convergence rate to a
stationary point is new for NMF. We showed that Algorithm 1 is able to compute
exact NMFs for several classes of nonnegative matrices (namely, randomly generated
matrices, inﬁnitesimally rigid matrices, and slack matrices of nested hexagons) and as
such demonstrate its competitiveness compared to recent methods from the literature.
However, we have only tested Algorithm 1 on a limited number of nonnegative matri-
16

ces for exact NMF. In the future we plan to test it on a larger library of nonnegative
matrices and also to compute approximate NMFs in data analysis applications, in or-
der to better understand the behavior of Algorithm 1 along with the two formulations,
(5) and (9).
Further works also include:
‚ The design of more advanced strategies for the initialization of pU, Tq.
‚ The elaboration of alternative formulations for (5) and (9) to deal with the
non-uniqueness of the NMF models. In particular, we plan to develop new for-
mulations that discard solutions of the form V “ ˜W ˜H “ pWEq
`
E´1H
˘
for
a given solution pW, Hq and for invertible matrices E such that WE ě 0 and
E´1H ě 0. For example, one could remove the permutation and scaling ambi-
guity for the columns of W and rows of H, to remove some degrees of freedom
in the formulations. We refer the interested reader to [11] and [13, Chapter 4],
and the references therein, for more information on the non-uniqueness of NMF.
‚ The use of our framework for other closely related problems; in particular the
computation of symmetric NMFs which requires H “ W J; this problem is closely
related to completely positive matrices [4]. Symmetric NMF can be used for data
analysis and in particular for various clustering tasks [21].
‚ The use of upper-approximations that are more accurate than linearizations for
concave functions such as second-order models, and study the convergence for
such models.
Acknowledgements
We thank the two reviewers for their careful reading and insightful feedback that
helped us improve the paper signiﬁcantly.
References
[1] H. Abbaszadehpeivasti, E. de Klerk, and M. Zamani, On the rate of convergence of the
diﬀerence-of-convex algorithm (DCA), arXiv preprint arXiv:2109.13566 (2021).
[2] S. Arora, R. Ge, R. Kannan, and A. Moitra, Computing a nonnegative matrix
factorization—provably, SIAM Journal on Computing 45 (2016), pp. 1582–1611.
[3] S. Basu, R. Pollack, and M.F. Roy, On the combinatorial and algebraic complexity of
quantiﬁer elimination, Journal of the ACM (JACM) 43 (1996), pp. 1002–1045.
[4] A. Berman and N. Shaked-Monderer, Completely positive matrices, World Scientiﬁc, 2003.
[5] A. Cichocki, R. Zdunek, A.H. Phan, and S.i. Amari, Nonnegative matrix and tensor factor-
izations: applications to exploratory multi-way data analysis and blind source separation,
John Wiley & Sons, 2009.
[6] K.L. Clarkson, Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm,
ACM Trans. Algorithms 6 (2010).
[7] J.E. Cohen and U.G. Rothblum, Nonnegative ranks, decompositions, and factorizations
of nonnegative matrices, Linear Algebra and its Applications 190 (1993), pp. 149–168.
[8] J. Dewez, Computational approaches for lower bounds on the nonnegative rank, Ph.D.
diss., UCLouvain, 2022.
[9] J. Dewez, N. Gillis, and F. Glineur, A geometric lower bound on the extension complexity
of polytopes based on the f-vector, Discrete Applied Mathematics 303 (2021), pp. 22–388.
[10] M. Frank and P. Wolfe, An algorithm for quadratic programming, Naval Research Logistics
Quarterly 3 (1956), pp. 95–110.
17

[11] X. Fu, K. Huang, N.D. Sidiropoulos, and W.K. Ma, Nonnegative matrix factorization
for signal and data analytics: Identiﬁability, algorithms, and applications., IEEE Signal
Process. Mag. 36 (2019), pp. 59–80.
[12] N. Gillis, Approximation et sous-approximation de matrices par factorisation positive :
algorithmes, complexite et applications, Master’s thesis, Universite Catholique de Louvain,
2007.
[13] N. Gillis, Nonnegative Matrix Factorization, SIAM, Philadelphia, 2020.
[14] N. Gillis and F. Glineur, Accelerated multiplicative updates and hierarchical ALS algo-
rithms for nonnegative matrix factorization, Neural Computation 24 (2012), pp. 1085–
1105.
[15] N. Gillis, et al., Nonnegative matrix factorization: Complexity, algorithms and applica-
tions, Unpublished doctoral dissertation, Universit´e catholique de Louvain. Louvain-La-
Neuve: CORE (2011).
[16] N. Gillis and F. Glineur, Using underapproximations for sparse nonnegative matrix fac-
torization, Pattern recognition 43 (2010), pp. 1676–1687.
[17] M. Jaggi, Sparse convex optimization methods for machine learning, Ph.D. diss., ETH
Zurich, 2011.
[18] M. Jaggi, Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization, in Pro-
ceedings of the 30th International Conference on Machine Learning, Proceedings of Ma-
chine Learning Research Vol. 28, 17–19 Jun, Atlanta, Georgia, USA. PMLR, 2013, pp.
427–435.
[19] J. Kim and H. Park, Fast nonnegative matrix factorization: An active-set-like method and
comparisons, SIAM Journal on Scientiﬁc Computing 33 (2011), pp. 3261–3281.
[20] R. Krone and K. Kubjas, Uniqueness of nonnegative matrix factorizations by rigidity
theory, SIAM Journal on Matrix Analysis and Applications 42 (2021), p. 134–164.
[21] D. Kuang, C. Ding, and H. Park, Symmetric Nonnegative Matrix Factorization for Graph
Clustering, in Proceedings of the 2012 SIAM International Conference on Data Mining.
pp. 106–117.
[22] S. Lacoste-Julien, Convergence rate of Frank-Wolfe for non-convex objectives, arXiv
preprint arXiv:1607.00345 (2016).
[23] H.A. Le Thi and T. Pham Dinh, DC programming and DCA: thirty years of developments,
Mathematical Programming 169 (2018), pp. 5–68.
[24] A. Moitra, An Almost Optimal Algorithm for Computing Nonnegative Rank, in Proc.
of the 24th Annual ACM-SIAM Symp. on Discrete Algorithms (SODA ’13). 2013, pp.
1454–1464.
[25] Mosek APS, Optimization software. Available at https://www.mosek.com/.
[26] R. Sharma and K. Sharma, A new dual based procedure for the transportation problem,
European Journal of Operational Research 122 (2000), pp. 611–624.
[27] Y. Shitov, The nonnegative rank of a matrix: Hard problems, easy solutions, SIAM Review
59 (2017), pp. 794–800.
[28] M. Tepper and G. Sapiro, Nonnegative matrix underapproximation for robust multiple
model ﬁtting, in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. 2017, pp. 2059–2067.
[29] A. Vandaele, N. Gillis, F. Glineur, and D. Tuyttens, Heuristics for exact nonnegative
matrix factorization, J. of Global Optim 65 (2016), p. 369–400.
[30] S.A. Vavasis, On the complexity of nonnegative matrix factorization, SIAM Journal on
Optimization 20 (2010), pp. 1364–1377.
18

Appendix A. Factorized matrices
In this appendix, we describe the matrices considered for the numerical experiments
in Section 5:
‚ Randomly generated matrices: It is standard in the NMF literature to use ran-
domly generated matrices to compare algorithms (see, e.g., [19]). In this pa-
per, we used the standard approach: V “ WH P RFˆN
`
where each entry of
W P RFˆK
`
and H P RKˆN
`
is generated using the uniform distribution in the in-
terval r0, 1s, and K ď minpF, Nq. With this approach, rankpV q “ rank`pV q “ K
with probability one. In the numerical experiments reported in Section 5, we used
F “ N “ 10 and K “ 5.
‚ Inﬁnitesimally rigid factorizations: in this paper, we consider four inﬁnitesimally
rigid factorizations for 5ˆ 5 matrices with positive entries and with nonnegative
rank equal to four from [20]:
Vinf1 “
¨
˚
˚
˚
˚
˝
573705
806520
167622
246500
531659
397096
39600
299176
63720
274120
131646
403260
30269
226915
264510
9114
85160
311182
827468
851798
147857
3200
351037
599025
697755
˛
‹‹‹‹‚
,
Vinf2 “
¨
˚
˚
˚
˚
˝
30893
319912
149770
873
111428
383490
87990
5580
628440
587250
560076
1030324
331070
288045
350647
203830
305184
277512
264376
205933
90911
142936
500784
618842
609633
˛
‹‹‹‹‚
,
Vinf3 “
¨
˚
˚
˚
˚
˝
948201
723609
958755
591858
397953
222448
218040
30429
348793
15825
329588
7189
623001
12012
469185
467424
160704
115092
835504
343912
1114797
932972
975775
997164
636096
˛
‹‹‹‹‚
,
Vinf4 “
¨
˚
˚
˚
˚
˝
88076
294646
658787
902872
244559
2216
4216
596705
652698
250465
279360
180864
769506
1051380
391634
553284
826606
765406
293965
883775
696039
897917
148301
832169
169525
˛
‹‹‹‹‚
.
These matrices have been shown to be challenging to factorize. We refer the
reader to [20] for more details.
‚ Nested hexagons problem: computing an exact NMF is equivalent to tackle a
well-known problem in computational geometry which is referred to as nested
polytope problem. Here we consider a family of input matrices whose exact NMF
corresponds to ﬁnding a polytope nested between two hexagons; see [13, Chapter
19

2] and the references therein. Given x ą 1, Va“x is deﬁned as
1
x
¨
˚
˚
˚
˚
˚
˚
˝
1
x
2x ´ 1
2x ´ 1
x
1
1
1
x
2x ´ 1
2x ´ 1
x
x
1
1
x
2x ´ 1
2x ´ 1
2x ´ 1
x
1
1
x
2x ´ 1
2x ´ 1
2x ´ 1
x
1
1
x
x
2x ´ 1
2x ´ 1
x
1
1
˛
‹‹‹‹‹‹‚
which satisﬁes rankpVa“xq “ 3 for any x ą 1. The inner hexagon is smaller than
the outer one with a ratio of a´1
a . We consider three values for a:
˝ a “ 2: the inner hexagon is twiced smaller than the outer one and we can
ﬁt a triangle between the two, thus rank`pVaq “ 3.
˝ a “ 3: the inner hexagon is 2{3 smaller than the outer one and we can ﬁt
a rectangle between the two, thus rank`pVaq “ 4.
˝ a “ 4: rank`pVaq “ 5.
˝ a Ñ `8, which gives:
V “
¨
˚
˚
˚
˚
˚
˚
˝
0
1
2
2
1
0
0
0
1
2
2
1
1
0
0
1
2
2
2
1
0
0
1
2
2
2
1
0
0
1
1
2
2
1
0
0
˛
‹‹‹‹‹‹‚
with rank`pV q “ 5.
Appendix B. Sparsity Patterns Integration
This appendix details the SPI procedure for quadratic cones, similar rationale has
been followed for exponential cones. Due to nonnegative constraints on the entries of
W and H, one can expect sparsity patterns for the solutions pW, Hq, as for the solution
pU, Tq of (9) since Wfk “ GpUfkq “ aUfk and Hkn “ GpTknq “ ?Tkn. Obviously,
the sparsity for the solutions is reinforced by the sparsity of the input matrix V . One
can observe that the objective function Φ from (9) is not L-smooth on the interior
of the domain, that is the non-negative orthant. In the case the pf, kq-entry of the
current iterate Ui´1 tends to zero, the corresponding entry of the gradient of Φ w.r.t.
U tends to 8 which therefore ends the optimization process. In order to tackle this
issue and enables the solution to reach the desired tolerance of 10´6, we integrated
an additional stage within the second building block of Algorithm 1. This additional
stage is referred to as ”Sparsity Pattern Integration”. Let us illustrate its principle on
the following case: the entry Ui´1
¯f,¯k tends to zero. Let us now ﬁx U ¯f,¯k to zero, drop this
variable from the optimization process and observe the impact on the constraints of
(7) in which variable U ¯f,¯k is involved; the inequality constraints identiﬁed by index
f “ ¯f are:
b
U ¯f,1
a
T1,n ` ... `
b
U ¯f,¯k
b
T¯k,n ` ... `
b
U ¯f,K
a
TK,n ě V ¯f,n for n P N.
20

First, since aU ¯f,¯k “ 0, there is no more constraints on aT¯k,n for N inequalities
identiﬁed by index f “ ¯f. Second, for the problem (9) and its successive lineariza-
tions, it is then clear that N conic variables t ¯f,¯k,n (and hence the N associated conic
constraints) can be dropped from the optimization process. Finally, the linear term
“
∇UΦ
`
Ui´1, T i´1˘‰
¯f,¯k U ¯f,¯k is removed from the current linearizations of Φ. The same
rationale is followed for the case entries of the current iterate for T tend to zero.
On a practical point of view, at each activation of SPI, Algorithm 1 checks if entries
of the current iterates pUi´1, T i´1q are below a threshold th deﬁned by the user.
Hence the corresponding entries of U and T are set to zero so that a sparsity pattern
is determined, that are the indices of these entries. The successive linearizations of (9)
are automatically updated based on the current sparsity pattern with the approach
explained above.
Let us illustrate the impact of triggering the SPI procedure on the solutions obtained
for the factorization of the following matrix V :
V “
¨
˚
˚
˚
˚
˚
˚
˝
0
1
2
2
1
0
0
0
1
2
2
1
1
0
0
1
2
2
2
1
0
0
1
2
2
2
1
0
0
1
1
2
2
1
0
0
˛
‹‹‹‹‹‹‚
.
(B1)
The nonnegative rank of (B1) is known and is equal to 5. Algorithm 1 is used to
compute an exact NMF of V with the following input parameters:
‚ K “ 5 “ rank`pV q,
‚ th “ 10´3,
‚ the maximum number of iterations deﬁned by parameter maxiter is set to 500
and the SPI procedure is triggered at iteration 400.
Figure B1 displays the evolution of }V ´WH}F
}V }F
along iterations for V (B1) with a fac-
torization rank K “ 5. One can observe that, once the SPI is activated, the relative
Frobenius error drops from 5 10´4 to 8 10´9, hence below the tolerance of 10´6 such
that we assume an exact NMF pW, Hq is found. For this experiment, we obtain:
W “
¨
˚
˚
˚
˚
˚
˚
˝
0
1.4748
0.9259
0
0
0.7824
0
1.8517
0
0
0
0
0.9259
0
1.4716
0
0
0
0.6024
1.4716
0.7824
0
0
1.2049
0
0
1.4748
0
0.6024
0
˛
‹‹‹‹‹‹‚
,
H “
¨
˚
˚
˚
˚
˝
0
0
1.2781
0
0
1.2781
0
0.6780
1.3561
0.6780
0
0
0
0
0
1.0801
1.0801
0
1.6599
1.6599
0
0
0
0
0.6796
0
0
0
0.6796
1.3591
˛
‹‹‹‹‚
.
21

0
100
200
300
400
500
10-10
10-8
10-6
10-4
10-2
100
Figure B1. Evolution of }V ´W H}F
}V }F
along iterations; SPI is triggered at iteration 400.
50
100
150
200
250
300
350
400
450
10-10
10-8
10-6
10-4
10-2
(a) Algorithm 1 for (5)
50
100
150
200
250
300
350
400
450
10-10
10-8
10-6
10-4
10-2
(b) Algorithm 1 for (9)
Figure C1.
Evolution of the minimum FW gap computed along iterations by Algorithm 1 for (5) (left) and
(9) (right) for a randomly geenrated matrix.
Appendix C. Additional empirical validations of the convergence rates
In this appendix, we report in Figures C1 and C2 additional empirical validations of
the convergence rate introduced in 4 for the two others classes on tested matrices,
namely the random matrices and the matrices related to nested hexagons problem.
For the later, we considered the particular case a “ 4.
22

50
100
150
200
250
300
350
400
450
10-8
10-6
10-4
10-2
(a) Algorithm 1 for (5)
50
100
150
200
250
300
350
400
450
10-8
10-6
10-4
10-2
(b) Algorithm 1 for (9)
Figure C2.
Evolution of the minimum FW gap computed along iterations by Algorithm 1 for (5) (left) and
(9) (right) for the matrix Va“4.
23
