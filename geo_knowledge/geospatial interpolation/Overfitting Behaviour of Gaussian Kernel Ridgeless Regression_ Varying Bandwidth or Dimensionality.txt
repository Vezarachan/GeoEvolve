Overfitting Behaviour of Gaussian Kernel Ridgeless
Regression: Varying Bandwidth or Dimensionality
Marko Medvedev
The University of Chicago
medvedev@uchicago.edu
Gal Vardi
Weizmann Institute of Science
gal.vardi@weizmann.ac.il
Nathan Srebro
TTI-Chicago
nati@ttic.edu
Abstract
We consider the overfitting behavior of minimum norm interpolating solutions
of Gaussian kernel ridge regression (i.e. kernel ridgeless regression), when the
bandwidth or input dimension varies with the sample size. For fixed dimensions,
we show that even with varying or tuned bandwidth, the ridgeless solution is
never consistent and, at least with large enough noise, always worse than the null
predictor. For increasing dimension, we give a generic characterization of the
overfitting behavior for any scaling of the dimension with sample size. We use
this to provide the first example of benign overfitting using the Gaussian kernel
with sub-polynomial scaling dimension. All our results are under the Gaussian
universality ansatz and the (non-rigorous) risk predictions in terms of the kernel
eigenstructure.
1
Introduction
A central question in learning theory is how learning algorithms can generalize well even when
returning models that perfectly fit (i.e. interpolate) noisy training data. This phenomenon was
observed empirically by Zhang et al. [65], and does not align with the traditional belief from
statistical learning theory that overfitting to noise leads to poor generalization. Consequently, it
attracted significant interest in recent years, and there has been much effort to understand the
overfitting behavior of linear models, kernel methods, and neural networks.
In this paper, we study the overfitting behavior of Kernel Ridge Regression (KRR) with Gaussian
kernel, namely, the behavior of the limiting test error when training on noisy data as the number
of samples tends to infinity by insisting on interpolation (achieving zero training error). When the
input dimension and bandwidth are fixed, the overfitting behavior is known to be “catastrophic” [38],
i.e. for any nonzero noise, the test risk tends to infinity as the sample size increases. However,
this is not how Gaussian Kernel Ridge Regression is typically used in practice. In fixed dimension,
the bandwidth is tuned, that is decreased, when the sample size increases [53, 18]. Additionally, it
makes sense to study the behaviour when the input dimension increases with sample size (as in, e.g.
linear models with proportional scaling [44]). This could be because when more data is available,
more input features are used, even with a kernel; because as more resources are available we scale
up both the input dimension and amount of data used; or to capture the fact that very large scale
problems typically involve both more samples and higher input dimension. But unlike with linear
models, where the dimension must scale linearly with the number of samples in order to allow for
interpolation, when a kernel is used we can study the behaviour even when the input dimensionality
increases much slower, and ask how slowly it could increase without catastrophic overfitting. Previous
studies on kernel ridgeless regression considered polynomial increasing dimension (i.e. dimension
∝sample-sizea, for 0 < a ≤1) [5, 66, 23, 39, 41], but not subpolynomial scaling.
We aim to provide a more comprehensive picture of overfitting with Gaussian KRR by studying
the overfitting behavior with varying bandwidth or with arbitrarily varying dimension, including
Preprint. Under review.
arXiv:2409.03891v1  [cs.LG]  5 Sep 2024

sub-polynomially. In particular, we show that for fixed dimension, even with varying bandwidth,
the interpolation learning is never consistent and generally not better than the null predictor (either
the test error tends to infinity or is finite but it is almost always not better than the null predictor).
For increasing dimension, we give an upper and lower bound on the test risk for any scaling of the
dimension with sample size, which indicates in many cases whether the overfitting is catastrophic
(test error tends to infinity), tempered (test error tends to a constant), or benign (consistent). Our
result agrees with the polynomial scaling of the dimension with sample size, showing tempered
overfitting for an exponent that is a reciprocal of an integer and benign overfitting for any other
exponent [5, 66]. Moreover, our result goes further, as we show the first example of sub-polynomially
scaling dimension that achieves benign overfitting for the Gaussian kernel. Additionally, we show that
a class of dot-product kernels on the sphere is inconsistent when the dimension scales logarithmically
with sample size. All our results are under the Gaussian universality ansatz and the non-rigorous but
well-established risk predictions in terms of the kernel eigenstructure [52, 68, 8, 59].
Related work
The test performance of overfitting models has been extensively studied for linear regression [28, 7,
3, 45, 47, 16, 33, 60, 55, 67, 31, 56, 13, 2, 51, 24], linear classification [12, 57, 10, 46, 43, 51, 37, 54,
58, 17], neural networks [19, 20, 11, 35, 62, 63, 40, 34, 22, 32, 30], and kernel methods. Below we
focus on overfitting in kernel ridge regression.
Overfitting in fixed dimension with fixed kernel.
In Mallinar et al. [38], the authors show
that the minimum norm interpolating solution for Gaussian kernel with fixed bandwidth overfits
catastrophically. In [15, 5, 66], the authors derive bounds on the test risk of minimum norm interpolant
with a fixed kernel under various assumptions. Our result for fixed dimension will only apply to the
Gaussian kernel, but it allows for any varying or adaptively chosen bandwidth.
Inconsistency of Kernel Ridge Regression.
The case of varying bandwidth has been considered
in Beaglehole et al. [6], Rakhlin and Zhai [49], Haas et al. [26]. In Beaglehole et al. [6], the authors
show that there exists a specific data distribution for which the minimum norm interpolanting solution
for a particular set of translation invariant kernels is not consistent. In Rakhlin and Zhai [49], the
authors show that for input distributions on the unit ball, the Laplace kernel is inconsistent, even with
varying bandwidth. In Haas et al. [25], the authors show that under different assumptions on the data
distribution, for a general class of (potentially varying) kernels in fixed dimension, any differentiable
function that overfits the data and is not much different from the minimum norm interpolant is
inconsistent. All of these works only consider whether we can achieve consistency. None of these
results apply in the case of data distributions that we are considering, and even if the predictor is
not consistent, we ask how bad is it by comparing it to the null predictor and whether it might be
tempered.
Overfitting in increasing dimensions.
Many papers have studied the setup where the dimension
increases with sample size [5, 66, 68, 39, 41, 29, 61], in particular when the dimension is a function
of sample size (or vice versa), but they all consider only the case of a polynomial scaling of dimension
and sample size. It was shown that in this case the minimum norm interpolating solution of dot
product kernels on the sphere can be benign, depending on if the exponent is not an integer. We
generalize these results to any scaling of the dimension and sample size. Our results recover the
existing results in the case of polynomially scaling dimension and show benign overfitting in a certain
sub-polynomial scaling. Having sub-polynomimal scaling of the dimension allows us to expand
the set of possible target functions from only polynomials of a bounded degree, as in the case of
polynomial scaling of dimension [5, 66, 68, 39, 41], to, in our case of sub-polynomially increasing
dimension, polynomials of any degree and even non-polynomial functions.
2
Problem formulation and assumptions
Kernel ridge(less) regression and the Gaussian kernel.
Let D be an unknown distribution over
X × Y ⊆Rd × R and let {(xi, yi)}m
i=1 ∼Dm be a dataset consisting of m samples. For simplicity,
we will assume that the distribution of the target is given by a target function f ∗of the input x ∈X
2

with zero mean independent noise ξ with variance σ2, that is y ∼f ∗(x) + ξ. We note that our results
can be extended to a distribution agnostic setting, as analyzed by Zhou et al. [68].
Let K : X × X →R be a positive semi-definite kernel function. Let ∥f∥K be the norm of f in the
RKHS HK corresponding to K. For a predictor f, let R(f) and ˆR(f) be the test and training risk of
f,
ˆR(f) = 1
m
m
X
i=1
(f(xi) −yi)2 and R(f) = ED

(f(x) −y)2
.
Two important risks to consider are the risk of the null predictor, f ≡0, which we will denote by
R(0) = ED[(y)2], and the Bayes (or irreducible) risk, which we will denote σ2 or R(f ∗). Using this
notation, the risk of the null predictor is R(0) = σ2 + ED[(y)2]. Bayes risk represents the minimum
possible risk that can be achieved by any predictor. For a regularization parameter δ, the regularized
ridge solution ˆfδ is given by
ˆfδ = argminf∈HK ˆR(f) + δ
m∥f∥2
K.
We are interested in the minimum norm interpolating (ridgeless) solution ˆf0 = limδ→0+ ˆfδ, namely
ˆf0 = argmin ˆ
R(f)=0;f∈HK∥f∥2
K.
We will focus on the Gaussian kernel, which is given by
K(x1, x2) = exp

−∥x1 −x2∥2
2
τ 2m

,
where m is the sample size and τm is a predetermined bandwidth parameter that can vary with sample
size. The Gaussian kernel is widely used and achieves good error rates for a variety of learning tasks
[53, 18]. Gaussian KRR achieves optimal convergence to the best possible (Bayes) error for learning
any function in a Besov space of high enough order (essentially bounded and twice differentiable in
the weak sense) under very mild assumptions on the distribution of the input and target X × Y [18].
For ridge regression with the Gaussian kernel and under a standard data distribution assumption, the
minimum distance between samples decreases with sample size so it makes sense to also decrease τm.
Additionally, decreasing τm with sample size helps to achieve good convergence rates theoretically
[18].
Main question.
We will consider the problem of learning using the minimum norm interpolating
solution ˆf0 of KRR. We want to understand the limiting behavior of test risk R( ˆf0) as the sample
size increases m →∞, that is limm→∞R( ˆf0). It suffices to understand lim supm→∞R( ˆf0) and
lim infm→∞R( ˆf0) and this way we do not assume the existence of the limit. In this work, we use
the taxonomy of benign, tempered, and catastrophic overfitting from Mallinar et al. [38], which
indicates whether limm→∞R( ˆf0) is the Bayes (optimal) error, a non-optimal but constant error, or
infinity. Note that in this taxonomy, the null predictor can be classified as tempered. Therefore, we
will compare the limiting risk to the risk of the null predictor R(0) in order to understand whether the
performance of the interpolating solution is non-trivial.
Main tool: Eigenframework and a closed form of the test risk.
Our main tool will be the closed
form of the test risk predicted by the eigenframwork [52]. Under the eigenframework, we can write
down the closed form of the test risk using Mercer’s theorem decomposition of a kernel function K.
Given a positive semi-definite kernel function K : X × X →R, we can decompose it as
K(x1, x2) =
∞
X
k=1
λkϕk(x1)ϕk(x2),
(1)
where λk and ϕk are the eigenvalues and eigenfunctions of the integral operator associated to K. The
eigenfunctions {ϕk} are an orthonormal basis of L2
DX (X), where DX is the marginal distribution of
X. We denote the Bayes optimal target function by f ∗, and expand it in the kernel basis {ϕi}∞
i=1 as
3

f ∗(x) = P∞
i=1 βiϕi(x). To state the close form of the test risk we will introduce a few quantities. Let
effective regularization, κδ, be the solution to P∞
i=1
λi
λi+κδ + δ
κδ = m. Furthermore, let Li,δ =
λi
λi+κδ
and Eδ =
m
m−P∞
i=1 L2
i,δ . Then, the predicted risk, i.e. the predicted closed form of the test risk of ˆfδ,
is given by
˜R( ˆfδ) = Eδ
 ∞
X
i=1
(1 −Li,δ)2 β2
i + σ2
!
,
(2)
where σ2 is the Bayes error of D. Note that [69] shows that the predicted closed form of the test risk
from [52] extends to more general targets, as stated above. There is strong evidence that the predicted
risk is a good estimate of the true test risk, namely R( ˆfδ) ≈˜R( ˆfδ). Indeed, a number of works use
the predicted risk closed form to estimate the test risk of KRR [60, 8, 68]. The following assumption
on Gaussian design ansatz is used by all of these works.
Assumption 1 (Gaussian design ansatz, cf. Zhou et al. [68]). When sampling (x, ·) ∼D, we have that
the Gaussian universality holds for the eigenfunctions in the sense that the expected risk is unchanged
if we replace ϕ with ˜ϕ, where ˜ϕ is Gaussian with appropirate parameters, i.e. ˜ϕ ∼N(0, diag{λi}).
This assumption appears to hold for real datasets as well, namely the predictions computed for
Gaussian design agree well with the experiments on kernel regression using real data [8, 52, 59].
As discussed in Zhou et al. [68], under this assumption, the equivalence R( ˆfδ) ≈˜R( ˆfδ) holds in a
few ways. First, in an appropriate asymptotic limit in which the sample size m and the number of
eigenmodes in a given eigenvalue grow proportionally, the equivalence holds [27, 1]. Second, if the
eigenstructure of the task is fixed, the error between the two can be bounded by a decaying function
of m [14]. Finally, various numerical experiments show that the error between the two is small even
for small sample size m [8, 52]. Specifically, Canatar et al. [8] (see Figure 5 there) gives empirical
evidence that the predicted risk closely approximates the true risk for the Gaussian kernel with data
uniform on a sphere, which is the setting that we consider in some of our results.
There has been some recent progress in bounding the error between R( ˆfδ) and ˜R( ˆfδ) unconditionally.
Misiakiewicz and Saeed [42] show that the error will tend to zero if the dimension d grows fast
enough with the sample size. Additionally, they provide strong empirical evidence that the predicted
risk is close to the test risk for a real dataset (MNIST) and Gaussian kernel, see Figure 1 in [42].
Formally, we will prove results about the predicted risk ˜R( ˆfδ), but as previously presented evidence
suggests, treating R( ˆfδ) ≈˜R( ˆfδ) as equivalence is sufficient for understanding the behavior of KRR.
We note that using the eigenframework might introduce restrictions for which kernels some of our
results apply, as concurrent work showed that the eigenframework prediction might not hold for the
NTK in fixed dimension [4, 15]. Our two main results concern the Gaussian kernel, for which we
described ample empirical evidence that the eigenlearning predictions hold [8, 42]. Understanding
the limitations of the eigenframework is an important future research direction.
3
Fixed dimension: Gaussian kernel with varying bandwidth
We will assume that the source distribution is uniform on a d dimensional sphere, that the target
function is square integrable, and that the target distribution is given by the target function with an
independent noise.
Assumption 2 (Target function and data distribution). Let D be the distribution over X × Y =
Sd−1 × R, such that the X marginal, denoted by DX , is Unif(Sd−1). We will assume that for a target
function f ∗∈L2
DX (Sd−1), the marginal Y distribution is given by y ∼f ∗(x) + ξ, where ξ has
mean zero and variance σ2 > 0. We write f ∗= P
i βiϕi, where ϕi is the L2
DX (Sd−1) eigenbasis
corresponding to the kernel K (Equation (1)). If we write β = (β1, β2, . . . ), then we have that
∥β∥2
2 = EDX
 (f ∗(x))2
. We will use the notation ∥f ∗∥2 = ∥β∥2
2.
The assumption on the distribution on X is common in the literature on KRR [39, 5, 66, 21]. The
assumption that x ∼Unif
 Sd−1
can be relaxed to a more general setting where x is uniformly
distributed on other manifolds that are diffeomorphic to the sphere using the results of Li et al. [36],
although that will not be the focus of this paper.
4

Note that here we vary the bandwidth τm so both ˆf0 and R( ˆf0) will depend on the bandwidth τm
as well as m. We will identify three different regimes of bandwidth scaling. We will show that the
minimum norm interpolating solution exhibits either tempered or catastrophic overfitting, and we
will argue that it is almost always worse than the risk of the null predictor.
Theorem 3 (Overfitting behavior of Gaussian kernel in fixed dimension). Under Assumption 2, the
following bounds hold for the predicted risk ˜R( ˆf0) of the minimum norm interpolating solution of
Gaussian KRR:
1. If τm = o(m−
1
d−1 ), then ˜
R(0) ≤lim inf m→∞˜
R( ˆ
f0) ≤lim supm→∞˜
R( ˆ
f0) < ∞.
More precisely, if τm ≤m−
1
d−1 t(m), where t(m) →0 as m →∞, then there is a scalar cd
that depends only on the dimension and m0 that depends on t(m) such that for all m > m0
we have ˜R( ˆf0) > σ2 + (1 −cdt(m)
d−1
2 )∥f ∗∥2.
2. If τm = ω(m−
1
d−1 ), then limm→∞˜
R( ˆ
f0) = ∞. Hence, for large enough m we have
˜R( ˆf0) > ˜R(0).
3. If τm = Θ(m−
1
d−1 ), then lim supm→∞R( ˆ
f0) < ∞.
Moreover, Suppose that
C1m−
1
d−1 ≤τm ≤C2m−
1
d−1 for some constants C1 and C2, then there exist η, µ > 0 that
depend only on d, C1, and C2, such that for all m we have ˜
R( ˆ
f0) > µ∥f ∗∥2+(1+η)σ2.
Consequently, ˜
R( ˆ
f0) > ˜
R(0) as long as σ2 > 1−µ
η ∥f ∗∥2.
Theorem 3 shows that the minimum norm interpolating solution of Gaussian KRR cannot be consistent
when data is distributed uniformly on the sphere, even with varying or adaptively chosen bandwidth.
Additionally, in the first two modes of bandwidth change, the minimum norm interpolating solution is
never better than the null predictor. In the third case, the interpolating solution is worse than null for
noise that is not too small. This shows that even though the minimum norm interpolating predictor is
classified as tempered in the first and third case of scaling of the bandwidth, it is still worse than the
trivial null predictor. Note that our analysis does not exclude the possibility that for τm = Θ(m
1
d−1 )
there exists small enough σ2 for which the interpolating solution is better than the null predictor.
We leave this as an open question. In Appendix A, we provide further empirical justification for
Theorem 3.
4
Increasing dimension
For the case of increasing dimension, we consider the problem of learning a sequence of distributions
D(d) over X × Y = Rd × R given by y ∼f ∗
d (x) + ξd using a sequence of kernels K(d). Here, ξd is
independent noise with mean 0 and variance σ2 > 0. Formally, the kernel and the target function
can change with the dimension d, but we will think of it as the same kernel and target with higher
dimensional input. Furthermore, d will increase with sample size m, i.e. d = d(m) (or analogously
m will increase with d). A common assumption, which we also adopt, is that the projections of the
target function f ∗
d onto the eigenfunctions ϕ(d)
k
of the kernels K(d) are uniformly bounded [5, 66].
Assumption 4 (Target function and distribution in increasing dimension). Consider learning a
sequence of target functions f ∗
d with a sequence of kernels K(d). Let the target function f ∗
d have only
Sd nonzero coefficients (where Sd can change with d), so f ∗
d = PSd
i=1 β(d)
i
ϕ(d)
i
where ϕ(d)
i
are from
Equation (1) and |β(d)
i
| ≤B, i.e. ∥β∥∞≤B, for B that is independent of d.
The functions that can be represented in this form depend on the number of nonzero coefficients, Sd,
and the kernel that we are using. In particular, for dot-product kernels on the sphere it includes all
polynomials of degree k ≤kd where kd is such that the multiplicities of first kd eigenfunctions are at
most Sd. If Sd grows with d, then this set will include much more general functions. See Remark 14
for further discussion.
First, we will consider a general kernel K since Theorem 7 and Theorem 9 will apply more generally.
Then, we will apply these results to the cases of dot-product and Gaussian kernels.
The multiplicities of eigenvalues will play an important role in bounding the test risk, both from
above and below, so we will introduce the following related notation.
5

Definition 5 (Lower and upper index). Let ˜λk be the k-th non-repeating eigenvalue of a kernel K
and let N(k) be its multiplicity. Let Nk = N(1) + · · · + N(k). Let m be the sample size. Let km be
defined as the maximal k such that there is less than m eigenvalues with index k, i.e.
km = max{k ∈N|N(1) + · · · + N(k) < m}.
Define the lower index Lm and the upper index Um as follows
Lm = N(1) + · · · + N(km) and Um = N(1) + · · · + N(km + 1).
When the dimension changes with sample size, we sometimes denote N(k) by N(d, k).
We will first state a generic bound on the test risk for any data distribution, kernel with bounded
sum of eigenvalues, sample size, and dimension. This bound will be informative when we scale the
dimension d with sample size m, but it holds for any kernel that satisfies the following assumption.
Assumption 6 (Bounded sum of eigenvalues). Assume that the kernel K has a bounded sum of
eigenvalues, i.e. there is a constant A such that P∞
i=1 N(i)˜λi ≤A. For a sequence of kernels K(d),
assume that all such A(d) are bounded by some constant A.1
This is a reasonable assumption for most dot-product kernels, as we show in Appendix C.4. It also
implicitly sets the scale of the kernel.
Theorem 7 (Test risk upper bound for kernel ridgeless regression). Let d and m be any dimension
and sample size. Define Lm, Um, km, N(i), Nl, and ˜λk as in Definition 5. Consider KRR with a
kernel K satisfying Assumption 6 for some A. Assume that for some integer l, the target function f ∗
satisfies Assumption 4 with at most Nl nonzero coefficients. Then, the predicted risk of the minimum
norm interpolating solution is bounded by the following:
˜R( ˆf0) ≤

1 −Lm
m
−1 
1 −m
Um
−1
σ2
(3)
+ B2

1 −Lm
m
−1 
1 −m
Um
−1 A2
m2
 
l
X
i=1
N(i) 1
˜λ2
i
!
.
(4)
Alternatively, we can bound the risk using
Pl
i=1 N(i) 1
˜λi

instead of
Pl
i=1 N(i) 1
˜λ2
i

(see Theo-
rem 20 in the appendix).
We also establish a generic inconsistency result for any data distribution, kernel with a bounded sum
of eigenvalues, sample size, and dimension based on the upper and lower indices from Definition 5.
We will further need to assume that the eigenvalues are bounded away from zero. Similarly, this will
be useful when scaling dimension d with sample size, but it holds generally.
Assumption 8 (Lower bound on eigenvalues). Assumme that the kernel K has eigenvalues that
are not too small, i.e. there is a constant b such that maxi≤km

1
˜λi

< m−Lm
b
. For a sequence of
kernels K(d), assume that for the corresponding m = m(d) (since d = d(m), we can also "invert"
the dependence) all such b(d) are bounded below by some b.
This assumption will hold for most dot-product kernels and we will show it for Gaussian kernel in
Appendix C.4.
Theorem 9 (Test risk lower bound for any kernel ridgeless regression). Let km and Lm be as
in Definition 5. Consider learning a target function f ∗, with some sample size m. Let K be a
kernel satisfying Assumption 6 and Assumption 8 for some A and b. Consider the minimum norm
interpolating solution of KRR (with any data distribution) with kernel K. Then, for the predicted risk
of minimum norm interpolating solution, the following lower bound holds:
˜R( ˆf0) >
 
1 −

b
b + 1
2 Lm
m
!−1
σ2.
1Note that this assumption implicitly sets the scale of the kernel.
6

To apply Theorem 7 for varying dimension d, we would additionally require that A is uniformly
bounded for all d and kernel K(d) and also that l = l(d) changes with d such that Assumption 4 holds
with Sd = Nl(d). For dot-product kernels K(d) on the sphere, if we let K(d)(x, y) = h(d)(∥x −y∥),
we will have A = supd h(d)(0), so if h(d) does not change with d we can take A = h(0) (see
Appendix C.4 for more details). Specifically, this holds for the Gaussian kernel on the sphere with
A = 1.
To apply Theorem 9 to the case of increasing dimension, we would require that the bounds
P
i N(d, i)˜λi ≤A and maxi≤km

1
˜λi

< m−Lm
b
hold for all d and kernels K(d). Usually, the
condition maxi≤km

1
˜λi

< m−Lm
b
will be satisfied for b = 1. We will show it for the two cases of
sub-polynomially scaling dimensions with a Gaussian kernel. For the polynomial scaling dimension,
it is reasonable to assume it for general dot-product kernels, as discussed in Appendix C.4.
Now, we will show that using Theorem 7 and Theorem 9 we can recover the behavior of the minimum
norm interpolating solution for polynomially increasing dimension [5, 66], i.e. tempered overfitting
for integer exponent and benign for non-integer exponent. Here, we will need to additionally assume
that the eigenvalue decay is not too fast.
Assumption 10 (Eigenvalue decay). The eigenvalues do not decrease too quickly, i.e. for km as in
Definition 5, we have that there is a constant c such that maxi≤km

1
˜λi

≤cN(km). For increasing
dimension, we require that maxi≤km

1
˜λi

≤cN(d, km) for all m (i.e. all d, as d and m both
increase).
This assumption is stronger than Assumption 8, but as we show in Appendix C.4, it is reasonable for
dot-product kernels on the sphere and even the NTK.
Corollary 11 (Dot-product kernels with polynomially increasing dimension, recovering the results of
[23, 39, 41, 5, 66]). Consider the problem of learning a sequence of target functions f ∗
d satisfying
Assumption 4 with Sd ≤Θ(d⌊α⌋) with a dot-product kernel K(x, y) = h(∥x−y∥) with h(0) = 1 on
the sphere Sd−1 (where h does not depend on d, i.e. A = 1 from Assumption 6)) that further satisfies
Assumption 10. Let dα
m = Θ(1) for α ∈(0, ∞). Then the overfitting behavior of the minimum norm
interpolating solution is benign if α is not an integer and tempered if α is an integer.
Additionally, we will show that for d = log m, we cannot get benign overfitting, i.e. consistency with
a class of dot-product kernels on the sphere. Similarly, as in the previous corollary, this will hold for
any sequence where d = log m even only asymptotically.
Corollary 12 (Inconsistency with dot-product kernels in logarithmically scaling dimension). Let
K(d) be a sequence of dot-product kernels on Sd−1 that satisfy Assumption 8. Let the dimension d
grows with sample size as d = log2 m (i.e. m = 2d). Then, the minimum norm interpolant cannot
exhibit benign overfitting for any such sequence K(d), i.e. there exists an absolute constant η > 0
such that for all d, m
˜R( ˆf0) > (1 + η)σ2.
On the other hand, using Theorem 7, we will establish the first case of sub-polynomial scaling
dimension with benign overfitting using the Gaussian kernel and data on the sphere. We will use
d = exp(√log m).
Corollary 13 (Benign overfitting with Gaussian kernel and sub-polynomial dimension). Let K be
the Gaussian kernel on the sphere Sd−1 with a fixed bandwidth, and take a sequence of dimensions d
and sample sizes m that scale as d = exp
 √log m

(in particular, we take l ∈N such that d = 22l
and m = 222l with l = 1, 2, 3 . . . ). Consider learning a sequence of target functions f ∗
d as in
Assumption 4 with Sd ≤m
1
4 . Then, we have that the minimum norm interpolating solution achieves
the Bayes error in the limit (m, d) →∞. In particular, for d ≥4 and m ≥16 we have
˜R( ˆf0) ≤

1 −
1
log m
−1 
1 −exp

−0.89
p
log m
−1
σ2 + 2B2 1
m.
7

Remark 14 (Allowed target functions). The set of allowed target functions f ∗
d in Corollary 13, i.e.
with sub-polynomial scaling dimension, is strictly larger than the set of allowed target functions for
polynomially scaling dimension, as in Corollary 11 and [39, 5, 66]. In particular, for polynomially
scaling dimension dα
m = Θ(1), the result holds only if the target function is a polynomial of degree at
most ⌊α⌋. On other hand, Corollary 13 shows that sub-polynomially scaling dimension allows for
the target function to be a polynomial of arbitrary degree, as well as non-polynomial functions. In
particular, in dimension d, we can represent polynomials of degree up to Θ(log2 d).
5
Proofs outline
In this section, we discuss the main proof ideas. All formal proofs are provided in the appendix.
By Equation (2), to understand how the test risk of the minimum norm interpolating solution behaves,
it suffices to understand how the eigenvalues corresponding to the kernel K and thus the quantities
E0, Li,0 behave. In Zhou et al. [68], the authors show that E0 is bounded both above and below in
terms of effective rank of the systems of eigenvalue {λ}∞
i=1, defined by rk :=
P∞
i=k+1 λi
λk+1
. We will
use this, along with directly bounding Li,0.
If K is a dot-product kernel on the sphere (such as the Gaussian kernel), we can take the eigen-
functions ϕi to be the spherical harmonics Yks, where k ≥0 and s ∈[1, N(d, k)].
Here
N(d, k) =
(2k+d−2)(k+d−3)!
k!(d−2)!
is the multiplicity of the k-th spherical harmonic. All Yks for the
same k will have the same eigenvalue, which we will denote ˜λk. In this case, we can write a
closed-form expression for the eigenvalues of Gaussian kernel, ˜λk, in terms of the bandwidth τm and
modified Bessel functions of the first kind Iv(x) [48] (see Appendix D). Using the closed form of
˜λi and the multiplicities of eigenvalues, we can understand how the test risk of the minimum norm
interpolating solution R( ˆf0) behaves as m →∞, which tells us its overfitting behavior.
Additionally, E0 appearing in Equation (2), will be informative. For example, if limm→∞E0 > 1
then the overfitting cannot be benign. The following bound using the effective rank rk holds [68]:
For k < m such that rk + k > m we have
E0 ≤

1 −k
m
−1 
1 −
m
k + rk
−1
.
(5)
For k ≥m it holds that
E0 ≥
1
1 −m
k

k−m
k−m+rk
.
(6)
Proof sketch of Theorem 3.
We will focus on the lower bounds in this case, as the result is
negative. The key elements to understanding the effective rank rk and the test risk Equation (2) is
to understand how the ratios of eigenvalues
˜λk+1
˜λk
and the multiplicities N(d, k) behave. Using the
closed form of the eigenvalues of Gaussian kernel and the properties of modified Bessel functions
[48, 50], with some computation (see Theorem 28 in the appendix for the computations), we can
derive the following bounds on ratios of eigenvalues

2
τ 2
m

2
 k + d
2

+

2
τ 2
m
 <
˜λk+1
˜λk
<

2
τ 2
m

 k + d
2 −1
2

+

2
τ 2
m
 .
(7)
From these bounds, we can derive tight bounds on
˜λk+j
˜λk
for indices k and j using simple but long
calculations (see Theorem 28 in the appendix). If k = o

1
τm

and j = o

1
τm

, then
˜λk+j
˜λk
≈1. If
k ≤Θ

1
τm

and j = Θ

1
τm

, then
˜λk+j
˜λk
= Θ(1). If k = ω

1
τm

, then
˜λk+j
˜λk
= o

1
jn

for any
integer n ∈N (i.e. it deceases super-polynomialy). For N(d, l) it holds that
N(d, l) = Θ(ld−2) and Nl := N(d, 1) + · · · + N(d, l) = Θ(ld−1).
8

Therefore if ˜l is the index such that ˜λ˜l = λl, we have that ˜l = Θ(l
1
d−1 ).
For τm ≥ω(m−
1
d−1 ), we will take l = (1 +
1
√m)m and show that rl = o(m). Then, the bound in
Equation (6) will imply that E0 ≥1 + √m, so from Equation (2), ˜R( ˆf0) > E0σ2 = √mσ2. Note
that for l = (1 +
1
√m)m, we have that ˜l = Θ(m
1
d−1 ) = ω

1
τm

. Note that for rl−1 (we consider
here l −1 for simplicity), we have that
rl−1 =
∞
X
i=0
λl+i
λl
< N(d, ˜l) +
∞
X
i=1
N(d, ˜l + i)
˜λ˜l+i
˜λ˜l
< Θ(m
d−2
d−1 ) (1 + Θ(1)) ,
since N(d, ˜l + i) < N(d, j)id−2 and
˜λ˜l+i
˜λ˜l
< 1
id . So indeed rl−1 = o(m) which implies rl = o(m).
For τm = o(m−
1
d−1 ) and τm = Θ(m−
1
d−1 ), we will directly analyze Equation (2). For k = Θ( 1
τm ),
we have that
˜λi
˜λ1 > 1
2 for all i ≤k. Let Li =
˜λi
˜λi+κ0 . Note that Li > 1
2L1 for i ≤k. Note that
m =
X
i
N(d, i)Li > 1
2L1 (N(d, 1) + · · · + N(d, k)) > Θ
  1
τm
d−1!
L1.
So, we have that for all i that Li < L1 <
m
Θ

(
1
τm )
d−1. From Equation (2), we have that
˜R( ˆf0) = E0
X
i
N(d, i)(1 −Li)2β2
i + E0σ2 > E0



1 −
m
Θ

1
τm
d−1




2
∥f ∗∥2 + E0σ2.
For τm = o(m
1
d−1 ) this is sufficient. Additionally, by a similar computation as above, in this
case, r1 = ω(m), so E0 is bounded by Equation (5). For τm = Θ(m
1
d−1 ), using Equation (5) and
Equation (6), by showing that for l = Θ(m), rl = Θ(m), we have Θ(1) > E0 > 1 + Θ(1).
Proof sketch of Theorem 7.
Let Li =
˜λi
˜λi+κ0 . Note that (1 −Li)2 =
κ2
0
(κ0+˜λi)2 . Then, Equation (2)
can be rewritten and bounded as (with an abuse of notation for βi)
˜R( ˆf0) = E0
X
i
N(d, i)(1 −Li)2β2
i + E0σ2 ≤E0B2
l
X
i=1
N(d, i)
κ2
0
(κ0 + ˜λi)2 + E0σ2.
Note that
κ2
0
(κ0+˜λi)2 < κ2
0
˜λ2
i and also Li ≤
˜λi
κ0 . Therefore, we have that
˜R( ˆf0) ≤E0B2κ2
0
 
l
X
i=1
N(d, i) 1
˜λ2
i
!
+ E0σ2.
Note that m = P
i N(d, i)Li < P
i N(d, i)
˜λi
κ0 <
A
κ0 , so κ0 <
A
m.
Finally, to bound E0,
note that in Equation (5) we can choose k = Lm < m, then rk + k > Um > m, so
E0 ≤
 1 −Lm
m
−1 
1 −
m
Um
−1
. Combining these with κ0 < A
m gives the claim of Theorem 7. The
proof of the alternative bound is harder and will be delayed to the appendix.
Proof sketch of Theorem 9.
By Theorem 3 from Zhou et al. [68], if k is the first k < m such that
k + brk ≥m, then
E0 ≥
 
1 −

b
b + 1
2 k
m
!−1
.
Since maxi≤km

1
˜λi

<
m−Lm
b
, for l < N(d, 1) + · · · + N(d, km), we have that brl + l ≤
N(d, 1) + · · · + N(d, km) −1 + b maxi≤km

1
˜λi

< Lm + m −Lm ≤m, so the first l for
which rl + l > m is l = Lm = N(d, 1) + · · · + N(d, km). Plugging in k = Lm we get that
E0 ≥

1 −

b
b+1

Lm
m
−1
, so from Equation (6), we have ˜R( ˆf0) ≥

1 −

b
b+1
2 Lm
m
−1
σ2.
9

Proof sketch of Corollary 11.
Note that an analogous proof holds for any dα
m →c, for a constant c,
but we take equality for simplicity. If k is a constant, i.e. it does not change with the dimension, we
have that N(d, k) = Θ(dk). Therefore, km = ⌊α⌋if α is a non-integer and α −1 if α is an integer.
If α is a non-integer, then Lm = N(d, km) + · · · + N(d, 1) = Θ(d⌊α⌋) and Um = N(d, km + 1) +
· · · + N(d, 1) = Θ(d⌊α⌋+1). So we have that Lm
m = d⌊α⌋−α,
m
Lm = dα−⌊α⌋−1, N(d, ⌊α⌋) = d⌊α⌋.
Note that km = ⌊α⌋. We have from Assumption 10 that maxi≤km
1
˜λi = O(N(d, km)). Note
that Theorem 7 holds with
Pl
i=1 N(d, i) 1
˜λi

instead of
Pl
i=1 N(d, i) 1
˜λ2
i

. Therefore, we have
1
m2 maxl≤km

1
˜λi N(d, i)

≤O(d(2⌊α⌋−2α)). This gives
˜R( ˆf0) ≤

1 −d⌊α⌋−α−1 
1 −dα−⌊α⌋−1−1
σ2
+ O

B2 
1 −d⌊α⌋−α−1 
1 −dα−⌊α⌋−1−1
d(2⌊α⌋−2α)

.
Therefore, limm→∞˜R( ˆf0) = σ2. So, if α is not an integer, we get benign overfitting. If α is an
integer, note that N(d, 1) + · · · + N(d, α −1) = o (dα), so km = α. Then there are cu, cl ∈(0, 1)
such that cl < Lm
m ≤cu < 1. Then, Theorem 9 holds for b = 1
2,
˜R( ˆf0) >
1
1 −c1
9
σ2.
This shows that lim infm→∞˜R( ˆf0) ≥
1
1−c1
9 σ2 > σ2. The upper bound follows as above. Since we
assumed that maxi≤km
1
˜λi = O(N(d, km)), and maxi≤km N(d, i) = N(d, km) = Θ(dα), we have
that Pkm
i=1 N(d, i) 1
˜λi = Θ(d2α), so
˜R( ˆf0) ≤(1 −cl)−1  1 −d−1−1 σ2 + Θ

B2  1 −d−1−1
.
So, we conclude that for integer α we have tempered overfitting and for non-integer α we have benign
overfitting. This recovers results of Ghorbani et al. [23], Mei et al. [39], Misiakiewicz [41], Barzilai
and Shamir [5], Zhang et al. [66].
Proof sketch of Corollary 12.
Note that this holds for any scaling of d and m where d = Θ(log m),
but we take this particular one for concreteness. As we show in the appendix (Theorem 21), it is not
hard to see that Lm ≈αlm for some constant αl < 1 and Lm ≈αum for some constant αu > 1.
Theorem 9 implies that we cannot get benign overfitting in this case, i.e. for all m
˜R( ˆf0) >
1
1 −

b
b+1
2
αl
σ2.
This shows that lim infm→∞˜R( ˆf0) ≥
1
1−(
b
b+1)
2αl σ2 > σ2.
Proof sketch of Corollary 13.
We have that for the Gaussian kernel on the sphere, Theorem 7
holds with A = 1 (see Appendix C.4 for further details). First, we will compute km and hence Lm
and Um. After some tedious calculation (Theorem 23 in the appendix), we see that for k ≤2l + l −1
we have N(d, 1) + · · · + N(d, k) = o(m), but for k = 2l + l we have N(d, 1) + · · · + N(d, k) >
m. This shows that km = 2l + l −1 and so again after long calculations Lm = Θ(
m
log m) and
Um > Θ(md0.89). To estimate Pk
i=1 N(d, i)˜λi, note that eigenvalues are decreasing and iN(d, i) =
iN(d, i+1) 2i+d−2
2i+d
i+1
i+d−2 = o(N(d, i+1)) (take k ≪d), so it suffices to estimate N(d, k) 1
˜λk . Now,
from Equation (7) and an estimate on the size of the first eigenvalue (Corollary 31) ˜λ1 = Θ( 1
dα ), for
fixed α > 0, we can estimate the size ˜λk. We have that
1
˜λk
< 1
˜λ1
(τm)k
d
2 + k + 2
τ 2m
k
< dk(1+ε),
for arbitrarily small ε. For k = 7km
24 , from additional calculation (Theorem 23 in the appendix), we
have that m
1
4 ≤N(d, k) ≤m
1
3 and dk < m
7
24 . So Pkm
i=1 N(d, i) 1
˜λ2
i < m. Plugging these back into
Theorem 7 gives the desired result.
10

6
Summary
In this paper, we considered the minimum norm interpolating solution of kernel ridge regression in
fixed dimension with Gaussian kernel and varying or adaptively chosen bandwidth, and in increasing
dimension with various kernels. In fixed dimension, we showed that if the source distribution is
uniform on the sphere, then the minimum norm interpolating solution is inconsistent for any choice
of bandwidth, and usually worse than the null predictor, except possibly in one particular scaling of
bandwidth and with small noise. Furthermore, we showed a general upper and lower bound on the
test risk, which we applied in the case of increasing dimension to recover the currently known results
about polynomially increasing dimension and show two new results: We showed that no dot-product
kernel on the sphere can achieve consistency for logarithmic scaling of the dimension, and obtained
the first case of sub-polynomially scaling dimension where the minimum norm interpolating solution
exhibits benign overfitting, namely with Gaussian kernel and dimension scaling as d = exp(√log m).
Acknowledgments and Disclosure of Funding
We would like to thank Theodor Misiakiewicz and Sam Buchanan for useful discussions.
Work done as part of the NSF-Simons funded collaboration on the Theoretical Foundations of Deep
Learning (https://deepfoundations.ai), and primarily while GV was at TTIC. GV is supported by
research grants from the Center for New Scientists at the Weizmann Institute of Science, and the
Shimon and Golde Picker – Weizmann Annual Grant.
11

References
[1] Francis Bach. High-dimensional analysis of double descent for linear regression with random projections.
arXiv preprint arXiv:2303.01372, 2023.
[2] Peter L Bartlett and Philip M Long. Failures of model-dependent generalization bounds for least-norm
interpolation. The Journal of Machine Learning Research, 22(1):9297–9311, 2021.
[3] Peter L. Bartlett, Philip M. Long, Gábor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020.
[4] Daniel Barzilai and Ohad Shamir. Generalization in kernel regression under realistic assumptions. arXiv
preprint arXiv:2312.15995, 2023.
[5] Daniel Barzilai and Ohad Shamir. Generalization in kernel regression under realistic assumptions. arXiv
preprint arXiv:2312.15995v2, 2024.
[6] Daniel Beaglehole, Mikhail Belkin†, and Parthe Pandit. On the inconsistency of kernel ridgeless regression
in fixed dimensions. arXiv preprint arXiv:2205.13525, 2023.
[7] Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM Journal on
Mathematics of Data Science, 2(4):1167–1180, 2020.
[8] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain
generalization in kernel regression and infinitely wide neural networks. Nature Communications, 12(1):
1–12, 2021.
[9] Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019.
[10] Yuan Cao, Quanquan Gu, and Mikhail Belkin. Risk bounds for over-parameterized maximum margin
classification on sub-gaussian mixtures. In Advances in Neural Information Processing Systems (NeurIPS),
2021.
[11] Yuan Cao, Zixiang Chen, Mikhail Belkin, and Quanquan Gu. Benign overfitting in two-layer convolutional
neural networks. arXiv preprint arXiv:2202.06526, 2022.
[12] Niladri S. Chatterji and Philip M. Long. Finite-sample analysis of interpolating linear classifiers in the
overparameterized regime. Journal of Machine Learning Research, 22(129):1–30, 2021.
[13] Niladri S Chatterji, Philip M Long, and Peter L Bartlett. The interplay between implicit bias and benign
overfitting in two-layer linear networks. arXiv preprint arXiv:2108.11489, 2021.
[14] Chen Cheng and Andrea Montanari. Dimension free ridge regression. arXiv preprint arXiv:2210.08571,
2022.
[15] Tin Sum Cheng, Aurelien Lucchi, Anastasis Kratsios, and David Belius. Characterizing overfitting in
kernel ridgeless regression through the eigenspectrum. arXiv preprint arXiv:2402.01297, 2024.
[16] Geoffrey Chinot and Matthieu Lerasle. On the robustness of the minimum ℓ2 interpolator. arXiv preprint
arXiv:2003.05838, 2020.
[17] Konstantin Donhauser, Nicolo Ruggeri, Stefan Stojanovic, and Fanny Yang. Fast rates for noisy interpo-
lation require rethinking the effect of inductive bias. In International Conference on Machine Learning
(ICML), 2022.
[18] Mona Eberts and Ingo Steinwart. Optimal regression rates for svms using gaussian kernels. Electronic
Journal of Statistics, page Vol. 7 1–42, 2013.
[19] Spencer Frei, Niladri S. Chatterji, and Peter L. Bartlett. Benign overfitting without linearity: Neural
network classifiers trained by gradient descent for noisy linear data. In Conference on Learning Theory
(COLT), 2022.
[20] Spencer Frei, Gal Vardi, Peter L Bartlett, and Nathan Srebro. Benign overfitting in linear classifiers and
leaky relu networks from kkt conditions for margin maximization. arXiv preprint arXiv:2303.01462, 2023.
[21] Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On the
similarity between the laplace and neural tangent kernels. arXiv preprint arXiv:2007.01580, 2020.
12

[22] Erin George, Michael Murray, William Swartworth, and Deanna Needell. Training shallow relu networks
on noisy data using hinge loss: when do we overfit and is it benign? Advances in Neural Information
Processing Systems, 36, 2024.
[23] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural
networks in high dimension. The Annals of Statistics, 49(2):1029–1054, 2021.
[24] Nikhil Ghosh and Mikhail Belkin. A universal trade-off between the model size, test loss, and training loss
of linear predictors. arXiv preprint arXiv:2207.11621, 2022.
[25] Moritz Haas, David Holzmüller, Ulrike von Luxburg, and Ingo Steinwart. Mind the spikes: Benign
overfitting of kernels and neural networks in fixed dimension. 37th Conference on Neural Information
Processing Systems (NeurIPS 2023), 2023.
[26] Moritz Haas, David Holzmüller, Ulrike Luxburg, and Ingo Steinwart. Mind the spikes: Benign overfitting
of kernels and neural networks in fixed dimension. Advances in Neural Information Processing Systems,
36, 2024.
[27] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional
ridgeless least squares interpolation. Annals of Statistics, 2019.
[28] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-dimensional
ridgeless least squares interpolation. Preprint, arXiv:1903.08560, 2020.
[29] Hong Hu, Yue M. Lu, and Theodor Misiakiewicz. Asymptotics of random feature regression beyond the
linear scaling regime. arXiv preprint arXiv:2403.08160v1, 2023.
[30] Nirmit Joshi, Gal Vardi, and Nathan Srebro. Noisy interpolation learning with shallow univariate relu
networks. arXiv preprint arXiv:2307.15396, 2023.
[31] Peizhong Ju, Xiaojun Lin, and Jia Liu. Overfitting can be harmless for basis pursuit, but only to a degree.
Advances in Neural Information Processing Systems, 33:7956–7967, 2020.
[32] Kedar Karhadkar, Erin George, Michael Murray, Guido Montúfar, and Deanna Needell. Benign overfitting
in leaky relu networks with moderate input dimension. arXiv preprint arXiv:2403.06903, 2024.
[33] Frederic Koehler, Lijia Zhou, Danica J Sutherland, and Nathan Srebro. Uniform convergence of in-
terpolators: Gaussian width, norm bounds, and benign overfitting. arXiv preprint arXiv:2106.09276,
2021.
[34] Guy Kornowski, Gilad Yehudai, and Ohad Shamir. From tempered to benign overfitting in relu neural
networks. Advances in Neural Information Processing Systems, 36, 2024.
[35] Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting for two-layer relu
networks. arXiv preprint arXiv:2303.04145, 2023.
[36] Yicheng Li, Zixiong Yu, Guhan Chen, and Qian Lin. Statistical optimality of deep wide neural networks.
arXiv preprint arXiv:2305.02657, 2023.
[37] Tengyuan Liang and Benjamin Recht. Interpolating classifiers make few mistakes. arXiv preprint
arXiv:2101.11815, 2021.
[38] Neil Mallinar, James B. Simon, Amirhesam Abedsoltan, Parthe Pandit, Mikhail Belkin, and Preetum Nakki-
ran. Benign, tempered, or catastrophic: A taxonomy of overfitting. arXiv preprint arXiv:2207.06569v2,
2022.
[39] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization error of random feature and
kernel methods: hypercontractivity and kernel matrix concentration. Applied and Computational Harmonic
Analysis, 59:3–84, 2022.
[40] Xuran Meng, Difan Zou, and Yuan Cao. Benign overfitting in two-layer relu convolutional neural networks
for xor data. arXiv preprint arXiv:2310.01975, 2023.
[41] Theodor Misiakiewicz. Spectrum of inner-product kernel matrices in the polynomial regime and multiple
descent phenomenon in kernel ridge regression. arXiv:2204.10425, 2022.
[42] Theodor Misiakiewicz and Basil Saeed. A non-asymptotic theory of kernel ridge regression: deterministic
equivalents, test error, and gcv estimator. arXiv preprint arXiv:2403.08938, 2024.
13

[43] Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-margin linear
classifiers: High-dimensional asymptotics in the overparametrized regime. Preprint, arXiv:1911.01544,
2020.
[44] Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-margin
linear classifiers: Benign overfitting and high dimensional asymptotics in the overparametrized regime.
Preprint arXiv:1911.01544v3, 2023.
[45] Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpolation of
noisy data in regression. IEEE Journal on Selected Areas in Information Theory, 2020.
[46] Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and Anant
Sahai. Classification vs regression in overparameterized regimes: Does the loss function matter? Journal
of Machine Learning Research, 22(222):1–69, 2021.
[47] Jeffrey Negrea, Gintare Karolina Dziugaite, and Daniel Roy. In defense of uniform convergence: General-
ization via derandomization with an application to interpolating predictors. In International Conference on
Machine Learning, pages 7263–7272, 2020.
[48] Minh Ha Quang and Yuan Yao. Mercer’s theorem, feature maps, and smoothing. Conference: Learning
Theory, 19th Annual Conference on Learning Theory, COLT, 2006.
[49] Alexander Rakhlin and Xiyu Zhai. Consistency of interpolation with laplace kernels is a high-dimensional
phenomenon. arXiv preprint arXiv:1812.11167, 2018.
[50] Javier Segura. Simple bounds with best possible accuracy for ratios of modified bessel functions. arXiv
preprint arXiv:2207.02713v3, 2023.
[51] Ohad Shamir. The implicit bias of benign overfitting. In Conference on Learning Theory, pages 448–478.
PMLR, 2022.
[52] James B. Simon, Madeline Dickens, Dhruva Karkada, and Michael R. DeWeese. The eigenlearning
framework: A conservation law perspective on kernel regression and wide neural networks. arXiv preprint
arXiv:2110.03922, 2021.
[53] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media,
2008.
[54] Christos Thrampoulidis, Samet Oymak, and Mahdi Soltanolkotabi. Theoretical insights into multiclass
classification: A high-dimensional asymptotic view. Advances in Neural Information Processing Systems,
33:8907–8920, 2020.
[55] A. Tsigler and P. L. Bartlett. Benign overfitting in ridge regression. Preprint, arXiv:2009.14286, 2020.
[56] Guillaume Wang, Konstantin Donhauser, and Fanny Yang. Tight bounds for minimum l1-norm interpolation
of noisy data. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.
[57] Ke Wang and Christos Thrampoulidis. Binary classification of gaussian mixtures: Abundance of support
vectors, benign overfitting and regularization. Preprint, arXiv:2011.09148, 2021.
[58] Ke Wang, Vidya Muthukumar, and Christos Thrampoulidis. Benign overfitting in multiclass classification:
All roads lead to interpolation. In Advances in Neural Information Processing Systems (NeurIPS), 2021.
[59] Alexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how real-
world neural representations generalize. In International Conference on Machine Learning, Proceedings of
Machine Learning Research, 2022.
[60] Denny Wu and Ji Xu. On the optimal weighted ℓ2 regularization in overparameterized linear regression.
Advances in Neural Information Processing Systems, 33:10112–10123, 2020.
[61] Lechao Xiao, Hong Hu, Theodor Misiakiewicz, Yue M. Lu, and Jeffrey Pennington. Precise learning curves
and higher-order scaling limits for dot product kernel regression. arXiv preprint: arXiv:2205.14846v3,
2023.
[62] Xingyu Xu and Yuantao Gu. Benign overfitting of non-smooth neural networks beyond lazy training. In
International Conference on Artificial Intelligence and Statistics, pages 11094–11117. PMLR, 2023.
[63] Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, and Wei Hu. Benign overfitting and grokking in relu
networks for xor cluster data. arXiv preprint arXiv:2310.02541, 2023.
14

[64] Zhen-Hang Yang and Yu-Ming Chu. On approximating the modified bessel function of the first kind and
toader-qi mean. Journal of Inequalities and Applications, 2016.
[65] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. In International Conference on Learning Representations,
2017.
[66] Haobo Zhang, Weihao Lu, and Qian Lin. The phase diagram of kernel interpolation in large dimensions.
arXiv preprint arXiv:2404.12597v1, 2024.
[67] Lijia Zhou, Frederic Koehler, Pragya Sur, Danica J Sutherland, and Nathan Srebro. A non-asymptotic
moreau envelope theory for high-dimensional generalized linear models. arXiv preprint arXiv:2210.12082,
2022.
[68] Lijia Zhou, James B. Simon, Gal Vardi, and Nathan Srebro. An agnostic view on the cost of overfitting in
(kernel) ridge regression. arXiv preprint arXiv:2306.13185, 2023.
[69] Lijia Zhou, James B Simon, Gal Vardi, and Nathan Srebro. An agnostic view on the cost of overfitting in
(kernel) ridge regression. arXiv preprint arXiv:2306.13185, 2023.
15

A
Empirical Justification
In Figure 1, we provide empirical jusification for the fixed dimension case considered in Theorem 3.
(a) τm = o(m−
1
d−1 ) with σ2 = 1 and d = 6. We see that the test error tends to something that is equal to or
larger than the test risk of the null predictor.
(b) τm = ω(m−
1
d−1 ) with σ2 = 10 and d = 4. We see that the test error is increasing with the sample size m,
suggesting that the test error diverges to infinity.
(c) τm = Θ(m−
1
d−1 ) with σ2 = 10000 and d = 6. We see that the test error for a large noise level, i.e. large
σ2, is well above the risk of the null predictor.
Figure 1: Using the setup of Theorem 3 in the paper, we plot the dependence of the test error on
the sample size for the Gaussian kernel ridgeless predictor. We consider y = f ∗(x) + ξ where
ξ ∼N(0, σ2), f ∗= 10, σ2 is the noise level, and x ∼Unif(Sd−1). We compare the test error with
the noise level (the Bayes risk) and the risk of the null predictor. We see that for all three cases, our
predictions agree with the experiments.
16

B
Fixed dimension
Cost of overfitting
The cost of overfitting, introduced in [68], is defined as the ratio of the test risk
of an interpolating solution and the test risk of the optimally regularized solution. It measures how
worse off are we interpolating rather than regularizing.
Definition 15 (Cost of overfitting). Let ˆfδ∗be the optimally regularized solution, i.e.
R( ˆfδ∗) = inf
δ≥0 R( ˆfδ).
For a distribution D over X × R, we will define the cost of overfitting C(D, m) as
C(D, m) = R( ˆf0)
R( ˆfδ∗)
.
Zhou et al. [68] show that agnostically to the distribution D, the behavior of the cost of overfitting is
tightly controlled by the effective ranks of systems of eigenvalue {λi}∞
i=1 of a given kernel (coming
from Mercer’s theorem)
rk :=
P∞
i=k+1 λi
λk+1
and Rk :=
 P∞
i=k+1 λi
2
P∞
i=k+1 λ2
i
.
We will assume additionally that the eigenvalues are nonzero, so rk and Rk are well defined on N.
Since we chose that the eigenvalues are sorted and since they square summable, we have that
rk ≤Rk ≤r2
k.
The following two results of [68] summarize how we will use effective ranks to control the cost of
overfitting. Proposition 16 gives a necessary and sufficient condition for the overfitting to benign in
terms of the cost of overfitting, i.e. that C(D, m) →1, in terms of effective ranks.
Proposition 16 (Necessary and sufficient condition for benign overfitting [68]). Let kn be the
smallest integer k ∈N ∪{0} for which n < k + rk. Then E0 →1 if and only if
lim
n→∞
kn
n = 0 and
lim
n→∞
n
Rkn
= 0.
Proposition 17 gives a sufficient condition for ˜R( ˆf0) →∞in terms of effective ranks.
Proposition 17 (Sufficient condition for catastrophic overfitting [68]). Let ε > 0 and let k = (1+ε)m.
If limk→∞
rk
k = 0 then E0 →∞, i.e. the overfitting is catastrophic.
Using sharp bounds on eigenvalues {λi}∞
i=1, derived in Theorem 28, we can derive bounds on
effective ranks so that we can apply the results of [68] and Equation (2) to bound the test risk of
minimum norm interpolant.
When the input is distributed uniformly on the sphere, the eigenfunctions {ϕk} in the Mercer
decomposition of the kernel function can be taken to be the spherical harmonics Yks. Using the Funk-
Hecke formula we can find the closed form of eigenvalues for the Gaussian kernel [48], which are
given in terms of bandwidth τm and modified Bessel function of the first kind Iv(x). In Appendix D,
we will derive bounds on the sizes of the eigenvalues using the properties of Bessel functions and
their multiplicities.
We need to take a particular input distribution, in order to able to compute the eigenvalues explicitly
and to know their multiplicities. The result of Cao et al. [9] offers one possible way of generalizing
our result to more general different distributions
We split the proof into two parts. First, we will bound the cost of overfitting. Then, we will provide
lower bounds on the test risk of Gaussian KRR.
17

Bounds on cost of overfitting
The bounds on cost of overfitting will be used to show whether the
test risk is bounded above or away from Bayes risk.
Lemma 18 (Cost of overfitting for Gaussian KRR). Let X ∼Unif(Sd−1). Then the following
bounds hold for the minimum norm interpolating solution of KRR and its cost of overfitting C(D, m)
and E0:
1. If τm ≤m−
1
d−1 t(m), where t(m) →0 as m →∞then there is a constant CI that depends
on d and C, such that
1 ≤C(D, m) ≤E0 ≤

1 −1
m
−2 
1 −CIt(m)
d−1
2
−1
.
2. If τm ≥m−
1
d−1 t(m), where t(m) →∞as m →∞then there is a integer m0 depending
on d and t(m) such that for all m ≥m0
E0 ≥√m.
3. If C1m−
1
d−1 ≤τm ≤C2m−
1
d−1 then there exist constants CI > 0 and CII depending on
C1, C2 and d such that
C(D, m) ≤E0 ≤CI
E0 ≥1 + CII.
Proof. We will break down the proof into the proof of the three parts.
Part 1:
We will use Proposition 16 to prove this that E0 →1. We can see the upper bound on E0
from Theorem 2 [68]. We have that for k < m
E0 ≤

1 −k
m
−2 
1 −m
Rk
−1
.
Now we will show that for k = 1, we have that k + rk > m and limm→∞m
R1 = 0. This implies that
conditions of Proposition 16 hold so E0 →1. We will rewrite rk =
P
i>k λk
λk+1
using ˜λi. We have that
r1 =
P
i>1 λi
λ2
= ˜N(d, ˜1) +
∞
X
j=1
N(d, ˜1 + j)
˜λ˜1+j
˜λi1
,
where N(d, 1) = d, so ˜1 = 1 and ˜N(d, ˜1) = d −1. Therefore, we have
r1 = (d −1) +
∞
X
j=1
N(d, 1 + j)
˜λ1+j
˜λ1
.
Let T =
2
τ 2
m . Taking
√
T g(m)
2
≤j ≤
√
Tg(m), k = 1, we can apply the bound Theorem 28 on
˜λ1+j
˜λ1 .
Therefore, we have that
exp(−5g(m)) <
˜λ1+j
˜λ1
< 1.
18

In particular, we have that for m large enough, for all such j it holds that
˜λ1+j
˜λ1
> 1
2. Going back to
r1, this means that
r1 = (d −1) +
∞
X
j=1
N(d, 1 + j)
˜λ1+j
˜λ1
>
√
T g(m)
X
j= 1
2
√
T g(m)
N(d, 1 + j)
˜λ1+j
˜λ1
>
√
T g(m)
X
j= 1
2
√
T g(m)
(1 + j)d−2 1
2
>
√
T g(m)
X
j= 1
2
√
T g(m)
1
2d+1 (
√
Tg(m))d−2 > 1
2
√
Tg(m)
1
2d+1 (
√
Tg(m))d−2
> 1
2(
√
Tg(m))d−1.
If
√
Tg(m) is not an integer, we can take its integer part and add a small constant since
√
Tg(m)
will be large. Note that τm ≤m−
1
d−1 t(m), so
T ≥m
2
d−1
1
t(m)2 .
Therefore
r1 > 1
2(
√
Tg(m))d−1 > 1
2

m
1
d−1 g(m)
t(m)
d−1
= m1
g(m)
t(m)
d−1
,
so as long as g(m)
t(m) →∞, r1 > m for m large enough. We can take g(m) =
p
t(m). Then, in
Proposition 16, we can select km = 1, since 1 + r1 > m for large enough m. Note also that R1 ≥r1,
so we also have that R1 ≥r1 > m1 
g(m)
t(m)
d−1
, so
lim
m→∞
1
m = 0 and
lim
m→∞
m
R1
= 0.
This finishes the proof of the first claim.
Part 2:
Let τm ≥m−
1
d−1 t(m), where t(m) →∞as m →∞. We will use Proposition 17 to prove
this claim. Using Theorem 5 from [68] we know that for any ε > 0 if rk = o(m) for k = (1 + ε)m
then the following bound holds
E0 ≥
1
1 −m
k

k−m
k−m+rk
 > 1 + 1
ε.
We will show that there is a constant U > 0 such that for any k ≥m, we have that
rk ≤Um

1
t(m)
 d−1
2
.
By Theorem 28, we have that for j ≥
√
Tg(m) where g(m) →∞as m →∞that
˜λ˜k+j
˜k
< exp

−g(m)2
4

.
Let k = (1 + ε)m. We have that
rk = ˜N(d, ˜k) +
∞
X
j=1
N(d, ˜k + j)
˜λ˜k+j
˜λ˜k
.
19

Now, if we take j ≥j0 =
√
Tg(m), it holds that
˜λ˜k+j
˜λ˜k
< exp(−g(m)2
4
).
Therefore, we have that
rk = ˜N(d, ˜k) +
∞
X
j=1
N(d, ˜k + j)
˜λ˜k+j
˜λ˜k
= ˜N(d, ˜k) +
j−1
X
j=1
N(d, ˜k + j)
˜λ˜k+j
˜λ˜k
+
∞
X
j=j
N(d, ˜k + j)
˜λ˜k+j
˜λ˜k
≤nujd−1 +
∞
X
j=j
N(d, ˜k + j)
˜λ˜k+j
˜λ˜k
.
Note that by Theorem 28, for j ≥l
√
Tg(m) we have that
˜λ˜k+j
˜k
< exp

−l2g(m)2
4

.
Note also that this bounds holds for all l simultaneously. Therefore, we can write
rk ≤nujd−1 +
∞
X
j=j
N(d, ˜k + j)
˜λ˜k+j
˜λ˜k
= nujd−1 +
∞
X
l=1
j
X
s=1
N(d, lj + s)
˜λ˜k+lj+s
˜λ˜k
≤nujd−1 +
∞
X
l=1
j
X
s=1
N(d, (l + 1)j)
˜λ˜k+lj
˜λ˜k
≤nujd−1 +
∞
X
l=1
(l + 1)d−1jd−1 exp(−l2g(m)2
4
)
≤m
g(m)
t(m)
d−1  
nu +
∞
X
l=1
(l + 1)d−1 exp(−l2
4 )
!
≤Um
g(m)
t(m)
d−1
= o(m).
Second to last inequality is true because g(m) →∞as m →∞. The last inequality follows from
the fact that P∞
l=1(l + 1)d−1 exp(−l2
4 ) is bounded by a constant. Note that the bound is independent
of k, so we can vary ε with m as well. If we choose ε =
1
√m, the desired result follows.
Part 3:
Let C1m−
1
d−1 ≤τm ≤C2m−
1
d−1 . Note that the following two bounds hold for E0 [68]:
1. For k < m such that rk + k > m
E0 ≤

1 −k
m
−1 
1 −
m
k + rk
−1
.
2. For k ≥m it holds that
1
m
X
i
Li,δ ≥m
k

k −m
k −m + rk

.
20

Note that in this case c′
1m
1
d−1 ≤
√
T =
q
2
τ 2
m ≤c′
2m
1
d−1 , where c′
2 =
q
2
C2
1 (and similarly for
indices 1 and 2 swapped with inequalities reversed).
Theorem 28 shows that for i is such that i ≤(k
√
T), we have that for ˜λi it holds that
˜λi
˜λ1
≥
 
1
1 +
k
√
T
!k
√
T
≥exp(−k2).
Take k = (1 + ε)m. We want to show that there is a constant B > 0 that depends on the dimension
d and the constant C such that
rk ≤Bm.
Let ˜k = ((1 + ε)m)
1
d−1 . Note that by an analogous proof to Theorem 28, we have that for the size of
eigenvalues it holds
˜λ˜k+j
˜λ˜k
≤
 
T
T + (˜k + j −1 + d
2 −1
2)
! j−1
2
.
For l˜k ≤j ≤(l + 1)˜k, we have that
˜λ˜k+j
˜λ˜k
≤
 
T
T + (˜k + j −1 + d
2 −1
2)
! j−1
2
<

T
T + ((l + 1)˜k)
 l˜k−1
2
<

T
T + ((l + 1)˜k)
 (l+1)˜k
4
.
We have that ˜k = 1
c′ (1+ε)
1
d−1 c′m
1
d−1 = 1
c′ (1+ε)
1
d−1 √
T = α
√
T for α = 1
c′ (1+ε)
1
d−1 . Therefore,
we have that
˜λ˜k+j
˜λ˜k
<

T
T + ((l + 1)˜k)
 (l+1)˜k
4
=

T
T + ((l + 1)α
√
T)
 (l+1)α
√
T
4
→exp

−(l + 1)2α2
4

,
as m →∞i.e.
√
T →∞. Therefore, for m large enough, for all l we have that if l˜k ≤j ≤(l + 1)˜k
then
˜λ˜k+j
˜λ˜k
≤
1
(l + 1)d+2 .
The last inequality is true because we can take m large enough so that α
√
T and
√
T
α are both greater
than 400d. Then we have that for all l
˜λ˜k+j
˜λ˜k
<

T
T + ((l + 1)α
√
T)
 (l+1)α
√
T
4
<
 
1
1 + l+1
100d
!(l+1)100d
<
1
(l + 1)d+2 .
The last inequality holds because of the following

1 + l + 1
100d
(l+1)100d
>
l + 1
100d
d+2 (l + 1)100d
d + 2

> (l + 1)d+2
1
(100d)d+2 (100d)d+2 > (l + 1)d+2.
Therefore, we can bound rk as follows
rk =
P
i>k λk
λk
= ˜N(d, ˜k) +
∞
X
j=1
N(d, ˜k + j)
˜λ˜k+j
˜λ˜k
< N(d, ˜k) +
∞
X
j=1
N(d, ˜k + j)
˜λ˜k+j
˜λ˜k
< (˜k)d−2 +
∞
X
l=1
((l + 1)˜k)d−2(˜k)
1
(l + 1)d+2
< ˜kd−1
 ∞
X
l=1
1
(l + 1)4
!
< ˜kd−1C = C(1 + ε)m.
21

Therefore, applying claim 1, we have that k −m + rk < (C(1 + ε) + ε)m, so we have that
k−m
k−m+rk >
ε
(C(1+ε)+ε), so it follows that E0 > (1 −
(1+ε)ε
(C(1+ε)+ε))−1 > 1.
Note that the above proof actually shows that for any k1 ≤k, we have that rk1 ≤Cm. Now we want
to show that rk is lower bounded and that we can take k < m such that k + rk > m. Again apply
Theorem 28, we can generalize
˜λi
˜λ1
≥
 
1
1 +
k
√
T
!k
√
T
≥exp(−k2).
to other ratios as well. Let j ≤k
√
T and let k
√
T ≤i ≤(k + 1)
√
T. Then we have that
˜λi
˜λj
≥
 
1
1 + 2k
√
T
!(k+1)
√
T
≥exp(−2k(k + 1)).
Note that c′
1m
1
d−1 ≤
√
T =
q
2
τ 2
m ≤c′
2m
1
d−1 , where c′
2 =
q
2
C2
1 (and similarly for indices 1 and 2
swapped with inequalities reversed). Therefore, we have that the following bound holds on rk for any
k < m:
rk =
P
i>k λi
λk+1
= ˜N(d,
˜
(k + 1)) +
∞
X
j=1
N(d,
˜
(k + 1) + j)
˜λ
˜
(k+1)+j
˜λ
˜
(k+1)
.
Note that since
√
T ≥c′
1m
1
d−1 then N√
T ≥nl(c′
1)d−1m, so ˜m ≤
1
c′
1
1
n
1
d−1
l
. Therefore, if we take
j = ˜k, l =
1
c′
1
1
n
1
d−1
l
, and any l
√
T ≤i ≤(l + 1)
√
T, we have that (since exp(−2l(l + 1)) <
exp(−l2))
˜λi
˜λj
≥exp(−2l(l + 1)).
Note that there is at least N(l+1)
√
T −Nl
√
T ≥nl
 (l + 1)d−1 −ld−1
m such indices i, we have
that
rk =
P
i>k λi
λk+1
= ˜N(d,
˜
(k + 1)) +
∞
X
j=1
N(d,
˜
(k + 1) + j)
˜λ
˜
(k+1)+j
˜λ
˜
(k+1)
.
>
X
l
√
T ≤i≤(l+1)
√
T
N(d, i)
˜λi
˜λk
> nl
 (l + 1)d−1 −ld−1
m exp(−2l(l + 1)) > βm,
where β = nl
 (l + 1)d−1 −ld−1
exp(−2l(l + 1)). Note that β is independent of k, so in particular
for k = (1 −β/2)m, we will have rk + k > m. Therefore we can use the upper bound on E0 to see
that
E0 ≤22
β2 (1 + β/2).
Since β only depends on d and C1 and C2, this finishes the proof.
In particular, Lemma 18 shows that when τm
=
o(m−
1
d−1 ) or τm
=
ω(m−
1
d−1 ), then
lim sup ˜R( ˆf0) ≤∞.
22

Lower bound for the test risk of Gaussian KRR
Now we will show lower bounds for the test risk
of Gaussian KRR.
Lemma 19 (Lower bound for the test risk). Under Assumption 2, the following bounds hold for the
risk ˜R( ˆf0) of the minimum norm interpolating solution of Gaussian KRR:
1. If τm = o(m−
1
d−1 ), then ˜
R(0) ≤lim inf ˜
R( ˆ
f0) ≤lim supm→∞˜
R( ˆ
f0) < ∞. More
precisely, if τm ≤m−
1
d−1 t(m), where t(m) →0 as m →∞, then there is a scalar cd that
depends only on the dimension so that for all such t(m) there is m0 that depends on t(m)
such that for all m > m0 we have ˜R( ˆf0) > σ2 + (1 −cdt(m)
d−1
2 )∥f ∗∥2.
2. If τm = ω(m−
1
d−1 ), then limm→∞˜
R( ˆ
f0) = ∞. More precisely, if τm ≥m−
1
d−1 t(m),
where t(m) →∞as m →∞then there is an integer m0 that depends on the dimension
and t(m) such that for m > m0, we have that
tR( ˆf0) →√m(σ2) (note that ˜R(0) is bounded). Hence, for large enough m we have
˜R( ˆf0) > ˜R(0).
3. If τm = Θ(m−
1
d−1 ), then lim supm→∞R( ˆ
f0) < ∞.
Moreover, suppose that
C1m−
1
d−1 ≤τm ≤C2m−
1
d−1 for some constants C1 and C2, then there exist η, µ > 0 that
depend only on d, C1, and C2, such that for all m we have ˜
R( ˆ
f0) > µ∥f ∗∥2+(1+η)σ2.
Consequently, ˜
R( ˆ
f0) > ˜
R(0) as long as σ2 > 1−µ
η ∥f ∗∥2.
Proof. All three proofs will similarly follow by directly analyzing Equation (2).
Part 1:
Note that for any i, we have that ˜λi + κ0 ≤˜λ1 + κ0, so we have that Li ≥
˜λi
˜λ1 L1. From
Theorem 28 we know that for j ≤
√
Tt(m)
1
2 , we have that
˜λj
˜λ1
≥exp(−t(m)),
so there is m0 that depends on t(m) such that for all m > m0
˜λj ≥1
2
˜λ1,
for all j ≤
√
Tt(m)
1
2 . Therefore we have that
m =
X
i
N(d, i)Li >

N(d, 1) + . . . N(d,
√
Tt(m))
 1
2L1.
Note that

N(d, 1) + . . . N(d,
√
Tt(m))

> cl
√
Tt(m)
1
2
d−1
= c′
lm(t(m))−d−1
2 , where c′
l =
√
2cl. Therefore, we have that L1 < 2
c′
l (t(m))
d−1
2 . Therefore, we have that
(1 −Li)2 ≥(1 −L1)2 >

1 −2
c′
l
(t(m))
d−1
2

.
From Equation (2), we have that then
˜R( ˆf0) ≥E0σ2 + E0(1 −cdt(m)
d−1
2 )∥β∥2.
This shows the first claim.
For the second part, note the following property of Li,δ =
λi
λi+κδ : if for j > i, we have that
Li,δ > Lj,δ. Let λj
λi > c for some fixed c > 0 and j > i, then we have that
Lj,δ > cLi,δ.
23

By Theorem 28, we have that for j ≤
√
Tg(m) with g(m) →0 as m →∞
˜λ˜k+j
˜λ˜k
> exp (−5g(m)) .
So there is m > m0 such that for m > m0
˜λ˜k+j
˜λ˜k
> exp (−5g(m)) > 1
2.
Note that
m
m −P∞
i=1 L2
i,δ
(1 −Lj,δ)2 > 1 if
∞
X
i=1
L2
i,δ > mLj,δ (2 −Lj,δ)
For this it suffices to have
∞
X
i=1
L2
i,δ > mL1,δ (2 −L1,δ) ,
since Li,δ are decreasing.
Note that there is at least N(d, 1) + · · · + N(d,
√
Tg(m)) >
nl(
√
Tg(m))d−1 > cum

g(m)
t(m)
d−1
of indices i such that
λi
λ1
=
˜λ1+j
˜λ1
> 1
2.
Therefore, if we select g(m) =
p
t(m), then we have that
∞
X
i=1
L2
i,δ > 1
2cum

1
t(m)
 d−1
2
> mL1,δ (2 −L1,δ) .
This implies that for all j simultaneously
m
m −P∞
i=1 L2
i,δ
(1 −Lj,δ)2 > 1.
This translates to
˜R( ˆf0) > ˜R(0) +
 
m
m −P∞
i=1 L2
i,δ
−1
!
σ2.
This finishes the proof of the first part.
Part 2:
Note that we showed that for m large enough, E0 > √m. This shows that
˜R( ˆf0) = E0
 X
i
(1 −L0)2v2
i + σ2
!
> √mσ2.
So, there is m0 that depends only on t(m) such that for m > m0 we have that ˜R( ˆf0) > √mσ2.
Part 3:
Note first that Li ≥Lj for i ≤j. Let T =
2
τ 2
m . From Lemma 26 it follows that
nl(α
√
T)d−1 ≤N(d, 1) + · · · + N(d, α
√
T) ≤nu(α
√
T)d−1.
24

Take α such that N(d, 1) + · · · + N(d, α
√
T) > 2m. Then, we have that
m =
X
i
N(d, i)Li > (N(d, 1) + · · · + N(d, α
√
T))Lα
√
T > 2mLα
√
T
Lα
√
T ≤1
2 =⇒λα
√
T ≤κ0.
Note that (1 −L1)2 < (1 −Li)2 and
(1 −L1)2 =
κ2
0
(λ1 + κ0)2 .
If κ0 > λ1, then 2κ0 > λ1 + κ0, so
1
2κ0 <
1
λ1+κ0 . This shows that
κ2
0
(λ1+κ0)2 > 1
4. Otherwise
κ0 ≤λ1, so κ0 + λ1 ≤2λ1. This implies that
κ2
0
(κ0 + λ1)2 > κ2
0
4λ2
1
> λα
√
T
4λ2
1
.
From Theorem 28 it follows that
λα
√
T
4λ2
1
> exp(−α2). Since

1
nu
2
C2
1

1
d−1
< α <

1
nl
2
C2
2

1
d−1
,
we know that exp
 −α2
> exp
 
−

1
nl
2
C2
2

2
d−1 !
, so µ = exp
 
−

1
nl
2
C2
2

2
d−1 !
. Combining
Lemma 18 and Lemma 19, we get Theorem 3.
C
Increasing dimension
In Appendix C.1 we will prove Theorem 7 and Theorem 9. In Appendix C.2, we will prove related
claims about the eigenvalue multiplicities for relevant scalings of the dimension Corollary 12 and
Corollary 13. In Appendix C.3, we will give a detailed proof of Corollary 13. In Appendix C.4, we
will discuss when the conditions posed on kernels are satisfied.
C.1
Proofs of Theorem 7 and Theorem 9
Theorem 20 (Error scaling for any kernel ridge regression). Let d and m be any dimension and
sample size. Define Lm, Um, km, N(i), Nl, and ˜λk as in Definition 5. Consider KRR with a kernel
K satisfying Assumption 6 for some A. Assume that for some integer l, the target function f ∗
satisfies Assumption 4 with at most Nl nonzero coefficients. Then, the risk of the minimum norm
interpolating solution is bounded by the following:
˜R( ˆf0) ≤

1 −Lm
m
−1 
1 −m
Um
−1
σ2
(8)
+ B2

1 −Lm
m
−1 
1 −m
Um
−1 A2
m2
 
l
X
i=1
N(i) 1
˜λ2
i
!
.
(9)
Additionally,
we
can
get
an
alternative
bound
if
we
swap
Pl
i=1 N(d, i) 1
˜λ2
i

with
Pl
i=1 N(d, i) 1
˜λi

, i.e.
˜R( ˆf0) ≤

1 −Lm
m
−1 
1 −m
Lm
−1
σ2
+B2

1 −Lm
m
−1 
1 −m
Lm
−1 A
m2
 Lm
X
i=1
N(d, i) 1
˜λi
!
.
Proofs of Theorem 7 and Theorem 20:
Note first that Equation (2) implies that
˜R( ˆf0) =
X
i
E0 (1 −Li,0)2 β2
i + E0σ2.
25

Let Li =
˜λi
˜λi+κ0 . Then, we can rewrite it as
˜R( ˆf0) =
X
i
E0N(d, i) (1 −Li)2 β2
i + E0σ2,
with a slight of abuse of notation for βi. From Equation (5), we have that for k < m
E0 ≤

1 −k
m
−1 
1 −
m
k + rk
−1
.
Take k = Lm. Note that by Definition 5, Lm < m Then λk+1 = ˜λkm+1 and it repeats N(d, km + 1)
times. Therefore, we have that
rk = N(d, km + 1) +
∞
X
i=1
N(d, km + i + 1)
˜λkm+i+1
˜λkm+1
.
Therefore, rk + k > N(d, km + 1) + Lm = Lm > m. Therefore, we can apply Equation (5). We
get that
E0 ≤

1 −k
m
−1 
1 −
m
k + rk
−1
=

1 −Lm
m
−1 
1 −m
Lm
−1
.
Note now that
m =
X
i
N(d, i)Li.
Note that ˜λi + κ0 > κ0 so we have that Li <
˜λi
κ0 . Therefore, we have that
m =
X
i
N(d, i)Li < 1
κ0
 X
i
N(d, i)˜λi
!
< A
κ0
.
Therefore, we have that κ0 < A
m. Since (1 −Li)2 =
κ2
0
(κ0+˜λ)2 < κ2
0
˜λ2 . Since there are only ld nonzero
βi, we have that
X
i
N(d, i)(1 −Li)2β2
i ≤B2 A2
m2
 
l
X
i=1
N(d, i) 1
˜λ2
i
!
.
Going back to Equation (2), we have
˜R( ˆf0) = E0
X
i
(1 −Li)2N(d, i)β2
i + E0σ2
≤E0B2 A2
m2
 
l
X
i=1
N(d, i) 1
˜λ2
i
!
+ E0σ2
≤

1 −Lm
m
−1 
1 −m
Lm
−1
σ2
+

1 −Lm
m
−1 
1 −m
Lm
−1
B2 A2
m2
 
l
X
i=1
N(d, i) 1
˜λ2
i
!
This shows the first bound. For the second bound, note that
∞
X
i=1
κ2
0
(κ0 + λi)2 β2
i ≤
∞
X
i=l+1
β2
i +
l
X
i=1
β2
i
1
P
i λi
1
λi
P
j>l λj
m
2
.
So if we take β to have only l nonzero terms, we get
l
X
i=1
κ2
0
(κ0 + λi)2 β2
i ≤
l
X
i=1
β2
i
1
P
i λi
1
λi
P
j>l λj
m
2
.
26

From this, we have that
l
X
i=1
κ2
0
(κ0 + λi)2 β2
i ≤A
λi
1
m2 .
Therefore, we have that
N(d, i)
κ2
0
(κ0 + ˜λi)2 β2
i ≤N(d, i)β2
i
A
λi
1
m2 .
Therefore, we have the improved inequality from
l
X
i=1
N(d, i)(1 −Li)2β2
i ≤
l
X
i=1
β2
i N(d, i) A
λi
1
m2 .
This gives the second bound on the test risk, as we can bound
l
X
i=1
N(d, i)(1 −Li)2β2
i ≤B2A
m2
 
l
X
i=1
N(d, i) 1
λi
!
.
[Lower bound for test risk in increasing dimension] Let km and Lm be as in Definition 5. Consider
learning a target function f ∗, with some sample size m. Let K be a kernel satisfying Assumption 8 for
some A and b. Consider the minimum norm interpolating solution of KRR (with any data distribution)
with kernel K. Then, for the risk of minimum norm interpolating solution, the following lower bound
holds:
˜R( ˆf0) >
 
1 −

b
b + 1
2 Lm
m
!−1
σ2.
Proof of Theorem 9:
By Theorem 3 from Zhou et al. [68], if k is the first k < m such that
k + brk ≥m, then
E0 ≥
 
1 −

b
b + 1
2 k
m
!−1
.
Therefore, it suffices to show that the first such k is actually Lm = N(d, 1) + · · · + N(d, km). Note
that
rk =
P
i>k λi
λk+1
< max
i≤k
 1
λi

(
X
i>k
λi) < A max
i≤k
 1
λi

.
Since maxi≤km

1
˜λi

<
m−Lm
b
, for l < N(d, 1) + · · · + N(d, km), we have that brl + l ≤
N(d, 1) + · · · + N(d, km) −1 + b maxi≤km

1
˜λi

< Lm + m −Lm ≤m, so the first l for
which rl + l > m is l = Lm = N(d, 1) + · · · + N(d, km). Plugging in k = Lm we get that
E0 ≥

1 −

b
b+1

Lm
m
−1
, so from Equation (6), we have ˜R( ˆf0) ≥

1 −

b
b+1
2 Lm
m
−1
σ2.
C.2
Dot-product kernels on the sphere
First we will prove results about the eigenvalue multiplicites Corollary 12 and Corollary 13.
Theorem 21 (Log-scaling multiplicity). Let d = log2 m, i.e. m = 2d, and let kd be an index for
which the following holds:
N(d, 1) + · · · + N(d, km) < m
N(d, 1) + · · · + N(d, km + 1) ≥m.
Then the following hold:
27

1.
dN(d, d
5 )
2d
is decreasing and has limd→∞
dN(d, d
5 )
2d
→0. This also holds for dN(d,k)
2d
with
k = f(d) ≤d
5 for all d.
2.
N(d, d
2 )
2d
is increasing and has limd→∞
N(d, d
2 )
2d
→∞.This also holds for N(d,k)
2d
with k =
f(d) ≥d
2 for all d.
3. There is an absolutet constant d0 such that for d > d0, we have that d
5 ≤km ≤d
2.
4. There are absolute constants cl−1 =
1
54, cl = 1
9, and cl+1 = 2
3 such that N(d, km + i) >
cl+im, for all i ∈{±1, 0}. Additionally, there are constants cu−1 = 1
3, cu = 1, cu+1 = 6,
such that N(d, km + i) < cu+im for all i ∈{±1, 0}.
Furthermore, if
d
log m = Θ(1), then we will also have Lm = Θ(m) and Lm = Θ(m).
Proof. We will split the proof into three parts. Note first that N(d, k) increases as k increases. This
can be seen from the ratio of consecutive multiplicities
N(d, k + 1)
N(d, k)
=
2k + d
2k + d −2
k + d −2
k + 1
.
Part 1:
We will use Stirling’s approximation to estimate N(d, k). It states that n! ≈
  n
e
n √
2πn.
Therefore, we have that
N(d, k) = (2k + d −2)(k + d −3)!
k!(d −2)!
≈
p
2π(k + d −3)
  k+d−3
e
k+d−3 (2k + d −2)
  k
e
k   d−2
e
d−2 √
k
√
d −2
.
Since N(d, k) is increasing in k and k ≤d
5, so we can just plug in k = d
5 in the expression. Therefore
N(d, d
5) ≈
p
2π(k + d −3)
  k+d−3
e
k+d−3 (2k + d −2)
  k
e
k   d−2
e
d−2 √
k
√
d −2
=
q
2π( d
5 + d −3)
 d
5 +d−3
e
 d
5 +d−3
(2 d
5 + d −2)
 d
5
e
 d
5   d−2
e
d−2 q
d
5
√
d −2
= C
√
ddd
d
5 +d−3  6
5 −3
d
e
 6
5 d−3
√
d
  d
5e
 d
5   d
e
d−2 √
d
√
d
= C 5
d
5   6
5
 6d
5
√
d
=
 51/5( 6
5)6/5d
√
d
.
Note that
 51/5( 6
5)6/5
< 1.8, so (51/5( 6
5 )6/5)
d
√
d
< 1.8d
√
d . In particular, we have that
dN(d, d
5)
2d
<
√
d(1.8)d
2d
→0.
Part 2:
The same calculation in this case gives
N(d, d
2) ≈C

2
1
2 ( 3
2)
3
2
d
√
d
but note that

2
1
2 ( 3
2)
3
2

> 2.5, so N(d, d
2 )
2d
>
  2.5
2
d
1
√
d →∞.
28

Part 3:
Note that if km ≤d
5 then for d large enough
N(d, 1) + · · · + N(d, km) ≤kmN(d, km) ≤dN(d, km) < d.
Similarly, note that if km > d
2, then N(d, km) > d for d large enough. Both of these imply hat
d
5 ≤km ≤d
2 when d > d0 (d0 is determined by when the inequalities above start holding).
Part 4:
Note that when d
5 ≤k ≤d
2, we have that 6 > N(d,k+1)
N(d,k)
=
2k+d
2k+d−2
k+d−2
k+1
> 3 for d large
enough, since the firs ratio is close to 1 and the second is 1 + d−2
k+1. This implies that
N(d,k)
N(d,k+1) < 1
3
whenever d is large enough (larger than an absolute constant) and k ≤d
2. Note now that
m < N(d, 1) + · · · + N(d, km + 1) < N(d, km + 1)

1 + 1
3 + (1
3)2 + . . .

< N(d, km + 1)3
2
2
3m < N(d, km + 1).
This now implies that N(d, km) > 1
6N(d, km + 1) > 1
9m, N(d, km −1) >
1
54m. Similarly we
have that N(d, km) < m, so N(d, km + 1) < 6m and N(d, km −1) < 1
3m.
Note that the same proof works even if d is not exactly equal to log2 m. This shows that whenever
d
log m = Θ(1), we have that Lm = Θ(m) and Lm = Θ(m).
Remark 22. Note that this does not say that we will always have tempered overfitting for
d
log m =
Θ(1) as sometimes it can happen that Lm →m, so we cannot apply Equation (5) and Equation (6).
In this case we expect the overfitting to be catastrophic.
Theorem 23 (Sub-polynomial scaling multiplicity). Let K be any dot product kernel on the sphere
Sd−1 (with nonzero eigenvalues). Let l ∈N and let m = 222l and let d = 22l. This corresponds to
the case d = exp
 √log m

, which is sub-polynomial. Then, for the upper and lower index, Lm and
Lm, we have the following
Lm
m ≤
3
2 log m
and
m
Lm
≤
1
d0.89 .
Additionally, for k = αkm, where α is a constant, we have that N(d, k) ≈mα, where ≈means up
to sub-polynomial factors.
Proof. Let km be the maximum k for which
N(d, 1) + · · · + N(d, km) < m
N(d, 1) + · · · + N(d, km + 1) ≥m.
We want to show that N(d, km) = o(m) and N(d, km + 1) = Ω(m). Then we can take k =
N(d, 1) + · · · + N(d, km) and so k
m →0 and
rk = N(d, km + 1) +
X
j=2
N(d, km + j)
˜λkm+j
˜λkm+1
= Ω(m)
m
Rk
→0.
29

Note that we just need to estimate N(d, km) precisely. Let k = 2l + l. We want to show that
N(d, k) > m. Note that by Stirling’s approximation n! ≈
  n
e
n √
2πn
N(d, k) ≈
p
2π(k + d −3)
  k+d−3
e
k+d−3 (2k + d −2)
  k
e
k   d−2
e
d−2 √
2πk
p
2π(d −2)
=
e
√
2π
dk+d−2√
d
q
2π(1 + k
d −3
d)
 1 + k
d −3
d
k+d−3  1 + 2k
d −2
d

kk(d −2)d−2√
k
√
d −2
=
e
√
2π
dk+d−2√
d
q
2π(1 + 2l+l
22l −
3
22l )

1 + 2l+l
22l −
3
22l
k+d−3  1 + 2k
d −2
d

kkdd−2(1 −2
d)d−2√
k
√
d
q
1 −2
d
=
e
√
2π
dk
q
2π(1 + k
d −3
d)
 1 + k
d −3
d
k+d−3  1 + 2k
d −2
d

(1 −2
d)d−2
q
1 −2
d
=
e
√
2π
q
2π(1 + k
d −3
d)
 1 + 2k
d −2
d

(1 −2
d)d−2
q
1 −2
d
dk
kk√
k

1 + k
d −3
d
k+d−3
.
Only the term
dk
kk√
k
 1 + k
d −3
d
k+d−3 is asymptotic here, as the rest tends to a constant as d →∞.
Note that since for all l > 5 (i.e. d)
2 >

1 + k
d −3
d
 d
k
> 1.95,
we have that

1 + k
d −3
d
k+d−3
> (1.95)
k2
d +k−3 k
d > (1.95)k > 2(0.9k)

1 + k
d −3
d
k+d−3
< 2
k2
d +k−3 k
d .
Note that we have that then
dk
kk√
k

1 + k
d −3
d
k+d−3
<
dk
kk√
k
2
k2
d +k−3 k
d
= 2k log d−k log k−1
2 log k+ k2
d +k−3k
d .
Consider now the expression k log d −k log k −1
2 log k + k2
d + k −3k
d . For k = 2l + l −1, we have
that
k log d −k log k −1
2 log k + k2
d + k −3k
d = k log d −k log k + k −1
2 log k −3k
d + k2
d
=
 2l + l −1

2l −
 2l + (l −1)
 
l + l −1
2l
+ o( 1
22l )

−1
2(l) + o( 1
2l ) + 2l + l −1 + o( 1
2l )
= 22l + 2l(l −1) −2l(l) + 2l −l + 1 −1
2l + l −1 + o(1) = 22l −1
2l = log2 m −log2 log2 m.
In particular, this implies that for k ≤2l + (l −1), we have that
N(d, k) < m2−1
2 l = o(m).
Now repeating the same computation for k = 2l + l, we have that
dk
kk√
k

1 + k
d −3
d
k+d−3
>
dk
kk√
k
20.9k
= 2k log d−k log k−1
2 log k+0.9k.
30

k log d −k log k −1
2 log k + k2
d + 0.9k −3k
d = k log d −k log k + k −1
2 log k −3k
d + k2
d
=
 2l + l

2l −
 2l + l
 
l + l
2l + o( 1
22l )

−1
2(l) + o( 1
2l ) + 0.9 · 2l + 0.9l + o( 1
2l )
= 22l + 2l(l) −2l(l) + 0.9 · 2l −l −1
2l + l + o(1) = 22l + 0.9 · 2l −1
2l
= log2 m + 0.9 ·
p
log2 m −log2 log2 m.
This shows that for k ≥2l + l,
N(d, k) > m20.9·2l−1
2 l = Ω(m).
To imply now that km = k = 2l + l −1, it suffices to show that N(d, 1) + · · · + N(d, k) = o(m).
But note that we have k ≪d, so
N(d, k −1) = 2k + d −2
2k + d
k + 1
k + d −2N(d, k) < 1
dN(d, k)
N(d, 1) + · · · + N(d, k −1) < k −1
d
N(d, k)
N(d, 1) + · · · + N(d, k) = o(m).
Therefore, we have computed with proof that km = 2l + l −1. So we can now compute Lm and Lm
with Lm = N(d, 1) + · · · + N(d, km). Note that
N(d, k + 1)
N(d, k)
=
2k + d
2k + d −2
k + d −2
k + 1
.
Since k << d, note that N(d,k+1)
N(d,k)
> 2, so we have that
N(d,k)
N(d,k+1) < 1
2, which implies that
N(d, 1) + N(d, 2) + · · · + N(d, km) < 3
2N(d, km) = o(m).
Therefore, we have that Lm
m = N(d,1)+N(d,2)+···+N(d,km)
m
< 3
2
N(d,km)
m
≤
3
2 log m. From what we
computed previously, we have that
Lm > N(d, k) > m20.9·2l−1
2 l
m
Lm
<
1
d0.9−1
2 log log dlog d <
1
d0.89 .
Note finally that for k = α2l, the leading term in the exponent of N(d, k) as derived above is α22l, all
other terms are at most l2l, which is sub-polynomial in m, so this shows that N(d, αk) ≈mα.
Remark 24. Note that for the polynomially increasing d, there are sequences (m, d) for which we do
not get benign overfitting, i.e. when d = mk, for integer k. Similarly in this case, there are sequences
of (m, d) for which we do not get benign overfitting, but is much harder to identify them.
C.3
Proof of Corollary 13
Corollary 3 (Benign overfitting with Gaussian kernel and sub-polynomial dimension). Let K be the
Gaussian kernel on the sphere Sd−1 with a fixed bandwidth, and suppose that for l ∈N the dimension
and sample size scale as d = 22l, m = 222l, i.e. d = exp
 √log m

. Consider learning a sequence
of target functions f ∗
d as in Assumption 4 with Sd ≤m
1
4 . Then, we have that the minimum norm
interpolating solution achieves the Bayes error in the limit (m, d) →∞. In particular, for d ≥4 and
m ≥16 we have
˜R( ˆf0) ≤

1 −
1
log m
−1 
1 −exp

−0.89
p
log m
−1
σ2 + 2B2 1
m.
If we take Sd = poly(d), then we can improve the 1
m error dependence to
1
m2−ε . Furthermore, we
can improve the bound on Sd by reducing the rate of convergence. Let sd be an integer such that
Sd ≤Nsd. Then error dependence is
1
m2 sdN(d, sd)d(1+ε)sd <
1
m2 d2(1+ε)sd.
31

Proof:
We have that from Theorem 23 that in this case km = 2l + l −1 = log d + log log d −1,
Lm = Θ(
m
log m) and Lm ≥Θ(md0.89). Therefore, we have that
E0 ≤

1 −
1
log m
−1 
1 −exp

−0.89
p
log m
−1
.
To finish the proof, we need to estimate N(d, 1) 1
˜λ2
1 +. . . N(d, km)
1
˜λ2
km
. Note that the eigenvalues are
sorted and N(d, k −1) = o(N(d, k + 1)) since km = o(d). This implies that it suffices to estimate
N(d, km)
1
˜λ2
km
. Note that for the first eigenvalue, we have from Corollary 31 for α = 1 +
2
τ 2
m .
1
dα <˜λ1
So, for ˜λk, it holds that
˜λk
˜λ1
>
k
Y
i=1
2
τ 2
m
2
τ 2
m + 2(i + d
2 −1).
Therefore, we have that
1
˜λk
< 1
˜λ1
k
Y
i=1
2
τ 2
m + 2(i + d
2 −1)
2
τ 2
m
= dα
τ 2
m
2
k Γ(d + 2km +
2
τ 2
m )
Γ( d
2)Γ(d +
2
τ 2
m )
< dα  τ 2
m
k d
2 + k + 2
τ 2m
k
.
Note also that if k = βkm for some constant β then
N(d, k) ≈mβ,
in the sense of Theorem 23. So, N(d, km)
1
˜λ2
km
< mβd2k(1+ε)−2α. We want to take k as large as
possible, so we will take it k = βkm. Then d2k ≈m2β. Therefore, as long as 3β < 2, we will have
a polynomially scaling error. So we can take β = 2
3. Note that with Theorem 20, we actually get
even better dependence of the error, like mβdk(1+ε)−α. Plugging the first estimate into Theorem 7,
for 3α = 1, we have that
˜R( ˆf0) ≤

1 −
1
log m
−1 
1 −
1
d0.89
−1
σ2
+ B2

1 −
1
log m
−1 
1 −
1
d0.89
−1 1
m2 m3β
≤

1 −
1
log m
−1 
1 −
1
d0.89
−1
σ2
+ B2

1 −
1
log m
−1 
1 −
1
d0.89
−1 1
m.
If we want to achieve Sd = poly(d), note that we just need to take k to not scale wit d. That is, if we
want deg Sd = n, then we can take k = n. The eigenvalue will be
1
˜λn
< dα+2n.
The multiplicity will be N(d, n) = Θ(dn). Note that then error scales as d5n
m2 , i.e. since d is sub poly
m, we can take any such n.
C.4
Kernel conditions
We have three main assumptions on kernels. First, the sum of its eigenvalues is bounded.
Assumption 6 (Bounded sum of eigenvalues). Assume that the kernel K has a bounded sum of
eigenvalues, i.e. there is a constant A such that P∞
i=1 N(i)˜λi ≤A. For a sequence of kernels K(d),
assume that all such A(d) are bounded by some constant A.2
2Note that this assumption implicitly sets the scale of the kernel.
32

Second, there is a lower bound on the eigenvalues.
Assumption 8 (Lower bound on eigenvalues). Assumme that the kernel K has eigenvalues that
are not too small, i.e. there is a constant b such that maxi≤km

1
˜λi

< m−Lm
b
. For a sequence of
kernels K(d), assume that for the corresponding m = m(d) (since d = d(m), we can also "invert"
the dependence) all such b(d) are bounded below by some b.
And third, the eigenvalues don’t decrease too fast.
Assumption 10 (Eigenvalue decay). The eigenvalues don’t decrease too quickly, i.e. for km as in
Definition 5, we have that there is a constant c such that maxi≤km

1
˜λi

≤cN(km). For increasing
dimension, we require that maxi≤km

1
˜λi

≤cN(d, km).
Note that Assumption 6 will hold quite broadly, in particular at least for dot product kernels on
the sphere (that are effectively the same kernel just in increasing dimensions). Note also that the
Assumption 10 is almost always stronger than Assumption 8. This follows from the definition of
Lm and km, i.e. since Lm = N(1) + · · · + N(km), we have that N(km) < m and as long as
Lm < Θ(m), we can take b large. This is usually the case (we show previously in Appendix C.2 for
dot-product kernels on the sphere). For d = ω(log m), we even have Lm = o(m). So in particular
N(d, km) < Θ(m −Lm) and for d = ω(log m), we have N(d, km) = o(m −Lm), so indeed
Assumption 10 is stronger than Assumption 8.
Assumption 6 and Assumption 10 are also taken to be true in the literature on the polynomially
scaling dimension [5, 66].
First assumption:
For dot-product kernels on the sphere, we know that the eigenfunction ϕi are
actually the spherical harmonics Yks. Let K(x, y) = h(∥x −y∥). Note that for x ∈Sd−1, it holds
that
N(d,k)
X
s=1
Yks(x)2 = N(d, k).
Therefore, since
K(x, y) =
X
i
λiϕi
h(0) = K(x, x) =
X
k
N(d,k)
X
s=1
˜λkYks(x)2 =
X
k
N(d, k)˜λk.
So the assumption in Theorem 7 holds with A = h(0). For the Gaussian kernel, this is A = 1.
Similarly, A = 1 for Laplace kernel.
Second Assumption:
For the eigenvalue assumption, maxi≤km

1
˜λi

< m−Lm
Ab
, note the following.
We will usually have Lm = o(m). In particular, whenever d = ω(log m) for a dot-product kernel on
the sphere this will hold.
The eigegnvalue assumption will hold for the Gaussian kernel in the non-integer polynomial regime,
dα = m. In Appendix C.3, we showed that
1
˜λk < dk, so since km = ⌊α⌋, we will have
1
˜λk < dα −
Θ(d⌊α⌋). Similarly, we can show this for the Gaussian kernel for d = log m and d = exp(√log m)
regime. Note that in Corollary 31, we showed that ˜λ1 >
1
dα and ˜λk >
1
dα+k , so then
1
˜λk Additionally,
in the d = exp(√log m) regime, we showed that Lm = o(m) and that
1
˜λk = Θ(Lm) for any
k = Θ(km) < km. The same proof shows that this assumption holds for any sub-polynomially
scaling dimension and Gaussian kernel.
Furthermore, this assumption can be weakened to the following: there exists b > 0 such for all
k ≤Lm, b P
i>k λi < λk+1(m −k). Additionally, since the assumption is specific to our approach,
it seems to be possible to weaken it even further.
33

Third assumption:
This assumption holds for dot-product kernels on the sphere. It was shown in
Zhang et al. [66] (Lemma 5.2.1) under another assumption similar to (1) that it holds for polynomially
scaling dimension. It is also assumed in Barzilai and Shamir [5] (page 11) for polynomially scaling
dimension. Furthermore, Cao et al. [9] (Theorem 4.3) shows that this holds for Neural Tangent Kernel
even more broadly, i.e. for all i and not only i ≤km.
D
Appendix B: Sharp bounds on eigenvalues corresponding to Gaussian and
Laplace Kernels
In this section, we will summarize and prove the results about eigenvalues of Gaussian and Laplace
kernels when X ∼Unif(Sd−1).
Given a positive semi-definite kernel function K : X × X →R, we can decompose it as
K(x, t) =
∞
X
i=1
λkϕk(x)ϕk(t),
where λk and ϕk are the eigenvalues and eigenfunctions of the operator associated to K, LK :
L2
DX (Sd−1) →L2
DX (Sd−1)
LK(f)(x) =
Z
X
K(x, t)f(t)dµ(t).
LK is a Hilbert-Schmidt operator and has a countable system of non-negative eigenvalues λk
satisfying P∞
k=1 λ2
k < ∞. The corresponding L2
DX (Sd−1)-normalized eigenfunctions {ϕk(x)}∞
k=1
form an orthonormal basis of L2
DX (Sd−1). The eigenfunctions in this case are given by spherical
harmonics, Yi,s. The eigenvalues corresponding to all Yi,s for fixed i have the same eigenvalue, ˜λi.
Eigenvalues ˜λi are reach repeated N(d, i) = (2i+d−2)(i+d−3)!
i!(d−2)!
times. So the sequence {λi}∞
i=1 =
˜λ1, . . . , ˜λ1, ˜λ2, . . . , ˜λ2, . . . . Using Funk-Hecke formula, we can compute ˜λk explicitly, both for
the case of Gaussian [48] kernel. For Gaussian kernel, we have the following theorem about the
eigenvalues ˜λk.
Theorem 25 (Eigenvalues of the Gaussian kernel [48]). Let X ∼Unif(Sd−1), with d ∈N and d ≥2.
For K(x, t) = exp

−∥x−t∥2
τ 2
m

, τm > 0, and k ∈N0 we have that
˜λk = e
−
2
τ2m τ d−2
m
Ik+ d
2 −1
 2
τ 2m

Γ
d
2

.
Each ˜λk occurs with multiplicity N(d, k) = (2k+d−2)(k+d−3)!
k!(d−2)!
(we use ˜λ notation to indicate that it
has multiplicity) and its corresponding eigenfunction are the spherical harmonics of order k on Sd−1.
Here, Iv(z), v, z ∈C is the modified Bessel function of the first kind
Iv(z) =
∞
X
j=0
1
j!Γ(v + j + 1)
z
2
v+2j
.
It will be useful to know the size of N(d, i). Also, the size of the sum of the first k multiplicities will
be useful. Let Nk = Pk
i=1 N(d, i).
Lemma 26 (Size of multiplicity). For N(d, i) = (2k+d−2)(k+d−3)!
k!(d−2)!
, it holds that
1
(d −2)!kd−2 ≤N(d, i) ≤2d−1kd−2.
Additionally, there exist constants nl, nu > 0 depending on the dimension d such that Nk is bounded
below and above by the following
nlkd−1 ≤Nk ≤nukd−1.
34

Proof. Since (2k + j) ≤(2k)j for j ≥2 and (k + 1) ≤2k, we have that
N(d, k) = (2k + d −2)(k + d −3) . . . (k + 1)
(d −2)!
≤(d −2)!(2k)d−3(2k)
(d −2)!
≤2d−1kd−2
and
N(d, k) = (2k + d −2)(k + d −3) . . . (k + 1)
(d −2)!
≥
1
(d −2)!kd−2.
Note that by Bernoulli’s formula
k
X
l=1
ld−2 =
1
d −1
 kd−1 + o(kd−1)

.
Therefore, we have that
nlkd−1 ≤
1
(d −1)(d −2)!
 kd−1 + o(kd−1)

≤Nk ≤2d−1
d −1
 kd−1 + o(kd−1)

≤nukd−1.
Lemma 27 (Inverting the index). If the index of an eigenvalue ˆλj is such that Nk−1 ≤j ≤Nk −1,
then ˆλj = λk. We will denote such j with ˜k, i.e. ˜k is an index such that ˆλ˜k = λk.
Proof. Immediate from the fact that {λi}∞
i=1 is a sequence with ˜λi repeating N(d, i) times, in that
order.
An interesting property of eigenvalues of the Gaussian kernel is that they are sorted because the
Modified Bessel functions are [50]. In particular, Iv+1(x) < Iv(x) for all v, x > 0, so ˜λi+1 < ˜λi.
This is not the case for the Laplace kernel.
D.1
Bounds on eigenvalues of the Gaussian kernel
Theorem 28 (Size of Ratios of Eigenvalues for Gaussian Kernel). Let T = ( 2
τ 2
m ). For j ≤
√
Tt(m), k ≤
√
Tt(m), where t(m) →0 as m →∞, we have that
exp(−6t(m)) <
˜λk+j
˜λk
< 1.
For j ≥
√
Tt(m), where t(m) →∞as m →∞we have that for any k
˜λk+j
˜λk
< exp

−t(m)2
4

.
Proof. First of all, note that from [50], we have that for v ≥1
2
(v) +
p
(v)2 + x2
x
> Iv−1(x)
Iv(x)
>
(v −1
2) +
q
(v −1
2)2 + x2
x
x
(v) +
p
(v)2 + x2 < Iv(x)
Iv−1(x) <
x
(v −1
2) +
q
(v −1
2)2 + x2
35

In particular, this means that for the eigenvalues ˜λk, we have
( 2
τ 2
m )
(k + d
2) +
q
(k + d
2)2 + ( 2
τ 2
m )2 <
˜λk+1
˜λk
=
Ik+1+ d
2 −1( 2
τ 2
m )
Ik+ d
2 −1( 2
τ 2
m )
<
( 2
τ 2
m )
(k + d
2 −1
2) +
q
(k + d
2 −1
2)2 + ( 2
τ 2
m )2 .
This can be bounded with a simpler expression as follows
( 2
τ 2
m )
2(k + d
2) + ( 2
τ 2
m ) <
˜λk+1
˜λk
=
Ik+1+ d
2 −1( 2
τ 2
m )
Ik+ d
2 −1( 2
τ 2
m )
<
( 2
τ 2
m )
(k + d
2 −1
2) + ( 2
τ 2
m ).
We will use these bounds to derive tight bounds for the ratios
˜λk+j
˜λk . Note the following
jY
i=1
( 2
τ 2
m )
2(k + i −1 + d
2) + ( 2
τ 2
m ) <
˜λk+j
˜λk
=
jY
i=1
˜λk+i
˜λk+i−1
<
jY
i=1
( 2
τ 2
m )
(k + i −1 + d
2 −1
2) + ( 2
τ 2
m ).
Note now that 
( 2
τ 2
m )
2(k + j −1 + d
2) + ( 2
τ 2
m )
!j
<
jY
i=1
( 2
τ 2
m )
2(k + i −1 + d
2) + ( 2
τ 2
m ) <
˜λk+j
˜λk
.
Note that since (x + j −i)(x + i) = x2 + ix + (j −i)i, we have that (x + j −i)(x + i) ≥(x + j)x,
therefore
jY
i=1
( 2
τ 2
m )
(k + i −1 + d
2 −1
2) + ( 2
τ 2
m )
<
 
( 2
τ 2
m )
(k + j −1 + d
2 −1
2) + ( 2
τ 2
m )
! j−1
2  
( 2
τ 2
m )
(k + d
2 −1
2) + ( 2
τ 2
m )
! j+1
2
.
We use j + 1 and j −1 to account for the fact that j might be odd when we split into (j −1)/2 pairs.
When j is even, we split into j
2 pairs and use the fact that the term with exponent j+1
2
is larger.
Let T = ( 2
τ 2
m ). We can bound the ratio
˜λk+j
˜λk
tightly now.
When j ≤
√
Tm−δ, k ≤
√
Tm−δ, we have that
exp(−6m−δ) <
˜λk+j
˜λk
< 1.
When j ≥
√
Tmδ, we have that for any k
˜λk+j
λk
< exp(−m2δ
4 ).
To see why this is true, note first that
 
( 2
τ 2
m )
2(k + j −1 + d
2) + ( 2
τ 2
m )
!j
=
 
T
2(k + j −1 + d
2) + T
!j
>
 
T
2(k + j −1 + d
2) + T
!√
T
>

T
5(
√
Tm−δ) + T
√
T
=
 
1
5( 1
√
T m−δ) + 1
!√
T
→exp(−5m−δ),
36

as m →∞. For the second inequality, note that
 
( 2
τ 2
m )
(k + d
2 −1
2) + ( 2
τ 2
m )
! j+1
2
< 1.
We also have that
 
( 2
τ 2
m )
(k + j −1 + d
2 −1
2) + ( 2
τ 2
m )
! j−1
2
=
 
T
(k + j −1 + d
2 −1
2) + T
! j−1
2
<

T
(
√
Tmδ) + T

√
T mδ−1
2
=
 
1
(
√
T mδ)
T
+ 1
!
√
T mδ
3
<
 
1
mδ
√
T + 1
!
√
T mδ
3
=



 
1
mδ
√
T + 1
!
√
T
mδ



m2δ
3
→exp(−m2δ
3 ) →0,
as m →∞. Note that we can turn the limits am →t(m) into inequalities of the form (1 −ε)t(m) <
am < (1 + ε)t(m) for some ε and all m since the convergence is uniform.
The following is a simple corollary.
Corollary 29. Let T = ( 2
τ 2
m ). If T > 1 and the index of the eigenvalue i is such that i ≤(k
√
T)d−1,
we have that for λi it holds that
λi
λ1
≥
 
1
1 +
k
√
T
!k
√
T
≥exp(−k2).
Proof. Note that

1
1+
k
√
T
k
√
T
is increasing in
√
T and note that
√
T increases as m increases. Note
that for
√
T = 1 it suffices to show e(k) > 1 + k which is true for all k as long as
√
T > 1.
The following simple bound also holds for eigenvalues of Gaussian kernel.
Proposition 30 (Ratio of eigenvalues bounded above [48]). For the eigenvalues associated to the
Gaussian Kernel ˜λk of bandwidth τm we have that
˜λk+1
˜λk
<
1
τ 2m(k + d
2).
It is straightforward to convert the bounds on ratios of eigenvalues Theorem 25 to bounds on the sizes
of the actual eigenvalues.
Corollary 31 (Sizes of the eigenvalues of Gaussian kernel). The following bounds hold for the
largest eigenvalue of the Gaussian kernel
1
τ 2m + 4
Γ(( 2
τ 2
m + 1
2))Γ( d
2)
Γ( 2
τ 2
m + d
2 + 2)
< ˜λ1 <
1
p
τ 4m + 4τ 2m
Γ(( 2
τ 2
m + 1
2))Γ( d
2)
Γ( 2
τ 2
m + d
2 + 3
2) .
Therefore, for ˜λk, we have that
1
τ 2k
m
Γ(( 2
τ 2
m + k + 1
2))
Γ( 2
τ 2
m + k + d
2 + 2)
˜λ1 < ˜λk
˜λk < 2k 1
τ 2k
m
Γ(( 2
τ 2
m + k + 1
2))
Γ( 2
τ 2
m + k + d
2 + 3
2)
˜λ1.
37

Furthermore, for τm fixed we have that
˜λ1 >
1
τ 2m + 4
1
(( d
2 +
2
τ 2
m )
1+
2
τ2m )
.
and
˜λk >
1
τ 2m + 4
1
(( d
2 +
2
τ 2
m )
1+
2
τ2m )
1
τ 2k
m
1
((k + d
2 +
2
τ 2
m )k).
Proof. Note that from [64] we have
1
1 +
4
τ 2
m
exp
 2
τ 2m

< I0
 2
τ 2m

<
1
q
1 +
4
τ 2
m
exp
 2
τ 2m

We also have from [50]
( 2
τ 2
m )
2(i + 1) + ( 2
τ 2
m ) <
Ii+1( 2
τ 2
m )
Ii( 2
τ 2
m )
<
( 2
τ 2
m )
2(i + 1
2).
Then
˜λk = e
−
2
τ2m τ d−2
m
Ik+ d
2 −1
 2
τ 2m

Γ
d
2

.
So then
1
τ dm
Γ(( 2
τ 2
m + 1
2))
Γ( 2
τ 2
m + d
2 + 2) <
d
2 −1
Y
i=0
( 2
τ 2
m )
2(i + 1) + ( 2
τ 2
m ) <
I1+ d
2 −1

2
τ 2
m

I0

2
τ 2
m

<
d
2 −1
Y
i=0
( 2
τ 2
m )
2(i + 1
2) = 1
τ dm
Γ( 1
2)
Γ( d
2).
So then
1
τ 2m + 4
Γ(( 2
τ 2
m + 1
2))Γ( d
2)
Γ( 2
τ 2
m + d
2 + 2)
< ˜λ1 <
1
p
τ 4m + 4τ 2m
Γ(( 2
τ 2
m + 1
2))Γ( d
2)
Γ( 2
τ 2
m + d
2 + 3
2) .
By repeating the same argument, the claim about ˜λk follows.
Note that
˜λ1 >
1
τ 2m + 4Γ(d
2)
d
2 −1
Y
i=0
1
(i + 1) + ( 2
τ 2
m ) >
1
τ 2m + 4
1
(( d
2 +
2
τ 2
m )
1+
2
τ2m )
.
Therefore, we have that
˜λk > ˜λ1
k−1
Y
i=0
2
τ 2
m
k + d
2 + ( 2
τ 2
m ) > ˜λ1
1
τ 2k
m
1
((k + d
2 +
2
τ 2
m )k).
38
