Stochastic (Approximate) Proximal Point Methods:
Convergence, Optimality, and Adaptivity1
Hilal Asi
John C. Duchi
asi@stanford.edu
jduchi@stanford.edu
Stanford University
Abstract
We develop model-based methods for solving stochastic convex optimization problems,
introducing the approximate-proximal point, or aProx, family, which includes stochastic
subgradient, proximal point, and bundle methods. When the modeling approaches we pro-
pose are appropriately accurate, the methods enjoy stronger convergence and robustness
guarantees than classical approaches, even though the model-based methods typically add
little to no computational overhead over stochastic subgradient methods. For example,
we show that improved models converge with probability 1 and enjoy optimal asymptotic
normality results under weak assumptions; these methods are also adaptive to a natural
class of what we term easy optimization problems, achieving linear convergence under
appropriate strong growth conditions on the objective. Our substantial experimental in-
vestigation shows the advantages of more accurate modeling over standard subgradient
methods across many smooth and non-smooth optimization problems.
1
Introduction
In this paper, we develop and analyze a family of model-based methods, moving beyond naive
stochastic gradient methods, for solving the stochastic convex optimization problem
minimize F(x) = EP [f(x; S)] =
Z
S
f(x; s)dP(s)
subject to x âˆˆX.
(1)
In problem (1), the set S is a sample space, and for each s âˆˆS, the function f(Â·; s) : Rn â†’R
is a closed convex function, subdiï¬€erentiable on the closed convex set X âŠ‚Rn.
Stochastic minimization problems, in which an optimizer has access to samples Si drawn
independently and identically distributed from P and uses these samples to minimize F,
have applications in numerous ï¬elds, including machine learning, statistical estimation, and
simulation-based optimization [65, 23, 54]. The current accepted methodology for such prob-
lems is the stochastic (sub)gradient method [66, 65, 42, 9, 53], which Robbins and Monro [48]
originally developed for smooth stochastic approximation problems, which iterates as follows:
beginning at an initial point x1, iteratively draw Sk
iid
âˆ¼P and update
xk+1 := xk âˆ’Î±kgk for some gk âˆˆâˆ‚f(xk; Sk).
(2)
The stochastic gradient method enjoys convergence guarantees [66, 42] and widespread empir-
ical success in large-scale convex and non-convex stochastic optimization [65, 9, 53, 16, 34]. In
spite of this success, there are notable diï¬ƒculties with the stochastic subgradient method (2):
it is sensitive to stepsize selection; it can diverge on objectives, such as F(x) = x4, that do
not obey its convergence criteria; and it is rarely adaptive to nuanced aspects of problem
1Research partially supported by NSF-CAREER Award 1553086, ONR-YIP N00014-19-1-2288, and the
Sloan Foundation.
1
arXiv:1810.05633v2  [math.OC]  3 Jul 2019

diï¬ƒculty. Engineers thus waste time and computation dealing with these issues and ï¬nding
appropriate stepsizes, which cascades into additional practical challenges.
An alternative to treating the stochastic gradient method (2) (SGM) as a noisy approx-
imation to gradient descent is to view it as minimizing a sequence of random models of the
functions F and f, and we leverage this view here. In this context, SGM makes a linear
approximation to the instantaneous function f around the point xk, setting
fxk(x; Sk) := f(xk; Sk) + âŸ¨gk, x âˆ’xkâŸ©
and choosing xk+1 to minimize the regularized model fxk(x; Sk)+
1
2Î±k âˆ¥x âˆ’xkâˆ¥2
2. More sophis-
ticated models are plausible. Most familiar is the stochastic proximal point method [50, 30, 7,
28, 8] (the least-mean-squares algorithm [63] for quadratic f), which makes no approximation,
using fxk(x; s) = f(x; s) and iterating
xk+1 = argmin
xâˆˆX

f(x; Sk) +
1
2Î±k
âˆ¥x âˆ’xkâˆ¥2
2

.
(3)
This modeling perspective is important in non-stochastic optimization, where (for example)
Newton, Gauss-Newton, bundle, and trust-region methods [e.g. 25, 10, 43, 44] explicitly build
sequences of easier-to-minimize models while minimizing the global function F. A substantial
body of work investigates this modeling perspective [11, 18], and recent work by Duchi and
Ruan [19] and Davis and Drusvyatskiy [14] demonstrates convergence for appropriate models
in weakly convex stochastic optimization, motivating our approach.
We show how to extend this modeling perspective to stochastic convex optimization prob-
lems, leveraging it to build a new family of algorithms for solving problem (1), which, in
homage to the stochastic proximal point iteration (3), we call the aProx (approximate prox-
imal point) algorithms, with substantially better theoretical guarantees and empirical perfor-
mance than naive stochastic subgradient methods. The aProx algorithms iterate as follows:
for k = 1, 2, . . ., we draw a random Sk
iid
âˆ¼P, then update the iterate xk by minimizing a
regularized approximation to f(Â·; Sk), setting
xk+1 := argmin
xâˆˆX

fxk(x; Sk) +
1
2Î±k
âˆ¥x âˆ’xkâˆ¥2
2

.
(4)
The function fx(Â·; s) is a model of f(Â·; s) at the point x, meaning that fx satisï¬es the following
conditions on its structure and local approximation properties for f:
(C.i) The function y 7â†’fx(y; s) is convex and subdiï¬€erentiable on X.
(C.ii) The model fx satisï¬es the equality fx(x; s) = f(x; s) and
fx(y; s) â‰¤f(y; s) for all y.
By the ï¬rst-order conditions for convexity, for any g âˆˆâˆ‚yfx(y; s)|y=x Conditions (C.i) and (C.ii)
imply f(y; s) â‰¥fx(y; s) â‰¥fx(x; s) + âŸ¨g, y âˆ’xâŸ©= f(x; s) + âŸ¨g, y âˆ’xâŸ©, yielding the containment
âˆ‚yfx(y; s)|y=x âŠ‚âˆ‚xf(x; s).
(5)
Davis and Drusvyatskiy [14] and Duchi and Ruan [19] consider similar modeling conditions,
and they inspire our treatment here. See Section 2 and Figure 1 for examples.
2

The aProx methodology (4) is ï¬‚exible in that it allows many possible modeling choices.
As we shall see, though stochastic gradient (2) and proximal point (3) methods are both special
cases, they possess quite diï¬€erent behavior. Thus, it is interesting to provide conditions on
the accuracy of the models fx, in addition to (C.i)â€“(C.ii), that imply stronger convergence
guarantees than those available for stochastic gradient and other simple methods. To describe
our contributions at a high level, we list two assumptions that we frequently make.
As
f(Â·; s) is subdiï¬€erentiable on X, there exist measurable selections fâ€²(x; s) âˆˆâˆ‚f(x; s), and
E[âˆ‚f(x; S)] = âˆ‚F(x) for x âˆˆX (cf. [6, Sec. 2]).
Assumption A1. The set X â‹†:= argminxâˆˆX {F(x)} is non-empty, and there exists Ïƒ2 < âˆ
such that E[âˆ¥fâ€²(xâ‹†; S)âˆ¥2
2] â‰¤Ïƒ2 for xâ‹†âˆˆX â‹†and all measurable selections fâ€²(xâ‹†; s) âˆˆâˆ‚f(xâ‹†; s).
Assumption A2. There exists a non-decreasing function Gbig : R+ â†’[0, âˆ) such that for
all x âˆˆX and measurable selections fâ€²(x; s) âˆˆâˆ‚f(x; s), E[âˆ¥fâ€²(x; S)âˆ¥2] â‰¤Gbig(dist(x, X â‹†)).
Assumption A2 makes no restrictions on the growth of the function Gbig, so the second moment
of the subgradient fâ€²(x; S) may grow arbitrarily. This contrasts with typical assumptions for
stochastic subgradient methods [e.g. 66, 42] which assume uniform boundedness or second-
moment conditions on subgradients. Within this context, we take three thrusts.
1. First, in Section 3, we develop conditions for the stability of iterates (4) under Assump-
tion A1, meaning that they remain in a bounded neighborhood of the optimal solution
set X â‹†of problem (1). We leverage this stability to prove convergence for the aProx
iteration (4) even for functions with substantial variation in their gradient estimates (e.g.
the gradient may grow super-exponentially in âˆ¥xâˆ¥) to which standard results do not ap-
ply. As a consequence, we extend Polyak and Juditskyâ€™s analysis of averaged stochastic
gradient methods to all aProx models (4)â€”showing asymptotic normality with optimal
covariance under weaker conditions than those necessary for classical situationsâ€”so long
as the iterates are bounded, highlighting the importance of this stability.
2. In our second thrust, in Section 4, we study the performance of aProx methods (4)
for what we term easy problems. In these problems there exists a shared minimizer xâ‹†
common to all the sampled functions.
This assumption is strong, yet many problems
are easy: in statistical learning, Belkin et al. [4, 3] show that functions that perfectly
interpolate the observed data (suï¬€ering zero loss on the observations) can achieve optimal
statistical convergence; Kaczmarz algorithms solve consistent over-parameterized linear
systems [55, 41, 40]; the problem of ï¬nding a point in the intersection of convex sets
assumes there exists a point in each of them [35, 2].
By incorporating a simple lower
bound conditionâ€”basically, that if f is non-negative, any model fx should also be non-
negativeâ€”in addition to conditions (C.i)â€“(C.ii), we show how aProx adapts to these easy
problems and achieves (near) linear convergence, even in stochastic settings, using methods
with no additional computational complexity beyond stochastic gradient methods.
3. Finally, in Section 5, we present representative non-asymptotic convergence guarantees.
Any aProx method (4) recovers standard convergence guarantees of stochastic gradient
and proximal point methods [42, 7] (see [14] for the basic convergence guarantee). We
show how stochastic proximal point methods enjoy fast convergence under (restricted)
strong convexity with only weak moment conditions on âˆ‚f(x; s), further emphasizing the
advantages of accurate modeling.
3

In addition to our theoretical results, we perform substantial simulations. Our experi-
ments consider a wide range of smooth, non-smooth, and super-polynomially-growing convex
problems: regression with squared and absolute losses, logistic and poisson regression, and
projection problems onto intersections of halfspaces (relating these to classiï¬cation problems).
The common refrain in each of these is that even slightly improved aProx models are much
more robust to stepsize choice than stochastic gradient methods, and more careful modeling
allows fast convergence in a much broader range of problems, including those with moderately
poor conditioning where stochastic gradient methods fail.
1.1
Related work
We situate our paper in relation to classical and modern work on stochastic optimization
problems.
Stochastic gradient methods are classical, beginning with the development by
Robbins and Monro [48] in the 1950s [46, 47, 66, 42, 65, 31]. A number of authors recognize
the challenges associated with stepsize selection and instability of stochastic gradient methods:
in the case of smooth strongly convex minimization, Nemirovski et al. [42] show how a slightly
mis-speciï¬ed stepsize can cause arbitrarily slow convergence guarantees. More recent work
(e.g. [1]) shows that even when assumptions suï¬ƒcient for convergence hold, stochastic gradient
methods can exhibit transient divergent behavior.
In eï¬€ort to alleviate some of these issues, there is recent work on more careful approaches
to stochastic optimization problems. Of most relevance to our work are stochastic proximal
point methods (3), which use the true function fx(y; s) = f(y; s) in the iteration (4). Bert-
sekas [7] analyzes stochastic proximal point algorithms in an incremental framework (when
S = {1, . . . , m} is a ï¬nite set), showing convergence results similar to subgradient methods,
while Kulis and Bartlett [30] and Karampatziakis and Langford [28] give theoretical and em-
pirical results in online convex optimization settings, demonstrating regret bounds similar to
classical results [66]. Toulis et al. [57, 58] study stochastic proximal point algorithms and
convergence guarantees for their ï¬nal iterates, a diï¬€erent approach than we take. Their re-
sults, however, assume that the functions under consideration are both globally Lipschitz and
globally strongly convex, a contradiction, severely limiting the applicability of their results.
(Their analysis explicitly and frequently uses both assumptions; under weaker assumptions,
their convergence guarantees exhibit the same potential for exponential divergence of stochas-
tic gradient methods.)
Patrascu and Necoara [45] also analyze stochastic proximal point
algorithms, providing non-asymptotic convergence results under the assumption that each
function f(Â·; s) is Lipschitz or strongly convex [45, Assumptions 1 & 9]; these assumptions
fail for many problems (including linear regression with f(x; (a, b)) = 1
2(âŸ¨a, xâŸ©âˆ’b)2), though
their results also apply to interesections of sets X = X1 âˆ©Â· Â· Â· âˆ©Xm.
Ryu and Boyd [51] also investigate the stochastic proximal point method, making argu-
ments on its stability stronger than classical results for stochastic gradient methods. Under
Assumption A1, Ryu and Boyd show that the stochastic proximal point method guarantees
E[âˆ¥xk+1 âˆ’xâ‹†âˆ¥2] â‰¤E[âˆ¥x1 âˆ’xâ‹†âˆ¥2] + Ïƒ Pk
i=1 Î±i, so that the iterates do not diverge exponen-
tially. Yet this result is not enough to explain or provide empirical or theoretical stability,
boundedness, or convergence guarantees.
Notation
For a convex function f, âˆ‚f(x) denotes its subgradient set at x, and fâ€²(x) âˆˆâˆ‚f(x)
denotes an arbitrary element of the subdiï¬€erential. Throughout, xâ‹†denotes a minimizer of
problem (1) and X â‹†= argminxâˆˆX F(x) its optimal set. We let Fk := Ïƒ(S1, . . . , Sk) be the
Ïƒ-ï¬eld generated by the ï¬rst k random variables Si, so xk âˆˆFkâˆ’1 for all k under iteration (4).
4

          
           
Linear
Truncated
x0
x1
(a)
(b)
Figure 1.
(a) Models of the function f(x) = log(1 + eâˆ’x): a linear model (6) built around
the point x0 and truncated model (8) built around the point x1.
(b) The bundle model,
maximum of linear functions, as in the iteration (9). The lighter lines represent individual
linear approximations, the darker line their maximum.
2
Methods
We begin our contributions by introducing diï¬€erent natural models for stochastic convex
optimization problems, as well as a few conditions in addition to (C.i)â€“(C.ii) that we can
use to demonstrate new aspects of convergence for the aProx family. While the stochastic
proximal point method (3) satisï¬es all the conditions in the paper, in some situations it may
be expensive or challenging to implement exactly. With that in mind, we provide a catalogue
of a few models to serve as a reference for the remainder of the paper.
Stochastic subgradient methods:
The starting point for any model-based methods are
the simple ï¬rst-order models. As we discuss in the introduction, the stochastic subgradient
method uses the model
fx(y; s) := f(x; s) + âŸ¨fâ€²(x; s), y âˆ’xâŸ©,
(6)
where fâ€²(x; s) âˆˆâˆ‚f(x; s) is an arbitrary element of the subdiï¬€erential. The model (6) satisï¬es
conditions (C.i)â€“(C.ii) by convexity.
Proximal point methods:
The stochastic proximal point method uses the â€œmodelâ€
fx(y; s) := f(y; s),
(7)
that is, the true function. The model (7) satisï¬es all the conditions we provide.
Truncated models:
The ï¬rst condition beyond (C.i)â€“(C.ii) builds out of the simple obser-
vation that, if one is minimizing a nonnegative function (for example, in most machine learning
and statistical applications with a loss function), then a priori a model of the function that
takes negative values cannot be accurate. If f(x; s) â‰¥0 for all x, a better approximation to
f than the linear model (6) is to take fâ€²(x; s) âˆˆâˆ‚f(x; s) and deï¬ne
fx(y; s) :=

f(x; s) + âŸ¨fâ€²(x; s), y âˆ’xâŸ©

+ ,
5

where fâ€²(x; s) âˆˆâˆ‚f(x; s).
More generally, we may consider models that provide a lower
guarantee:
(C.iii) For all s âˆˆS, the models fx(Â·; s) satisfy
fx(y; s) â‰¥inf
zâˆˆX f(z; s).
Thus, if we are given an oracle that, for each ï¬xed s âˆˆS, can compute the minimal value
infzâˆˆX f(z; s), we may consider the truncated models
fx(y; s) := max

f(x; s) + âŸ¨fâ€²(x; s), y âˆ’xâŸ©, inf
zâˆˆX f(z; s)

.
(8)
See Figure 1(a) for an illustration of this model.
Many statistical, machine learning, and signal-processing examples support this model, be-
cause for any individual sample s we have infzâˆˆX f(z; s) = 0. For example, in linear regression,
s = (a, b) âˆˆRn Ã— R, and infz(âŸ¨a, zâŸ©âˆ’b)2 = 0 when a Ì¸= 0. In logistic regression [23], we have
s = (a, b) âˆˆRn Ã—{âˆ’1, 1}, and f(x; (a, b)) = log(1+exp(âˆ’bâŸ¨a, xâŸ©)) satisï¬es infz f(z; (a, b)) = 0
unless a = 0. Support vector machines use f(x; (a, b)) = [1 âˆ’bâŸ¨a, xâŸ©]+, which again has inï¬-
mal value 0. Even in more complex scenarios, these inï¬mal values may be easy to compute;
see, for example, our discussion of poisson regression in Section 3.3, Example 3.
Relatively accurate models:
Now we consider an additional condition on accuracy, which
allows less accurate models than the exact model (3); for example, this allows the bundle
model, which approximates f(Â·; s) by the maximum of aï¬ƒne lower bounds, to come. As we
see in the sequel (Theorem 1), this condition is suï¬ƒcient for strong stability and convergence
guarantees for any aProx method using the model-based updates (4). We require a bit more
notation. Let fx0(Â·; s) be a model centered at x0 satisfying Conditions (C.i)â€“(C.ii). For Î± > 0
deï¬ne
xÎ± := argmin
xâˆˆX

fx0(x; s) + 1
2Î± âˆ¥x âˆ’x0âˆ¥2
2

,
which is the result of a single update (leaving dependence on s implicit). Then we consider
(C.iv) For some Ïµ > 0, there exists a function C : S â†’R+ with E[C(S)] < âˆsuch that for
all x0 âˆˆX, the updated point xÎ± and model fx0(Â·; s) satisfy
f(xÎ±; s) â‰¤fx0(xÎ±; s) + 1 âˆ’Ïµ
2Î± âˆ¥xÎ± âˆ’x0âˆ¥2
2 + C(s)Î±.
The lower bound condition (C.ii) guarantees that f(x; s) â‰¥fx0(x; s) for all x, x0, so (C.iv)
provides a complementary upper bound. It is clear that the full (proximal point) model (7)
satisï¬es Condition (C.iv) with Ïµ = 1 and C(s) = 0, as fx = f. We term Condition (C.iv) a
â€œrelativeâ€ accuracy because the necessary approximation scales with Î± so that higher accuracy
is necessary as Î± â†“0, though âˆ¥xÎ± âˆ’x0âˆ¥= O(Î±) by standard results [18].
While aside from proximal-point models, it is not clear a priori how to guarantee that
Condition (C.iv) holds, one approach is to use bundle methods [25, 56], identical to Kelleyâ€™s
cutting plane method [29].
In this situation, we begin from the linear model f0
x(y; s) =
f(x; s) + âŸ¨fâ€²(x; s), y âˆ’xâŸ©, iteratively construct the lower piecewise-linear models
xi
Î± := argmin
yâˆˆX

fiâˆ’1
x
(y) + 1
2Î± âˆ¥y âˆ’xâˆ¥2
2

and fi
x(y) := max

fiâˆ’1
x
(y; s), f(xi
Î±; s) + âŸ¨fâ€²(xi
Î±; s), y âˆ’xi
Î±âŸ©
	
.
(9)
6

Whenever the iterate xi
Î± satisï¬es Condition (C.iv), we may terminate the iteration, as fiâˆ’1
x
(y)
satisï¬es Conditions (C.i)â€“(C.ii) by construction. While we do not address this in this pa-
per, the number of iterations to achieve a solution satisfying Condition (C.iv) is at most
O(âˆ¥fâ€²(x0; s)âˆ¥2), as each step solves a strongly convex optimization problem (see [56, Sec. 2.4]).
3
Stability and its consequences
The ï¬rst of our main thrusts, upon which we focus in this section, is the stability and bound-
edness of the iterates for stochastic proximal point methods and their relatives in the aProx
family of methods. These stability guarantees are in strong contrast to standard stochastic
subgradient methods, which may diverge for problems on which aProx methods converge.
We begin with our deï¬nition of stability.
In this deï¬nition, we let A denote the set
of positive stepsize sequences {Î±k} with P
k Î±2
k < âˆ. We call a pair (F, P) a collection of
problems if P is a collection of probability measures on a sample space S, and F is a collection
of functions f : X Ã— S â†’R, where f(Â·; s) is convex. We make the following deï¬nition.
Deï¬nition 3.1. An algorithm generating iterates xk according to the model-based update (4)
is stable in probability for the collection of problems (F, P) if for all f âˆˆF and P âˆˆP deï¬ning
F(x) = EP [f(x; S)] and X â‹†= argminxâˆˆX F(x), and for all stepsize sequences {Î±k} âˆˆA
sup
k
dist(xk, X â‹†) < âˆwith probability 1.
(10)
Classical results coupled with the Robbins-Siegmund supermartingale convergence theo-
rem [49] guarantee the stochastic subgradient method satisï¬es supk dist(xk, X â‹†) < âˆwhen-
ever
E
hfâ€²(x; S)
2
2
i
â‰¤C0 + C1 dist(x, X â‹†)2
for all x âˆˆX,
which typical smoothness or boundedness conditions imply (cf. [49, 47, 5]). Stochastic (ap-
proximate) proximal point approaches allow us to move beyond these quadratic growth as-
sumptions; in contrast, for objectives for which the gradients grow more than quadratically,
condition (10) typically fails for gradient methods:
Example 1 (Divergence for non-quadratics):
Let F(x) = 1
4x4, and consider any sequence of
stepsizes Î±k > 0 satisfying Î±k+1 â‰¥1
4Î±k for all k, and let xk+1 = xk âˆ’Î±kF â€²(xk) be generated
by the gradient method. Then whenever the initial iterate x1 satisï¬es |x1| â‰¥
p
3/Î±1, we have
|xk| â‰¥2|xkâˆ’1| for all k, so |xk| â‰¥2k|x1| for all k âˆˆN. 3
When the objective grows faster than polynomially, even worse behavior is possible; for ex-
ample, for the objective F(x) = (ex +eâˆ’x), for any polynomially decreasing stepsize sequence,
if x1 is large enough we have the super-exponential divergence |xk| â‰¥22k|x1|.
Standard
stochastic gradient methods may also exhibit undesirable behavior for easier problems; even
in situations for which the objective is smooth and in which there is no noise, (sub)gradient
methods may suï¬€er transient exponential growth, as the following example demonstrates.
Example 2 (Instability for quadratics):
Let F(x) = 1
2x2. Then the gradient method iterates
xk+1 = (1âˆ’Î±k)xk. Let us assume that Î±k = Î±0kâˆ’Î², and let Î±0 â‰¥3KÎ² for some K âˆˆN. Then
assuming x1 Ì¸= 0, for all k â‰¤K we have |xk+1| = |1 âˆ’Î±k||xk| â‰¥2|xk|, so that |xk+1| â‰¥2k for
k â‰¤K. Classical guarantees for the (stochastic) gradient method show that xk will converge
eventually, but even for smooth quadratics, gradient descent suï¬€ers from exponential growth
behavior if the stepsize is mis-speciï¬ed. 3
While stylized, these examples highlight the diï¬ƒculty of naive application of gradient
methods; in the sequel, we show how accurate models alleviate the issues in the examples.
7

3.1
Stability of (approximate) proximal methods
The starting point of almost all of what follows are suï¬ƒcient conditions on the models we
use to guarantee stability as in Deï¬nition 3.1. For this ï¬rst result, in addition to the two
conditions (C.i)â€“(C.ii), we assume the models are accurate at their updated points, that is,
Condition (C.iv). The full model (stochastic proximal point method) satisï¬es these conditions,
and so too do bundle models (9). In the theorem, recall the Ïƒ-ï¬eld Fk := Ïƒ(S1, . . . , Sk).
Theorem 1. Let Assumption A1 hold and xk be generated by the iteration (4) with any model
satisfying Conditions (C.i)â€“(C.ii) and (C.iv). Then for all xâ‹†âˆˆX â‹†,
E
h
âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 | Fkâˆ’1
i
â‰¤âˆ¥xk âˆ’xâ‹†âˆ¥2
2 + Î±2
k
Ïƒ2
Ïµ + E[C(S)]

.
Before providing the proof of the theorem (see Section 3.1.1), we present a few of its conse-
quences for stability. By taking xâ‹†to be the projection of xk onto X â‹†, Theorem 1 implies
E[dist(xk+1, X â‹†)2 | Fkâˆ’1] â‰¤dist(xk, X â‹†)2 + Î±2
k
Ïƒ2
Ïµ + E[C(S)]

.
A few somewhat more consequential corollaries, at least from the perspective of our stability
Deï¬nition 3.1, follow. We ï¬rst have that the expected distance is non-divergent.
Corollary 3.1. Let the conditions of Theorem 1 hold. For each k âˆˆN,
E

dist(xk+1, X â‹†)2
â‰¤E

dist(x1, X â‹†)2
+
Ïƒ2
Ïµ + E[C(S)]

k
X
i=1
Î±2
i .
A second corollary establishes that the iterates of appropriately accurate aProx methods
are stable. We require the Robbins-Siegmund almost supermartingale convergence lemma.
Lemma 3.1 ([49]). Let Ak, Bk, Ck, Dk â‰¥0 be non-negative random variables adapted to
the ï¬ltration Fk and satisfying E[Ak+1 | Fk] â‰¤(1 + Bk)Ak + Ck âˆ’Dk. Then on the event
{P
k Bk < âˆ, P
k Ck < âˆ}, there is a random Aâˆ< âˆsuch that Ak
a.s.
â†’Aâˆand P
k Dk < âˆ.
By applying Theorem 1 with Ak = dist(xk+1, X â‹†)2, Ck = Î±2
k+1(Ïƒ2/Ïµ + E[C(S)]), and Bk =
Dk = 0 in Lemma 3.1, we have
Corollary 3.2. Let the conditions of Theorem 1 hold and assume P
k Î±2
k < âˆ. Then
sup
kâˆˆN
dist(xk, X â‹†) < âˆ
and dist(xk, X â‹†) converges to some ï¬nite value with probability 1.
Combining Corollaries 3.1 and 3.2, we see that the stochastic proximal point method
and its aProx variantsâ€”as long as they satisfy the accuracy condition (C.iv)â€”are stable
according to Deï¬nition 3.1. This is in strong contrast to stochastic gradient methods and
their relatives, which can be unstable even for relatively simple problems.
8

3.1.1
Proof of Theorem 1
We now return to the promised proof of Theorem 1. In giving the proof, we present lemmas on
the progress of individual iterates for subsequent use. These results are typical of Lyapunov-
type arguments for convergence of stochastic gradient methods [66, 42].
Lemma 3.2. Let h be convex and subdiï¬€erentiable on a closed convex set X and let Î² > 0.
Then for all x0, x1, y âˆˆX, and hâ€²(y) âˆˆâˆ‚h(y),
h(y) âˆ’h(x1) â‰¤âŸ¨hâ€²(y), y âˆ’x0âŸ©+ 1
2Î² âˆ¥x1 âˆ’x0âˆ¥2 + Î²
2
hâ€²(y)
2
Proof
By the ï¬rst-order conditions for convexity, we have
h(y) âˆ’h(x1) â‰¤âŸ¨hâ€²(y), y âˆ’x1âŸ©= âŸ¨hâ€²(y), y âˆ’x0âŸ©+ âŸ¨hâ€²(y), x0 âˆ’x1âŸ©
â‰¤âŸ¨hâ€²(y), y âˆ’x0âŸ©+ 1
2Î² âˆ¥x1 âˆ’x0âˆ¥2 + Î²
2
hâ€²(y)
2 ,
where the second line uses Youngâ€™s inequality.
We also have the following lemma, which gives a one-step progress guarantee for any
algorithm using models satisfying Conditions (C.i)â€“(C.ii).
Lemma 3.3. Let Condition (C.i) hold. In each step of the method (4), for any x âˆˆX,
1
2 âˆ¥xk+1 âˆ’xâˆ¥2
2 â‰¤1
2 âˆ¥xk âˆ’xâˆ¥2
2 âˆ’Î±k [fxk(xk+1; Sk) âˆ’fxk(x; Sk)] âˆ’1
2 âˆ¥xk âˆ’xk+1âˆ¥2
2 .
Proof
By the ï¬rst-order conditions for convex optimization, for some gk âˆˆâˆ‚fxk(xk+1; Sk)
we have that âŸ¨Î±kgk + (xk+1 âˆ’xk), y âˆ’xk+1âŸ©â‰¥0 for all y âˆˆX. Setting y = x, we obtain
Î±kâŸ¨gk, xk+1 âˆ’xâŸ©â‰¤âŸ¨xk+1 âˆ’xk, x âˆ’xk+1âŸ©= 1
2
h
âˆ¥xk âˆ’xâˆ¥2
2 âˆ’âˆ¥xk+1 âˆ’xâˆ¥2
2 âˆ’âˆ¥xk+1 âˆ’xkâˆ¥2
2
i
.
As fxk(x; Sk) â‰¥fxk(xk+1; Sk) + âŸ¨gk, x âˆ’xk+1âŸ©by Condition (C.i), this gives the result.
With Lemmas 3.2 and 3.3 in place, we can prove the theorem. Let xâ‹†âˆˆX â‹†be an otherwise
arbitrary optimal point. Applying Lemma 3.3 with x = xâ‹†, we have
1
2 âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 â‰¤1
2 âˆ¥xk âˆ’xâ‹†âˆ¥2
2 âˆ’Î±k [fxk(xk+1; Sk) âˆ’fxk(xâ‹†; Sk)] âˆ’1
2 âˆ¥xk âˆ’xk+1âˆ¥2
2
(i)
â‰¤1
2 âˆ¥xk âˆ’xâ‹†âˆ¥2
2 âˆ’Î±k [f(xk+1; Sk) âˆ’fxk(xâ‹†; Sk)] âˆ’Ïµ
2 âˆ¥xk âˆ’xk+1âˆ¥2
2 + C(Sk)Î±2
k
(ii)
â‰¤1
2 âˆ¥xk âˆ’xâ‹†âˆ¥2
2 âˆ’Î±k [f(xk+1; Sk) âˆ’f(xâ‹†; Sk)] âˆ’Ïµ
2 âˆ¥xk âˆ’xk+1âˆ¥2
2 + C(Sk)Î±2
k,
where inequality (i) is a consequence of the accurate model condition (C.iv) and (ii) because
fx(xâ‹†; s) â‰¤f(xâ‹†; s) by the lower model condition (C.ii). Now, we apply Lemma 3.2 with
x1 = xk+1, x0 = xk, y = xâ‹†, and Î² = Î±k
Ïµ to ï¬nd
1
2 âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 â‰¤1
2 âˆ¥xk âˆ’xâ‹†âˆ¥2
2 + Î±kâŸ¨fâ€²(xâ‹†; Sk), xâ‹†âˆ’xkâŸ©+ Î±2
k
2Ïµ
fâ€²(xâ‹†; Sk)
2
2 + C(Sk)Î±2
k
for all fâ€²(xâ‹†; Sk) âˆˆâˆ‚f(xâ‹†; Sk).
9

For some F â€²(xâ‹†) âˆˆâˆ‚F(xâ‹†), we have âŸ¨F â€²(xâ‹†), y âˆ’xâ‹†âŸ©â‰¥0 for all y âˆˆX. As our choice of
fâ€²(xâ‹†; s) âˆˆâˆ‚f(xâ‹†; s) above was arbitrary, we may take fâ€²(xâ‹†; Sk) so that E[fâ€²(xâ‹†; Sk)] = F â€²(xâ‹†)
for any desired F â€²(xâ‹†) âˆˆâˆ‚F(xâ‹†) (cf. [6]). Thus, taking expectations with respect to Fkâˆ’1,
1
2E[âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 | Fkâˆ’1] â‰¤1
2 âˆ¥xk âˆ’xâ‹†âˆ¥2
2+Î±2
k
2Ïµ E
hfâ€²(xâ‹†; S)
2
2
i
+E[C(S)]Î±2
k+Î±kâŸ¨F â€²(xâ‹†), xâ‹†âˆ’xkâŸ©.
As âŸ¨F â€²(xâ‹†), xâ‹†âˆ’xkâŸ©â‰¤0, we obtain the theorem.
3.2
Convergence of aProx methods
The key consequence of Theorem 1 is that the iterates xk are stable in probability (10),
remaining bounded with probability 1. Iterate boundedness of algorithms does not guaran-
tee convergence in general; however, in this section, we show that as a consequence of this
boundedness, any algorithm satisfying Conditions (C.i)â€“(C.ii) is convergent. Assumptions A1
and A2 are insuï¬ƒcient to guarantee convergence of subgradient methods, whichâ€”as our ex-
amples showâ€”may diverge without uniform boundedness conditions on the subgradients. In
contrast, any aProx method that is stable in probability (10) guarantees convergence with
probability 1. Throughout this section and Section 3.3, we without comment make the as-
sumption that the stepsizes Î±k satisfy the standard summability conditions
Î±k > 0 for all k,
âˆ
X
k=1
Î±k = âˆ,
and
âˆ
X
k=1
Î±2
k < âˆ.
Proposition 1. Let Assumptions A1 and A2 hold.
Let the iterates xk be generated by
any method satisfying Conditions (C.i)â€“(C.ii) and F â‹†= infxâˆˆX F(x).
On the event that
supk dist(xk, X â‹†) < âˆ, with probability one both P
k Î±k(F(xk) âˆ’F â‹†) < âˆ, and the iterates
xk are convergent: there exists xâ‹†âˆˆX â‹†such that âˆ¥xk âˆ’xâ‹†âˆ¥a.s.
â†’0 and F(xk) a.s.
â†’F(xâ‹†).
Proposition 1 implies an asymptotic convergence rate on (weighted averages of) the iterates
xk. Indeed, let {Î³k}âˆ
k=1 âŠ‚R+ be a non-decreasing sequence with Î³k > 0, and Î³k â†’âˆ.
Deï¬ning the weighted averages xk = Pk
i=1 Î³iÎ±ixi/(Pk
i=1 Î³iÎ±i), we have
Corollary 3.3. Let the conditions of Proposition 1 hold. Then with probability 1,
lim
kâ†’âˆ
1
Î³k

k
X
i=1
Î³iÎ±i

[F(xk) âˆ’F â‹†] = 0.
Proof
We have (Pk
i=1 Î³iÎ±i)(F(xk) âˆ’F â‹†) â‰¤Pk
i=1 Î³iÎ±i(F(xi) âˆ’F â‹†) by Jensenâ€™s inequality.
Kroneckerâ€™s lemma gives the result.
For example, taking Î³k = Î±âˆ’1
k , we obtain that the average xk = 1
k
Pk
i=1 xi satisï¬es
kÎ±k (F(xk) âˆ’F â‹†) a.s.
â†’0.
To prove Proposition 1, we present a lemma giving a one-step progress guarantee for any
method satisfying Conditions (C.i)â€“(C.ii).
Lemma 3.4. Let Conditions (C.i)â€“(C.ii) hold and let xk be generated by the updates (4).
Then for any x âˆˆX,
1
2 âˆ¥xk+1 âˆ’xâˆ¥2
2 â‰¤1
2 âˆ¥xk âˆ’xâˆ¥2
2 âˆ’Î±k[f(xk; Sk) âˆ’f(x; Sk)] + Î±2
k
2
fâ€²(xk; Sk)
2
2 .
10

Proof
Using Lemma 3.3, it suï¬ƒces to show that for any Î± > 0 and x0, x1, x âˆˆX
âˆ’Î±[fx0(x1; s) âˆ’fx0(x; s)] âˆ’1
2 âˆ¥x1 âˆ’x0âˆ¥2
2 â‰¤âˆ’Î±[f(x0; s) âˆ’f(x; s)] + Î±2
2
fâ€²(x0; s)
2
2 .
To see this, recall that Conditions (C.i)â€“(C.ii) imply the containment (5), which in turn
implies
âˆ’fx0(x1; s) + fx0(x; s) = âˆ’[fx0(x0; s) âˆ’fx0(x; s)] + fx0(x0; s) âˆ’fx0(x1; s)
â‰¤âˆ’[fx0(x0; s) âˆ’fx0(x; s)] + âŸ¨fâ€²(x0; s), x0 âˆ’x1âŸ©
(C.ii)
â‰¤
âˆ’[f(x0; s) âˆ’f(x; s)] + âŸ¨fâ€²(x0; s), x0 âˆ’x1âŸ©.
Then we use that for any vector v, Î±âŸ¨v, âˆ†âŸ©âˆ’1
2 âˆ¥âˆ†âˆ¥2
2 â‰¤Î±2
2 âˆ¥vâˆ¥2
2, which gives the result.
Proof of Proposition 1
By Assumption A2, E[âˆ¥fâ€²(x; S)âˆ¥2] â‰¤Gbig(r) for all x such that
dist(x, X â‹†) â‰¤r. Lemma 3.4 implies that for any xâ‹†âˆˆX â‹†,
E[âˆ¥xk+1 âˆ’xâ‹†âˆ¥2 | Fkâˆ’1] â‰¤âˆ¥xk âˆ’xâ‹†âˆ¥2 âˆ’2Î±k(F(xk) âˆ’F â‹†) + Î±2
kGbig(dist(xk, X â‹†)).
(11)
On the event that supk dist(xk, X â‹†) < âˆ, we have P
k Î±2
kGbig(dist(xk, X â‹†)) < âˆ, and so the
Robbins-Siegmund Lemma 3.1 implies for any xâ‹†âˆˆX â‹†, there is some (random) V (xâ‹†) < âˆ
such that âˆ¥xk âˆ’xâ‹†âˆ¥a.s.
â†’V (xâ‹†), and P
k Î±k(F(xk) âˆ’F(xâ‹†)) < âˆ.
We now show that dist(xk, X â‹†) a.s.
â†’0. Letting xâ‹†be the projection of xk onto X â‹†, inequal-
ity (11) gives E[dist(xk+1, X â‹†)2 | Fkâˆ’1] â‰¤dist(xk, X â‹†)2âˆ’2Î±k(F(xk)âˆ’F â‹†)+Î±2
kGbig(dist(xk, X â‹†)).
Thus using Lemma 3.1, there exists a random Dâˆ< âˆsuch that dist(xk, X â‹†) a.s.
â†’Dâˆ. For
Ïµi âˆˆR+, deï¬ne the gap function
Î“(xâ‹†, Ïµ1, Ïµ2) := inf
xâˆˆX {F(x) âˆ’F â‹†| Ïµ1 â‰¤âˆ¥x âˆ’xâ‹†âˆ¥â‰¤4Ïµ1, dist(x, X â‹†) â‰¥Ïµ2} .
The set {x âˆˆX | âˆ¥x âˆ’xâ‹†âˆ¥âˆˆ[Ïµ1, 4Ïµ1], dist(x, X â‹†) â‰¥Ïµ2} is compact, so the inï¬mum in the
deï¬nition of the gap Î“ is attained and Î“(xâ‹†, Ïµ1, Ïµ2) > 0 for Ïµi > 0. When the a.s. limits V (xâ‹†)
and Dâˆare positive, there exists a (random) K < âˆsuch that âˆ¥xk âˆ’xâ‹†âˆ¥âˆˆ[V (xâ‹†)/2, 2V (xâ‹†)]
and dist(xk, X â‹†) â‰¥Dâˆ/2 for all k â‰¥K. Thus with probability 1,
âˆ>
X
k
Î±k(F(xk) âˆ’F â‹†) â‰¥
X
kâ‰¥K
Î±kÎ“(xâ‹†, V (xâ‹†)/2, Dâˆ/2).
As P
k Î±k = âˆand Î“(xâ‹†, Ïµ1, Ïµ2) > 0 for Ïµi > 0, we have Dâˆ= 0 with probability 1.
Finally, we show the sequence xk converges. We begin by noting that V (xâ‹†) = limk âˆ¥xk âˆ’xâ‹†âˆ¥
is 1-Lipschitz. Indeed, let xâ‹†
1, xâ‹†
2 âˆˆX â‹†. By the a.s. limit deï¬nition of V ,
|V (xâ‹†
1) âˆ’V (xâ‹†
2)| â‰¤lim sup
k
|âˆ¥xk âˆ’xâ‹†
1âˆ¥âˆ’âˆ¥xk âˆ’xâ‹†
2âˆ¥| â‰¤âˆ¥xâ‹†
1 âˆ’xâ‹†
2âˆ¥.
Let us now show that for some xâ‹†âˆˆX â‹†, we have V (xâ‹†) = 0. Let B = {x âˆˆRn | âˆ¥xâˆ¥â‰¤1}
be the ball. As V (xâ‹†) < âˆ, there exists a (random) r < âˆsuch that the xk âˆˆrB for all k.
As dist(xk, X â‹†) a.s.
â†’0, the compactness of B implies that rB âˆ©X â‹†Ì¸= âˆ…. Let xâ‹†âˆˆrB âˆ©X â‹†. For
any x âˆˆrB, the projection Ï€(x) of x onto X â‹†satisï¬es âˆ¥Ï€(x) âˆ’xâˆ¥â‰¤âˆ¥xâ‹†âˆ’xâˆ¥â‰¤2r, and so
11

âˆ¥Ï€(x)âˆ¥â‰¤3r, and deï¬ning the set C = 3rB, we have dist(xk, X â‹†) = dist(xk, X â‹†âˆ©C) for all k.
Now, ï¬x Ïµ > 0, and let {xâ‹†
i }N
i=1 be an Ïµ-net of C âˆ©X â‹†, where N < âˆ. As xk âˆˆC for all k,
min
iâˆˆ[N] âˆ¥xk âˆ’xâ‹†
i âˆ¥âˆ’Ïµ â‰¤dist(xk, X â‹†âˆ©C) = dist(xk, X â‹†) a.s.
â†’0,
and miniâˆˆ[N] âˆ¥xk âˆ’xâ‹†
i âˆ¥a.s.
â†’miniâˆˆ[N] V (xâ‹†
i ). Thus for any Ïµ > 0, there exists xÏµ âˆˆX â‹†âˆ©C such
that V (xÏµ) â‰¤Ïµ, so that infxâˆˆCâˆ©X â‹†V (x) = 0. By the continuity of V , the inï¬mum is attained
and there is xâ‹†such that V (xâ‹†) = 0. The sequence xk thus converges to this xâ‹†âˆˆX â‹†, and
F(xk) a.s.
â†’F(xâ‹†) = F â‹†.
3.3
Asymptotic normality
Without additional conditions, it is challenging to provide more precise convergence rate
guarantees than those of Corollary 3.3, which (at best) provides an asymptotic rate scaling
as 1/(Î±kk) for Î±k â‰kâˆ’Î².
With this in mind, we introduce an additional assumption of
smoothness and strong convexity in a neighborhood of the optimal point, and we consider
distributional convergence to establish asymptotic normality of the averaged iterates, extend-
ing results of Polyak and Juditsky [47]. In most analyses of stochastic convex optimization
problems yielding asymptotic normality, typical assumptions are that the random functions
f(Â·; s) have globally Lipschitz gradients (e.g. [47, Sec. 5], [22, 32, 51]), which is reasonable
when X is compact, but may fail for non-compact X. In contrast, we consider
Assumption A3. The functions F and f satisfy the following.
(i) The function F is C2 in a neighborhood of xâ‹†= argminxâˆˆX F(x), and âˆ‡2F(xâ‹†) â‰»0.
(ii) There exists Ïµ > 0 such that f(Â·; s) is L(s)-smooth on the set X â‹†
Ïµ := {x | âˆ¥x âˆ’xâ‹†âˆ¥â‰¤Ïµ},
meaning that x 7â†’âˆ‡f(x; s) is L(s) Lipschitz on X â‹†
Ïµ , and E[L(S)2] = L2 < âˆ.
Assumption A3 says that in a neighborhood of xâ‹†, the random functions f have Lip-
schitz gradients with probability 1.
We will apply Assumption A3 in conjunction with
Assumption A2, which enforces a type of local Lipschitz continuity of f and F.
Typ-
ical Lipschitz conditions on âˆ‡f imply Assumption A2:
if âˆ‡f(Â·; s) is Lr(s) Lipschitz on
X â‹†
r = {x âˆˆX | âˆ¥x âˆ’xâ‹†âˆ¥â‰¤r}, where the smoothness constant Lr is square integrable
for ï¬nite r, Assumption A2 holds whenever E[âˆ¥âˆ‡f(xâ‹†; S)âˆ¥2] < âˆ. Indeed,
âˆ¥âˆ‡f(x; s)âˆ¥â‰¤âˆ¥âˆ‡f(xâ‹†; s)âˆ¥+ âˆ¥âˆ‡f(xâ‹†; s) âˆ’âˆ‡f(x; s)âˆ¥â‰¤âˆ¥âˆ‡f(xâ‹†; s)âˆ¥+ Lr(s) âˆ¥x âˆ’xâ‹†âˆ¥,
so we have the moment bound Gbig(r) â‰¤2E[âˆ¥âˆ‡f(xâ‹†; S)âˆ¥2]+2E[Lr(S)2]r2. To further motivate
Assumption A3, we provide a brief example.
Example 3 (Poisson regression):
In problems with count data b1, b2, . . . , bm âˆˆN, we may
wish to predict counts based on a covariate vector ai âˆˆRn. A standard model is poisson
regression, a generalized linear model [38, 23], where we model b âˆˆN conditional on a âˆˆRn
as coming from a poisson distribution with parameter Î» = eâŸ¨a,xâŸ©so p(b | a, x) = eâˆ’Î»Î»b/b!.
The negative log likelihood is f(x; (a, b)) = âˆ’log p(b | a, x) = log(b!) + exp(âŸ¨a, xâŸ©) âˆ’bâŸ¨a, xâŸ©,
and it is easy to compute the truncated model (8), as f satisï¬es
inf
z f(x; (a, b)) = log(b!) + inf
t {et âˆ’bt} = log(b!) + b âˆ’b log b.
12

Because fâ€²(x; (a, b)) = aeâŸ¨a,xâŸ©âˆ’ba, using |et âˆ’es| â‰¤emax{s,t}|s âˆ’t| shows that f satisï¬es
Assumption A3 as soon as we have the covariance condition Cov(a) â‰»0 and E[erâˆ¥aâˆ¥2] < âˆ
for r < âˆ. Such moment conditions are not suï¬ƒcient for SGM to converge. 3
We have the following asymptotic normality theorem; we present the proof in Appendix A.
Theorem 2. Let Assumptions A1â€“A3 hold. Let the iterates xk be generated by any method
satisfying Conditions (C.i)â€“(C.ii) with stepsizes Î±k = Î±0kâˆ’Î² for some Î² âˆˆ(1
2, 1) and Î±0 > 0.
Assume additionally that the iterates are bounded: with probability 1, supk âˆ¥xkâˆ¥< âˆ. Then
1
âˆš
k
k
X
i=1
(xi âˆ’xâ‹†) dâ†’N
 0, âˆ‡2F(xâ‹†)âˆ’1 Cov(âˆ‡f(xâ‹†; S))âˆ‡2F(xâ‹†)âˆ’1
.
To prove Theorem 2, we use two main insights. The ï¬rst is that, if the iterates remain
bounded, then Proposition 1 guarantees convergence. The second is a gradient approximation
result that shows that even if the models in the iterations (4) are non-smooth, they locally
behave as ï¬rst-order Taylor approximations to the functions f, and thus eventually the iterates
approximate the stochastic gradient method on quadratics.
From this, we can apply the
techniques of Polyak and Juditsky [47] to guarantee asymptotic normality.
The asymptotic convergence guarantee in Theorem 2 is unimprovable. It achieves the
local asymptotic minimax bound for stochastic optimization [20] (the analogue of the stan-
dard Fisher information in classical statistical problems [33, 60, Sec. 8.7]). In contrast to
stochastic gradient schemes, however, Theorem 2 requires essentially only that the iterates
remain bounded. In the end, then, the important consequence of the aProx family is that,
with appropriately accurate models, we can guarantee stability (Deï¬nition 3.1). By leverag-
ing these stability guarantees, we can then show that model-based iteration schemes (4) are
(asymptotically) optimal.
4
Fast convergence for easy problems
The stability and asymptotic results in Section 3 provide some evidence for the beneï¬ts
of using better models within stochastic optimization problems: if the models are accurate
enough that the iterates remain bounded, then we obtain asymptotic optimality results even
when standard gradient methods diverge. In this section, we study a diï¬€erent collection of
problems, which we term easy optimization problems. More precisely, we say that a stochastic
minimization problem is easy if there are shared global minimizers:
Deï¬nition 4.1. Let F(x) := EP [f(x; S)]. Then F is easy to optimize if for each xâ‹†âˆˆX â‹†:=
argminxâˆˆX F(x) and P-almost all s âˆˆS we have
inf
xâˆˆX f(x; s) = f(xâ‹†; s).
Deï¬nition 4.1 places strong restrictions on the class of functions we consider, but a number
of examples satisfy its conditions. Other researchers have considered similar conditions to
Deï¬nition 4.1; for example, Schmidt and Le Roux [52] study stochastic optimization problems
of the form F(x) =
1
m
Pm
i=1 fi(x) where âˆ‡fi(xâ‹†) = 0 for all i. We brieï¬‚y enumerate a few
examples to motivate what follows, returning in Section 4.3 to ï¬‚esh them out fully.
Example 4 (Overdetermined linear systems):
In an overdetermined linear system, we
have a matrix A âˆˆRmÃ—n, with m â‰¥n, and we wish to solve Ax = b, where we assume
13

the system of equalities is feasible. Letting ai âˆˆRn denote the rows of A, both objectives
F(x) =
1
2m âˆ¥Ax âˆ’bâˆ¥2
2 and F(x) = 1
m âˆ¥Ax âˆ’bâˆ¥1 satisfy Deï¬nition 4.1, where we take samples
s = (ai, bi) âˆˆRn Ã— R, and any solution xâ‹†to Ax = b satisï¬es f(xâ‹†; (ai, bi)) = 0 for all i. See
Section 4.3.1.
3
Example 5 (Finding a point in the intersection of convex sets):
Let C1, C2, . . . , Cm be
closed convex sets with non-empty intersection X â‹†:= âˆ©m
i=1Ci. Then the objective F(x) =
1
m
Pm
i=1 dist(x, Ci) is convex, and treating the sample space S = {1, . . . , m}, we have F(x) = 0
and f(x; i) := dist(x, Ci) = 0 for all i if and only if x âˆˆX â‹†. See Section 4.3.2.
3
Example 6 (Data interpolation):
A more involved example arises out of recent results in
statistical machine learning. In this area, substantial recent success in deep learning [34, 64]
arises out of models that ï¬t a training sample of data perfectly. In settings more amenable
to analysis, Belkin et al. [3, 4] study statistical algorithms that minimize convex losses and
perfectly interpolate the data, that is, given a sample {S1, . . . , Sm} drawn i.i.d. from an
underlying population, they ï¬nd points xâ‹†satisfying f(xâ‹†; Si) = infx f(x; Si) for each i =
1, . . . , m. See Section 4.3.3.
3
Given Examples 4, 5, and 6, it is of interest to investigate the aProx family for problems
satisfying Deï¬nition 4.1. In this case, we show that any model satisfying the local approxima-
tion conditions (C.i)â€“(C.ii) and the additional lower bound condition (C.iii) possesses strong
adaptivity and convergence properties. This is in contrast to subgradient methods, which
(given a precise stepsize choice) can exhibit fast convergence, but are typically non-adaptive.
To highlight the types of models we consider, without loss of generality, we may assume
that infxâˆˆX f(x; s) = 0, as given an oracle that provides the value inf f(Â·; s) we can replace
f with f(Â·; s) âˆ’inf f(Â·; s). As we discuss in Section 2 and Example 3, it is frequently easy to
compute the inï¬mum infz f(z; s) for an individual sample s. The results in this section thus
apply to the lower-truncated model (8),
fx(y; s) :=

f(x; s) + âŸ¨fâ€²(x; s), y âˆ’xâŸ©

+ .
The updates for this model are easy to compute when X = Rn; indeed, in this case, the
guarded model (8) yields the update
xk+1 = xk âˆ’min
(
Î±k,
f(xk; Sk)
âˆ¥fâ€²(xk; Sk)âˆ¥2
2
)
fâ€²(xk; Sk).
This update is reminiscent of the classical Polyak subgradient method [46], which chooses
â€œoptimalâ€ stepsizes in the subgradient method when the value F(xâ‹†) is known.
In the remainder of this section, we analyze the performance of the aProx family of models
on easy problems, showing that in a number of settings, these methods even enjoy linear
convergence. The starting point of each of our results is the following lemma, whose proof we
defer to Section B.1, that shows that if a problem has shared minimizers as in Deï¬nition 4.1,
iterates of any method satisfying the lower bound condition (C.iii) are guaranteed to make
progress toward the optimal set.
Lemma 4.1. Let F be easy to optimize (Deï¬nition 4.1). Let xk be generated by the updates (4)
using a model satisfying Conditions (C.i)â€“(C.iii). Then for any xâ‹†âˆˆX â‹†,
1
2 âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 â‰¤1
2 âˆ¥xk âˆ’xâ‹†âˆ¥2
2 âˆ’1
2[f(xk; Sk) âˆ’f(xâ‹†; Sk)] min
(
Î±k, f(xk; Sk) âˆ’f(xâ‹†; Sk)
âˆ¥fâ€²(xk; Sk)âˆ¥2
2
)
.
14

In the next two sections, we use this lemma to derive conditions on the growth of F that we
can leverage for fast convergence of aProx models. We return to our examples in Section 4.3,
demonstrating that common problems satisfy the assumptions in Sections 4.1 and 4.2.
4.1
Sharp growth with shared minimizers
For our ï¬rst set of problems, we consider objectives that exhibit sharp growth away from the
optimal set X â‹†; classical and recent optimization literature highlights the importance of such
conditions for the convergence of deterministic optimization methods [12, 18]. As we shall see,
these conditions are suï¬ƒcient to guarantee linear convergence of aProx models in stochastic
settings. We begin with the following assumption, which we use for its ease of applicability
following the structure of the progress guarantee in Lemma 4.1.
Assumption A4 (Expected sharp growth). There exist constants Î»0, Î»1 > 0 such that for
all Î± âˆˆR+ and x âˆˆX and xâ‹†âˆˆX â‹†,
E
"
min
(
Î±[f(x; S) âˆ’f(xâ‹†; S)], (f(x; S) âˆ’f(xâ‹†; S))2
âˆ¥fâ€²(x; S)âˆ¥2
2
)#
â‰¥dist(x, X â‹†) min {Î»0Î±, Î»1 dist(x, X â‹†)} .
While Assumption A4 is somewhat complex, a quick calculation shows that a for it to hold,
it is suï¬ƒcient that there exist constants Î» > 0 and p > 0 such that
P
 f(x; S) âˆ’f(xâ‹†; S) â‰¥Î» dist(x, X â‹†)

â‰¥p
and E[âˆ¥fâ€²(x; S)âˆ¥2
2] â‰¤M2 for x âˆˆX. That is, f(x; S) â‰¥f(xâ‹†; S) + Î» dist(x, X â‹†) with non-
zero probability, which is reasonably easy to check (for example, using the Paley-Zygmund
inequality and Mendelsonâ€™s small-ball conditions [39]).
Under these conditions, we have the following fast convergence guarantee for the aProx
family of methods; we provide the proof in Section B.2.
Proposition 2. Let F be easy to optimize, Assumption A4 hold, and xk be generated by
the stochastic iteration (4) using any model satisfying Conditions (C.i)â€“(C.iii), where the
stepsizes Î±k = Î±0kâˆ’Î² for some Î² âˆˆ(âˆ’âˆ, 1). Deï¬ne K0 :=

(Î»0Î±0/(Î»1 dist(x1, X â‹†)))1/Î²
.
Then
E[dist(xk+1, X â‹†)2] â‰¤
ï£±
ï£²
ï£³
exp

âˆ’Î»1 min{k, K0} âˆ’
Î»0
dist(x1X â‹†)
Pk
i=K0+1 Î±i

dist(x1, X â‹†)2
if Î² â‰¥0
exp

âˆ’Î»1 [k âˆ’K0]+ âˆ’
Î»0
dist(x1,X â‹†)
Pkâˆ§K0
i=1
Î±i

dist(x1, X â‹†)2
if Î² < 0
and with probability 1, we have the linear convergence
lim sup
kâ†’âˆ
dist(xk, X â‹†)2
(1 âˆ’Î»1)k
< âˆ.
Without careful stepsize choices, even non-stochastic subgradient methods do not achieve such
convergence guarantees. Indeed, consider the simple objective F(x) = |x|, which certainly sat-
isï¬es the sharp growth conditions, and apply the subgradient method xk+1 = xk âˆ’Î±k sign(xk)
(where we treat sign(0) = +1). Then the convergence can be no faster than O(Î±k); this is
the typical jamming behavior of subgradient methods. In contrast, Proposition 2 shows that
by leveraging the knowledge that infx F(x) = 0, we achieve linear convergence.
15

4.2
Quadratic growth with shared minimizers
As an alternative to sharp growth conditions, we also consider optimization problems that
exhibit quadratic growthâ€”strong convexityâ€”away from their optima. As is the case for the
sharpness conditions in Section 4.1, strong convexity conditions play an important role in the
analysis and implementation of methods for convex optimization [25, 46, 43, 18] as well as
stochastic optimization problems [24, 21]. It is thus of interest to develop an understanding
of the behavior of the aProx family of methods under strong convexity conditions, so that
we make the following assumption (which is slightly weaker than strong convexity).
Assumption A5 (Quadratic growth with shared minimizers). There exist constants Î»0, Î»1 >
0 such that for all x âˆˆX and Î± > 0,
E
"
(f(x; S) âˆ’f(xâ‹†; S)) min
(
Î±, f(x; S) âˆ’f(xâ‹†; S)
âˆ¥fâ€²(x; S)âˆ¥2
2
)#
â‰¥min {Î»0Î±, Î»1} dist(x, X â‹†)2.
Let us give more intuitive conditions suï¬ƒcient for A5 to hold. Suppose the standard condi-
tions that âˆ‡F is L-Lipschitz and that F has quadratic growth: F(x)âˆ’F(xâ‹†) â‰¥c0 dist(x, X â‹†)2.
In addition, assume there exist constants 0 < c, C < âˆ, p > 0 such that
P

âˆ¥âˆ‡f(x; S)âˆ¥2
2 â‰¤C âˆ¥âˆ‡F(x)âˆ¥2
2 and f(x; S) âˆ’f(xâ‹†; S) â‰¥c(F(x) âˆ’F(xâ‹†))

â‰¥p > 0.
The Lipschitz condition implies F(xâ‹†) â‰¤F(y) â‰¤F(x) + âŸ¨âˆ‡F(x), y âˆ’xâŸ©+ L
2 âˆ¥y âˆ’xâˆ¥2
2, and
setting y = x âˆ’1
Lâˆ‡F(x) gives F(x) âˆ’F(xâ‹†) â‰¥
1
2Lâˆ¥âˆ‡F(x)âˆ¥2
2 or
2L
âˆ¥âˆ‡F(x)âˆ¥2
2 â‰¥
1
F(x)âˆ’F(xâ‹†). Thus,
E
"
(f(x; S) âˆ’f(xâ‹†; S)) min
(
Î±, f(x; S) âˆ’f(xâ‹†; S)
âˆ¥fâ€²(x; S)âˆ¥2
2
)#
â‰¥cp (F(x) âˆ’F(xâ‹†)) min
(
Î±, F(x) âˆ’F(xâ‹†)
C âˆ¥âˆ‡F(x)âˆ¥2
2
)
â‰¥pcc0 min

Î±,
1
2CL

dist(x, X â‹†)2.
Under the quadratic growth Assumption A5, whenever the problem is easy (Deï¬nition 4.1)
Lemma 4.1 implies the following proposition, which gives nearly linear convergence of the
aProx family whenever the lower bound condition (C.iii) holds.
Proposition 3. Let Assumption A5 hold and xk be generated by the stochastic iteration (4)
by any model satisfying Conditions (C.i)â€“(C.iii), where the stepsizes Î±k = Î±0kâˆ’Î² for some
Î² âˆˆ(âˆ’âˆ, âˆ). Deï¬ne K0 =

(Î»0Î±0/Î»1)1/Î²
. If Î² â‰¥0, then
E[dist(xk+1, X â‹†)2] â‰¤exp
ï£«
ï£­âˆ’Î»1 min{k, K0} âˆ’Î»0
k
X
i=K0+1
Î±i
ï£¶
ï£¸dist(x1, X â‹†)2,
while if Î² < 0, then
E[dist(xk+1, X â‹†)2] â‰¤exp
 
âˆ’Î»1 [k âˆ’K0]+ âˆ’Î»0
K0âˆ§k
X
i=1
Î±i
!
dist(x1, X â‹†)2.
Proof
Under Assumption A5, the distance recursion for Dk := dist(xk, X â‹†) in Lemma 4.1
then becomes Dk â‰¤Dkâˆ’1 and
E[D2
k+1 | Fkâˆ’1] â‰¤max{1 âˆ’Î»0Î±k, 1 âˆ’Î»1}D2
k.
16

The claim follows by algebraic manipulations and that 1 âˆ’t â‰¤eâˆ’t for all t.
Under similar strong convexity assumptions, Schmidt and Le Roux [52] and Ma et al. [37]
show that stochastic gradient methods can achieve linear or near-linear convergence for easy
convex optimization problems. As is typical in the analysis of stochastic gradient methods,
however, this requires precise stepsize choices that reï¬‚ect typically unknown constants, such
as global Lipschitz conditions and the strong convexity parameter Î»0. In contrast, the aProx
family of methods is adaptive to the easiness of the problemâ€”achieving optimal asymptotic
behavior (as in Section 3) while providing strong ï¬nite-sample guarantees and nearly linear
convergence (Proposition 3) when problems satisfy strong growth conditions.
4.3
Examples of easy problems with strong growth
We now return to the three examples 4â€“6 ï¬tting our framework of easy (Deï¬nition 4.1)
problems with shared minimizers, exhibiting quantitative growth conditions for each.
4.3.1
Overdetermined linear systems and Kaczmarz algorithms
Kaczmarz algorithms [55, 35, 41, 40] for overdetermined linear systems, as in Example 4, are
eï¬€ective, solving feasible systems Ax = b (where A âˆˆRmÃ—n, m â‰¥n) using careful stochastic
gradient steps on the objective âˆ¥Ax âˆ’bâˆ¥2
2 to achieve fast convergence. We consider instead
the mean absolute error F(x) :=
1
m âˆ¥Ax âˆ’bâˆ¥1, which typically satisï¬es the sharp growth
condition A4, so that the aProx method (4) using the truncated model (8) achieves linear
convergence. Specialized Kaczmarz algorithms typically achieve slightly better convergence
rates [55, 41, 40], but in this case, we have the additional beneï¬t that the aProx methods
are adaptive: they still converge outside of linear systems.
We provide conditions suï¬ƒcient to demonstrate Assumption A4. Let the vectors ai âˆˆRn
be drawn independently from a distribution with âˆ¥aiâˆ¥2 â‰¤M, and assume for small c > 0
there exists pc > 0 such that P(|âŸ¨ai, vâŸ©| â‰¥c âˆ¥vâˆ¥2) â‰¥pc for v âˆˆRn. Letting bi = âŸ¨ai, xâ‹†âŸ©and
fi(x) = |âŸ¨ai, xâŸ©âˆ’bi|, we have the following lemma; see Appendix B.3 for its proof.
Lemma 4.2. Let the preceding conditions on the vectors ai hold. There exists a numerical
constant C < âˆsuch that for c > 0 and t â‰¥0, if we deï¬ne
Î»0 := c
 
pc âˆ’C
r
n + t
m
!
and Î»1 := c2
M2
 
pc âˆ’C
r
n + t
m
!
,
then with probability at least 1 âˆ’eâˆ’t over the randomness in the ai, simultaneously for all x
1
m
m
X
i=1
min
(
Î±[fi(x) âˆ’fi(xâ‹†)], (fi(x) âˆ’fi(xâ‹†))2
âˆ¥fâ€²
i(x)âˆ¥2
2
)
â‰¥âˆ¥x âˆ’xâ‹†âˆ¥2 min {Î»0Î±, Î»1 âˆ¥x âˆ’xâ‹†âˆ¥2} .
That is, with high probability over the choice of A âˆˆRmÃ—n, Assumption A4 holds with
parameters Î»0 and Î»1. As a more concrete example, suppose the vectors ai are uniform on
âˆšnÂ·Snâˆ’1, the sphere of radius âˆšn. Then P(|âŸ¨ai, vâŸ©| â‰¥1
2 âˆ¥vâˆ¥2) â‰¥1
2, so that Î»1 â‰³1/n with high
probability, and Proposition 2 implies that lim supk âˆ¥xk âˆ’xâ‹†âˆ¥2
2 /(1 âˆ’C/n)k < âˆ. Roughly,
then, O(1) Â· n log 1
Ïµ iterations of the aProx update (4) are suï¬ƒcient to achieve Ïµ-accuracy in
the solution of Ax = b, each of which requires time O(n), yielding a total operation count
of O(1) Â· n2 log 1
Ïµ. This is a less precise version of the bound Strohmer and Vershynin [55,
Sec. 2.1] attain for Kaczmarz methods on well-conditioned problems.
17

4.3.2
Finding a point in the intersection of convex sets
We return now to Example 5, building connections with randomized projection algorithms [2,
35, 36]. Let C1, C2, . . . , Cm be closed convex sets, where X â‹†:= âˆ©m
i=1Ci is non-empty. The
conditioning of the problem of ï¬nding a point in this intersection is related to the ratio
dist(x, X â‹†)/ maxi dist(x, Ci) (cf. [35, 36]).
As a simple special case, if the sets Ci are all
halfspaces of the form Ci = {x | âŸ¨ai, xâŸ©â‰¤bi}, then the Hoï¬€man error bound [26] implies that
âˆ¥[Ax âˆ’b]+âˆ¥âˆâ‰¥c dist(x, X â‹†) for a constant c > 0; this result also holds in inï¬nite dimensions
under a constraint qualiï¬cation [27]. Abstracting away the particulars of the sets Ci, consider
F(x) = 1
m
Pm
i=1 dist(x, Ci), and assume the analog of the Hoï¬€man bound that for some Î» > 0,
dist(x, X â‹†) â‰¥max
i
dist(x, Ci) â‰¥Î» dist(x, X â‹†),
where the ï¬rst inequality always holds and Î» is a condition number [35]. For example, for
halfspaces Ci = {x | âŸ¨ai, xâŸ©â‰¤bi}, i = 1, 2, with âˆ¥aiâˆ¥= 1, then Î» =
p
(1 + âŸ¨a1, a2âŸ©)/2 suï¬ƒces.
To understand the growth properties of F(x) = 1
m
Pm
i=1 dist(x, Ci), recall [25, Ex. VI.3.3]
that âˆ‡dist(x, Ci) = (x âˆ’Ï€Ci(x))/ âˆ¥Ï€Ci(x) âˆ’xâˆ¥2 for x Ì¸âˆˆCi, where Ï€Ci(x) denotes projection
onto Ci, and âˆ‚dist(x, Ci) consists of those v in the normal cone to Ci at x with âˆ¥vâˆ¥2 â‰¤1 if
x âˆˆCi, so that the subgradients of the component functions dist(x, Ci) have norm bounded
by 1.
Then letting I be uniform in {1, . . . , m}, we obtain for any Î± > 0 that
E[dist(x, CI) min{Î±, dist(x, CI)}] = 1
m
m
X
i=1
dist(x, Ci) min{Î±, dist(x, Ci)}
â‰¥1
m max
i
dist(x, Ci) min

Î±, max
i
dist(x, Ci)

â‰¥dist(x, X â‹†) min
Î±Î»
m , Î»
m dist(x, X â‹†)

.
In particular, Assumption A4 holds with constant Î»1 = Î»/m, and thus Proposition 2 implies
the convergence lim supk dist(xk, X â‹†)2/(1 âˆ’Î»/m)k < âˆ. Roughly m
Î» log 1
Ïµ steps are suï¬ƒcient
to ï¬nd a point xk that is within distance Ïµ of the set X â‹†using the aProx family.
4.3.3
Interpolation problems
Finally, we consider statistical machine learning problems in which one interpolates the data,
Example 6. Belkin and collaborators [3, 4, 37] suggest that in a number of statistical machine
learning problems, it is possible to achieve zero error on a training sample while still achieving
optimal convergence rates for the population objective. To give a simple example, we study
underparameterized least-squares.
Our data comes in m pairs (ai, bi) âˆˆRn Ã— R, fi(x) =
1
2(âŸ¨ai, xâŸ©âˆ’bi)2, where n > m. The minimum norm interpolant xâ‹†:= argminx{âˆ¥xâˆ¥2 | Ax = b},
or equivalently, the minimizer
xâ‹†= argmin
xâˆˆRn
( m
X
i=1
â„“(âŸ¨ai, xâŸ©âˆ’bi) | x âˆˆspan{a1, . . . , am}
)
for any loss â„“: R â†’R+ uniquely minimized at 0, possesses certain statistical optimality
properties while exhibiting strong empirical prediction performance [3, 4, 37, 64, Sec. 5]. In
18

our case, the truncated models (8) guarantee that the iterates of the aProx family lie in the
span of the vectors ai. With our choice fi above, if we let M := maxi âˆ¥aiâˆ¥2 then
1
m
m
X
i=1
fi(x) min
(
Î±,
fi(x)
âˆ¥âˆ‡fi(x)âˆ¥2
2
)
â‰¥
1
2m âˆ¥A(x âˆ’xâ‹†)âˆ¥2
2 min

Î±, 1
M2

.
Let UÎ£V T = A be the singular value decomposition of A, so
âˆ¥A(x âˆ’xâ‹†)âˆ¥2
2 =
m
X
i=1
Ïƒi(A)2âŸ¨vi, x âˆ’xâ‹†âŸ©2 â‰¥Ïƒm(A)2 V V T (x âˆ’xâ‹†)
2
2 .
As xâ‹†âˆˆspan{ai}, whenever x âˆˆspan{ai}, we have
V V T (x âˆ’xâ‹†)

2 = âˆ¥x âˆ’xâ‹†âˆ¥2, and
Assumption A5 holds for such x: if I is uniform on {1, . . . , m} then
E

(fI(x) âˆ’fI(xâ‹†)) min

Î±, fI(x) âˆ’fI(xâ‹†)
âˆ¥âˆ‡fI(x)âˆ¥2
2

â‰¥Ïƒm(A)2
m
min

Î±, Mâˆ’2	
âˆ¥x âˆ’xâ‹†âˆ¥2
2 .
Random matrices A âˆˆRmÃ—n with independent rows typically satisfy an inequality of the form
Ïƒm(A) â‰³E[âˆ¥aâˆ¥2
2]1/2(1 âˆ’
p
m/n) with high probability [62]. Assuming that M2 â‰²E[âˆ¥aâˆ¥2
2], we
see that in this case, Assumption A5 then holds with constants Î»0 â‰³1
mE[âˆ¥aâˆ¥2
2] and Î»1 â‰³1
m.
Summarizing, under typical scenarios, Proposition 3 guarantees the following
Corollary 4.1. Consider the underdetermined least squares problem, where the matrix A has
rows with constant norm âˆ¥aiâˆ¥2 = âˆšn and Ïƒm(A) â‰¥âˆšm, and let xk be generated by the
iteration (4) with the truncated model (8) and stepsizes Î±k = Î±0kâˆ’Î² for some Î² > 0. Then
there exists a constant C depending on n, m, Î±0, and Î² such that for all k âˆˆN
E[âˆ¥xk âˆ’xâ‹†âˆ¥2
2] â‰¤C max

exp

âˆ’k
m

, exp

âˆ’Î±0
1 âˆ’Î² k1âˆ’Î²

âˆ¥x1 âˆ’xâ‹†âˆ¥2
2 .
Corollary 4.1 shows that the iterates exhibit nearly linear convergence. With a more careful
stepsize choice, gradient methods achieve linear convergence [52, 37, Section 4], but these
recommended stepsizes cause non-convergence except on the narrow set of â€œeasyâ€ problems
considered. In our scenario, however, aProx methods achieve these convergence rates while
enjoying convergence in other problems.
5
Non asymptotic convergence results
For our ï¬nal set of theoretical results, we provide two propositions on the non-asymptotic
convergence of aProx methods, describing their behavior on Lipschitz objectives, and then
showing that for any functions exhibiting a type of strong convexity, the stochastic proximal
point method achieves strong non-asymptotic guarantees.
The ï¬rst result, on convergence for Lipschitzian objectives, is essentially known [14,
Thm. 4.1], and it generalizes the standard results in most treatments of stochastic convex
optimization, where one makes some type of Lipschitzian assumptions on the random func-
tions f (e.g. [66, 42, 7]):
Assumption A6. There exists M2 < âˆsuch that for each x âˆˆX, E[âˆ¥fâ€²(x; S)âˆ¥2
2] â‰¤M2.
Under this Lipschitzian assumption, the following extension of the well-known results on
convergence of the stochastic gradient method [66, 42] (also generalizing Bertsekas [7]) holds.
19

Proposition 4. Let Assumptions A1 and A6 hold, and let the iterates xk be generated by algo-
rithm (4) by any model satisï¬ng Conditions (C.i)â€“(C.ii). Deï¬ne xk = (Pk
i=1 Î±i)âˆ’1 Pk
i=1 Î±ixi.
Then
E[F(xk)] âˆ’F(xâ‹†) â‰¤
1
2 Pk
i=1 Î±i
âˆ¥x0 âˆ’xâ‹†âˆ¥2
2 +
M2
2 Pk
i=1 Î±i
k
X
i=1
Î±2
i .
If X is compact with R := supxâˆˆX âˆ¥x âˆ’xâ‹†âˆ¥2, then the average xk := 1
k
Pk
i=1 xi satisï¬es
E[F(xk)] âˆ’F(xâ‹†) â‰¤
R2
2kÎ±k
+ M2
2k
k
X
i=1
Î±i.
The proof is a more or less standard application of Lemma 3.4, so we defer it to Appendix C.1;
the only contribution beyond Davis and Drusvyatskiy [14] is the claim under compact X.
We present one ï¬nal theoretical result, showing how the stochastic proximal point method
(the full model (7)) achieves reasonable non-asymptotic convergence guarantees even when
the functions f may be non-Lipschitz and non-smooth, as long as they obey a (restricted)
strong convexity condition. The convergence guarantees we present here are impossible with
standard stochastic gradient methods, which can diverge under the assumptions we consider.
Assumption A7 (Restricted strong convexity). The functions f(Â·; s) are strongly convex
over X with respect to the matrix Î£(s) âª°0, that is,
f(y; s) â‰¥f(x; s) + âŸ¨fâ€²(x; s), y âˆ’xâŸ©+ 1
2(x âˆ’y)T Î£(s)(x âˆ’y) for x, y âˆˆX
for all fâ€²(x; s) âˆˆâˆ‚f(x; s). The matrix Î£ satisï¬es E[Î£(S)] âª°Î»minInÃ—n, where Î»min > 0.
We analyze the stochastic proximal point method under Assumption A7; we begin with
a technical lemma, which provides a guarantee on the one-step progress of the method. We
present the proof of Lemma 5.1 in Appendix C.2.
Lemma 5.1. Let Assumption A7 hold and the iterates xk be generated by the stochastic
proximal point method (3). Deï¬ne
Î£k := E

1
1 + 2Î±kÎ»max(Î£(S))Î£(S)

.
Then
1
2E
h
âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 | Fkâˆ’1
i
â‰¤1
2(xk âˆ’xâ‹†)T  I âˆ’Î±kÎ£k

(xk âˆ’xâ‹†) + Î±2
kE
hfâ€²(xâ‹†; S)
2
2
i
.
Lemma 5.1 guarantees that the proximal-point method makes progress whenever the re-
stricted strong convexity conditions hold, irrespective of smoothness of the objective func-
tions. First, we have 0 â‰ºÎ£0 âª¯Î£k for all k âˆˆN, and deï¬ning Î»k := Î»min(Î£k) > 0, we
have Î»k â†‘Î»âˆ:= Î»min(E[Î£(S)]) when Î±k â†“0.
Under the conditions of Lemma 5.1 and
Assumption A1, we thus have
E
h
âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 | Fkâˆ’1
i
â‰¤(1 âˆ’Î±kÎ»k) âˆ¥xk âˆ’xâ‹†âˆ¥2
2 + Î±2
kÏƒ2 â‰¤(1 âˆ’Î±kÎ»0) âˆ¥xk âˆ’xâ‹†âˆ¥2
2 + Î±2
kÏƒ2.
Applying this inequality recursively gives
E[âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2] â‰¤
k
Y
i=1
(1 âˆ’Î±iÎ»i) âˆ¥x1 âˆ’xâ‹†âˆ¥2
2 +
k
X
i=1
Î±2
i
k
Y
j=i+1
(1 âˆ’Î±jÎ»j)Ïƒ2,
where we note that Î±jÎ»j < 1 for all j. An inductive argument [47] implies the next proposition.
20

Proposition 5. Let Assumptions A1 and A7 hold, and let xk be generated by the stochastic
proximal point method (3) with stepsizes Î±k = Î±0kâˆ’Î² for some Î² âˆˆ(0, 1).
Then for a
numerical constant C < âˆ,
E[âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2] â‰¤exp
 
âˆ’Î»0
k
X
i=1
Î±i
!
âˆ¥x1 âˆ’xâ‹†âˆ¥2
2 + C Â· Ïƒ2
Î»0
Î±k Â· log k.
An asymptotic argument [47] gives convergence E[âˆ¥xk âˆ’xâ‹†âˆ¥2
2] â‰²
Ïƒ2
Î»âˆÎ±k for large k.
For
stochastic proximal point methods, if the objective is strongly convex, choosing Î±k = C/k
for a large constant C yields asymptotic convergence bounds on âˆ¥xk âˆ’xâ‹†âˆ¥2
2 of the form Ïƒ2
Î»âˆ
1
k.
In comparison to convergence results available for stochastic gradient methods [46, 42, 1],
Proposition 5 holds whenever the subgradient fâ€²(xâ‹†; S) has ï¬nite second moment, and we
require this only at the point xâ‹†; moreover, it holds no matter the stepsize sequence.
6
Experiments
The ï¬nal component of this paper is an empirical evaluation of the aProx methods. Our
goal in the experiments is to evaluate the relative merits of diï¬€erent approximate models in
the iteration (4), and accordingly, we consider the four approximations below.
(i) Stochastic gradient method (SGM): uses the linear model (6).
(ii) Proximal: uses the full model (7).
(iii) Truncated: uses the lower truncated model (8).
(iv) Bundle: uses the bundle (cutting plane) model (9) with two lines, that is, with i = 1.
We present several experiments, each comparing diï¬€erent aspects of the aProx models.
Throughout, we consider step size sequences of the form Î±k = Î±0kâˆ’Î², where Î² âˆˆ(1/2, 1).
We wish to evaluate the robustness and stability of each of the models (6)â€“(9) for diï¬€erent
problems, investigating both well- and poorly-conditioned instances, as well as problems sat-
isfying the â€œeasyâ€ conditions of Deï¬nition 4.1. Let us provide some guidance toward expected
results. Roughly, for problems with globally Lipschitz gradients (such as linear regression
problems with noise), we expect the methods to have fairly similar performanceâ€”stochastic
gradient, proximal point, truncated, and bundle models are all asymptotically normal with
optimal covariance. However, as problems become either (i) easierâ€”closer to satisfying Deï¬-
nition 4.1â€”or (ii) more poorly conditioned or harder because of large or unbounded Lipschitz
constants, we expect stochastic gradient methods to (i) converge more slowly or (ii) be sub-
stantially more sensitive to stepsize choices.
Within each of our experiments, we run each model-based iteration (4) for K total iter-
ations across multiple initial stepsizes Î±0. For a ï¬xed accuracy Ïµ > 0, we record the number
of steps k required to achieve F(xk) âˆ’F(xâ‹†) â‰¤Ïµ, reporting these times (where we terminate
each run at iteration K). We perform T experiments for each initial stepsize choice, reporting
the median of the time-to-Ïµ-accuracy; the shaded areas in each plot cover the 5th to 95th
percentile of convergence times (90% coverage sets). Finally, while we restrict our exper-
iments to the single-sample batch size setting (i.e. no â€œmini-batchesâ€), we note that using
multiple samples to decrease the variance of (sub)gradients does not necessarily improve the
robustness of standard stochastic subgradient methods. Indeed, Examples 1 and 2 show that
the deterministic gradient method can be sensitive to stepsize choice. In experiments we do
21

not include because of the additional space they require, we verify this intuitionâ€”stochastic
gradient methods remain similarly sensitive to stepsize choice even using large batch sizes.
Additionally, it is possible to consider the iteration k at which the averaged iterate xk achieves
F(xk) âˆ’F(xâ‹†) â‰¤Ïµ; this does not change the qualitative aspects of our plots in any way.
6.1
Linear Regression
10
1
100
101
102
103
104
105
0
200
400
600
800
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
Initial stepsize Î±0
Time to accuracy Ïµ = .05
10
1
100
101
102
103
104
105
0
200
400
600
800
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
Initial stepsize Î±0
Time to accuracy Ïµ = .05
(a)
(b)
Figure 2.
The number of iterations to achieve Ïµ-accuracy versus initial stepsize Î±0 for linear
regression with m = 1000, n = 40, and condition number Îº(A) = 1. (a) The noiseless setting
with Ïƒ = 0. (b) Noisy setting with Ïƒ = 1
2.
10
1
100
101
102
103
104
105
200
400
600
800
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
Initial stepsize Î±0
Time to accuracy Ïµ = .05
10
1
100
101
102
103
104
105
300
400
500
600
700
800
900
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
Initial stepsize Î±0
Time to accuracy Ïµ = .05
(a)
(b)
Figure 3.
The number of iterations to achieve Ïµ-accuracy versus initial stepsize Î±0 for linear
regression with m = 1000, n = 40, and condition number Îº(A) = 15. (a) The noiseless setting
with Ïƒ = 0. (b) Noisy setting with Ïƒ = 1
2.
In our linear regression experiments, we let A âˆˆRmÃ—n, b âˆˆRm, and F(x) =
1
2m âˆ¥Ax âˆ’bâˆ¥2
2,
where in each individual experiment we generate xâ‹†âˆ¼N(0, In) âˆˆRn and set b = Axâ‹†+ Ïƒv
for v âˆ¼N(0, Im). We choose Ïƒ diï¬€erently depending on the experiment, setting Ïƒ = 0 in
22

noiseless experiments and Ïƒ = 1
2 otherwise. We generate A as A = QD, where Q âˆˆRmÃ—n
has uniformly random orthogonal columns, and D = diag(1, 1 + (Îº âˆ’1)/(n âˆ’1), . . . , Îº) is a
diagonal matrix with linearly spaced entries between 1 and a desired condition number Îº â‰¥1.
In Figure 2, we plot the results of our experiments on well-conditioned problems, which
use matrices A with condition number Îº(A) = 1, while Figure 3 shows identical results
except that we use condition number Îº(A) = 15. Plot (a) of each ï¬gure demonstrates the
results for the noiseless setting with Ïƒ = 0. In Fig. 2(a), we see the expected result that the
stochastic gradient method has good performance for a precise range of stepsizes in [10âˆ’1, 1],
while the better approximations of the proximal point (7) and the truncated (8) models
yield better convergence over a range of stepsizes with six orders of magnitude. The bundle
model (9) shows somewhat more robustness than SGM, but for large stepsizes also exhibits
some oscillation. In the noisy cases, plots (b) in each ï¬gure, the results are similar, except that
the full proximal model is somewhat less robust to stepsize choice; roughly, in the stochastic
proximal point (SPPM) case, the model trusts the instantaneous function too much, and
overï¬ts to the noise at each iteration.
Figure 3 tells a similar story to Figure 2, except that the stochastic gradient method is
essentially not convergent: in no experiment did it ever achieve accuracy below Ïµ = .05 in
the noisy or noiseless settings. This problem is not in any real sense challenging: a condition
number of Îº(A) = 15 is not particularly poorly conditioned [59], yet stochastic gradient
methods exhibit very poor behavior.
These plots suggest that the reliance on stochastic
gradient methods in much of the statistical and machine learning literature may be misplaced.
6.2
Absolute Loss Regression
10
1
100
101
102
103
104
105
0
200
400
600
800
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
10
1
100
101
102
103
104
105
0
200
400
600
800
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
(a)
(b)
Figure 4.
The number of iterations to achieve Ïµ-accuracy as a function of the initial step size
Î±0 for absolute loss regression with m = 1000, n = 40, and condition number Îº(A) = 1. (a)
Noiseless setting with Ïƒ = 0. (b) Noisy setting with Ïƒ = 1
2.
For our second set of experiments, we consider regression with an absolute loss, using
F(x) =
1
m âˆ¥Ax âˆ’bâˆ¥1 and f(x; (a, b)) = |âŸ¨a, xâŸ©âˆ’b|, which is non-smooth but Lipschitz. We
generate A and b identically to the linear regression experiments in Sec. 6.1. Based on our
results in Section 4.3.1, we expect that in the noiseless case, the proximal-point (7), trun-
cated (8), and multi-line (9) methods should have very fast convergence, which is indeed
23

what we see in Figure 4(a). (For this objective, the multi-line model functions identically
to the stochastic proximal point method.) When the problems are easy, even when they are
non-smooth, a more careful (even simple truncated) model yields substantial improvements
over naive linear models (6). In Figure 4(b), we display results with noise with standard de-
viation Ïƒ = 1
2; in this case, the better approximations to f achieve convergence for a slightly
larger range of stepsizesâ€”statistically signiï¬cant as their conï¬dence regions exhibitâ€”but the
diï¬€erence is less substantial. Experiments with condition numbers Îº(A) = 15 yielded results
completely parallel to those in Figure 3.
6.3
Logistic Regression
10
1
100
101
102
103
104
105
200
400
600
800
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
Initial stepsize Î±0
Time to accuracy Ïµ = .05
10
1
100
101
102
103
104
105
0
200
400
600
800
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
Initial stepsize Î±0
Time to accuracy Ïµ = .05
(a)
(b)
Figure 5.
The number of iterations to achieve Ïµ-accuracy as a function of the initial step
size Î±0 for logistic regression with n = 40, m = 1000, and condition number Îº(A) = 1. (a)
Noiseless experiment. (b) Labels ï¬‚ipped with probability p = 0.01.
We now turn to classiï¬cation experiments, beginning with a logistic regression experiment.
In logistic regression, widely used for ï¬tting models for binary classiï¬cation in statistics and
machine learning [23], we have data pairs (ai, bi) âˆˆRn Ã— {Â±1}, and we wish to minimize
F(x) := 1
m
m
X
i=1
f(x; (ai, bi))
where
f(x; (a, b)) = log

1 + eâˆ’bâŸ¨a,xâŸ©
.
We generate the data as follows: we sample ai
iid
âˆ¼N(0, In) and uâ‹†âˆ¼N(0, In), labeling bi =
sign(âŸ¨ai, uâ‹†âŸ©); in the noisy setting we ï¬‚ip each labelâ€™s sign independently with probability p.
We present the results of this experiment in Figures 5 and 6, including plots for both the
noiseless (perfectly separated) and noisy cases (plots (a) and (b) in each ï¬gure, respectively),
where Fig. 6 displays results when the condition number of the data matrix A is Îº(A) = 15.
These plots demonstrate similar results to those in the preceding sections, though there are a
few diï¬€erences. First, in the noiseless setting, there is no optimizer xâ‹†, as the optimal value is
limtâ†’âˆF(tuâ‹†) = 0, yet still we see the beneï¬ts of the more accurate models in Figures 5(a)
and 6(a). Moreover, the truncated and proximal models exhibit a wider range of convergent
stepsizes than the simple stochastic gradient method does even in the noisy case.
24

10
2
10
1
100
101
102
103
104
200
400
600
800
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
Initial stepsize Î±0
Time to accuracy Ïµ = .05
10
2
10
1
100
101
102
103
104
0
200
400
600
800
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
Initial stepsize Î±0
Time to accuracy Ïµ = .05
(a)
(b)
Figure 6.
The number of iterations to achieve Ïµ-accuracy as a function of the initial step size
Î±0 for logistic regression with parameters n = 40, m = 1000, and condition number Îº(A) = 15.
(a) Noiseless experiment. (b) Labels ï¬‚ipped with probability p = .01.
6.4
Multi Class Hinge Loss
10
1
100
101
102
103
104
105
0
200
400
600
800
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
Initial stepsize Î±0
Time to accuracy Ïµ = .05
10
1
100
101
102
103
104
105
0
200
400
600
800
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
Initial stepsize Î±0
Time to accuracy Ïµ = .05
(a)
(b)
Figure 7.
The number of iterations to achieve Ïµ-accuracy as a function of the initial step
size Î±0 for multi class hinge loss with parameters n = 15, m = 2000, K = 10 and (a) label
randomization probability p = 0 and (b) label randomization probability p = 0.01.
In our second classiï¬cation experiment, we focus on a somewhat more complex multi-class
setting, using the multi-class hinge loss [13]. In this setting, we receive m vectors ai âˆˆRn
and a correct label â„“i âˆˆ[K] for each i, where K is the number of classes. We wish to ï¬nd a
classiï¬er, represented as a collection of K vectors X = [x1 Â· Â· Â· xK] âˆˆRnÃ—K that minimizes
F(X) = 1
m
m
X
i=1
max
jÌ¸=â„“i [1 + âŸ¨ai, xj âˆ’xâ„“iâŸ©]+ .
In the case that the data is separable with a positive margin, meaning that there exists Xâ‹†
25

such that âŸ¨ai, xâ„“iâŸ©â‰¥1 + âŸ¨ai, xjâŸ©for all j Ì¸= â„“i, this problem is equivalent to ï¬nding a point
in the intersection of halfspaces (recall Section 4.3.2).
Accordingly, we expect to see fast
convergence for the truncated (8) and proximal-point (7) models for large stepsizes.
To generate the data, we draw vectors ai
iid
âˆ¼N(0, In), then generate an â€œoptimalâ€ classiï¬er
Uâ‹†âˆˆRnÃ—K with i.i.d. N(0, 1) entries. In the non-noisy setting, we set â„“i = argmaxjâŸ¨uâ‹†
j, aiâŸ©,
while in the noisy setting, for each i âˆˆ{1, . . . , m} we resample a value â„“i uniformly at random
with probability p. We present the results of this experiment in Figure 7. The experiment
reinforces the conclusions of the previous experiments: better models (4) in the aProx family
are signiï¬cantly more robust to the step size values and achieve generally faster convergence
than more naive subgradient methods.
6.5
Poisson Regression
10
1
100
101
102
103
104
105
0
200
400
600
800
1000
Accuracy epsilon = 0.055
Proximal
SGM
Truncated
Bundle
Initial stepsize Î±0
Time to accuracy Ïµ = .05
Figure 8.
The number of iterations to achieve Ïµ-accuracy as a function of the initial step size
Î±0 for Poisson regression with parameters n = 40, and m = 1000.
For our ï¬nal experiment, we consider a poisson regression problem (Example 3), for
which classical results on stochastic approximation do not apply because of the exponen-
tial objective.
In this case, we model counts bi âˆˆN as coming from a distribution p(b |
a, x) = exp(âˆ’eâŸ¨a,xâŸ©) exp(bâŸ¨a, xâŸ©)/b!, giving loss f(x; (a, b)) = exp(âŸ¨a, xâŸ©) âˆ’bâŸ¨a, xâŸ©. We gen-
erate the data by ï¬rst drawing u âˆ¼âˆšn Â· Uni(Snâˆ’1), then drawing ai
iid
âˆ¼N(0, (1/n)In) and
bi âˆ¼Poisson(eâŸ¨ai,uâŸ©).
In this experiment, the proximal update has no closed-form, as it
involves minimizing a quadratic plus exponential term, but the minimizer of the proximal
update lies on {xk + ta | t âˆˆR}, yielding a 1-dimensional convex optimization problem. As
Example 3 shows, however, it is simple to implement the truncated model (8) by computing
infx f(x; (a, b)) = log(b!) + b âˆ’b log b.
We present the results in Figure 8. It is surprising to us that the stochastic gradient method
converges at all on this problem, but with low noise scenarios and small enough stepsizes, it
seems that SGM does not leave a region around zero and so is convergent. Importantly, the
truncated model (8), in spite of its substantially easier calculation, enjoys convergence nearly
as robust as that of the stochastic proximal point method.
26

Acknowledgments
We thank two anonymous reviewers for careful reading and feedback on earlier versions of this
paper. In particular, both suggested strengthenings of Proposition 1 we have implemented.
A
Proof of Theorem 2
The proof of the theorem proceeds in a series of lemmas, each of which requires some work.
Roughly, the outline is as follows: ï¬rst, we develop a recursion for the iterates xk+1, which
parallels a noisy gradient recursion, except that the errors implicitly depend on the next iter-
ate. This allows a decomposition (see Eq. (14)) of 1
k
Pk
i=1(xi âˆ’xâ‹†) into a leading term, which
is obviously asymptotically normal, and several error terms. Two of these errors are standard
stochastic approximation errors (similar, for example, to those in Polyak and Juditsky [47]),
though we require care in showing they are negligible (see Lemmas A.1, A.2, and A.3). The
last error term involves subgradients of the models fxk(Â·; Sk) at the point xk+1, causing an
implicit and potentially non-smooth dependence in the errors. To address this, we provide a
gradient comparison result (Lemma A.4), based on Davis et al. [15], which shows that even if
the method generating the iterates xk uses a non-smooth approximation to f(Â·; Sk), near xk
the subgradients of the model approximate âˆ‡f. This allows us to adapt the results of Polyak
and Juditsky [47] on asymptotic optimality of stochastic gradient methods.
Let âˆ†k = xk âˆ’xâ‹†, and for H = âˆ‡2F(xâ‹†), deï¬ne the remainder R(x) = âˆ‡F(x)âˆ’H(xâˆ’xâ‹†),
so that âˆ¥R(x)âˆ¥= O(âˆ¥x âˆ’xâ‹†âˆ¥2) as x â†’xâ‹†. We perform an expansion to rewrite the implicit
iteration xk+1 = xk âˆ’Î±kfâ€²
xk(xk+1; Sk). Deï¬ning the localized (sub)gradient errors
Î¶k = Î¶(xk, Sk) :=
 âˆ‡fâ€²(xk; Sk) âˆ’âˆ‡f(xâ‹†; Sk)

âˆ’
 F â€²(xk) âˆ’âˆ‡F(xâ‹†)

,
(12)
we obtain
xk+1 = xk âˆ’Î±kfâ€²
xk(xk+1; Sk)
= xk âˆ’Î±k
h
H(xk âˆ’xâ‹†) + R(xk) + fâ€²(xk; Sk) âˆ’F â€²(xk) + fâ€²
xk(xk+1; Sk) âˆ’fâ€²(xk; Sk)
|
{z
}
=:Îµk
i
= xk âˆ’Î±kH(xk âˆ’xâ‹†) âˆ’Î±kâˆ‡f(xâ‹†; Sk) âˆ’Î±k [R(xk) + Î¶k + Îµk] .
(13)
Subtracting xâ‹†to use âˆ†k = xk âˆ’xâ‹†, the implicit iteration (13) becomes
âˆ†k+1 = (I âˆ’Î±kH)âˆ†k âˆ’Î±kâˆ‡f(xâ‹†; Sk) âˆ’Î±k[R(xk) + Î¶k + Îµk].
Deï¬ning the matrices
Bk
i := Î±i
k
X
j=i
jY
l=i+1
(I âˆ’Î±lH) and Ak
i := Bk
i âˆ’Hâˆ’1,
Polyak and Juditsky [47, Lemma 2] show that âˆ†k = 1
k
Pk
i=1 âˆ†i satisï¬es the equality
âˆš
kâˆ†k =
1
âˆš
k
k
X
i=1
Hâˆ’1âˆ‡f(xâ‹†; Sk)
+ 1
âˆš
k
k
X
i=1
Ak
i âˆ‡f(xâ‹†; Si) + 1
âˆš
k
k
X
i=1
Bk
i [R(xi) + Î¶i + Îµi] + O(1/
âˆš
k)
(14)
27

and additionally supi,k âˆ¥Bk
i âˆ¥< âˆand limk 1
k
Pk
i=1 âˆ¥Ak
i âˆ¥= 0. Evidently, equality (14) implies
the theorem as soon as we show each of the terms except kâˆ’1/2Hâˆ’1 Pk
i=1 âˆ‡f(xâ‹†; Sk) are
oP (1). We thus bound each of the terms in (14) in turn, which gives Theorem 2.
Lemma A.1. Under the conditions of Theorem 2, âˆ¥âˆ†kâˆ¥a.s.
â†’0, Pâˆ
k=1 Î±k(F(xk)âˆ’F(xâ‹†)) < âˆ,
and Pâˆ
k=1 Î±k âˆ¥âˆ†kâˆ¥2 < âˆ.
Lemma A.2. Under the conditions of Theorem 2,
1
âˆš
k
Pk
i=1 âˆ¥R(xi)âˆ¥a.s.
â†’0.
Lemma A.3. Under the conditions of Theorem 2,
1
âˆš
k
Pk
i=1 Î¶i
a.s.
â†’0 and
1
âˆš
k
Pk
i=1 Ak
i Î¶i
a.s.
â†’0.
See Sections A.1, A.2, and A.3 for proofs of each of these lemmas, respectively.
Controlling the implicit modeling errors Îµk = fâ€²
xk(xk+1; Sk) âˆ’âˆ‡f(xk; Sk) is the most
important challenge, and for this, we use the following gradient comparison lemma.
Lemma A.4 (Davis et al. [15], Theorem 6.1). Let f and h be convex and subdiï¬€erentiable
on Rn and Ïµ > 0, r < âˆ. Assume that on the set {x | âˆ¥x âˆ’xâ‹†âˆ¥â‰¤Ïµ}, the function f has
L-Lipschitz gradient. Assume additionally that f â‰¥h and at the point x0, hâ€²(x0) âˆˆâˆ‚f(x0).
Then for any x and hâ€²(x) âˆˆâˆ‚h(x), if âˆ¥x âˆ’xâ‹†âˆ¥â‰¤Ïµ/4 and âˆ¥x0 âˆ’xâ‹†âˆ¥â‰¤Ïµ/4, then
âˆ‡f(x) âˆ’hâ€²(x)
 â‰¤2L âˆ¥x âˆ’x0âˆ¥.
Key to the application of Lemma A.4 is that individual iterates move very little.
Lemma A.5. Let Conditions (C.i) and (C.ii) hold. Then âˆ¥xk âˆ’xk+1âˆ¥â‰¤Î±k âˆ¥fâ€²(xk; Sk)âˆ¥for
some fâ€²(xk; Sk) âˆˆâˆ‚f(xk; Sk).
Proof
Let gk+1 âˆˆâˆ‚fxk(xk+1; Sk) satisfy âŸ¨Î±kgk+1 + (xk+1 âˆ’xk), y âˆ’xk+1âŸ©â‰¥0 for all y âˆˆX.
Then, using âŸ¨gk âˆ’gk+1, xk âˆ’xk+1âŸ©â‰¥0 for any gk âˆˆâˆ‚fxk(xk; Sk) âŠ‚âˆ‚f(xk; Sk) and setting
y = xk, we have Î±kâŸ¨gk, xk âˆ’xk+1âŸ©â‰¥Î±kâŸ¨gk+1, xk âˆ’xk+1âŸ©â‰¥âˆ¥xk âˆ’xk+1âˆ¥2. Cauchy-Schwarz
gives the result.
With Lemmas A.4 and A.5 in place, we can control the ï¬nal error terms in the expansion (14).
We provide the proof of the next lemma in Section A.5.
Lemma A.6. Under the conditions of Theorem 2,
1
âˆš
k
Pk
i=1 âˆ¥Îµiâˆ¥a.s.
â†’0.
This completes the proof of Theorem 2.
A.1
Proof of Lemma A.1
The ï¬rst two results are consequences of Proposition 1. By the local strong convexity of F
(Assumption A3(i)), for all r âˆˆR+ there exists Î»r > 0 such that that âˆ¥x âˆ’xâ‹†âˆ¥â‰¤r implies
F(x) âˆ’F(xâ‹†) â‰¥Î»r
2 âˆ¥x âˆ’xâ‹†âˆ¥2. Thus for some (random) r < âˆ, we have P
k Î±k(F(xk) âˆ’
F(xâ‹†)) â‰¥Î»r
2
P
k Î±k âˆ¥âˆ†kâˆ¥2, which gives the last result of the lemma.
A.2
Proof of Lemma A.2
By Assumption A3, R(x) = âˆ‡F(x)âˆ’H(xâˆ’xâ‹†) satisï¬es R(x) = O(âˆ¥x âˆ’xâ‹†âˆ¥2) as x â†’xâ‹†, and
thus for each r âˆˆR+, there exists some Cr < âˆsuch that âˆ¥x âˆ’xâ‹†âˆ¥â‰¤r implies âˆ¥R(x)âˆ¥â‰¤
Cr âˆ¥x âˆ’xâ‹†âˆ¥2. Now, for r âˆˆR+ deï¬ne the stopping time
Ï„r := inf{k âˆˆN | âˆ¥âˆ†kâˆ¥â‰¥r},
(15)
28

so that {Ï„r > k} âˆˆFk, as xk+1 âˆˆFk. Then using Lemma 3.4 exactly as in the proof of
Proposition 1, we have
E[âˆ¥âˆ†k+1âˆ¥21 {Ï„r > k + 1} | Fkâˆ’1] â‰¤E[âˆ¥âˆ†k+1âˆ¥21 {Ï„r > k} | Fkâˆ’1]
â‰¤1 {Ï„r > k}
 âˆ¥âˆ†kâˆ¥2 âˆ’2Î±k(F(xk) âˆ’F â‹†) + Î±2
kGbig(r)

.
Again using the local strong convexity of F (Assumption A3), there exists Î»r > 0 such that
F(xk) âˆ’F â‹†â‰¥(Î»r/2) âˆ¥xk âˆ’xâ‹†âˆ¥2 on the event {Ï„r > k}, so we obtain
E[âˆ¥âˆ†k+1âˆ¥21 {Ï„r > k + 1} | Fkâˆ’1] â‰¤1 {Ï„r > k}

(1 âˆ’Î±kÎ»r)âˆ¥âˆ†kâˆ¥2 + Î±2
k
2 Gbig(r)

.
(16)
A technical lemma, whose proof we defer to section A.6, helps to control this term.
Lemma A.7. Let Î±k = Î±0kâˆ’Î² for some Î² âˆˆ(0, 1) and Î±0 > 0, and let Î» > 0 and Ï > 1
Î².
Deï¬ne pk := Pk
i=1 Î±Ï
i
Qk
j=i+1 |1 âˆ’Î»Î±j|. Then lim supk pk/(Î±Ïâˆ’1
k
log k) < âˆ. If additionally
Î»Î±1 â‰¤1, then there exists a numerical constant C < âˆsuch that
pk â‰¤CÎ±Ï
0
1
k +
1
kÏÎ²

+ C log k
Î»
Î±Ïâˆ’1
k
.
A recursive application of inequality (16) and (see also [47, Pg. 851]) yields the bound
E[âˆ¥âˆ†kâˆ¥2 1 {Ï„r > k}] â‰¤o(Î±k) + C Pk
i=1 Î±2
i
Qk
j=i+1 |1 âˆ’cÎ±j| for constants 0 < c, C < âˆ, which
may depend on r, and therefore Lemma A.7 implies
E[âˆ¥âˆ†kâˆ¥2 1 {Ï„r > k}] â‰¤CrÎ±k Â· log k
(17)
for a ï¬nite Cr < âˆ. Once we note that âˆ¥R(x)âˆ¥â‰¤Cr âˆ¥x âˆ’xâ‹†âˆ¥2, the remainder of the argument
is completely identical to that of Polyak and Juditsky [47, Pg. 851], which demonstrates that
P
k âˆ¥âˆ†kâˆ¥2 /
âˆš
k < âˆwith probability 1 once there exists a (random) r < âˆsuch that Ï„r = âˆ,
then applies the Kronecker lemma.
A.3
Proof of Lemma A.3
We begin with the ï¬rst statement. By Lemma A.1, we have P
k Î±k âˆ¥âˆ†kâˆ¥2 < âˆ. Now we
apply Dembo [17, Exercise 5.3.35], which states that if Mk is an Fk-adapted martingale and
bk â†‘âˆare non-random, if P
k bâˆ’2
k E[âˆ¥Mk âˆ’Mkâˆ’1âˆ¥2 | Fkâˆ’1] < âˆthen bâˆ’1
k Mk
a.s.
â†’0. Recall
the deï¬nition (12) of Î¶k = Î¶(xk, Sk) for Î¶(x, s) = (fâ€²(x; s) âˆ’âˆ‡f(xâ‹†; s)) âˆ’(F â€²(x) âˆ’âˆ‡F(xâ‹†)),
where fâ€²(x; s) âˆˆâˆ‚f(x; s) and F â€²(x) âˆˆâˆ‚F(x) (which are both singleton sets near xâ‹†by
Assumption A3). Then if âˆ¥x âˆ’xâ‹†âˆ¥â‰¤Ïµ, Assumption A3(ii) guarantees that âˆ¥Î¶(x, s)âˆ¥â‰¤(L(s)+
E[L(S)]) âˆ¥x âˆ’xâ‹†âˆ¥, while if âˆ¥x âˆ’xâ‹†âˆ¥â‰¤r we have âˆ¥Î¶(x, s)âˆ¥â‰¤âˆ¥fâ€²(x; s)âˆ¥+âˆ¥âˆ‡f(xâ‹†; s)âˆ¥+Gbig(r)
by Assumption A2. Combining these inequalities, we have that whenever âˆ¥x âˆ’xâ‹†âˆ¥â‰¤r,
E[âˆ¥Î¶(x, S)âˆ¥2] â‰¤Cr âˆ¥x âˆ’xâ‹†âˆ¥2
for a constant Cr < âˆ. Recall the stopping times (15), Ï„r = inf{k | âˆ¥âˆ†kâˆ¥â‰¥r}, and deï¬ne
the truncated variables
Î¶(r)
i
:= Î¶i1 {Ï„r > i} âˆˆFi.
(18)
With probability 1, there is some ï¬nite r such that Î¶(r)
i
= Î¶i for all i by assumption in
Theorem 2. Moreover, E[Î¶(r)
i
| Fiâˆ’1] = E[Î¶(xi, Si) | Fiâˆ’1]1 {Ï„r > i} = 0, and
X
i
1
i E[âˆ¥Î¶(r)
i
âˆ¥2 | Fiâˆ’1] â‰¤Cr
X
i
1
i âˆ¥âˆ†iâˆ¥2 â‰¤Cr
X
i
Î±i âˆ¥âˆ†iâˆ¥2 < âˆ
29

with probability 1 (by Lemma A.1). This gives kâˆ’1/2 Pk
i=1 Î¶i
a.s.
â†’0 as desired, and also shows
that kâˆ’1/2 Pk
i=1 Hâˆ’1Î¶i
a.s.
â†’0.
We now turn to the second claim of the lemma.
We ï¬rst argue that for all r < âˆ,
kâˆ’1/2 Pk
i=1 Ak
i Î¶(r)
i
pâ†’0, and then use continuity to argue that the sequence converges almost
surely, which implies the result as kâˆ’1/2 Pk
i=1 Ak
i Î¶(r)
i
= kâˆ’1/2 Pk
i=1 Ak
i Î¶i with probability 1 for
some r âˆˆN, r < âˆ. We begin with the convergence in probability. First, recall inequality (17)
in the proof of Lemma A.2, which states that E[âˆ¥âˆ†kâˆ¥2 1 {Ï„r > k}] â‰¤CrÎ±k log k. Then for
r < âˆ, we have that
E[âˆ¥Î¶(r)
i
âˆ¥2] = E

E[âˆ¥Î¶(xi, Si)âˆ¥2 | Fiâˆ’1]1 {Ï„r > i}

â‰¤E[Cr âˆ¥xi âˆ’xâ‹†âˆ¥2 1 {Ï„r > i}] â‰¤CrÎ±i log i
for a constant Cr < âˆwhose value may change from line to line. Thus
E
"
1
âˆš
k
k
X
i=1
Ak
i Î¶(r)
i

2#
= 1
k
k
X
i=1
âˆ¥Ak
i âˆ¥2E[âˆ¥Î¶(r)
i
âˆ¥2] â‰¤C
k
k
X
i=1
âˆ¥Ak
i âˆ¥,
where we have used that Î¶(r)
i
still form a martingale diï¬€erence sequence. This converges to
zero [47, Lemma 2].
Now, we come to the second claim on the almost sure convergence. Recall the deï¬nition
Bk
i = Î±i
Pk
j=i
Qj
l=i+1(I âˆ’Î±lH), so that Ak
i = Bk
i âˆ’Hâˆ’1. For r < âˆ, deï¬ne the sequence
Zk,r :=
k
X
i=1
Bk
i Î¶(r)
i
.
The lemma will be proved if we can show that for any ï¬nite r, we have kâˆ’1/2Zk,r
a.s.
â†’0. Note
that Bk+1
i
âˆ’Bk
i = Î±i
Qk+1
j=i+1(I âˆ’Î±jH), and so if we deï¬ne
W k
i =
k
Y
j=i
(I âˆ’Î±jH),
Vk,r :=
k
X
i=1
Î±iW k+1
i+1 Î¶(r)
i
,
then Vk,r âˆˆFk and
Zk,r = Zkâˆ’1,r + Vkâˆ’1,r + Î±kÎ¶(r)
k
=
kâˆ’1
X
i=1
Vi,r +
k
X
i=1
Î±iÎ¶(r)
i
.
(19)
The second sum Pk
i=1 Î±iÎ¶(r)
i
is a square-integrable martingale with summable squared incre-
ments, so it converges with probability 1 [17, Ex. 5.3.35], and 1/
âˆš
k Pk
i=1 Î±iÎ¶(r)
i
a.s.
â†’0. It thus
suï¬ƒces to show that
1
âˆš
k
Pk
i=1 Vi,r converges almost surely, which we do by showing that it is
a Cauchy sequence. Now, we note that
E[âˆ¥Vk,râˆ¥2] =
k
X
i=1
Î±2
i âˆ¥W k+1
i+1 âˆ¥2E[âˆ¥Î¶(r)
i
âˆ¥2] â‰¤Cr
k
X
i=1
Î±2
i exp
ï£«
ï£­âˆ’c
k+1
X
j=i+1
Î±j
ï£¶
ï£¸Î±i log i,
where we have used that E[âˆ¥Î¶(r)
i
âˆ¥2] â‰²Î±i log i as above. As Î±3
i log i â‰ªÎ±3âˆ’Ïµ
i
for all Ïµ > 0,
Lemma A.7 implies that for all Ïµ > 0,
E[âˆ¥Vk,râˆ¥2] â‰¤
Cr
k2Î²âˆ’Ïµ .
(20)
30

We use inequality (20) to demonstrate that the sequence Tk,r :=
1
âˆš
k
Pk
i=1 Vi,r is Cauchy.
Indeed, we have for all Ïµ > 0 that
E [âˆ¥Tk,r âˆ’Tk+1,râˆ¥] â‰¤

1
âˆš
k + 1 âˆ’1
âˆš
k

k
X
i=1
E[âˆ¥Vi,râˆ¥] +
1
âˆš
k + 1E[âˆ¥Vk+1,râˆ¥]
â‰¤
C
k3/2
k
X
i=1
1
iÎ²âˆ’Ïµ + 1
âˆš
k
1
kÎ²âˆ’Ïµ + 1
âˆš
k
1
kÎ²âˆ’Ïµ â‰¤Ckâˆ’Î²âˆ’1/2+Ïµ.
As Î² âˆˆ(1
2, 1), we have P
k E[âˆ¥Tk,r âˆ’Tk+1,râˆ¥] â‰²P
k kâˆ’Î²âˆ’1/2+Ïµ < âˆfor suï¬ƒciently small Ïµ > 0.
Thus for all r < âˆ, the sequence Tk,r is Cauchy with probability 1, so that kâˆ’1/2 Pk
i=1 Vk,r
converges. As we know that kâˆ’1/2 Pk
i=1 Vk,r
pâ†’0 by the previous arguments, the limit point
must be zero.
A.4
Proof of Lemma A.4
The result is a consequence of Davis et al. [15]. A simpliï¬ed variant of their theorem follows:
Lemma A.8 (Theorem 6.1 [15]). Let f and h by convex and assume there exists a function
u such that 0 â‰¤f(y) âˆ’h(y) â‰¤u(y) for all y. Then for any x and Î³ > 0, there exists bx such
that for all hâ€²(x) âˆˆâˆ‚h(x), there is fâ€²(bx) âˆˆâˆ‚f(bx) such that and
âˆ¥x âˆ’bxâˆ¥â‰¤2Î³ and âˆ¥hâ€²(x) âˆ’fâ€²(bx)âˆ¥â‰¤u(x)
Î³
.
In our context, we note that by assumption,
f(y) â‰¤f(x0) + âŸ¨âˆ‡f(x0), y âˆ’x0âŸ©+ L
2 âˆ¥x0 âˆ’yâˆ¥2
if âˆ¥x0 âˆ’xâ‹†âˆ¥â‰¤Ïµ and âˆ¥y âˆ’xâ‹†âˆ¥â‰¤Ïµ, while otherwise certainly f(y) < âˆ. Noting that our
function h satisï¬es h(y) â‰¥h(x0) + âŸ¨hâ€²(x0), y âˆ’x0âŸ©= f(x0) + âŸ¨fâ€²(x0), y âˆ’x0âŸ©, we may take the
upper bound function
u(y) =
(
L
2 âˆ¥y âˆ’x0âˆ¥2
if âˆ¥y âˆ’xâ‹†âˆ¥â‰¤Ïµ, âˆ¥x0 âˆ’xâ‹†âˆ¥â‰¤Ïµ
+âˆ
otherwise,
so that 0 â‰¤f(y) âˆ’h(y) â‰¤u(y) for all y.
When âˆ¥x âˆ’xâ‹†âˆ¥â‰¤Ïµ/4 and âˆ¥x0 âˆ’xâ‹†âˆ¥â‰¤Ïµ/4. In this case, we choose Î³ = 1
2 âˆ¥x âˆ’x0âˆ¥â‰¤Ïµ/4,
so that âˆ¥x âˆ’bxâˆ¥â‰¤2Î³ â‰¤Ïµ/2 implies that âˆ¥bx âˆ’xâ‹†âˆ¥â‰¤3Ïµ/4 < Ïµ. Thus, we have
hâ€²(x) âˆ’âˆ‡f(x)
 â‰¤âˆ¥hâ€²(x) âˆ’âˆ‡f(bx)âˆ¥+ âˆ¥âˆ‡f(bx) âˆ’âˆ‡f(x)âˆ¥
â‰¤L
2Î³ âˆ¥x âˆ’x0âˆ¥2 + Lâˆ¥x âˆ’bxâˆ¥â‰¤L
2Î³ âˆ¥x âˆ’x0âˆ¥2 + 2LÎ³
by Lemma A.8 and our deï¬nition of u. The choice Î³ = 1
2 âˆ¥x âˆ’x0âˆ¥gives the lemma.
31

A.5
Proof of Lemma A.6
In the implicit iteration (13), if xk, xk+1 âˆˆ{x : âˆ¥x âˆ’xâ‹†âˆ¥â‰¤Ïµ/4}, Lemma A.4 shows that
Îµk = fâ€²
xk(xk+1; Sk) âˆ’âˆ‡f(xk; Sk) satisï¬es
âˆ¥Îµkâˆ¥â‰¤2L(Sk) âˆ¥xk âˆ’xk+1âˆ¥
(âˆ—)
â‰¤2Î±kL(Sk) âˆ¥âˆ‡f(xk; Sk)âˆ¥
â‰¤Î±kL(Sk)2 + Î±k âˆ¥âˆ‡f(xk; Sk)âˆ¥2 ,
(21)
where inequality (âˆ—) follows by the single step guarantee in Lemma A.5.
We show that each of the two terms in inequality (21) has small sum. We have E[L2(S)] <
âˆand E[P
k Î±kkâˆ’1/2L2(Sk)] < âˆ, so the Kronecker lemma implies kâˆ’1/2 Pk
i=1 Î±iL2(Si) a.s.
â†’0.
For the second term, let Ïµ > 0 be such that P
k Î±kkâˆ’1/2+Ïµ < âˆ, which must exist as Î±k =
Î±0kâˆ’Î² for Î² âˆˆ(1/2, 1). Deï¬ne
Zk :=
1
k1/2âˆ’Ïµ
k
X
i=1
Î±i
fâ€²(xi; Si)
2 ,
which is adapted to Fk. Then
E[Zk+1 | Fk] â‰¤
k1/2âˆ’Ïµ
(k + 1)1/2âˆ’Ïµ Zk + (k + 1)âˆ’1/2+ÏµÎ±k+1E[
fâ€²(xk+1; Sk+1)
2 | Fk]
â‰¤Zk + (k + 1)âˆ’1/2+ÏµÎ±k+1Gbig(âˆ¥âˆ†k+1âˆ¥),
the last inequality following by Assumption A2. On the event that supk âˆ¥âˆ†kâˆ¥< âˆ, we have
P
k kâˆ’1/2+ÏµÎ±kGbig(âˆ¥âˆ†kâˆ¥) < âˆ, so that the Robbins-Siegmund Lemma 3.1 applies, and thus
Zk
a.s.
â†’Zâˆfor some ï¬nite random variable Zâˆ. Thus kâˆ’ÏµZk =
1
âˆš
k
Pk
i=1 Î±i âˆ¥fâ€²(xi; Si)âˆ¥2 a.s.
â†’0.
The deï¬nition (4) of the iteration for xk+1 implies that fâ€²
xk(xk+1; Sk) = Î±âˆ’1
k (xk âˆ’xk+1),
and Lemma A.5 gives âˆ¥fâ€²
xk(xk+1; Sk)âˆ¥= Î±âˆ’1
k âˆ¥xk âˆ’xk+1âˆ¥â‰¤âˆ¥âˆ‚f(xk; Sk)âˆ¥. Thus, we obtain
1
âˆš
k
k
X
i=1
âˆ¥Îµiâˆ¥â‰¤
2
âˆš
k
k
X
i=1
1 {âˆ¥xi âˆ’xâ‹†âˆ¥â‰¥Ïµ/4 or âˆ¥xi+1 âˆ’xâ‹†âˆ¥â‰¥Ïµ/4}
fâ€²(xi; Si)

+ 1
âˆš
k
k
X
i=1
(Î±iL(Si)2 + Î±i
fâ€²(xi; Si)
2),
where the second term is a consequence of inequality (21). Because xk
a.s.
â†’xâ‹†(Lemma A.1),
both of these terms converge to zero with probability 1.
A.6
Proof of Lemma A.7
Because Î±k is decreasing in k, there exists some K0 such that k > K0 implies Î»Î±k â‰¤1, so
that deï¬ning C0 = QK0
i=1(|1 âˆ’Î»Î±i| âˆ¨1) exp(Î» PK0
i=1 Î±i), we have
k
Y
j=i+1
|1 âˆ’Î»Î±j| â‰¤C0 exp
ï£«
ï£­âˆ’Î»
k
X
j=i+1
Î±j
ï£¶
ï£¸.
(22)
32

Using that Î±j = Î±0jâˆ’Î², we have Pk
j=i+1 Î±j â‰¥Î±0
c
1âˆ’Î²(k1âˆ’Î² âˆ’(i + 1)1âˆ’Î²) for a numerical
constant c > 0. Consequently, by our deï¬nition of pk we have
pk â‰¤C0
k
X
i=1
Î±Ï
i exp

âˆ’c Î»Î±0
1 âˆ’Î² (k1âˆ’Î² âˆ’(i + 1)1âˆ’Î²)

.
Now, for B > 0 and k0 = BkÎ² log k, we have for a numerical constant c that k1âˆ’Î²âˆ’(i+1)1âˆ’Î² â‰¥
cB log k for all i â‰¤k0. Taking B â‰¥c(1âˆ’Î²)/(Î»Î±0), where c is a numerical constant, we obtain
Î»
k
X
j=i+1
Î±j â‰¥c Î»Î±0
1 âˆ’Î² (k1âˆ’Î² âˆ’(i + 1)1âˆ’Î²) â‰¥log k
for all i â‰¤k0. Returning to the deï¬nition of pk, we thus obtain
pk â‰¤C0
k0
X
i=1
Î±Ï
i exp(âˆ’log k) + C0
k
X
i=k0+1
Î±Ï
i â‰¤cC0Î±Ï
0
k
Z k
1
tâˆ’ÏÎ²dt + cC0
Î±0Î»Î±Ï
k Â· kÎ² log k
= c
C0Î±Ï
0
k
1
1 âˆ’ÏÎ² Â·

k1âˆ’ÏÎ² âˆ’1

+ C0 log k
Î»
Î±Ïâˆ’1
k

.
Finally, if Î»Î±1 â‰¤1, then in inequality (22) we may take C0 = 1, giving the second result
of the lemma.
B
Proofs of fast convergence on easy problems
B.1
Proof of Lemma 4.1
We assume without loss of generality that f(xâ‹†; s) = 0 for all xâ‹†âˆˆX â‹†, as we may replace f
with f âˆ’inf f. By Lemma 3.3, the update (4) satisï¬es
1
2 âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 â‰¤1
2 âˆ¥xk âˆ’xâ‹†âˆ¥2
2 + Î±k [fxk(xâ‹†; Sk) âˆ’fxk(xk+1; Sk)] âˆ’1
2 âˆ¥xk+1 âˆ’xkâˆ¥2
2 .
For shorthand, let gk = fâ€²(xk; Sk) and fk = f(xk; Sk). As fxk(xâ‹†; Sk) â‰¤f(xâ‹†; Sk) = 0, and
by Condition (C.iii) we have fxk(xk+1; Sk) â‰¥[fk + âŸ¨gk, xk+1 âˆ’xkâŸ©]+, we have
âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 â‰¤âˆ¥xk âˆ’xâ‹†âˆ¥2
2 +2Î±k

f(xâ‹†; Sk) âˆ’[fk + âŸ¨gk, xk+1 âˆ’xkâŸ©]+

âˆ’âˆ¥xk+1 âˆ’xkâˆ¥2
2 . (23)
If we let exk+1 denote the unconstrained minimizer
exk+1 = argmin
x

[fk + âŸ¨gk, x âˆ’xkâŸ©]+ +
1
2Î±k
âˆ¥x âˆ’xkâˆ¥2
2

= xkâˆ’Î»kgk for Î»k := min
(
Î±k,
fk
âˆ¥gkâˆ¥2
2
)
,
then because xk+1 âˆˆX we have
âˆ’Î±kfxk(xk+1; Sk) âˆ’1
2 âˆ¥xk+1 âˆ’xkâˆ¥2
2 â‰¤âˆ’Î±kfxk(exk+1; Sk) âˆ’1
2 âˆ¥exk+1 âˆ’xkâˆ¥2
2 .
By inspection, the guarded stepsize Î»k guarantees that [fk + âŸ¨gk, exk+1 âˆ’xkâŸ©]+ = fkâˆ’Î»k âˆ¥gkâˆ¥2
2,
and thus inequality (23) (setting f(xâ‹†; Sk) = 0) yields
âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 â‰¤âˆ¥xk âˆ’xâ‹†âˆ¥2
2 âˆ’2Î±kfxk(exk+1; Sk) âˆ’âˆ¥exk+1 âˆ’xkâˆ¥2
2
â‰¤âˆ¥xk âˆ’xâ‹†âˆ¥2
2 âˆ’2Î»kfk + Î»2
k âˆ¥gkâˆ¥2
2 .
33

We have two possible cases: whether fk/ âˆ¥gkâˆ¥2
2 â‰¶Î±k. In the case that fk/ âˆ¥gkâˆ¥2
2 â‰¤Î±k, we
have Î»k = fk/ âˆ¥gkâˆ¥2
2 and so âˆ’2Î»kfk + Î»2
k âˆ¥gkâˆ¥2
2 = âˆ’f2
k/ âˆ¥gkâˆ¥2
2. In the alternative case that
fk/ âˆ¥gkâˆ¥2
2 > Î±k, we have Î»k = Î±k and Î±2
k âˆ¥gkâˆ¥2
2 â‰¤Î±kfk. Combining these cases gives
âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 â‰¤âˆ¥xk âˆ’xâ‹†âˆ¥2
2 âˆ’min
(
Î±kfk,
f2
k
âˆ¥gkâˆ¥2
2
)
.
B.2
Proof of Proposition 2
We adopt a bit of shorthand notation. Let Dk = dist(xk, X â‹†), so Dk âˆˆFkâˆ’1. Then Lemma 4.1
implies that under Assumption A4, we have
E[D2
k+1 | Fkâˆ’1] â‰¤D2
k âˆ’min

Î»0Î±kDk, Î»1D2
k
	
= max {(1 âˆ’Î»1), (1 âˆ’Î»0Î±k/Dk)} D2
k
â‰¤max {(1 âˆ’Î»1), (1 âˆ’Î»0Î±k/D1)} D2
k,
(24)
where we have used that D1 â‰¥Dk for all k â‰¥1 by Lemma 4.1. Inequality (24) immediately
implies that if Î² â‰¥0, then
K0 = sup{k âˆˆN | Î»0Î±k > Î»1D1} =
$ Î»0Î±0
Î»1D1
1/Î²%
is the index for which k â‰¥K0 implies Î»0Î±k/D1 â‰¤Î»1.
This gives the ï¬rst result of the
proposition, E[D2
k+1] â‰¤exp(âˆ’Î»1 min{k, K0} âˆ’Î»0
D1
Pk
i=K0+1 Î±i)D2
1 if Î² â‰¥0. For Î² < 0, the
same choice of K0 gives E[D2
k+1] â‰¤exp(âˆ’Î»1 [k âˆ’K0]+ âˆ’Î»0
D1
Pkâˆ§K0
i=1
Î±i)D2
1.
For the second result, we prove only the case that Î² > 0, as the other case is similar. Note
that Pk
i=1 Î±i â‰³k1âˆ’Î², so for any Ïµ > 0 we have for constants 0 < c, C < âˆdepending on Î±0,
Î², Î»0 and Î»1 that
âˆ
X
k=1
P(Dk > ÏµÎ±k) â‰¤1
Ïµ
âˆ
X
k=1
exp

C âˆ’ck1âˆ’Î² + log 1
Î±k

< âˆ.
The Borel Cantelli lemma implies that Dk/Î±k
a.s.
â†’0. The ï¬rst inequality (24) implies that if
Vk = D2
k+1/(1 âˆ’Î»1)k+1, then
E[Vk | Fkâˆ’1] â‰¤
D2
k
(1 âˆ’Î»)k max

1, 1 âˆ’Î±k/Dk
1 âˆ’Î»1

â‰¤

1 +
1 âˆ’Î±k/Dk
1 âˆ’Î»1

+

Vkâˆ’1.
As [(1 âˆ’Î±k/Dk)/(1 âˆ’Î»1)]+ = 0 eventually with probability 1 and is Fkâˆ’1-measurable, the
Robbins-Siegmund Lemma 3.1 implies that Vk
a.s.
â†’Vâˆfor some VâˆâˆˆR+.
B.3
Proof of Lemma 4.2
Let âˆ†= x âˆ’xâ‹†for shorthand, noting that fi(xâ‹†) = 0, and note that for any c > 0 we have
1
m
m
X
i=1
min
(
Î±fi(x),
fi(x)2
âˆ¥fâ€²
i(x)âˆ¥2
2
)
â‰¥1
m
m
X
i=1
min
(
Î±|âŸ¨ai, âˆ†âŸ©|, |âŸ¨ai, âˆ†âŸ©|2
âˆ¥aiâˆ¥2
2
)
â‰¥1
m
m
X
i=1
c âˆ¥âˆ†âˆ¥2 1 {|âŸ¨ai, âˆ†âŸ©| â‰¥c âˆ¥âˆ†âˆ¥2} min

Î±, c âˆ¥âˆ†âˆ¥2
M2

,
34

where the ï¬nal inequality uses that âˆ¥aiâˆ¥2 â‰¤M by assumption.
Now, the ai âˆˆRn are
independent from some distribution on Rn satisfying P(|âŸ¨ai, âˆ†âŸ©| â‰¥c âˆ¥âˆ†âˆ¥2) â‰¥pc > 0 by
assumption. Using that the VC-dimension of linear functions v 7â†’âŸ¨ai, vâŸ©is O(n) a standard
VC argument [61, Ch. 2.6] implies that for a numerical constant C < âˆ, for any t â‰¥0 with
probability at least 1 âˆ’eâˆ’t over the random draw of the ai, we have
1
m
m
X
i=1
1 {|âŸ¨ai, âˆ†âŸ©| â‰¥c âˆ¥âˆ†âˆ¥2} â‰¥pc âˆ’C
r
n + t
m
simultaneously for all âˆ†âˆˆRn. This implies the lemma.
C
Proofs of results from Section 5
C.1
Proof of Proposition 4
The proposition follows nearly directly from Lemma 3.4. Indeed, applying that lemma, for
any xâ‹†âˆˆX â‹†we have
1
2E[âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 | Fkâˆ’1] â‰¤1
2 âˆ¥xk âˆ’xâ‹†âˆ¥2
2 âˆ’Î±k[F(xk) âˆ’F(xâ‹†)] + Î±2
k
2 M2,
where we have used Assumption A6 and that xk+1 âˆˆFkâˆ’1. Rearranging, we have
Î±kE[F(xk) âˆ’F(xâ‹†)] â‰¤1
2E
h
âˆ¥xk âˆ’xâ‹†âˆ¥2
2 âˆ’âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2
i
+ Î±2
k
2 M2.
Summing and telescoping yields Pk
i=1 Î±iE[F(xi)âˆ’F(xâ‹†)] â‰¤1
2 âˆ¥x1 âˆ’xâ‹†âˆ¥2
2+ 1
2
Pk
i=1 Î±2
i M2, and
dividing by Pk
i=1 Î±i and using convexity gives the ï¬rst result of the proposition.
For the second result, we rearrange the ï¬rst display above to see that
E[F(xk) âˆ’F(xâ‹†)] â‰¤
1
2Î±k
E[âˆ¥xk âˆ’xâ‹†âˆ¥2
2 âˆ’âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2] + Î±k
2 M2.
Summing this quantity, we obtain
k
X
i=1
E[F(xi) âˆ’F(xâ‹†)] â‰¤
k
X
i=2
 1
2Î±i
âˆ’
1
2Î±iâˆ’1

E[âˆ¥xi âˆ’xâ‹†âˆ¥2
2] +
1
2Î±1
âˆ¥x1 âˆ’xâ‹†âˆ¥2
2 + M2
2
k
X
i=1
Î±i.
Noting that E[âˆ¥xi âˆ’xâ‹†âˆ¥2
2] â‰¤R2 by assumption, dividing by k and applying Jensenâ€™s inequality
to F(xk) â‰¤1
k
Pk
i=1 F(xi) gives the result.
C.2
Proof of Lemma 5.1
Let Î£k = Î£(Sk), so that for all gk âˆˆâˆ‚f(xk+1; Sk) and y âˆˆX we have
f(y; Sk) â‰¥f(xk+1; Sk) + âŸ¨gk, y âˆ’xk+1âŸ©+ 1
2(y âˆ’xk+1)T Î£k(y âˆ’xk+1).
Using this inequality in place of the last step of the proof of Lemma 3.3 yields
1
2 âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 + Î±k
2 (xk+1 âˆ’xâ‹†)T Î£k(xk+1 âˆ’xâ‹†)
(25)
â‰¤1
2 âˆ¥xk âˆ’xâ‹†âˆ¥2
2 âˆ’Î±k [f(xk+1; Sk) âˆ’f(xâ‹†; Sk)] âˆ’1
2 âˆ¥xk âˆ’xk+1âˆ¥2
2 .
35

Using that
1
2(xk+1 âˆ’xâ‹†)T Î£k(xk+1 âˆ’xâ‹†)
= 1
2(xk âˆ’xâ‹†)T Î£k(xk âˆ’xâ‹†) + 1
2(xk+1 âˆ’xk)T Î£k(xk+1 âˆ’xk) + (xk+1 âˆ’xk)T Î£k(xk âˆ’xâ‹†)
â‰¥1 âˆ’Î·
2
(xk âˆ’xâ‹†)T Î£k(xk âˆ’xâ‹†) + Î· âˆ’1
2Î· (xk+1 âˆ’xk)T Î£k(xk+1 âˆ’xk)
for all Î· > 0, where the inequality follows from Youngâ€™s inequality.
Inequality (25) then
implies
1
2 âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 â‰¤1
2(xk âˆ’xâ‹†)T (I âˆ’Î±k(1 âˆ’Î·)Î£k) (xk âˆ’xâ‹†) âˆ’Î±k [f(xk+1; Sk) âˆ’f(xâ‹†; Sk)]
âˆ’1
2(xk âˆ’xk+1)T

I + Î±k(Î· âˆ’1)
Î·
Î£k

(xk âˆ’xk+1)
for all Î· > 0. The choice Î·k =
2Î±kÎ»max(Î£k)
1+2Î±kÎ»max(Î£k) âˆˆ(0, 1) is suï¬ƒcient to guarantee I + Î±k(Î·kâˆ’1)
Î·k
Î£k âª°
1
2I. Substituting this choice of Î·k above, we obtain that
1
2 âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2 â‰¤1
2(xk âˆ’xâ‹†)T (I âˆ’Î±k(1 âˆ’Î·k)Î£k) (xk âˆ’xâ‹†)
âˆ’Î±k [f(xk+1; Sk) âˆ’f(xâ‹†; Sk)] âˆ’1
4 âˆ¥xk âˆ’xk+1âˆ¥2
2 .
Applying Lemma 3.2 with y = xâ‹†, x1 = xk+1, x0 = xk, Î² = 2Î±k, and g(Â·) = f(Â·; Sk) implies
1
2 âˆ¥xk+1 âˆ’xâ‹†âˆ¥2
2
â‰¤(xk âˆ’xâ‹†)T (I âˆ’Î±k(1 âˆ’Î·k)Î£k) (xk âˆ’xâ‹†) + 2Î±kâŸ¨fâ€²(xâ‹†; Sk), xâ‹†âˆ’xkâŸ©+ Î±2
k
fâ€²(xâ‹†; Sk)
2
2 .
Taking expectations and using that E[âŸ¨fâ€²(xâ‹†; S), xâ‹†âˆ’xâŸ©] â‰¤0 for all x (as in the proof of
Theorem 1) gives the desired result.
References
[1] F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algo-
rithms for machine learning. In Advances in Neural Information Processing Systems 25,
pages 451â€“459, 2011.
[2] H. Bauschke and J. Borwein.
On projection algorithms for solving convex feasibility
problems. SIAM Review, 38(3):367â€“426, 1996.
[3] M. Belkin, D. Hsu, and P. Mitra. Overï¬tting or perfect ï¬tting? Risk bounds for classiï¬-
cation and regression rules that interpolate. arXiv:1806.05161 [stat.ML], 2018.
[4] M. Belkin, A. Rakhlin, and A. B. Tsybakov. Does data interpolation contradict statistical
optimality? arXiv:1806.09471 [stat.ML], 2018.
[5] D. Bertsekas. Nonlinear Programming. Athena Scientiï¬c, 1999.
[6] D. P. Bertsekas. Stochastic optimization problems with nondiï¬€erentiable cost functionals.
Journal of Optimization Theory and Applications, 12(2):218â€“231, 1973.
36

[7] D. P. Bertsekas.
Incremental proximal methods for large scale convex optimization.
Mathematical Programming, Series B, 129:163â€“195, 2011.
[8] P. Bianchi. Ergodic convergence of a stochastic proximal point algorithm. SIAM Journal
on Optimization, 26(4):2235â€“2260, 2016.
[9] L. Bottou and O. Bousquet. The tradeoï¬€s of large scale learning. In Advances in Neural
Information Processing Systems 20, 2007.
[10] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[11] J. Burke. Descent methods for composite nondiï¬€erentiable optimization problems. Math-
ematical Programming, 33:260â€“279, 1985.
[12] J. Burke and M. Ferris. A Gauss-Newton method for convex composite optimization.
Mathematical Programming, 71:179â€“194, 1995.
[13] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Research, 2:265â€“292, 2001.
[14] D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex
functions. SIAM Journal on Optimization, 29(1):207â€“239, 2019.
[15] D. Davis, D. Drusvyatskiy, and C. Paquette. The nonsmooth landscape of phase retrieval.
arXiv:1711.03247 [math.OC], 2017.
[16] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato,
A. Senior, P. Tucker, K. Yang, and A. Y. Ng. Large scale distributed deep networks. In
Advances in Neural Information Processing Systems 26, 2012.
[17] A. Dembo. Lecture notes on probability theory: Stanford statistics 310. Accessed October
1, 2016, 2016. URL http://statweb.stanford.edu/~adembo/stat-310b/lnotes.pdf.
[18] D. Drusvyatskiy and A. Lewis. Error bounds, quadratic growth, and linear convergence
of proximal methods. Mathematics of Operations Research, 43(3):919â€“948, 2018.
[19] J. C. Duchi and F. Ruan. Stochastic methods for composite and weakly convex opti-
mization problems. SIAM Journal on Optimization, 28(4):3229â€“3259, 2018.
[20] J. C. Duchi and F. Ruan. Asymptotic optimality in stochastic optimization. Annals of
Statistics, To Appear, 2019.
[21] S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex
stochastic composite optimization, I: a generic algorithmic framework. SIAM Journal on
Optimization, 22(4):1469â€“1492, 2012.
[22] S. Ghadimi and G. Lan. Stochastic ï¬rst- and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341â€“2368, 2013.
[23] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer,
second edition, 2009.
[24] E. Hazan and S. Kale. An optimal algorithm for stochastic strongly convex optimization.
In Proceedings of the Twenty Fourth Annual Conference on Computational Learning
Theory, 2011. URL http://arxiv.org/abs/1006.2425.
[25] J. Hiriart-Urruty and C. LemarÂ´echal. Convex Analysis and Minimization Algorithms I
& II. Springer, New York, 1993.
[26] A. J. Hoï¬€man. On approximate solutions of systems of linear inequalities. Journal of
Research of the National Bureau of Standards, 49(4):263â€“265, 1952.
[27] H. Hu and Q. Wang. On approximate solutions of inï¬nite systems of linear inequalities.
37

Linear Algebra and its Applications, 114â€“115:429â€“438, 1989.
[28] N. Karampatziakis and J. Langford. Online importance weight aware updates. In Pro-
ceedings of the 27th Conference on Uncertainty in Artiï¬cial Intelligence, 2011.
[29] J. E. Kelley. The cutting-plane method for solving convex programs. Journal of the
Society for Industrial and Applied Mathematics, 8(4):703â€“712, 1960.
[30] B. Kulis and P. Bartlett. Implicit online learning. In Proceedings of the 27th International
Conference on Machine Learning, 2010.
[31] H. J. Kushner and G. Yin.
Stochastic Approximation and Recursive Algorithms and
Applications. Springer, second edition, 2003.
[32] G. Lan. An optimal method for stochastic composite optimization. Mathematical Pro-
gramming, Series A, 133(1â€“2):365â€“397, 2012.
[33] L. Le Cam and G. L. Yang. Asymptotics in Statistics: Some Basic Concepts. Springer,
2000.
[34] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436â€“444, 2015.
[35] D. Leventhal and A. S. Lewis. Randomized methods for linear constraints: convergence
rates and conditioning. Mathematics of Operations Research, 35(3):641â€“654, 2010.
[36] A. S. Lewis, D. R. Luke, and J. Malick. Local linear convergence for alternating and
averaged nonconvex projections. Foundations of Computational Mathematics, 9(4):485â€“
513, 2009.
[37] S. Ma, R. Bassily, and M. Belkin. The power of interpolation: Understanding the ef-
fectiveness of SGD in modern over-parametrized learning. In Proceedings of the 35th
International Conference on Machine Learning, 2018.
[38] P. McCullagh and J. Nelder. Generalized Linear Models. Chapman and Hall, London,
1989.
[39] S. Mendelson. Learning without concentration. In Proceedings of the Twenty Seventh
Annual Conference on Computational Learning Theory, 2014.
[40] D. Needell and J. Tropp. Paved with good intentions: Analysis of a randomized block
Kaczmarz method. Linear Algebra and its Applications, 441:199â€“221, 2014.
[41] D. Needell, R. Ward, and N. Srebro. Stochastic gradient descent, weighted sampling,
and the randomized Kaczmarz algorithm. In Advances in Neural Information Processing
Systems 28, pages 1017â€“1025, 2014.
[42] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574â€“1609,
2009.
[43] Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publish-
ers, 2004.
[44] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 2006.
[45] A. Patrascu and I. Necoara. Nonasymptotic convergence of stochastic proximal point
algorithms for constrained convex optimization. arXiv:1706.06297 [math.OC], 2017.
[46] B. T. Polyak. Introduction to Optimization. Optimization Software, Inc., 1987.
[47] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging.
SIAM Journal on Control and Optimization, 30(4):838â€“855, 1992.
[48] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical
38

Statistics, 22:400â€“407, 1951.
[49] H. Robbins and D. Siegmund. A convergence theorem for non-negative almost super-
martingales and some applications. In Optimizing Methods in Statistics, pages 233â€“257.
Academic Press, New York, 1971.
[50] R. T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal
on Control and Optimization, 14:877â€“898, 1976.
[51] E. Ryu and S. Boyd. Stochastic proximal iteration: A non-asymptotic improvement upon
stochastic gradient descent, 2014.
URL http://www.math.ucla.edu/~eryu/papers/
spi.pdf.
[52] M. Schmidt and N. Le Roux. Fast convergence of stochastic gradient descent under a
strong growth condition. arXiv:1308.6370 [math.OC], 2013.
[53] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter.
Pegasos: primal estimated
sub-gradient solver for SVM. Mathematical Programming, Series B, 127(1):3â€“30, 2011.
[54] A. Shapiro, D. Dentcheva, and A. RuszczyÂ´nski. Lectures on Stochastic Programming:
Modeling and Theory. SIAM and Mathematical Programming Society, 2009.
[55] T. Strohmer and R. Vershynin. A randomized Kaczmarz algorithm with exponential
convergence. Journal of Fourier Analysis and Applications, 15(2):262â€“278, 2009.
[56] C. H. Teo, S. Vishwanthan, A. J. Smola, and Q. V. Le. Bundle methods for regularized
risk minimization. Journal of Machine Learning Research, 11:311â€“365, 2010.
[57] P. Toulis and E. Airoldi. Asymptotic and ï¬nite-sample properties of estimators based on
stochastic gradients. Annals of Statistics, 45(4):1694â€“1727, 2017.
[58] P. Toulis, D. Tran, and E. Airoldi. Towards stability and optimality in stochastic gradient
descent. In Proceedings of the 19th International Conference on Artiï¬cial Intelligence and
Statistics, 2016.
[59] L. N. Trefethen and D. Bau III. Numerical Linear Algebra. SIAM, 1997.
[60] A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Proba-
bilistic Mathematics. Cambridge University Press, 1998.
[61] A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes:
With Applications to Statistics. Springer, New York, 1996.
[62] R. Vershynin.
Introduction to the non-asymptotic analysis of random matrices.
In
Compressed Sensing: Theory and Applications, chapter 5, pages 210â€“268. Cambridge
University Press, 2012.
[63] B. Widrow and M. E. Hoï¬€. Adaptive switching circuits. 1960 IRE WESCON Convention
Record, pages 96â€“104, 1960. Reprinted in Neurocomputing (MIT Press, 1988).
[64] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning
requires rethinking generalization. In Proceedings of the Fifth International Conference
on Learning Representations, 2017.
[65] T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent
algorithms. In Proceedings of the Twenty-First International Conference on Machine
Learning, 2004.
[66] M. Zinkevich. Online convex programming and generalized inï¬nitesimal gradient ascent.
In Proceedings of the Twentieth International Conference on Machine Learning, 2003.
39
