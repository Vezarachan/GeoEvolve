Human-Timescale Adaptation in an
Open-Ended Task Space
Adaptive Agents Team1
1DeepMind
Foundation models have shown impressive adaptation and scalability in supervised and self-supervised
learning problems, but so far these successes have not fully translated to reinforcement learning (RL).
In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning
algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a
vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-Ô¨Çy hypothesis-
driven exploration, eÔ¨Écient exploitation of acquired knowledge, and can successfully be prompted
with Ô¨Årst-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement
learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale
attention-based memory architecture, and (3) an eÔ¨Äective automated curriculum that prioritises tasks
at the frontier of an agent‚Äôs capabilities. We demonstrate characteristic scaling laws with respect to
network size, memory length, and richness of the training task distribution. We believe our results lay
the foundation for increasingly general and adaptive RL agents that perform well across ever-larger
open-ended domains.
Trial 2
20 seconds of experience
Trial 1 
No test-time experience
40 seconds of experience
Trial 3
DEAD
END
REWARD
REWARD
Score: 0
Score: 13
Score: 16
Success
Exploration
ReÔ¨Ånement
Object caried
AdA‚Äôs trajectory
Object dropped
Figure 1 | Human timescale adaptation. Example trajectories of our agent (AdA) solving a held-out
task in a complex 3D environment within minutes of test-time experience without any further agent
training. Initial trials (Exploration) show a policy that uncovers hidden environment dynamics.
After just seconds of test-time experience (Success), AdA Ô¨Ånds a valid solution to the task. Later
(ReÔ¨Ånement), it improves this solution, gradually Ô¨Ånding a more rewarding behaviour. The solid
white lines show agent movement. The dashed coloured lines show the agent carrying an object of
the corresponding colour. For a full description of the task, see Figure B.1. Videos of AdA‚Äôs behaviour
are available on our microsite and accompanying results reel.
Corresponding authors: Feryal Behbahani (feryal@deepmind.com) and Edward Hughes (edwardhughes@deepmind.com).
¬© 2023 DeepMind. All rights reserved
arXiv:2301.07608v1  [cs.LG]  18 Jan 2023

Human-Timescale Adaptation in an Open-Ended Task Space
1. Introduction
The ability to adapt in minutes is a deÔ¨Åning characteristic of human intelligence and an important
milestone on the path towards general intelligence. Given any level of bounded rationality, there will
be a space of tasks in which it is impossible for agents to succeed by just generalising their policy
zero-shot, but where progress is possible if the agent is capable of very fast in-context learning from
feedback. To be useful in the real world, and in interaction with humans, our artiÔ¨Åcial agents should
be capable of fast and Ô¨Çexible adaptation given only a few interactions, and should continue to adapt
as more data becomes available. Operationalising this notion of adaptation, we seek to train an agent
that, given few episodes in an unseen environment at test time, can accomplish a task that requires
trial-and-error exploration and can subsequently reÔ¨Åne its solution towards optimal behaviour.
Meta-RL has been shown to be eÔ¨Äective for fast in-context adaptation (e.g. Yu et al. (2020);
Zintgraf (2022)). However, meta-RL has had limited success in settings where the reward is sparse
and the task space is vast and diverse (Yang et al., 2019). Outside RL, foundation models in semi-
supervised learning have generated signiÔ¨Åcant interest (Bommasani et al., 2021) due to their ability
to adapt in few shots from demonstrations across a broad range of tasks. These models are designed
to provide a strong foundation of general knowledge and skills that can be built upon and adapted to
new situations via Ô¨Åne-tuning or prompting with demonstrations (Brown et al., 2020). Crucial to
this success has been attention-based memory architectures like Transformers (Vaswani et al., 2017),
which show power-law scaling in performance with the number of parameters (Tay et al., 2022).
XLand task pool
Random sample
Passed evaluation
Training set
Uniform
sample
Select the tasks
above the
Ô¨Åtness threshold
Add them to
the training set
Evaluate the tasks
(compute the Ô¨Åtness)
Train the agent
Level
Goal
Task
1 trial
1 episode = k trials
AdA
Agent
RL update
Distillation update
Transformer-XL
obst
obsT
actiont
valuet
actionT
valueT
Figure 2 | Training our Adaptive Agent (AdA). We train a large Transformer model with meta-RL
in XLand. During training, tasks are uniformly sampled, and subsequently Ô¨Åltered to produce an
ever-changing training pool of tasks at the frontier of the agent‚Äôs capabilities. After training on these
tasks, the agent is capable of adapting to unseen hand-authored tasks as eÔ¨Äectively and eÔ¨Éciently as
humans.
In this work, we pave the way for training an RL foundation model; that is, an agent that has
been pre-trained on a vast task distribution and that, at test time, can adapt few-shot to a broad
range of downstream tasks. We introduce Adaptive Agent (AdA), an agent capable of human-timescale
adaptation in a vast open-ended task space with sparse rewards. AdA does not require any prompts
(Reed et al., 2022), Ô¨Åne-tuning (Lee et al., 2022) or access to oÔ¨Ñine datasets (Laskin et al., 2022; Reed
et al., 2022). Instead, AdA exhibits hypothesis-driven exploratory behaviour, using information gained
on-the-Ô¨Çy to reÔ¨Åne its policy and to achieve close to optimal performance. AdA acquires knowledge
eÔ¨Éciently, adapting in minutes on challenging held-out sparse-reward tasks in a partially-observable
3D environment with a Ô¨Årst-person pixel observation. A human study conÔ¨Årms that the timescale of
2

Human-Timescale Adaptation in an Open-Ended Task Space
AdA‚Äôs adaptation is comparable to that of trained human players. AdA‚Äôs adaptation behaviour in a
representative held-out task can be seen in Figure 1. AdA can also achieve improved performance
through zero-shot prompting with Ô¨Årst-person demonstrations, analogously to foundation models in
the language domain.
We use Transformers as an architectural choice to scale in-context fast adaptation via model-based
RL2 (Duan et al., 2017; Melo, 2022; Wang et al., 2016). Foundation models typically require large,
diverse datasets to achieve their generality (Brown et al., 2020; Mahajan et al., 2018; Schuhmann
et al., 2022; Sun et al., 2017; Zhai et al., 2022). To make this possible in an RL setting, where
agents collect their own data, we extend the recent XLand environment (OEL Team et al., 2021),
producing a vast open-ended world with over 1040 possible tasks. These tasks require a range of
diÔ¨Äerent online adaptation capabilities, including experimentation, navigation, coordination, division
of labour and coping with irreversibility. Given the wide range of possible tasks, we make use of
adaptive auto-curricula, which prioritise tasks at the frontier of an agent‚Äôs capabilities (Jiang et al.,
2021a; OEL Team et al., 2021). Finally, we make use of distillation (Schmitt et al., 2018), which
enables scaling to models with over 500M parameters, to the best of our knowledge the largest model
trained from scratch with RL at the time of publication (Ota et al., 2021). A high level overview of
our method is shown in Figure 2.
Our main contributions are as follows:
‚Ä¢ We introduce AdA, an agent capable of human-timescale adaptation in a wide range of chal-
lenging tasks.
‚Ä¢ We train AdA using meta-RL at scale in an open-ended task space with an automated curriculum.
‚Ä¢ We show that adaptation is inÔ¨Çuenced by memory architecture, curriculum, and the size and
complexity of the training task distribution.
‚Ä¢ We produce scaling laws in both model size and memory, and demonstrate that AdA improves
its performance with zero-shot Ô¨Årst-person prompting.
2. Adaptive Agent (AdA)
To achieve human timescale adaptation across a vast and diverse task space, we propose a general
and scalable approach for memory-based meta-RL, producing an Adaptive Agent (AdA). We train and
test AdA in XLand 2.0, an environment supporting procedural generation of diverse 3D worlds and
multi-player games, with rich dynamics that necessitate adaptation. Our training method combines
three key components: a curriculum to guide the agent‚Äôs learning, a model-based RL algorithm to
train agents with large-scale attention-based memory, and distillation to enable scaling. An overview
of our approach is shown in Figure 2. In the following sections, we describe each component and
how it contributes to eÔ¨Écient few-shot adaptation.
2.1. Open-ended task space: XLand 2.0
In order to demonstrate fast adaptation across an open-ended task space, we extend the procedurally-
generated 3D environment XLand (OEL Team et al., 2021), which we refer to here as XLand 1.0. In
XLand, a task consists of a game, a world, and a list of co-player policies (if any). The game consists
of a goal per player, deÔ¨Åned as a boolean function (predicate) on the environment state. An agent
receives reward if and only if the goal is satisÔ¨Åed. Goals are deÔ¨Åned in a synthetic language, and the
agent receives an encoding. The world speciÔ¨Åes a static Ô¨Çoor topology, objects the player can interact
with, and spawn locations for players. The agent observes the world, and any co-players therein, via
a Ô¨Årst-person pixel observation. All fundamental details of the game, world and co-player system are
3

Human-Timescale Adaptation in an Open-Ended Task Space
Figure 3 | XLand 2.0: a vast, smooth and diverse task space of adaptation problems. DiÔ¨Äerent
tasks have diÔ¨Äerent adaptation requirements, such as experimentation, tool use or division of labour.
For instance, in a task requiring experimentation, a player might be required to identify which objects
can usefully combine, avoiding dead-ends, and then optimise the way in which they combine objects,
like a toy version of experimental chemistry. Each task can be run for one or more trials, where the
environment is reset between trials, but agent memory is not. Highlighted are two example tasks,
Wrong Pair Disappears and Pass Over Wall Repeatedly, showing the goal, initial objects,
production rules (‚Äúrules‚Äù in the Ô¨Ågure) and how agents need to interact with them to solve the task.
For full task descriptions see Appendix F.1.
inherited from the original XLand; see OEL Team et al. (2021) for a full description and Appendix A.1
for details of the new features we added.
XLand 2.0 extends XLand 1.0 with a system called production rules. Each production rule expresses
an additional environment dynamic, leading to a much richer and more diverse array of diÔ¨Äerent
transition functions than in XLand 1.0. The production rules system can be thought of as a domain-
speciÔ¨Åc language (DSL) to express this diverse array of dynamics. Each production rule consists
of:
1. A condition, which is a predicate, for example near(yellow sphere,black cube),
2. A (possibly empty) list of spawns, which are objects, like purple cube, black cube.
When condition is satisÔ¨Åed, the objects present in condition get removed from the environ-
ment, and the ones in spawns appear. Each game can have multiple production rules. Production
rules can be observable to players, or partially or fully masked, depending on the task conÔ¨Åguration.
More precisely, there are three distinct mechanisms for hiding production rule information from the
players:
1. Hiding a full production rule, where the player only gets information that a rule exists, but
neither knows the condition nor what spawns.
4

Human-Timescale Adaptation in an Open-Ended Task Space
2. Hiding an object, where a particular object is hidden from all production rules. The hidden
objects are numbered such that if multiple objects are hidden, the agent can distinguish them.
3. Hiding a condition‚Äôs predicate, where the agent gets to know the objects that need to satisfy
some predicate, but it does not know which one. The hidden predicates are also numbered.
Instead of procedurally generating tasks on the Ô¨Çy, we pre-sample a large pool of tasks. For more
details about the speciÔ¨Åc mechanism we use for pre-sampling tasks, see Appendix A.2. We visualise
the XLand 2.0 task space in Figure 3.
2.2. Meta-RL
We use a black-box meta-RL problem setting (Duan et al., 2017; Wang et al., 2016). We deÔ¨Åne the
task space M to be a set of partially-observable Markov decision processes (POMDPs). For a given
task ùëö‚ààM we deÔ¨Åne a trial to be any sequence of transitions from an initial state ùë†0 to a terminal
state ùë†ùëá.1 In XLand, tasks terminate if and only if a certain time period ùëá‚àà[10s, 40s] has elapsed,
speciÔ¨Åed per-task. The environment ticks at 30 frames-per-second and the agent observes every 4th
frame, so task lengths in units of timesteps lie in the range [75, 300].
An episode consists of a sequence of ùëòtrials for a given task ùëö. At trial boundaries, the task is reset
to an initial state. In our domain, initial states are deterministic except for the rotation of the agent,
which is sampled uniformly at random. The trial and episode structure is depicted in Figure 3.
In black-box meta-RL training, an agent uses experience of interacting with a wide distribution
of tasks to update the parameters of its neural network, which parameterises the agent‚Äôs policy
distribution over actions given a state observation. If an agent possesses dynamic internal state
(memory), then meta-RL training endows that memory with an implicit online learning algorithm, by
leveraging the structure of repeated trials (Mikulik et al., 2020).
At test time, this online learning algorithm enables the agent to adapt its policy without any
further updates to the neural network weights. Therefore, the memory of the agent is not reset at
trial boundaries, but is reset at episode boundaries. To generate an episode, we sample a pair (ùëö,
ùëò) where ùëò‚àà{1, 2, . . . 6}. As we will discuss later, at test time AdA is evaluated on unseen, held-out
tasks across a variety of ùëòvalues, including on held-out ùëònot seen during training. For full details on
AdA‚Äôs meta-RL method, see Appendix D.1.
2.3. Auto-curriculum learning
Given the vastness and diversity of our pre-sampled task pool, it is challenging for an agent to learn
eÔ¨Äectively with uniform sampling. Most randomly sampled tasks are likely going to be too hard (or
too easy) to beneÔ¨Åt an agent‚Äôs learning progress. Instead, we use automatic approaches to select
‚Äúinteresting‚Äù tasks at the frontier of the agent‚Äôs capabilities, analogous to the ‚Äúzone of proximal
development‚Äù in human cognitive development (Vygotsky, 1978). We propose extensions to two
existing approaches, both of which strongly improve agent performance and sample eÔ¨Éciency (see
Section 3.3), and lead to an emergent curriculum, selecting tasks with increasing complexity over
time.
No-op Ô¨Åltering.
We extend the dynamic task generation method proposed in OEL Team et al. (2021,
Section 5.2) to our setup. When a new task is sampled from the pool, it is Ô¨Årst evaluated to assess
1Note that we use a reversed naming convention to Duan et al. (2017). In our convention, the term ‚Äútrial‚Äù maps well
onto the related concept in the human behavioural literature (Barbosa et al., 2022).
5

Human-Timescale Adaptation in an Open-Ended Task Space
whether AdA can learn from it. We evaluate AdA‚Äôs policy and a ‚ÄúNo-op‚Äù control policy (which takes
no action in the environment) for a number of episodes. The task is used for training if and only if
the scores of the two policies meet a number of conditions. We expanded the list of conditions from
the original no-op Ô¨Åltering and used normalised thresholds to account for diÔ¨Äerent trial durations.
See Appendix D.5 for further details.
Prioritised level replay (PLR).
We modify ‚ÄúRobust PLR‚Äù (referred to here as PLR, Jiang et al.
(2021a)) to Ô¨Åt our setup. By contrast to no-op Ô¨Åltering, PLR uses a Ô¨Åtness score (Schmidhuber, 1991)
that approximates the agent‚Äôs regret for a given task. We consider several potential estimates for
agent regret, ranging from TD errors as used in Jiang et al. (2021b), to novel approaches using
dynamics-model errors from AdA (see Appendix D.5 and Figure D.1).
PLR operates by maintaining a Ô¨Åxed-sized archive containing tasks with the highest Ô¨Åtness. We
only train AdA on tasks sampled from the archive, which occurs with probability ùëù. With probability
1 ‚àíùëù, a new task is randomly sampled and evaluated, and the Ô¨Åtness is compared to the lowest value
in the archive. If the new task has higher Ô¨Åtness, it is added to the archive, and the lowest Ô¨Åtness task
is dropped. Thus, PLR can also be seen as a form of Ô¨Åltering, using a dynamic criteria (the lowest
Ô¨Åtness value of the archive). It diÔ¨Äers to no-op Ô¨Åltering in that tasks can be repeatedly sampled from
the archive as long as they maintain high Ô¨Åtness. To apply PLR in our heterogeneous task space,
we normalise Ô¨Åtness at each trial index by using rolling means and variances, and use the mean
per-timestep Ô¨Åtness value rather than the sum, to account for varying trial duration. Finally, since we
are interested in tasks at the frontier of an agent‚Äôs capabilities after across-trial adaptation, we use
only the Ô¨Åtness from the last trial. See Appendix D.5 for further details.
2.4. RL agent
Learning algorithm.
We use Muesli (Hessel et al., 2021) as our RL algorithm. We brieÔ¨Çy describe
the algorithm here, but refer the reader to the original publication for details. Taking a history-
dependent encoding as input, in our case the output of an RNN or Transformer, AdA learns a sequence
model (an LSTM) to predict the values ÀÜùë£ùëñ, action-distributions ÀÜùúãùëñand rewards ÀÜùëüùëñfor the next ùêºsteps.
Here, ùëñ= 0, . . . , ùêºdenotes the prediction ùëñsteps ahead. ùêºis typically small and in our case ùêº= 4. For
each observed step ùë°, the model is unrolled for ùêºsteps and updated towards respective targets:
Lùë°
ùëü=
ùêº‚àëÔ∏Å
ùëñ=0
 ÀÜùëüùë°
ùëñ‚àíùëüùë°+ùëñ
2 , Lùë°
ùë£=
ùêº‚àëÔ∏Å
ùëñ=0
 ÀÜùë£ùë°
ùëñ‚àíùê∫ùë°+ùëñ
2 , Lùë°
ùúã=
ùêº‚àëÔ∏Å
ùëñ=0
KL

ùúãùë°+ùëñ
CMPO
 ÀÜùúãùë°
ùëñ

.
(1)
Here, ùëüùë°+ùëñrefers to the observed rewards. ùê∫ùë°+ùëñrefers to value-targets which are obtained using
Retrace (Munos et al., 2016) based on Q-values obtained from one-step predictions of the model.
The action-targets ùúãùë°
CMPO are obtained by re-weighting the current policy2 using clipped, nor-
malised, exponentially transformed advantages. Muesli furthermore incorporates an additional
auxiliary policy-gradient loss based on these advantages to help optimise immediate predictions of
action-probabilities. Finally, Muesli maintains a target network which trails the sequence model and
is used for acting and to compute Retrace targets and advantages.
Memory architecture.
Memory is a crucial component for adaptation as it allows the agent to store
and recall information learned and experienced in the past. In order for agents to eÔ¨Äectively adjust to
2The prior distribution is actually a mixture of the current estimate of the policy, the (outdated) policy used to produce
the sample and the uniform distribution where the latter two are mixed in as regularisers.
6

Human-Timescale Adaptation in an Open-Ended Task Space
Observations
Embedding
Value
Head
Policy
Head
Value
Head
Policy
Head
Observations
Embedding
Transformer-XL
t
T
vt
vT
RGB
Goals
Hand Sensor
Rules
Trial Info  
t-1 rt-1
RGB
Goals
Hand Sensor
Rules
Trial Info  
rT-1
T-1
Figure 4 | Agent architecture. For each timestep, we embed and combine the pixel observation,
goal, hand, trial and time information, production rules, previous action, and previous reward into a
single vector. These observations embeddings pass in sequence to the Transformer-XL, whose output
embeddings feed into an MLP value head, MLP policy head, and the Muesli LSTM model step (omitted
in the diagram for brevity). See Appendix C.1 for more details about our agent architecture.
the changes in task requirements, memory should allow the agent to recall information from both the
very recent and the more distant past. While slow gradient-based updates are able to capture the
latter, they are often not fast enough to capture the former, i.e. fast adaptation. The majority of work
on memory-based meta-RL has relied on RNNs as a mechanism for fast adaptation (Parisotto, 2021).
In this work, we show that RNNs are not capable of adaptation in our challenging partially-observable
embodied 3D task space. We experiment with two memory architectures to address this problem:
1. RNN with Attention stores a number of past activations (in our case 64) in an episodic memory
and attends over it, using the current hidden state as query. The output of the attention module
is then concatenated with the hidden state and fed into the RNN. We increase eÔ¨Äective memory
length of the agent by storing only every 8th activation in its episodic memory.3
2. Transformer-XL (TXL) (Dai et al., 2019) is a variant of the Transformer architecture (Vaswani
et al., 2017) which enables the use of longer, variable-length context windows to increase
the model‚Äôs ability to capture long-term dependencies. To increase the stability of training
Transformers with RL, we follow Parisotto et al. (2020) in performing normalisation before each
layer, and use gating on the feedforward layers as in Shazeer (2020).
Both memory modules operate on a sequence of learned timestep embeddings, and produce
a sequence of output embeddings that are fed into the Muesli architecture, as shown in Figure 4
with a Transformer-XL module. In Section 3.2 we show that both attention-based memory modules
signiÔ¨Åcantly outperform a vanilla RNN in tasks that require adaptation. Transformer-XL performs the
best and therefore is used as the default memory architecture in all our experiments unless stated
otherwise.
3We arrived at these numbers as a compromise between performance and speed. Note that the resulting architecture is
slower than an equivalently sized Transformer.
7

Human-Timescale Adaptation in an Open-Ended Task Space
Going beyond few shots. We propose a simple modiÔ¨Åcation to our Transformer-XL architecture
to increase the eÔ¨Äective memory length without additional computational cost. Since observations
in visual RL environments tend to be highly temporally correlated, we propose sub-sampling the
sequence as described for RNN with Attention, allowing the agent to attend over 4 times as many
trials. To ensure that observations which fall between the sub-sampled points can still be attended to,
we Ô¨Årst encode the entire trajectory using an RNN with the intention of summarising recent history
at every step. We show that the additional RNN encoding does not aÔ¨Äect the performance of our
Transformer-XL variant but enables longer range memory (see Section 3.7).
2.5. Distillation
For the Ô¨Årst four billion steps of training, we use an additional distillation loss (Czarnecki et al., 2019;
Schmidhuber, 1992; Schmitt et al., 2018) to guide AdA‚Äôs learning with the policy of a pre-trained
teacher, in a process known as kickstarting; iterating this process leads to a generational training
regime (OEL Team et al., 2021; Wang et al., 2021). The teacher is pre-trained from scratch via RL,
using an identical training procedure and hyperparameters as AdA, apart from the lack of initial
distillation and a smaller model size (23M Transformer parameters for the teacher and 265M for
multi-agent AdA). Unlike aforementioned prior work, we do not employ shaping rewards or Population
Based Training (PBT, Jaderberg et al. (2017)) in earlier generations. During distillation, AdA acts
according to its own policy and the teacher provides target logits given the trajectories observed by
AdA. Distillation allows us to amortise an otherwise costly initial training period, and it allows the
agent to overcome harmful representations acquired in the initial phases of training; see Section 3.6.
To integrate the distillation loss with Muesli, we unroll the model from every transition observed
by the student. We minimise the KL-divergence between all of the action-probabilities predicted by
the model and the action-probabilities predicted by the teacher‚Äôs policy at the corresponding timestep.
Analogously to Muesli‚Äôs policy-loss LùúãdeÔ¨Åned in (1), we deÔ¨Åne
Ldist =
ùêº‚àëÔ∏Å
ùëñ=0
KL

Àúùúãùë°+ùëñ
0
 ÀÜùúãùë°
ùëñ

,
(2)
where Àúùúãcorresponds to the predicted action-logits provided by the teacher given the same observed
history. Furthermore, we found it useful to add additional ùêø2 regularisation during distillation.
3. Experiments and Results
We evaluate our agents in two distinct regimes: on a set of 1000 test tasks sampled from the same
distribution as the training tasks, and on a set of 30 single-agent and 28 multi-agent hand-authored
probe tasks. A rejection sampling procedure guarantees that the procedural test tasks and probe
tasks are outside the training set. The probe tasks represent situations that are particularly intuitive
to humans, and deliberately cover a wide range of qualitatively diÔ¨Äerent adaptation behaviours.
Example probe tasks are depicted in Figures B.1 to B.3 in the Appendix, and a full description of
every probe task is available in Appendix F.
The total achievable reward on each task varies, so whenever we present aggregated results on
the test or hand-authored task set, we normalise the total per-trial reward for each task against the
reward obtained by Ô¨Åne-tuning AdA on the respective task set. We refer to this normalised reward
as a score. We stipulate that an adaptive agent must have two capabilities: zero-shot generalisation
and few-shot adaptation. Zero-shot generalisation is assessed by the score in the case of only being
given 1 trial of interaction with a held-out task. Few-shot adaptation is assessed by the improvement
8

Human-Timescale Adaptation in an Open-Ended Task Space
Generalisation: positive zero-shot return on 88% of test tasks
Adaptation: the agent achieves > 0.8 score 
on 40% of tasks zero-shot but this 
improves to 72% of tasks after 13 trials
Adaptation gap: 
we measure the 
20th percentile 
normalized score, 
here 0.04 on the 
Ô¨Årst trial and 0.61 
on the 13th
Figure 5 | Zero-shot generalisation and few-shot adaptation. We report the distribution of nor-
malised task scores over the single-agent test set when evaluated with various numbers of trials.
On the ùë¶-axis is the total last-trial reward relative to that of an agent Ô¨Åne-tuned on the test tasks
(approximating ‚ÄúinÔ¨Ånite trials‚Äù performance). Curves moving further towards the top right corner
indicate better performance. When given more trials, the agent achieves higher scores in the last trial,
showing test-time adaptation across most of the task distribution (shaded regions). The dashed line
indicates the zero-shot performance of an agent trained in a regime where every episode consists of
only a single trial.
in score as the agent is given progressively more trials (ùëò) of interaction with the task. More precisely,
for each ùëòwe report the score in the last trial, showing whether or not an agent is able to make use of
additional experience on-the-Ô¨Çy to perform better, i.e. measuring adaptation.
We aggregate scores across a task set using (one or more) percentiles. When presenting individual
probe tasks we report unnormalised total last trial rewards per task for agents and for human players
where applicable. For full details of our evaluation methodology see Appendix B.
The space of training conÔ¨Ågurations for AdA is large, comprising model size, auto-curriculum,
memory architecture, memory length, number of tasks in the XLand task pool, single vs multi-agent
tasks, distillation teacher, and number of training steps. We use a consistent training conÔ¨Åguration
within each experimental comparison, but diÔ¨Äerent conÔ¨Ågurations across diÔ¨Äerent experimental
comparisons. We therefore caution the reader against directly comparing results between diÔ¨Äerent
sections. For convenience, all experimental conÔ¨Ågurations are tabulated in Appendix D.
3.1. AdA shows human-timescale adaptation
Single-agent.
In Figure 5 we show the performance of AdA when trained in the single-agent setting
described in Table 1. Examine Ô¨Årst AdA‚Äôs zero-shot performance (ùëò= 1, red line). This matches the
performance of a baseline agent, trained only in a regime where each episode consists of a single
trial. In other words, AdA does not suÔ¨Äer any degradation in zero-shot performance, despite being
9

Human-Timescale Adaptation in an Open-Ended Task Space
Table 1 | Experimental setup for agent experiments in Section 3.1.
# players
Model parameters
Memory
Task pool
Curriculum
Teacher
Steps
1
169M TXL / 353M total
1800
25B
PLR D.5
D.1
100B
2
265M TXL / 533M total
1800
see App. D.3
PLR D.5
D.2
70B
(a)
(b)
Figure 6 | Human-timescale adaptation. We report median normalised last-trial score across 30
hand-authored tasks as a function of number of trials for AdA and human players. Both AdA and
the human players improve their performance with increasing number of trials, indicating that
AdA is capable of human-timescale adaptation. (a) shows the results using our standard per-task
normalisation scheme. (b) re-normalises the results by the maximum score per player-type to account
for systematic diÔ¨Äerences between the agent and human players. In particular, human players
reported lag while playing which may have resulted in lower scores.
trained on a distribution over number of trials ùëò‚àà{1, 2, . . . 6}. Now turn your attention to AdA‚Äôs
few-shot performance (ùëò‚àà{2, 3, 5, 8, 13}, orange to purple lines). Given more trials, AdA improves
its performance on over 80% of the task set, clearly adapting at test time. The improvements are
particularly strong when comparing zero-shot performance to the two trial setting, but AdA keeps on
improving when given more trials.
We compare the performance of AdA to that of a set of human players on 30 held-out hand-
authored probe tasks, seeking to assess whether AdA adapts on the same timescale as humans. Figure
6a shows the median scores for AdA and for human players as a function of number of trials. Both
AdA and human players were able to improve their score as they experienced more trials of the tasks,
indicating that AdA exhibits human-timescale adaptation on this set of probe tasks. We provide more
details of the scores obtained on each task in Figure F.1. This reveals a small set of tasks which
humans can solve but AdA can‚Äôt, such as the Spacer Tool task: in this task one object must be
used as a tool to move another, a situation which is extremely rare in XLand. There are also tasks
like Small Workstation, All Rules Visible which can be solved by AdA but not by humans,
likely due to complex control requirements. The majority of tasks, however, show adaptation from
both humans and AdA, with the slopes of AdA‚Äôs score being as steep as, if not steeper than, those of
the human players, especially for lower numbers of trials. For full details of our human experiment
design, see Appendix B.4.
Figure 7 analyses the behaviour of AdA in more detail on a speciÔ¨Åc held-out task. The increase
in score with a larger number of trials indicates that the task is solved more consistently and more
quickly when given a larger number of trials. Examining the trajectories for diÔ¨Äerent numbers of trials,
10

Human-Timescale Adaptation in an Open-Ended Task Space
DEAD
END
REWARD
REWARD
1 Trial
2 Trials
8 Trials
Figure 7 | Experimentation, success and reÔ¨Ånement. We report average performance and represen-
tative behaviour of AdA on the probe task Wrong Pair Disappears when evaluated with various
numbers of trials. AdA‚Äôs performance increases when given more trials, showing test-time adaptation.
The top-down view images show representative last-trial trajectories when given diÔ¨Äerent numbers of
total trials. A corresponding video for the case ùëò= 3 shows the behaviour across all trials within one
episode.
we can explain this eÔ¨Äect in terms of the behaviour of AdA. When given 1 or 2 trials AdA‚Äôs behaviour
shows structured hypothesis-driven exploration: trying out diÔ¨Äerent combinations of objects and
coming across the solution or a dead end. Once the solution is found, AdA reÔ¨Ånes its strategy on
subsequent trials, gathering the correct objects with more eÔ¨Éciency and combining them in the
right way. Thus AdA is able to generate a higher last-trial score when provided with more trials for
reÔ¨Ånement. When given 8 trials, the last-trial performance is close to that of the Ô¨Åne-tuned agent. We
observe this pattern of behaviour consistently across many of our held-out probe tasks; see videos on
our microsite.
Multi-agent.
We train a separate agent on a mixture of fully-cooperative multi-agent and single-
agent tasks to explore adaptation in the multi-agent setting. In fully-cooperative multi-agent tasks,
both players have the same goal. Such tasks typically have multiple Nash equilibria (Dafoe et al.,
2020). When faced with a new problem, agents must adapt on-the-Ô¨Çy to agree on a single equilibrium
of maximal mutual beneÔ¨Åt (Christianos et al., 2022; Hu et al., 2020; Stone et al., 2010). This gives
rise to a variety of interesting strategic novelties that are absent in the purely single-agent setting,
including emergent division-of-labour and physical coordination. Both of these behaviours have
received extensive study in the multi-agent RL literature (e.g. Gronauer and Diepold (2022); Strouse
et al. (2021); Wang et al. (2020b); Yang et al. (2020)); here for the Ô¨Årst time to our knowledge, we
demonstrate that these behaviours can emerge at test time in few-shot on held-out tasks. Co-players
for our training tasks are generated using Ô¨Åctitious self-play (Heinrich et al., 2015) and then curated
using PLR, as in Samvelyan et al. (2022). For more details, see Table 1 and Appendix D.3.
Analogously with the single-agent setting, we Ô¨Ånd strong evidence of adaptation across almost
90% of the space of held-out test tasks (Figure E.1). Futhermore, we evaluate the resulting agent
on a held-out test set of cooperative multi-agent tasks in two ways: in self-play and in co-play with
a random-action policy. As shown in Figure 8, self-play outperforms co-play with a random-action
policy by a large margin both in a zero-shot and in a few-shot setting. This indicates that the agents
are dividing the labour required to solve the tasks, thereby solving the task more quickly (or at all)
and improving their shared performance.
Examples of emergent social behaviour in self-play are shown in Figures 9 and E.2. When given
only a few trials, the agents explore the space of possible solutions, sometimes operating independently
11

Human-Timescale Adaptation in an Open-Ended Task Space
(a)
(b)
Figure 8 | Two heads are better than one. Cooperative self-play outperforms single-agent perfor-
mance on the test set of two-player cooperative held-out tasks. For this evaluation we restrict ourselves
to tasks whose goals and production rules do not refer to players and which are solvable by a single
player (216/1000 test tasks). To produce the purple curve, we evaluate AdA twice per task when
playing with a random-action policy co-player, once playing as the Ô¨Årst and once as the second player,
and take the maximum score over both evaluations before cross-task aggregation. This accounts for
possible advantages playing as one player might have over playing as the other in a task. (a) Median
score. (b) 20th percentile score.
GRAB
1
PASS
PASS
1
2
REWARD
1 Trial
8 Trials
Figure 9 | Multi-agent coordination. We report average performance and representative behaviour
of AdA on the probe task Pass Over the Wall Repeatedly when evaluated in self-play with
various numbers of trials. AdA‚Äôs performance increases when given more trials, showing test-time
adaptation. The top-down view images show representative last-trial trajectories when given diÔ¨Äerent
numbers of total trials. A corresponding video for the case ùëò= 5 shows the behaviour across all trials
within one episode.
and sometimes together. Given more trials, once the agents Ô¨Ånd a solution, they optimise their paths
by coordinating physically and dividing labour to solve the task eÔ¨Éciently. This behaviour emerges
from adaptation at test time and was not explicitly incentivised during training, other than through
the high-level fully cooperative reward function. Videos of such behavior in a variety of tasks are
available on our microsite.
12

Human-Timescale Adaptation in an Open-Ended Task Space
1
2
3
5
8
13
Number of Trials
0.0
0.2
0.4
0.6
0.8
1.0
Median Score
Transformer
RNN with Attention
RNN
(a) Impact of architecture
1
2
3
5
8
13
Number of Trials
0.0
0.2
0.4
0.6
0.8
1.0
Median Score
PLR
No-op
Uniform
(b) Impact of curriculum
Figure 10 | (a) Adaptation over increasing numbers of trials for diÔ¨Äerent choices of architectures.
Incorporating attention modules is essential to achieve adaptation, with Transformer-XL architectures
performing best. (b) Adaptation over increasing numbers of trials for diÔ¨Äerent choices of curricula.
No-op Ô¨Åltering and PLR greatly improve both zero-shot generalisation and few-shot adaptation over
the uniform sampling baseline.
3.2. Architecture inÔ¨Çuences performance
We now dive deeper into understanding which components of our method are critical, via a series
of ablation studies. In these studies we use a single initialisation seed, because we see low variance
across seeds when training AdA (see Appendix F.3). All ablations are in the single-agent setting,
unless stated otherwise.
First, we empirically contrast diÔ¨Äerent choices of architectures: Transformer-XL, RNN, and RNN
with Attention. To implement the RNN, we use a GRU (Cho et al., 2014). To facilitate comparison, we
match the total network size for all architectures. Table D.3 shows details on the experimental setup.
Figure 10a shows that while the Transformer-XL is the best performing architecture in this comparison,
incorporating a multi-head attention module into an RNN recovers most of the performance of the
Transformer, highlighting the eÔ¨Äectiveness of attention modules.
3.3. Auto-curriculum learning improves performance
To establish the importance of automated curriculum learning, we compare adaptation when training
with the curricula methods outlined in Section 2.3: no-op Ô¨Åltering and PLR. Figure 10b shows
the median last-trial score of agents trained with diÔ¨Äerent curricula. Both no-op Ô¨Åltering and PLR
curricula strongly outperform a baseline trained with uniformly sampled tasks. Moreover, PLR
outperforms No-op Ô¨Åltering, particularly at a higher number of trials, indicating that a regret-based
curriculum is especially helpful for learning longer-term adaptation. In Appendix D.5 we detail
training conÔ¨Åguration, and also compare the sample eÔ¨Éciency of our methods, where we see that
both auto-curriculum approaches are more sample-eÔ¨Écient than uniform sampling, in terms of both
learning steps and FLOPs.
In Figure 11 we show the evolution of task complexity for both methods. In both cases, simpler
tasks are initially prioritised, with a clear curriculum emerging. Neither method explicitly optimises
to increase these metrics, yet the task complexity increases as a result of the agent‚Äôs improving
capabilities. See Figure D.3 for additional metrics of task complexity.
13

Human-Timescale Adaptation in an Open-Ended Task Space
0G
10G
20G
30G
40G
Training Steps
2.5
3.0
3.5
4.0
Num Rules
0G
10G
20G
30G
40G
Training Steps
2.4
2.6
2.8
3.0
3.2
3.4
Num Trials
0G
10G
20G
30G
40G
Training Steps
0.6
0.8
1.0
1.2
Num Deadend Rules
0G
10G
20G
30G
40G
Training Steps
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Num Hidden Critical Rules
No-op Filtering
PLR
Test
Hand-authored
Figure 11 | Emergent curricula for no-op Ô¨Åltering and PLR. Plots show a selection of task metrics for
the dynamic training set, averaged over all tasks in the set, with standard error shaded. In all plots, a
higher metric value corresponds to greater task diÔ¨Éculty. For example, tasks with a higher number of
rules require more trial-and-error to Ô¨Ånd the correct rules to trigger. Horizontal lines show the same
metric values averaged over the test (dashed) and hand-authored (dotted) evaluation task sets.
(a)
(b)
Figure 12 | Scaling Transformer parameters increases both median (a) and 20th percentile (b) test
score. Both axes are log-scaled, according to the functions log(ùë•) and ‚àílog(1 ‚àíùë¶), respectively, and
the relationship between model size and performance appears roughly linear on this scale. The slope
is steeper when evaluating higher numbers of trials, showing that scaling the model is particularly
eÔ¨Äective at encouraging stronger adaptation, as opposed to stronger zero-shot generalisation.
3.4. Scaling the agent increases performance
Methods that scale well are critical for continued progress in machine learning, and understanding
how methods scale is important for deciding where to spend time and compute in the future. Scaling
laws have been determined for many foundation models (see Section 4), where performance is related
to model size and other factors as a power law, which can be seen as a linear relationship on a log-log
plot. Inspired by such analyses, we investigate how adaptation scales with Transformer model size
and memory length.
Scaling network size.
We show how performance scales with the size of AdA‚Äôs Transformer model,
experimenting with the model sizes shown in Table D.8. When investigating scaling laws for model
size, we follow Kaplan et al. (2020) in measuring only Transformer (i.e. non-embedding) parameters,
which range across 3 orders of magnitude, from 6M to 265M Transformer parameters (i.e. from 41M
to 533M total parameters). A complete list of hyperparameters is shown in Table D.9.
14

Human-Timescale Adaptation in an Open-Ended Task Space
(a)
(b)
Figure 13 | Scaling Transformer-XL memory length increases both median (a) and 20th percentile (b)
test score. Both axes are log-scaled, according to the functions log(ùë•) and ‚àílog(1 ‚àíùë¶), respectively,
and the relationship between memory length and performance appears roughly linear on this scale.
The slope is steeper when evaluating higher numbers of trials, showing that scaling the memory is
particularly eÔ¨Äective at encouraging stronger few-shot adaptation.
Figure 12 shows that larger networks increase performance, especially when given more test-time
trials to adapt. Though larger models seem to help in the median test-set score (Figure 12a), model
scale particularly has impact on the lower percentiles of the test set (Figure 12b). This indicates that
larger models allow the agent to generalise its adaptation to a broader range of tasks. The roughly
linear relationship between model size and performance on the log-log plot is indicative of a power
law scaling relationship, albeit only shown across two to three orders of magnitude. That the curves
are not exactly linear may be due to several factors: that we haven‚Äôt trained to convergence (though
performance increases had slowed for all models), and that we use a 23M parameter distillation
teacher across experiments for all model sizes.
Appendix D.7 details the computational costs of the various model sizes, and shows FLOPs adjusted
results. While larger models do indeed have better zero-shot score and adaptation than smaller ones
for the same number of training steps, and are more sample eÔ¨Écient, the biggest model may not
always be the best choice when compute cost is taken into account.
Scaling memory length.
Performance also scales with the length of AdA‚Äôs memory. The experimen-
tal setting is shown in Table D.10, where we examine the number of previous network activations we
cache, investigating values from 100 to 700, which, with 6 Transformer-XL blocks, yields an eÔ¨Äective
timestep range of 600 to 4200 timesteps.4
Figure 13 shows that, as with model size, scaling memory length helps performance, especially in
the lower test percentiles, pushing performance on the tails of the distribution. For any of our tasks,
the maximum trial duration is 300 timesteps, so it is interesting that performance on, for example,
5 trials (1500 timesteps) continues to increase for ‚ÄúeÔ¨Äective memory lengths‚Äù between 1800 and
4200. This indicates that it is easier for the Transformer-XL to make use of explicitly given memory
activations rather than relying on theoretically longer-range information implicit in those activations.
4Transformer-XL enables the use of longer, variable-length context windows by concatenating a cached memory of
previous attention layer inputs to the keys and values during each forward pass. Since inputs to intermediate layers
are activations from the previous layer, which in themselves contain information about the past, caching ùëÄactivations
theoretically allows for an eÔ¨Äective memory horizon of ùëÄ√ó ùêø, where ùêøis the number of attention layers in the network.
15

Human-Timescale Adaptation in an Open-Ended Task Space
(a)
(b)
Figure 14 | Median (a) and 20th percentile (b) adaptation scales with the size of the task pool. The
eÔ¨Äect is especially prominent for larger models. We show the ùë¶-axis on a logarithmic scale as in the
other scaling experiments. Here, we plot number of trials on the ùë•-axis and examine the gaps between
the curves for the two task distributions (triangle markers vs. circular markers).
3.5. Scaling the task pool increases performance
Another important factor to scale is the amount of data a model is trained on. For example, HoÔ¨Ämann
et al. (2022) showed that in order to get the most out of scaling a language model, one must scale
the amount of training data at the same rate as the number of parameters. In our case, relevant
data come from interaction with diÔ¨Äerent tasks, so we examine the eÔ¨Äect of scaling the number and
complexity of diÔ¨Äerent tasks in the XLand pool.
Scaling size of task pool.
Here we examine the eÔ¨Äect of varying the number of training tasks from
which the auto-curriculum can sample. Recall that in XLand, a task is the combination of a world (the
physical layout of terrain and objects) and a game (specifying the goal and production rules). We
investigate the eÔ¨Äects of training on tasks sampled from a small pool of 200M distinct tasks (4,000
worlds √ó 50,000 games) compared with a large pool of 25B distinct tasks (50,000 worlds √ó 500,000
games). Table D.11 shows the full experimental setup for these comparisons.
Figure 14 shows higher test score for identically sized models on the larger task pool. As in the
other scaling experiments, we especially see improved performance on the 20th percentile. The results
are shown for two diÔ¨Äerent sizes of models, with the larger Transformer yielding a larger gap when
scaling the size of the task pool. This suggests that the large models are especially prone to overÔ¨Åtting
to a smaller task pool.
Scaling complexity of task pool.
One Ô¨Ånal axis along which it is possible to scale our method is
the overall complexity of the task distribution. For example, tasks with a Ô¨Çat terrain will be, on
average, less complex to solve than tasks with terrain variation. In Figure E.3, we show that low
environment complexity can be a bottleneck to scaling, by comparing the eÔ¨Äectiveness of model
scaling between agents trained on two distributions of the same size but diÔ¨Äerent complexity and
evaluated on their respective test sets. Open-ended settings with unbounded environment complexity,
such as multi-agent systems, may therefore be particularly important for scaling up adaptive agents.
16

Human-Timescale Adaptation in an Open-Ended Task Space
(a)
(b)
Figure 15 | Adaptation over increasing numbers of trials when training from scratch or when kick-
starting with distillation, for models with 23M and 265M Transformer parameters. Circle markers
show training from scratch while triangle markers show training kickstarted with 4 billion frames of
distillation. For this ablation, agents were trained in the multi-agent setup described in Section 3.1
and evaluated on the multi-agent test set after 22 billion total training frames. (a) Median score. (b)
20th percentile score.
0G
20G
40G
60G
80G
100G
Training Steps
0.0
0.2
0.4
0.6
0.8
1.0
Median Score
Teacher
Student
(a)
0G
20G
40G
60G
80G
100G
Training Steps
0.0
0.2
0.4
0.6
0.8
1.0
20th Percentile Score
Teacher
Student
(b)
Figure 16 | Normalised last-trial score for ùëò= 13 using the 23M parameter Transformer-XL. The
teacher is trained from scratch, while the otherwise identical student is distilled from a snapshot
of the teacher, taken after 25 billion steps of training. The ùë•-axis counts the combined amount of
experience, including experience used to train the teacher. The comparison shows that distillation
can greatly increase the performance of the student, even if the combined amount of experience and
updates are equivalent. This is true for median score (a), but even more so for the 20th percentile (b).
3.6. Distillation improves performance and enables scaling agents
All of the scaling comparison experiments shown in the previous section use an identical distillation
teacher for the Ô¨Årst frames of training, as detailed in Appendix D.6. Now, we look at the role distillation
plays in scaling. In short, we Ô¨Ånd that kickstarting training with a distillation period is crucial when
scaling up model size. As shown in Figure 15, training a 265M parameter Transformer model without
distillation results in poor performance compared to a much smaller 23M parameter Transformer
trained in the same way. However, when training with distillation from a 23M parameter teacher
for the Ô¨Årst 4 billion training frames, the 265M model clearly outperforms the 23M variant. See
17

Human-Timescale Adaptation in an Open-Ended Task Space
experiment details in Appendix D.11.
Additionally, we Ô¨Ånd that even when the model size is the same for both student and teacher, we
observe large gains from distillation, for a constant total frame budget (Figure 16). We speculate that
this is due to bad representations learned early on by the student agent (Cetin et al., 2022; Nikishin
et al., 2022), which can be avoided by using distillation. This is also consistent with Ô¨Åndings in oÔ¨Ñine
RL, where additional data is often required to eÔ¨Äectively scale the model (Reid et al., 2022). The
eÔ¨Äect is largest for the Ô¨Årst round of distillation, with diminishing returns in subsequent rounds of
distillation (Figure E.5).
3.7. Training on more trials with skip memory enables many-shot adaptation
So far, we have considered the few-shot regime in which we train on 1 to 6 trials and evaluate up
to 13 trials. In this section, we evaluate AdA‚Äôs ability to adapt over longer time horizons. We Ô¨Ånd
that when trained with ùëò‚àà{1, 2, . . . 6}, agents do not continue to adapt past 13 trials; however, this
long-term adaptation capability is greatly improved by increasing the maximum number of trials
during training to 24 and increasing the eÔ¨Äective length of the memory accordingly. These results
show that our method naturally extends to many-shot timescales, with episodes lasting in excess of
30 minutes.5 In this section, we ablate both factors separately, and show that both are important for
long-range adaptation. The training conÔ¨Åguration (which is identical to that of the memory scaling
experiments save for the number of training steps) is detailed in Table D.14.
As we noted in Section 3.4, increasing the memory length leads to increased capacity that beneÔ¨Åts
the agent even when the entire episode Ô¨Åts in memory, but also comes at the cost of increased
computation. To disentangle these factors, we propose a simple change to the memory architecture
described in Section 2.4 which increases eÔ¨Äective memory length without increasing computational
cost. We use a GRU to encode trajectories before feeding them to the Transformer-XL. This allows us
to sub-sample timesteps from the encoded trajectories, enabling the agent to attend over 4 times as
many trials without additional computation. We show that the additional GRU on its own does not
aÔ¨Äect the performance of the agent greatly.
As can be seen in Figure 17a, increasing the number of trials in the training distribution signiÔ¨Åcantly
boosts performance in later trials, especially when the memory length is scaled accordingly. In other
words, the adaptation strategy learned by AdA beneÔ¨Åts from experiencing a large number of trials,
rather than just very recent ones. Therefore we can conclude that AdA is capable of adaptation based
on long-term knowledge integrated into memory across many trials, as opposed to merely encoding a
simple meta-strategy that only depends on the trajectory from the previous trial.
Increasing the number of trials in training leads to better adaptation even in the absence of
increased memory. This indicates that the agent is able to learn better exploration and reÔ¨Ånement
strategies when aÔ¨Äorded longer training episodes consisting of more trials. Note that increasing
eÔ¨Äective memory without increasing the number of training trials does not improve performance, as
the agent has not been trained to make use of the additional memory capacity.
3.8. AdA can leverage prompting with Ô¨Årst-person demonstrations
To determine whether AdA can learn in zero-shot from Ô¨Årst-person demonstrations, we prompted
it with a Ô¨Årst-person demonstration by a Ô¨Åne-tuned teacher, as follows. The teacher took control of
the avatar in the Ô¨Årst trial, while AdA continued to receive observations as usual, conditioning its
548 trials of a 40s task lasts for 32 minutes. By contrast, the average length of a Starcraft 2 game is between 10 and 15
minutes, and AlphaStar acted less frequently per-second than AdA does (Vinyals et al., 2019).
18

Human-Timescale Adaptation in an Open-Ended Task Space
1
5
13
24
36
48
Trials
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
20th Percentile Score
+4x effective memory and trials in training
+4x trials in training +RNN encoder
+4x trials in training
+4x effective memory length
Baseline
(a)
(b)
Figure 17 | (a) Ablation showing the 20th percentile of test scores as we vary the maximum number
of training trials (from a ùëò= 6 baseline to ùëò= 24) and increase the eÔ¨Äective memory size via sub-
sampling (from 1800 steps to 7200 steps). Together, these factors enable the agent to adapt over a
larger number of trials (lasting over 30 minutes). Increasing the number of training trials has the
biggest eÔ¨Äect and is a prerequisite for sub-sampling to be eÔ¨Äective. This Ô¨Ågure furthermore shows
that adding an RNN encoder to facilitate sub-sampling does not by itself greatly aÔ¨Äect performance.
(b) Median hand-authored task score of AdA prompted with a Ô¨Årst-person demonstration in the Ô¨Årst
trial of each episode, compared with an unprompted baseline. The prompted score lies strictly above
the baseline which indicates that AdA is able to use information from a demonstration prompt to
improve its performance. However, the score lies below that of the demonstration which suggests that
it is not able to make perfect use of the demonstration.
Transformer memory. AdA was then allowed to proceed on its own for the remaining trials and its
scores recorded in the usual manner.
Figure 17b shows the median score on our hand-authored test set of prompted AdA compared to
an unprompted baseline. Prompted AdA is unable to exactly mimic the teacher‚Äôs demonstration in the
second trial of a median task, shown by a drop in score. It does, however, outperform an unprompted
baseline across all numbers of trials, indicating that it is able to proÔ¨Åtably incorporate information
from the demonstration into its policy. This process is analogous to prompting of large language
models, where the agent‚Äôs memory is primed with an example of desired behaviour from which it
continues. We note that AdA was never trained with such oÔ¨Ä-policy Ô¨Årst-person demonstrations, yet
its in-context learning algorithm is still able to generalise to these.
In Figure F.4 we provide prompting results for all single-agent hand-authored tasks and discuss
the circumstances under which prompting is eÔ¨Äective. In Appendix F.4 we also provide early results
investigating prompting with human demonstrations on a subset of tasks. These reveal remarkable
success in some cases, but also conÔ¨Årm that human demonstrations cannot overcome inherent
limitations of AdA‚Äôs task distribution. Two videos compare the behaviour when prompted and when
not prompted on the task Object permanence:
yellow cube.
4. Related Work
In this work, we leverage advances in attention-based models for meta-learning in an open-ended
task space. Our agent learns a form of in-context RL algorithm, while also automatically curating the
training task distribution; thus we combine two pillars of an AI generating algorithm (AI-GA, Clune
19

Human-Timescale Adaptation in an Open-Ended Task Space
(2019)). The most similar work to ours is OEL Team et al. (2021), which also considers training
in a vast multi-agent task space with auto-curricula and generational learning. A key diÔ¨Äerence
in our work is that we focus on adaptation (vs. zero-shot performance), and make use of large
Transformer models. Akkaya et al. (2019) also demonstrated the eÔ¨Äectiveness of adaptive curricula
while meta-learning a policy to control a robot hand, however they focused on a speciÔ¨Åc sim-to-real
setting rather than a more generally capable agent. We now summarise literature related to each
component of our work in turn.
Procedural environment generation.
We make use of procedural content generation (PCG) to
generate a vast, diverse task distribution. PCG has been studied for many years in the games
community (Risi and Togelius, 2020; Togelius and Schmidhuber, 2008) and more recently has
been used to create testbeds for RL agents (Cobbe et al., 2018; Justesen et al., 2018; Raileanu and
Rockt√§schel, 2020). Indeed, in the past few years a series of challenging PCG environments have
been proposed (Chevalier-Boisvert et al., 2018; Cobbe et al., 2020; Deitke et al., 2022; Hafner, 2022;
Juliani et al., 2019; K√ºttler et al., 2020; Samvelyan et al., 2021), mostly focusing on testing and
improving generalisation in RL (Bhatt et al., 2022; Fontaine et al., 2021; Kirk et al., 2021). More
recently there has been increased emphasis on open-ended worlds: Albrecht et al. (2022) proposed
Avalon, a 3D world supporting complex tasks, while Minecraft (Johnson et al., 2016) has been
proposed as a challenge for Open-Endedness and RL (Fan et al., 2022; Grbic et al., 2021; Kanervisto
et al., 2022), but unlike XLand it does not admit control of the full simulation stack, thereby limiting
the smoothness of the task space.
Open-ended learning.
A series of recent works have demonstrated the eÔ¨Äectiveness of agent-
environment co-adaptation with a distribution of tasks (Parker-Holder et al., 2022; Wang et al., 2019,
2020a). Our approach bears resemblance to the unsupervised environment design (UED, Dennis et al.
(2020)) paradigm, since we seek to train a generalist agent without knowledge of the test tasks. One
of the pioneering methods in this space was PAIRED (Dennis et al., 2020), which seeks to generate
tasks with an RL-trained adversary. We build on Prioritised Level Replay (Jiang et al., 2021a,b), a
method which instead curates randomly sampled environments which have high regret. Our work
also relates to curriculum learning (Campero et al., 2021; Fang et al., 2021; Matiisen et al., 2020;
Mu et al., 2022; OpenAI et al., 2021; Portelas et al., 2019; Sukhbaatar et al., 2018), with the key
diÔ¨Äerence that these methods typically have a speciÔ¨Åc downstream goal or task in mind. There have
also been works training agents with auto-curricula over co-players, although these typically focus
on singleton environments (Berner et al., 2019; Vinyals et al., 2019) or uniformly sampled tasks
(Baker et al., 2020; Cultural General Intelligence Team et al., 2022; Jaderberg et al., 2019; Liu et al.,
2019). Similar to XLand 2.0‚Äôs production rule system, Zhong et al. (2020) train agents to generalise
to unobserved environment dynamics. However, they investigate zero-shot generalisation where the
agent has to infer underlying environment dynamics from language descriptions, whereas AdA agents
discovery these rules at test time via on-the-Ô¨Çy hypothesis-driven exploration over multiple trials.
Adaptation.
This work focuses on few-shot adaptation in control problems, commonly framed
as meta-RL. We focus on memory-based meta-RL, and build upon the work of Duan et al. (2017)
and Wang et al. (2016), who showed that if an agent observes rewards and terminations, and the
memory does not reset, a memory-based policy can implement a learning algorithm. This has proven
to be an eÔ¨Äective approach that can learn Bayes-optimal strategies (Mikulik et al., 2020; Ortega
et al., 2019) and may have neurological analogues (Wang et al., 2018). Indeed, our agents learn
conceptual exploration strategies, something that would require the outer learner of a meta-gradient
approach to estimate the return of the inner learner (Stadie et al., 2018). Solutions in this space
20

Human-Timescale Adaptation in an Open-Ended Task Space
either rely on high-variance Monte Carlo returns (Garcia and Thomas, 2019; Stadie et al., 2018;
Vuorio et al., 2021) or history-dependent estimators (Zheng et al., 2020). Our work is also inspired
by Alchemy (Wang et al., 2021), a meta-RL benchmark domain whose mechanics have inspired the
production rules in our work. The authors use memory-based meta-RL with a small Transformer,
but Ô¨Ånd that the agent‚Äôs performance is only marginally better than that of a random heuristic.
Transformers have also been shown to be eÔ¨Äective for meta-RL on simple domains (Melo, 2022) and
for learning RL algorithms (Laskin et al., 2022) from oÔ¨Ñine data. Other approaches for meta-RL
include meta-gradients (Andrychowicz et al., 2016; Finn et al., 2017; Flennerhag et al., 2022; Xu
et al., 2018), which can be eÔ¨Écient but often suÔ¨Äer from instability and myopia (Flennerhag et al.,
2022; Metz et al., 2021; Vuorio et al., 2021), and latent-variable based approaches (Finn et al., 2018;
Humplik et al., 2019; Rakelly et al., 2019; Zintgraf et al., 2019). Adaptation also plays a critical role
in robotics, with agents trained to adapt to varying terrain (Clavera et al., 2019; Kumar et al., 2021)
or damaged joints (Cully et al., 2015).
Transformers in RL and beyond.
Transformer architectures have recently shown to be highly
eÔ¨Äective for oÔ¨Ñine RL (Chen et al., 2021; Janner et al., 2021; Reed et al., 2022), yet successes in
the online setting remain limited. One of the few works to successfully train Transformer-based
policies was Parisotto et al. (2020), who introduced several heuristics to stabilise training in a simpler,
smaller-scale setting. Indeed, while we make use of a similar Transformer-XL architecture (Dai
et al., 2019; Vaswani et al., 2017), we demonstrate scaling laws for online meta-RL that resemble
those seen in other communities, such as language (Brown et al., 2020; Devlin et al., 2019; Kaplan
et al., 2020; Rae et al., 2021). Similarly, Melo (2022) use Transformers for fast adaptation in a
smaller-scale meta-RL setting, interpreting the self-attention mechanism as a means of building an
episodic memory from timestep embeddings, through the recursive application of Transformer layers.
Transformer architectures have also been used in meta-learning outside of RL, for example learning
general-purpose algorithms (Kirsch et al., 2022) or hyperparameter optimisers (Chen et al., 2022).
Transformers are also ubiquitous in modern large language models, which have been shown to be
few-shot learners (Brown et al., 2020).
5. Conclusion
Adaptation to new information across a range of timescales is a crucial ability for generally intelligent
agents. Foundation models in particular have demonstrated an ability to acquire a large knowledge-
base of information, and apply this rapidly to new scenarios. Thus far, they have relied mainly on
supervised and self-supervised learning. As such, they require access to large datasets. An alternative
to collecting datasets is to have an agent learn from its own experience via reinforcement learning,
provided that suÔ¨Éciently rich physical worlds or open-ended simulations are available. This raises
the question: can large-scale, generally adaptive models be trained with RL?
In this paper, we demonstrate, for the Ô¨Årst time to our knowledge, an agent trained with RL that
is capable of rapid in-context adaptation across a vast, open-ended task space, at a timescale that is
similar to that of human players. This Adaptive Agent (AdA) explores held-out tasks in a structured way,
reÔ¨Åning its policy towards optimal behaviour given only a few interactions with the task. Further, AdA
is amenable to contextual Ô¨Årst-person prompting, strengthening its few-shot performance, analogous
to prompting in large language models. AdA shows scaleable performance as a function of number of
parameters, context length and richness of the training task distribution.
Our training method is based on black-box meta-RL, previously thought of as hard to scale. We
show that state-of-the-art automatic curriculum techniques can shape the data distribution to provide
21

Human-Timescale Adaptation in an Open-Ended Task Space
suÔ¨Écient signal for learning to learn in an open-ended task space. Moreover, we demonstrate that
attention-based architectures can take advantage of this signal much more eÔ¨Äectively than purely
recurrent networks, illustrating the importance of co-adapting data-distribution and agent architecture
for facilitating rapid adaptation. Finally, distillation enables us to realise the potential of large-scale
Transformer architectures.
The future of AI research will inevitably involve training increasingly large models with increasingly
general and adaptive capabilities. In this direction, we have provided a recipe for training a 500M
parameter model, which we hope can pave the way for further advances at the intersection of RL
and foundation models. AdA shows rapid and scalable adaptation of myriad kinds, from tool use to
experimentation, from division of labour to navigation. Given scaling law trends, such models may
in future become the default foundations for few-shot adaptation and Ô¨Åne-tuning on useful control
problems in the real world.
6. Authors and Contributions
We list authors alphabetically by last name. Please direct all correspondence to Feryal Behbahani
(feryal@deepmind.com) and Edward Hughes (edwardhughes@deepmind.com).
6.1. Core contributors
‚Ä¢ Jakob Bauer: technical leadership, curriculum research, infrastructure engineering, task au-
thoring, paper writing
‚Ä¢ Kate Baumli: agent research, scaling, agent analysis, task authoring, paper writing
‚Ä¢ Feryal Behbahani: research vision, team leadership, agent research, paper writing
‚Ä¢ Avishkar Bhoopchand: technical leadership, evaluation research, infrastructure engineering,
task authoring, paper writing
‚Ä¢ Michael Chang: visualisation, agent analysis, human experiments
‚Ä¢ Adrian Collister: XLand development, human experiments
‚Ä¢ Edward Hughes: research vision, team leadership, evaluation research, paper writing
‚Ä¢ Sheleem Kashem: infrastructure engineering, curriculum research, human experiments
‚Ä¢ Jack Parker-Holder: curriculum research, paper writing
‚Ä¢ Yannick Schroecker: agent research, scaling, task authoring, agent analysis, paper writing
‚Ä¢ Jakub Sygnowski: infrastructure engineering, curriculum research, agent analysis, paper
writing
‚Ä¢ Alexander Zacherl: design leadership, agent analysis, task authoring, visualisation, human
experiments
‚Ä¢ Lei Zhang: curriculum research, agent analysis, paper writing
6.2. Partial contributors
‚Ä¢ Nathalie Bradley-Schmieg: project management
‚Ä¢ Natalie Clay: QA testing, human experiments
‚Ä¢ Vibhavari Dasagi: evaluation research
‚Ä¢ Lucy Gonzalez: project management
‚Ä¢ Karol Gregor: agent research
‚Ä¢ Maria Loks-Thompson: XLand development, human experiments
‚Ä¢ Hannah Openshaw: project management
‚Ä¢ Shreya Pathak: agent analysis
22

Human-Timescale Adaptation in an Open-Ended Task Space
‚Ä¢ Nicolas Perez-Nieves: agent analysis, task authoring
‚Ä¢ Nemanja Rakicevic: curriculum research, agent analysis
‚Ä¢ Tim Rockt√§schel: strategic advice, paper writing
‚Ä¢ Sarah York: QA testing, human experiments
6.3. Sponsors
‚Ä¢ Satinder Baveja: strategic advice
‚Ä¢ Karl Tuyls: strategic advice
7. Acknowledgements
We thank Max Jaderberg for early guidance on the project vision. We are grateful to Wojciech Marian
Czarnecki for an early version of the production rules formalism and Catarina Barros for a prototype
implementation. We thank Dawid G√≥rny for support on implementing visualisation tools. We are
grateful to Alex Platonov for artistic rendering of the Ô¨Ågures and accompanying videos. We thank
Nathaniel Wong, Tom Hudson and the Worlds Team for their engineering support. Further, we
thank Andrew Bolt, Max Cant, Valentin Dalibard, Richard Everett, Nik Hemmings, Shaobo Hou, Jony
Hudson, Errol King, George-Cristian Muraru, Alexander Neitz, Valeria Oliveira, Doina Precup, Drew
Purves, Daniel Tanis, Roma Patel, and Marcus Wainwright for useful discussions and support. We are
grateful to Sebastian Flennerhag and Raia Hadsell for reviewing a draft of the paper.
References
R. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville, and M. G. Bellemare. Deep reinforcement
learning at the edge of the statistical precipice. CoRR, abs/2108.13264, 2021.
I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert,
G. Powell, R. Ribas, et al. Solving rubik‚Äôs cube with a robot hand. arXiv preprint arXiv:1910.07113,
2019.
J. Albrecht, A. J. Fetterman, B. Fogelman, E. Kitanidis, B. Wr√≥blewski, N. Seo, M. Rosenthal, M. Knutins,
Z. Polizzi, J. B. Simon, and K. Qiu. Avalon: A benchmark for RL generalization using procedurally
generated worlds. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track, 2022.
M. Andrychowicz, M. Denil, S. Gomez, M. W. HoÔ¨Äman, D. Pfau, T. Schaul, B. Shillingford, and
N. De Freitas. Learning to learn by gradient descent by gradient descent. Advances in neural
information processing systems, 29, 2016.
I. Babuschkin, K. Baumli, A. Bell, S. Bhupatiraju, J. Bruce, P. Buchlovsky, D. Budden, T. Cai, A. Clark,
I. Danihelka, C. Fantacci, J. Godwin, C. Jones, R. Hemsley, T. Hennigan, M. Hessel, S. Hou,
S. Kapturowski, T. Keck, I. Kemaev, M. King, M. Kunesch, L. Martens, H. Merzic, V. Mikulik,
T. Norman, J. Quan, G. Papamakarios, R. Ring, F. Ruiz, A. Sanchez, R. Schneider, E. Sezener,
S. Spencer, S. Srinivasan, L. Wang, W. Stokowiec, and F. Viola. The DeepMind JAX Ecosystem,
2020. URL http://github.com/deepmind.
B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. McGrew, and I. Mordatch. Emergent tool
use from multi-agent autocurricula. In International Conference on Learning Representations, 2020.
23

Human-Timescale Adaptation in an Open-Ended Task Space
D. Balduzzi, K. Tuyls, J. Perolat, and T. Graepel. Re-evaluating evaluation. Advances in Neural
Information Processing Systems, 31, 2018.
J. Barbosa, H. Stein, S. Zorowitz, Y. Niv, C. SummerÔ¨Åeld, S. Soto-Faraco, and A. HyaÔ¨Ål. A practical
guide for studying human behavior in the lab. Behavior Research Methods, pages 1‚Äì19, 2022.
C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme,
C. Hesse, R. J√≥zefowicz, S. Gray, C. Olsson, J. Pachocki, M. Petrov, H. P. de Oliveira Pinto, J. Raiman,
T. Salimans, J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, and S. Zhang. Dota
2 with large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019.
V. Bhatt, B. Tjanaka, M. C. Fontaine, and S. Nikolaidis. Deep surrogate assisted generation of
environments. In Advances in Neural Information Processing Systems, 2022.
R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg,
A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. S. Chatterji, A. S. Chen,
K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy,
K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. D. Goodman, S. Grossman,
N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard,
S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. S.
Krass, R. Krishna, R. Kuditipudi, and et al. On the opportunities and risks of foundation models.
CoRR, abs/2108.07258, 2021.
J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-
derPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy
programs, 2018. URL http://github.com/google/jax.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,
D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,
C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot
learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in
Neural Information Processing Systems, volume 33, pages 1877‚Äì1901. Curran Associates, Inc., 2020.
A. Campero, R. Raileanu, H. Kuttler, J. B. Tenenbaum, T. Rockt√§schel, and E. Grefenstette. Learning
with AMIGo: Adversarially motivated intrinsic goals. In International Conference on Learning
Representations, 2021.
M. Carroll, R. Shah, M. K. Ho, T. L. GriÔ¨Éths, S. A. Seshia, P. Abbeel, and A. Dragan. On the utility of
learning about humans for human-ai coordination, 2019.
E. Cetin, P. J. Ball, S. Roberts, and O. Celiktutan. Stabilizing oÔ¨Ä-policy deep reinforcement learning
from pixels. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors,
Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of
Machine Learning Research, pages 2784‚Äì2810. PMLR, 17‚Äì23 Jul 2022.
L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch.
Decision transformer: Reinforcement learning via sequence modeling. In M. Ranzato, A. Beygelz-
imer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing
Systems, volume 34, 2021.
Y. Chen, X. Song, C. Lee, Z. Wang, Q. Zhang, D. Dohan, K. Kawakami, G. Kochanski, A. Doucet,
M. Ranzato, S. Perel, and N. de Freitas. Towards learning universal hyperparameter optimizers
with transformers. In Neural Information Processing Systems (NeurIPS) 2022, 2022.
24

Human-Timescale Adaptation in an Open-Ended Task Space
M. Chevalier-Boisvert, L. Willems, and S. Pal. Minimalistic gridworld environment for OpenAI Gym.
https://github.com/maximecb/gym-minigrid, 2018.
K. Cho, B. Van Merri√´nboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine
translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.
F. Christianos, G. Papoudakis, and S. V. Albrecht. Pareto actor-critic for equilibrium selection in
multi-agent reinforcement learning. arXiv, 2022. doi: 10.48550/ARXIV.2209.14344.
I. Clavera, A. Nagabandi, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn. Learning to adapt in
dynamic, real-world environments through meta-reinforcement learning. In International Conference
on Learning Representations, 2019.
J. Clune. AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artiÔ¨Åcial
intelligence. CoRR, abs/1905.10985, 2019.
K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in reinforcement
learning. CoRR, abs/1812.02341, 2018.
K. Cobbe, C. Hesse, J. Hilton, and J. Schulman. Leveraging procedural generation to benchmark
reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning,
pages 2048‚Äì2056, 2020.
A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret. Robots that can adapt like animals. Nature, 521:
503‚Äì507, 2015.
Cultural General Intelligence Team, A. Bhoopchand, B. BrownÔ¨Åeld, A. Collister, A. D. Lago, A. Edwards,
R. Everett, A. Frechette, Y. G. Oliveira, E. Hughes, K. W. Mathewson, P. Mendolicchio, J. Pawar,
M. Pislar, A. Platonov, E. Senter, S. Singh, A. Zacherl, and L. M. Zhang. Learning robust real-time
cultural transmission without human data, 2022.
W. M. Czarnecki, R. Pascanu, S. Osindero, S. Jayakumar, G. Swirszcz, and M. Jaderberg. Distilling
policy distillation. In The 22nd International Conference on ArtiÔ¨Åcial Intelligence and Statistics, pages
1331‚Äì1340. PMLR, 2019.
A. Dafoe, E. Hughes, Y. Bachrach, T. Collins, K. R. McKee, J. Z. Leibo, K. Larson, and T. Graepel. Open
problems in cooperative AI. CoRR, abs/2012.08630, 2020.
Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov. Transformer-XL: Attentive
language models beyond a Ô¨Åxed-length context. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pages 2978‚Äì2988, Florence, Italy, July 2019. Association
for Computational Linguistics. doi: 10.18653/v1/P19-1285.
M. Deitke, E. VanderBilt, A. Herrasti, L. Weihs, J. Salvador, K. Ehsani, W. Han, E. Kolve, A. Farhadi,
A. Kembhavi, and R. Mottaghi. ProcTHOR: Large-scale embodied AI using procedural generation.
In Advances in Neural Information Processing Systems, 2022. doi: 10.48550/ARXIV.2206.06994.
M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch, and S. Levine. Emergent complexity
and zero-shot transfer via unsupervised environment design. In Advances in Neural Information
Processing Systems, volume 33, 2020.
J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers
for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1
(Long and Short Papers). Association for Computational Linguistics, 2019.
25

Human-Timescale Adaptation in an Open-Ended Task Space
Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. RL2: Fast reinforcement
learning via slow reinforcement learning, 2017.
L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu, and
A. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge.
In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track,
2022.
K. Fang, Y. Zhu, S. Savarese, and F.-F. Li. Adaptive procedural task generation for hard-exploration
problems. In International Conference on Learning Representations, 2021.
G. Farquhar, K. Baumli, Z. Marinho, A. Filos, M. Hessel, H. P. van Hasselt, and D. Silver. Self-consistent
models and values. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors,
Advances in Neural Information Processing Systems, volume 34, pages 1111‚Äì1125, 2021.
A. Filos, E. V√©rtes, Z. Marinho, G. Farquhar, D. Borsa, A. L. Friesen, F. M. P. Behbahani, T. Schaul,
A. Barreto, and S. Osindero. Model-value inconsistency as a signal for epistemic uncertainty. In
K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv√°ri, G. Niu, and S. Sabato, editors, International
Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume
162 of Proceedings of Machine Learning Research, pages 6474‚Äì6498. PMLR, 2022.
C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In
Proceedings of the 34th International Conference on Machine Learning, ICML, Sydney, NSW, Australia,
6-11 August, volume 70 of Proceedings of Machine Learning Research. PMLR, 2017.
C. Finn, K. Xu, and S. Levine.
Probabilistic model-agnostic meta-learning.
Advances in neural
information processing systems, 31, 2018.
S. Flennerhag, Y. Schroecker, T. Zahavy, H. van Hasselt, D. Silver, and S. Singh. Bootstrapped
meta-learning. In International Conference on Learning Representations, 2022.
M. Fontaine, Y.-C. Hsu, Y. Zhang, B. Tjanaka, and S. Nikolaidis. On the importance of environments
in human-robot coordination. 07 2021.
F. Garcia and P. S. Thomas. A meta-mdp approach to exploration for lifelong reinforcement learning.
Advances in Neural Information Processing Systems, 32, 2019.
D. Grbic, R. Palm, E. Najarro, C. Glanois, and S. Risi. EvoCraft: A New Challenge for Open-Endedness,
pages 325‚Äì340. 04 2021.
S. Gronauer and K. Diepold. Multi-agent deep reinforcement learning: a survey. ArtiÔ¨Åcial Intelligence
Review, 55(2):895‚Äì943, 2022.
D. Hafner. Benchmarking the spectrum of agent capabilities. In International Conference on Learning
Representations, 2022.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pages 770‚Äì778, 2016. doi: 10.1109/
CVPR.2016.90.
J. Heinrich, M. Lanctot, and D. Silver. Fictitious self-play in extensive-form games. In International
conference on machine learning, pages 805‚Äì813. PMLR, 2015.
D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv: Learning, 2016.
26

Human-Timescale Adaptation in an Open-Ended Task Space
M. Hessel, I. Danihelka, F. Viola, A. Guez, S. Schmitt, L. Sifre, T. Weber, D. Silver, and H. Van Hasselt.
Muesli: Combining improvements in policy optimization. In International Conference on Machine
Learning. PMLR, 2021.
J. HoÔ¨Ämann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.
Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint
arXiv:2203.15556, 2022.
H. Hu, A. Peysakhovich, A. Lerer, and J. Foerster. ‚Äúother-play‚Äùfor zero-shot coordination. In Proceedings
of Machine Learning and Systems 2020, pages 9396‚Äì9407, 2020.
J. Humplik, A. Galashov, L. Hasenclever, P. A. Ortega, Y. W. Teh, and N. Heess. Meta reinforcement
learning as task inference. arXiv preprint arXiv:1905.06424, 2019.
M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue, A. Razavi, O. Vinyals, T. Green,
I. Dunning, K. Simonyan, et al. Population based training of neural networks. arXiv preprint
arXiv:1711.09846, 2017.
M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G. Casta√±eda, C. Beattie, N. C.
Rabinowitz, A. S. Morcos, A. Ruderman, N. Sonnerat, T. Green, L. Deason, J. Z. Leibo, D. Silver,
D. Hassabis, K. Kavukcuoglu, and T. Graepel. Human-level performance in 3d multiplayer games
with population-based reinforcement learning. Science, 364(6443):859‚Äì865, 2019.
M. Janner, Q. Li, and S. Levine. OÔ¨Ñine reinforcement learning as one big sequence modeling problem.
In Advances in Neural Information Processing Systems, 2021.
M. Jiang, M. Dennis, J. Parker-Holder, J. Foerster, E. Grefenstette, and T. Rockt√§schel. Replay-guided
adversarial environment design. In Advances in Neural Information Processing Systems, 2021a.
M. Jiang, E. Grefenstette, and T. Rockt√§schel. Prioritized level replay. In The International Conference
on Machine Learning, 2021b.
M. Johnson, K. Hofmann, T. Hutton, and D. Bignell. The Malmo platform for artiÔ¨Åcial intelligence
experimentation. In Proceedings of the Twenty-Fifth International Joint Conference on ArtiÔ¨Åcial
Intelligence. AAAI Press, 2016.
A. Juliani, A. Khalifa, V. Berges, J. Harper, E. Teng, H. Henry, A. Crespi, J. Togelius, and D. Lange.
Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning. In IJCAI, 2019.
N. Justesen, R. R. Torrado, P. Bontrager, A. Khalifa, J. Togelius, and S. Risi. Procedural level generation
improves generality of deep reinforcement learning. CoRR, abs/1806.10729, 2018.
A. Kanervisto, S. Milani, K. Ramanauskas, N. Topin, Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, W. Yang, W. Hong,
Z. Huang, H. Chen, G. Zeng, Y. Lin, V. Micheli, E. Alonso, F. Fleuret, A. Nikulin, Y. Belousov,
O. Svidchenko, and A. Shpilman. Minerl diamond 2021 competition: Overview, results, and lessons
learned, 2022.
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,
and D. Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020.
R. Kirk, A. Zhang, E. Grefenstette, and T. Rockt√§schel. A survey of generalisation in deep reinforcement
learning. CoRR, abs/2111.09794, 2021.
L. Kirsch, J. Harrison, J. Sohl-Dickstein, and L. Metz. General-purpose in-context learning by meta-
learning transformers. arXiv, 2022.
27

Human-Timescale Adaptation in an Open-Ended Task Space
A. Kumar, Z. Fu, D. Pathak, and J. Malik. RMA: Rapid motor adaptation for legged robots. In Robotics:
Science and Systems, 2021.
H. K√ºttler, N. Nardelli, A. H. Miller, R. Raileanu, M. Selvatici, E. Grefenstette, and T. Rockt√§schel. The
NetHack Learning Environment. In Proceedings of the Conference on Neural Information Processing
Systems (NeurIPS), 2020.
M. Laskin, L. Wang, J. Oh, E. Parisotto, S. Spencer, R. Steigerwald, D. Strouse, S. Hansen, A. Filos,
E. Brooks, M. Gazeau, H. Sahni, S. Singh, and V. Mnih. In-context reinforcement learning with
algorithm distillation, 2022.
K.-H. Lee, O. Nachum, S. Yang, L. Lee, C. D. Freeman, S. Guadarrama, I. Fischer, W. Xu, E. Jang,
H. Michalewski, and I. Mordatch. Multi-game decision transformers. In A. H. Oh, A. Agarwal,
D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022.
S. Liu, G. Lever, N. Heess, J. Merel, S. Tunyasuvunakool, and T. Graepel. Emergent coordination
through competition. In International Conference on Learning Representations, 2019.
D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri, Y. Li, A. Bharambe, and L. van der Maaten.
Exploring the limits of weakly supervised pretraining. In Computer Vision ‚Äì ECCV 2018: 15th
European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part II, page 185‚Äì201,
Berlin, Heidelberg, 2018. Springer-Verlag. ISBN 978-3-030-01215-1.
T. Matiisen, A. Oliver, T. Cohen, and J. Schulman. Teacher-student curriculum learning. IEEE Trans.
Neural Networks Learn. Syst., 31(9):3732‚Äì3740, 2020.
L. C. Melo. Transformers are meta-reinforcement learners. In International Conference on Machine
Learning, pages 15340‚Äì15359. PMLR, 2022.
L. Metz, C. D. Freeman, S. S. Schoenholz, and T. Kachman. Gradients are not all you need. arXiv
preprint arXiv:2111.05803, 2021.
V. Mikulik, G. Del√©tang, T. McGrath, T. Genewein, M. Martic, S. Legg, and P. Ortega. Meta-trained
agents implement bayes-optimal agents. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and
H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18691‚Äì18703.
Curran Associates, Inc., 2020.
J. Mu, V. Zhong, R. Raileanu, M. Jiang, N. Goodman, T. Rockt√§schel, and E. Grefenstette. Improving
intrinsic exploration with language abstractions. In Advances in Neural Information Processing
Systems, 2022.
R. Munos, T. Stepleton, A. Harutyunyan, and M. Bellemare. Safe and eÔ¨Écient oÔ¨Ä-policy reinforcement
learning. Advances in neural information processing systems, 29, 2016.
E. Nikishin, M. Schwarzer, P. D‚ÄôOro, P.-L. Bacon, and A. Courville. The primacy bias in deep rein-
forcement learning. In International Conference on Machine Learning, pages 16828‚Äì16847. PMLR,
2022.
OEL Team, A. Stooke, A. Mahajan, C. Barros, C. Deck, J. Bauer, J. Sygnowski, M. Trebacz, M. Jaderberg,
M. Mathieu, N. McAleese, N. Bradley-Schmieg, N. Wong, N. Porcel, R. Raileanu, S. Hughes-Fitt,
V. Dalibard, and W. M. Czarnecki. Open-ended learning leads to generally capable agents. CoRR,
abs/2107.12808, 2021.
28

Human-Timescale Adaptation in an Open-Ended Task Space
OpenAI, M. Plappert, R. Sampedro, T. Xu, I. Akkaya, V. Kosaraju, P. Welinder, R. D‚ÄôSa, A. Petron,
H. P. de Oliveira Pinto, A. Paino, H. Noh, L. Weng, Q. Yuan, C. Chu, and W. Zaremba. Asymmetric
self-play for automatic goal discovery in robotic manipulation, 2021.
P. A. Ortega, J. X. Wang, M. Rowland, T. Genewein, Z. Kurth-Nelson, R. Pascanu, N. Heess, J. Ve-
ness, A. Pritzel, P. Sprechmann, et al. Meta-learning of sequential strategies. arXiv preprint
arXiv:1905.03030, 2019.
K. Ota, D. K. Jha, and A. Kanezaki. Training larger networks for deep reinforcement learning, 2021.
E. Parisotto. Meta Reinforcement Learning through Memory. PhD thesis, Carnegie Mellon University
Pittsburgh, PA, 2021.
E. Parisotto, F. Song, J. Rae, R. Pascanu, C. Gulcehre, S. Jayakumar, M. Jaderberg, R. L. Kaufman,
A. Clark, S. Noury, et al. Stabilizing transformers for reinforcement learning. In International
conference on machine learning, pages 7487‚Äì7498. PMLR, 2020.
J. Parker-Holder, M. Jiang, M. Dennis, M. Samvelyan, J. Foerster, E. Grefenstette, and T. Rockt√§schel.
Evolving curricula with regret-based environment design. In The International Conference on
Machine Learning, 2022.
M. Pislar, D. Szepesvari, G. Ostrovski, D. L. Borsa, and T. Schaul. When should agents explore? In
International Conference on Learning Representations, 2022.
R. Portelas, C. Colas, K. Hofmann, and P. Oudeyer. Teacher algorithms for curriculum learning of
deep RL in continuously parameterized environments. In L. P. Kaelbling, D. Kragic, and K. Sugiura,
editors, 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November
1, 2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pages 835‚Äì853.
PMLR, 2019.
J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. HoÔ¨Ämann, F. Song, J. Aslanides, S. Henderson, R. Ring,
S. Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv
preprint arXiv:2112.11446, 2021.
R. Raileanu and T. Rockt√§schel. Ride: Rewarding impact-driven exploration for procedurally-generated
environments. In International Conference on Learning Representations, 2020.
K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen. EÔ¨Écient oÔ¨Ä-policy meta-reinforcement
learning via probabilistic context variables. In International conference on machine learning, pages
5331‚Äì5340. PMLR, 2019.
S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-maron, M. Gim√©nez, Y. Sulsky,
J. Kay, J. T. Springenberg, T. Eccles, J. Bruce, A. Razavi, A. Edwards, N. Heess, Y. Chen, R. Hadsell,
O. Vinyals, M. Bordbar, and N. de Freitas. A generalist agent. Transactions on Machine Learning
Research, 2022.
M. Reid, Y. Yamada, and S. S. Gu. Can wikipedia help oÔ¨Ñine reinforcement learning? CoRR, 2022.
S. Risi and J. Togelius. Increasing generality in machine learning through procedural content genera-
tion. Nature Machine Intelligence, 2, 08 2020. doi: 10.1038/s42256-020-0208-z.
M. Samvelyan, R. Kirk, V. Kurin, J. Parker-Holder, M. Jiang, E. Hambro, F. Petroni, H. Kuttler,
E. Grefenstette, and T. Rockt√§schel. Minihack the planet: A sandbox for open-ended reinforcement
learning research. In Thirty-Ô¨Åfth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track, 2021.
29

Human-Timescale Adaptation in an Open-Ended Task Space
M. Samvelyan, A. Khan, M. D. Dennis, M. Jiang, J. Parker-Holder, J. N. Foerster, R. Raileanu, and
T. Rockt√§schel. MAESTRO: Open-ended environment design for multi-agent reinforcement learning.
In Deep Reinforcement Learning Workshop NeurIPS 2022, 2022.
T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. In The International
Conference on Learning Representations, 2015.
J. Schmidhuber. Curious model-building control systems. In [Proceedings] 1991 IEEE International Joint
Conference on Neural Networks, pages 1458‚Äì1463 vol.2, 1991. doi: 10.1109/IJCNN.1991.170605.
J. Schmidhuber. Learning complex, extended sequences using the principle of history compression.
Neural Computation, 4(2):234‚Äì242, 1992. doi: 10.1162/neco.1992.4.2.234.
S. Schmitt, J. J. Hudson, A. Zidek, S. Osindero, C. Doersch, W. M. Czarnecki, J. Z. Leibo, H. Kuttler,
A. Zisserman, K. Simonyan, et al.
Kickstarting deep reinforcement learning.
arXiv preprint
arXiv:1803.03835, 2018.
C. Schuhmann, R. Beaumont, R. Vencu, C. W. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta,
C. Mullis, M. Wortsman, P. Schramowski, S. R. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk,
and J. Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models.
In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track,
2022.
N. Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.
B. C. Stadie, G. Yang, R. Houthooft, X. Chen, Y. Duan, Y. Wu, P. Abbeel, and I. Sutskever. Some consid-
erations on learning to explore via meta-reinforcement learning. arXiv preprint arXiv:1803.01118,
2018.
P. Stone, G. A. Kaminka, S. Kraus, and J. S. Rosenschein. Ad hoc autonomous agent teams: Collabora-
tion without pre-coordination. In M. Fox and D. Poole, editors, Proceedings of the Twenty-Fourth
AAAI Conference on ArtiÔ¨Åcial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010. AAAI
Press, 2010.
D. Strouse, K. McKee, M. Botvinick, E. Hughes, and R. Everett. Collaborating with humans without
human data. Advances in Neural Information Processing Systems, 34:14502‚Äì14515, 2021.
S. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, and R. Fergus. Intrinsic motivation and
automatic curricula via asymmetric self-play. In International Conference on Learning Representations,
2018.
C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable eÔ¨Äectiveness of data in deep
learning era. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 843‚Äì852,
2017.
Y. Tay, M. Dehghani, S. Abnar, H. W. Chung, W. Fedus, J. Rao, S. Narang, V. Q. Tran, D. Yogatama, and
D. Metzler. Scaling laws vs model architectures: How does inductive bias inÔ¨Çuence scaling?, 2022.
J. Togelius and J. Schmidhuber. An experiment in automatic game design. In 2008 IEEE Symposium
On Computational Intelligence and Games, pages 111‚Äì118, 2008. doi: 10.1109/CIG.2008.5035629.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin.
Attention is all you need. Advances in neural information processing systems, 30, 2017.
30

Human-Timescale Adaptation in an Open-Ended Task Space
O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell,
T. Ewalds, P. Georgiev, J. Oh, . D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P.
Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden, Y. Sulsky,
J. Molloy, T. L. Paine, √á. G√ºl√ßehre, Z. Wang, T. PfaÔ¨Ä, Y. Wu, R. Ring, D. Yogatama, D. W√ºnsch,
K. McKinney, O. Smith, T. Schaul, T. P. Lillicrap, K. Kavukcuoglu, D. Hassabis, C. Apps, and D. Silver.
Grandmaster level in starcraft II using multi-agent reinforcement learning. Nat., 575(7782):
350‚Äì354, 2019. doi: 10.1038/s41586-019-1724-z.
R. Vuorio, J. A. Beck, G. Farquhar, J. N. Foerster, and S. Whiteson. No dice: An investigation of the
bias-variance tradeoÔ¨Äin meta-gradients. In Deep RL Workshop NeurIPS 2021, 2021.
L. Vygotsky. Interaction between learning and development. Readings on the Development of Children,
pages 34‚Äì40, 1978.
J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran,
and M. Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.
J. X. Wang, Z. Kurth-Nelson, D. Kumaran, D. Tirumala, H. Soyer, J. Z. Leibo, D. Hassabis, and
M. Botvinick. Prefrontal cortex as a meta-reinforcement learning system. Nature neuroscience, 21
(6):860‚Äì868, 2018.
J. X. Wang, M. King, N. Porcel, Z. Kurth-Nelson, T. Zhu, C. Deck, P. Choy, M. Cassin, M. Reynolds,
F. Song, et al. Alchemy: A structured task distribution for meta-reinforcement learning. arxiv,
2021.
R. Wang, J. Lehman, J. Clune, and K. O. Stanley. Paired open-ended trailblazer (POET): endlessly
generating increasingly complex and diverse learning environments and their solutions. CoRR,
abs/1901.01753, 2019.
R. Wang, J. Lehman, A. Rawal, J. Zhi, Y. Li, J. Clune, and K. Stanley. Enhanced POET: Open-ended
reinforcement learning through unbounded invention of learning challenges and their solutions. In
H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning Research, pages 9940‚Äì9951. PMLR, 13‚Äì18 Jul
2020a.
T. Wang, H. Dong, V. Lesser, and C. Zhang. Roma: Multi-agent reinforcement learning with emergent
roles. arXiv preprint arXiv:2003.08039, 2020b.
Z. Xu, J. Modayil, H. P. van Hasselt, A. Barreto, D. Silver, and T. Schaul. Natural value approximators:
Learning when to trust past estimates. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017.
Z. Xu, H. P. van Hasselt, and D. Silver. Meta-gradient reinforcement learning. Advances in neural
information processing systems, 31, 2018.
J. Yang, B. Petersen, H. Zha, and D. Faissol. Single episode policy transfer in reinforcement learning.
In International Conference on Learning Representations, 2019.
J. Yang, A. Li, M. Farajtabar, P. Sunehag, E. Hughes, and H. Zha. Learning to incentivize other learning
agents. In Advances in Neural Information Processing Systems. arXiv, 2020. doi: 10.48550/ARXIV.
2006.06051.
W. Yu, J. Tan, Y. Bai, E. Coumans, and S. Ha. Learning fast adaptation with meta strategy optimization.
IEEE Robotics and Automation Letters, 5(2):2950‚Äì2957, 2020.
31

Human-Timescale Adaptation in an Open-Ended Task Space
X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision transformers. In 2022 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), Los Alamitos, CA, USA, jun 2022.
IEEE Computer Society.
Z. Zheng, J. Oh, M. Hessel, Z. Xu, M. Kroiss, H. Van Hasselt, D. Silver, and S. Singh. What can learned
intrinsic rewards capture? In International Conference on Machine Learning, pages 11436‚Äì11446.
PMLR, 2020.
V. Zhong, T. Rockt√§schel, and E. Grefenstette. RTFM: generalising to new environment dynamics
via reading. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020, 2020.
L. Zintgraf. Fast adaptation via meta reinforcement learning. PhD thesis, University of Oxford, 2022.
L. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson. Varibad: A very good
method for bayes-adaptive deep rl via meta-learning. arXiv preprint arXiv:1910.08348, 2019.
32

Human-Timescale Adaptation in an Open-Ended Task Space
Appendix
A. Environment Details
A.1. XLand 2.0
In this section we describe the diÔ¨Äerences between XLand 2.0 and the original XLand environment of
OEL Team et al. (2021). We modify the conÔ¨Åguration space as follows:
‚Ä¢ We introduce a new relation touching(a,b). It is satisÔ¨Åed if objects a and b are in contact,
as determined by Unity‚Äôs collision detection with a distance threshold of 1 millimeter.
‚Ä¢ We exclude all relations that refer to Ô¨Çoors. As a more Ô¨Çexible alternative we introduce the
option of spawning objects in a permanently frozen state, rendering them immobile. Frozen
objects can be used as anchor points in the environment which require players to navigate to
them by including them in goals or production rules.
‚Ä¢ We only use predicates consisting of a single relation or its negation, excluding conjunctions or
disjunctions of multiple relations. Note that production rules (Section 2.1) allow us to specify
tasks which require the player to sequentially satisfy predicates, or to give them multiple ways
to reach a desired state.
For the reader‚Äôs convenience, Tables A.1 and A.2 respectively list all shapes and colours we used
for objects in XLand. Table A.3 lists all predicates we used for goals and production rules.
Table A.1 | Shapes used for objects.
Shape name
Wall
Cube
Sphere
Pyramid
Table A.2 | Colours used for objects.
Colour name
Black
Purple
Yellow
Table A.3 | Predicates used for goals and production rules.
Predicate name
Meaning
touching(a,b)
Whether a and b are in contact.
near(a,b)
Whether a and b are at most 1m apart.
hold(a,b)
Whether player a holds b.
see(a,b)
If a is a player, whether it can see b. If not, whether the line
connecting the centres of mass of a and b is not obstructed by
another object.
not(p)
Whether predicate p is not satisÔ¨Åed.
33

Human-Timescale Adaptation in an Open-Ended Task Space
A.2. Pre-sampling tasks for training
The space of goals and production rules can generate at least 1040 distinct tasks, even given quite
restrictive bounds on the number of objects and rules.6 For convenience, we pre-sample a subset of
this space using the method described below. In Section 3.5 we evaluate the eÔ¨Äect of the size of the
sampled set. For each task we sample a world using the procedure outlined in OEL Team et al. (2021)
and combine it with a game and production rules as follows.
Single-player tasks.
We start by uniformly sampling a player‚Äôs goal, consisting of a predicate with
optional negation and two objects. Then, for a Ô¨Åxed number of steps (which we sample uniformly
between 1 and 4), we add new production rules, such that they need to be triggered in sequence
to get the objects present in the goal. We initialise the world to contain the objects present in the
condition of the Ô¨Årst production rule, together with up to 10 distractor objects, not present in any
production rule from this chain (nor in the goal).
Next, we introduce dead-end production rules. We sample them such that their condition contains
either distractor objects or ones that are ultimately necessary for satisfaction of the goal, yet the
spawns are always distractors. As such, triggering a dead end may put the game in an unsolvable state.
Including them in the training games creates pressure on the agent to avoid indiscriminately triggering
all production rules. Finally, we sample a hiding mask, specifying which part of the production rules
will be visible or hidden from the player. The sampling of both the mechanism for hiding (described
in Section 2.1) and the actual parts to hide is uniform random.
Multi-player tasks.
For this work we restrict our multi-player games to fully cooperative two-player
games. Such games are known to be particularly challenging, as they have multiple equilibria, and
thus feature an equilibrium selection problem (Dafoe et al., 2020). To sample such a game, we start
by sampling a single-player game as outlined above and randomly replace references to the Ô¨Årst
player in the goal or production rules with references to player two. We copy the goal and sample a
new production rule hiding mask for player two, resulting in a fully cooperative two-player game
with potentially asymmetric information provided about the task‚Äôs production rules.
B. Evaluation
B.1. Test scores
We evaluate our agents on a suite of 1000 held out test tasks sampled from the same distribution as
the training games, using held-out world topologies. The procedure for pre-generating the XLand
task pool is detailed in Appendix A.2. Rejection sampling ensures that no game (goal and production
rules) in the test set is contained in the training set.
In XLand, rewards are obtained on every frame in which a goal is satisÔ¨Åed, making total rewards
incomparable between tasks. To account for this, we Ô¨Åne-tune AdA on the test-task set. We compute
the Ô¨Åne-tuned agent‚Äôs maximum total last-trial reward (over any number of trials up to 13) and use
this as an estimate of the maximum possible reward obtainable in a single trial of each test task. We
call this quantity the normaliser. We deÔ¨Åne the test score ùëÜùëñ
ùëöof an agent ùëñon task ùëöwith ùëòtrials to be
the total reward obtained in trial ùëòdivided by the normaliser. This normalises rewards roughly to the
6This is an order of magnitude lower-bound estimate, assuming 4 shapes, 3, colours, 7 predicates, a maximum of 5
production rules with a maximum of 3 objects on the right-hand side, 7 blanking options, and a maximum of 20 objects in
the scene.
34

Human-Timescale Adaptation in an Open-Ended Task Space
interval [0, 1]. Note that it is possible for an agent under evaluation to obtain a score greater than
1, both due to noise in the evaluation process and the fact that the agent under evaluation may be
better than the one used for creating the normaliser.
When reporting the scores of our agents on a game with ùëòtrials, we always use the total reward of
the last trial. This is a good measure of whether the agent has successfully navigated the exploration-
exploitation tradeoÔ¨Äin a novel MDP, given knowledge of the number of trials ùëò. If an agent is capable
of adaptation, we expect to see the performance in the last (ùëòth) trial increase as a function of ùëò:
that is to say, the agent is able to make use of additional experience on-the-Ô¨Çy to perform better. We
evaluate on ùëò‚àà{1, 2, 3, 5, 8, 13}, where 8 and 13 are held out values of ùëòthat were not seen during
training.
To aggregate the scores of an agent across games, we use a Ô¨Åxed (usually 20th) percentile score
(Agarwal et al., 2021). This gives us a lower bound guarantee on the performance of the agent across
most of the tasks: for example, if the 20th percentile score of an agent is 0.5, then the agent gets the
score of at least 0.5 on 80% of the games. Using an aggregation method like this (as opposed to an
average) allows us to concentrate on the coverage of many tasks, as opposed to focusing the eÔ¨Äort on
improving the performance on outlier tasks. Empirically, we Ô¨Ånd that our results are robust across a
range of diÔ¨Äerent percentiles.
B.2. Hand-authored probe tasks
Evaluation using test tasks can only give us information with respect to the pre-sampled distribution in
Appendix A.2. While this is certainly vast and diverse, an arbitrary task sampled from the test set is not
necessarily easily understandable for humans. So in addition to quantitative evaluation on 1000 test
tasks we also investigate speciÔ¨Åc human-level capabilities of our agent on two sets of 30 single-agent
and 28 multi-agent probe tasks. These are based on situations that are intuitive to humans and which
require qualitatively diÔ¨Äerent behaviours, which we can inspect in more detail ‚Äúwith the naked eye‚Äù.
A full description of all 58 probe tasks can be found in Appendix F. Representative single-agent and
multi-agent probe tasks are described in detail in Figures B.1 to B.3.
B.3. Adaptation metric
We introduce an adaptation metric to rank our agents based on their few-shot adaptation capability
across the hand-authored probe task set. We collect last-trial total reward for all (ùëö, ùëò) pairs where
ùëöis a probe task and ùëò‚àà{1, 2, 3, 5, 8, 13}. We normalise the per-task scores just as for the test set
above. We then aggregate over tasks using the 50th percentile (median). Finally, we aggregate over
ùëòby saying that agent ùê¥ranks higher than agent ùêµif and only if ùê¥‚Äôs task-aggregated scores are a
Pareto improvement over ùêµ‚Äôs. That is to say, we would like agents that are both capable of high-quality
zero-shot generalisation where possible (ùëò= 1), and also that can use additional trial information to
eÔ¨Éciently improve their policy in few-shots ùëò> 1; we don‚Äôt ‚Äútrade-oÔ¨Ä‚Äù between these.
A convenient way of using the Pareto improvement criterion to compute a scalar metric is the Nash
average method of Balduzzi et al. (2018). We construct a competitive meta-game of ‚Äúagents vs. ùëò‚Äù,
and compute the maximum entropy Nash equilibrium for the game. The Nash payoÔ¨Äis then used as
the adaptation metric, and agents are ranked by this metric. As desired, this metric has the property
that if neither agent ùê¥nor agent ùêµPareto-dominate the other, the ùê¥and ùêµreceive the same Nash
payoÔ¨Äand are therefore ranked equally. This adaptation metric was used as the means of selecting
hyperparameters for training our best performing agent in Section 3.1.
35

Human-Timescale Adaptation in an Open-Ended Task Space
Figure B.1 | Wrong Pair Dis-
appears: The player‚Äôs goal is
to hold a black cube, which
does not exist among the ini-
tial objects.
But there are
two (hidden) production rules.
The player needs to identify
the correct world state which
triggers the rule that creates
the cube and not the one
which destroys the necessary
inputs. All this is embedded
in a challenging world layout
with one-way drops and lim-
ited visibility.
Figure B.2 | Pyramid in a
Haystack: To create the nec-
essary yellow pyramid, the
player needs to Ô¨Ånd and hold
the purple cube. There are sev-
eral distractor objects and dis-
tractor rules in this world, re-
quiring the player to deal not
just with a hard exploration
challenge but also a very noisy
environment.
Figure B.3 | Push, don‚Äôt lift:
The vast majority of training
and evaluation tasks require
lifting objects. Here two hid-
den rules destroy any object
when lifted. In order to cre-
ate the goal state, some ‚Äúlat-
eral thinking‚Äù is necessary: the
player needs to identify that
pushing the cubes with their
body is possible.
36

Human-Timescale Adaptation in an Open-Ended Task Space
Figure B.4 | Irreversible pro-
duction for two: Both play-
ers score when the Ô¨Årst player
holds a yellow sphere. There is
no yellow sphere initially, but it
can be produced from execut-
ing the Ô¨Årst, second and fourth
production rules in order. The
other two rules are dead ends,
destroying key input objects.
Note that some input objects
exist multiple times in the ini-
tial state, so there are multiple
solution paths.
(a)
(b)
Figure B.5 | The human player interface. (a) Prior to each trial players are told how many trials they
have remaining on the current task, and are given unlimited time to read the goal and production
rules (subject to any hiding). (b) During the trial players observe the same Ô¨Årst-person camera view
as agents, but at a higher 800 √ó 600 pixel resolution. The goal, production rules, current score, and
time remaining in the trial are displayed via UI elements.
B.4. Human data collection
To provide a benchmark for human-timescale adaptation, we collected score data from a pool of
100 human players on the 30 single-agent probe tasks. Before attempting the probe tasks, each
player completed a graded training curriculum of 23 tasks to acquire familiarity with the mouse-
and-keyboard control scheme and user interface (Figure B.5), and the particular game mechanics of
XLand. Since humans cannot undergo a short-term memory ‚Äúreset‚Äù on episode boundaries, individual
players attempted each of the probe tasks for a single ùëòonly, with ùëò‚àà{1, 2, 3, 5, 8}. All players
experienced a variety of ùëòacross the task set. We assigned each player a unique ordering of the
30 probe tasks to average out any knowledge transfer from earlier tasks to later, and, within those
orderings, we imposed separation between tasks with known similarity. Technical problems (e.g.
internet dropout) prevented completion of 3.4% of the 3000 episodes, leaving an average of 19.3
samples per (task, ùëò) pair, and a minimum of 17 samples for any individual (task, ùëò).
37

Human-Timescale Adaptation in an Open-Ended Task Space
C. Agent Details
Here we provide technical details of our agent architecture, observation and action speciÔ¨Åcations.
C.1. Agent Architecture
Here we provide an overview of the agent architecture, and provide important hyperparameters for
the agent, noting that we performed minimal hyperparameter tuning in this work.
Observation encoder.
The Ô¨Årst-person view RGB observation (Table C.1) is passed through a ResNet
(He et al., 2016) with [16, 32, 32] convolutional channels, each consisting of 2 √ó 2 blocks and a Ô¨Ånal
output size of 256. Max-pooling is used, in addition to scalar residual multipliers. relu activations
are used throughout.
The goal observation is passed through a goal embedder, which is the same in OEL Team et al.
(2021). This maps each of the 6 goal elements (negation, predicate, shape of object 1, colour of object
1, shape of object 2, colour of object 2) in the goal representation to a dense embedding vector of
size 8. These are concatenated together and passed through a 3-layer MLP of size [8, 8, 8], resulting
in a Ô¨Ånal goal embedding of size 8.
Production rules are encoded in the same manner as the goal, but mapped through a larger Ô¨Ånal
MLP of shape [512, 256] resulting in a Ô¨Ånal production rule embedding vector of size 256.
The encoded RGB, goal, and production rules observations are concatenated together with all
remaining scalar and vector observations (including previous reward, previous action, proprioception
observations, and trial timing information) and passed though a Ô¨Ånal MLP. This results in an encoded
observation vector of a size matching the hidden dimension of the Transformer memory.
Transformer memory.
We use a Transformer-XL with causal masking (Dai et al., 2019). For the
actor step we use a context window of 1, and in the learner step we use a rollout context window of
80. The Transformer-XL memory uses 300 previous cached activations (1800 eÔ¨Äective timesteps) of
memory in all experiments unless otherwise stated. We apply layer normalisation before attention
operations as in Parisotto et al. (2020), use gating in the feedforward component as in Shazeer (2020),
and apply relative positional embeddings as in Dai et al. (2019). As is common with Transformers,
we use the gelu activation function throughout (Hendrycks and Gimpel, 2016).
Muesli sequence model and prediction heads.
Next, we take the Transformer-XL output embed-
ding, and use two MLPs of width 1000 to produce the hidden and cell values for the initial state of the
Muesli model LSTM. On top of the hidden value, we apply MLP heads of width 1000 for the policy and
value respectively. The policy MLP is followed by 6 linear softmaxed outputs corresponding to 6 action
groups in a decomposed action space for the policy (as in OEL Team et al. (2021)). The value MLP is
followed by a 601-unit binned logit prediction as in Hessel et al. (2021). Finally, the Muesli sequence
model is unrolled for a further 4 steps, starting from the LSTM state embedding and producing a
1000-dimensional output vector on each LSTM step. This feeds into a further 1000-dimensional MLP
followed by a 601-unit binned reward prediction.
38

Human-Timescale Adaptation in an Open-Ended Task Space
C.2. Observations
We summarise all the observations received by the agent when running inference in Table C.1. In the
descriptions, ‚Äúlegacy reasons‚Äù refers to an observation format that was inherited from OEL Team et al.
(2021).
Table C.1 | Agent observations.
Observation name
Shape
Meaning
RGB
72√ó96√ó3
RGB values of the Ô¨Årst-person view of the agent.
IS HOLDING
1
Integer in {0, 1} indicating whether the agent is hold-
ing an object.
HAND DISTANCE
1
Distance to the held object as a fraction of the agent‚Äôs
maximum reach (0 while no object is held).
HAND FORCE
1
The force exerted by the agent on the held object
as a fraction of its maximum grip force (0 while no
object is held).
LAST ACTION
10
Last action performed by the agent.
GOAL ATOMS
6 √ó 6
Agent-readable description of the goal. For legacy
reasons, only the Ô¨Årst row is non-zero. The Ô¨Årst row
(6 numbers) describes the goal with elements:
1: whether the goal is negated or not,
2: index of the binary predicate,
3-4: shape and colour of the Ô¨Årst objects,
5-6: shape and colour of the second object.
GOAL SOP MATRIX
6 √ó 6
Always the same, kept for legacy reasons.
ATOM REWARDS
6
The reward of the agent in the previous frame (1
number), padded with zeros for legacy reasons.
OPTION REWARDS
6
Same as ATOM REWARDS, kept for legacy reasons.
PRODUCTION RULES
16 √ó 26
A description of up to 16 production rules. A single
production rule is described as: 3 √ó 6 = 18 numbers
describing up to three object-predicate-object trig-
gers and 4 √ó 2 = 8 numbers describing up to 4 spawn
objects. We only ever use a single object-predicate-
object trigger for all tasks in this paper. Hiding (Sec-
tion 2.1) is implemented by adding extra predicate
and shape indices meaning hidden production rule,
Ô¨Årst hidden object, etc.
REWARD
1
The environment reward obtained in the previous
step.
TRIALS REMAINING
5
One-hot encoding of the number of trials remaining
in the episode, up to a maximum of 5 trials remaining.
MORE THAN 5 TRIALS
1
Integer in {0, 1} indicating whether there are more
than 5 trials remaining in the episode.
TIME UNTIL LAST
TRIAL
1
Time remaining (in seconds) until the Ô¨Ånal trial in
the current episode.
TIME LEFT IN CURRENT
TRIAL
1
Time remaining (in seconds) until the end of the
current trial.
39

Human-Timescale Adaptation in an Open-Ended Task Space
DURATION OF LAST
TRIAL
1
The duration (in seconds) of the Ô¨Ånal trial in this
episode. In our setting, all trials for the same task
have the same duration.
DURATION OF NEXT
TRIAL
1
The duration (in seconds) of the next trial in the
current episode. In our setting, all trials for the same
task have the same duration.
D. Training Details
D.1. Meta-RL
AdA is trained with a meta-RL setup, in which episodes of experience for the agent comprise multiple
trials of interaction with the task environment, where the task is reset on trial boundaries. In this
setting, it is known that the agent‚Äôs policy can converge to Bayes-optimal behaviour, experimenting
on-the-Ô¨Çy to reduce its epistemic uncertainty, and reusing discovered information to achieve goals
increasingly eÔ¨Éciently. It is important that the agent‚Äôs memory is not reset at trial boundaries, but only
at episode boundaries. Similarly, the agent trains with a Ô¨Åxed discount factor ùõæ‚â†0 throughout the
episode, including on trial boundaries. For training, we sample (ùëö, ùëò) pairs according to a factorised
distribution (ùúåM, ùúåùêæ), the parameters of which are controlled by an automatic curriculum (Section
2.3). The MDPs ùëöare all drawn from a procedurally generated domain M, called XLand (Section 2.1).
We choose as our space of trials ùêæ= {1, 2, . . . 6}, and provide our agent with ùëòas a conditioning input.
After training, our agent is capable of few-shot adaptation across a wide-range of MDPs, including in
held-out ùëö‚ààM on which ùúåM puts no probability mass, and when ùëò> 6.
D.2. Single-agent training
Our single-agent training setup uses a task pool generated as described in Section 2.1. The ex-
perimental setup for the single-agent distillation teacher is summarised in Table D.1. Single-agent
training used an earlier version of XLand 2.0 than multi-agent experiments, without the frozen objects
described in Section A.1. Frozen objects were also therefore excluded from the test and hand-authored
probe task sets. AdA was implemented using JAX (Bradbury et al., 2018) and the DeepMind JAX
Ecosystem (Babuschkin et al., 2020) and trained on 64 Google TPUv3 devices. The wall-clock time
for training this version of AdA from scratch was approximately 5 weeks: 1 week to train the teacher,
and 4 weeks to train AdA. Even after this amount of training, AdA had not reached convergence,
illustrating the beneÔ¨Åts of open-ended learning methods.
Table D.1 | Distillation teacher for the single-agent experiments in Section 3.1.
Model Parameters
Memory
Task pool
Curriculum
Teacher
Steps
23M TXL / 76M total
1800
25B
PLR D.5
None
25B
D.3. Multi-agent training
Starting from the 25B-sized task pool used for single-agent training, we generate a two-player task
pool of the same size by the procedure described in Appendix A.2. For all our multi-agent experiments,
we use a half-half mixture of single-player and two-player tasks, as described in Section 2.1. For each
task we decide whether to spawn some of the initial objects permanently frozen (see Appendix A.1)
40

Human-Timescale Adaptation in an Open-Ended Task Space
with 50% probability. For tasks with frozen objects, we iterate over the initially spawned object types
and freeze all spawned instances of this type with a probability of 20%, while ensuring that the task
remains solvable.
During training we uniformly sample a co-player policy from a pool generated using Ô¨Åctitious
self-play. The co-player pool is initialised with a random-action policy. Every 500M training frames we
add a snapshot of the learning agent to the pool, thereby adding more and more capable co-players
over time. Finally we apply the PLR auto-curriculum method (see Section 2.3) to curate the tasks
(worlds, games and co-players) using the agent‚Äôs TD-error based Ô¨Åtness. The experimental setup for
the multi-agent distillation teacher is summarised in Table D.2.
Table D.2 | Distillation teacher for the multi-agent experiments in Section 3.1.
Model Parameters
Memory
Task pool
Curriculum
Teacher
Steps
23M TXL / 76M total
1800
see Sec D.3
PLR D.5
None
22B
D.4. Architecture experiments
Table D.3 shows the experimental setup for the experiments comparing diÔ¨Äerent memory architectures
in Section 3.2.
Table D.3 | Experimental setup for comparing diÔ¨Äerent memory architectures.
Architecture
Parameters
Memory
Task pool
Curriculum
Teacher
Steps
Transformer-XL
76M total
1800
25B
No-op
None
50B
GRU with Attention
-
GRU
-
D.5. Auto-curriculum learning
No-op Ô¨Åltering details.
Here we provide additional details of No-op Ô¨Åltering. For each task from
the XLand training pool, we evaluate the learning agent (without sending any experience to the
learner) and no-op policy on the task for 10 independent episodes, each of length 1 trial, producing
scores {ùëÖ0, . . . , ùëÖ9}, {ùëÖ‚Ä≤
0, . . . , ùëÖ‚Ä≤
9}, respectively. We admit the proposal task for training if it satisÔ¨Åes
the following criteria:
1. max ùëÖ‚Ä≤
ùëñ‚â§ùúñ1 (No-op is not too good.)
2. |{ùëñ: ùëÖùëñ‚â•ùúñ2}| ‚â§ùúñ3 (Agent is not too good.)
3. |{ùëñ: ùëÖùëñ‚â•max ùëÖ‚Ä≤
ùëñ+ ùúñ0}| ‚â•ùúñ4 or |{ùëñ: ùëÖùëñ‚â§min ùëÖ‚Ä≤
ùëñ‚àíùúñ0}| ‚â•ùúñ5 (Agent is suÔ¨Éciently diÔ¨Äerent from
no-op.)
4. max ùëÖùëñ‚àímin ùëÖùëñ‚â•ùúñ6 (Agent scores have suÔ¨Écient variance.)
The ùúñùëñ‚Äôs are thresholds and become hyperparameters in our training setup. Since diÔ¨Äerent tasks have
diÔ¨Äerent durations in general, we use relative thresholds deÔ¨Åned as a fraction of trial duration for ùúñ0,
ùúñ1, ùúñ2, ùúñ6 and absolute thresholds for the rest. Once a task is admitted for training, it is run using the
full number of trials speciÔ¨Åed by the task and for 30 episodes. All experience from these runs are sent
to the learner. See Table D.4 for the hyperparameters used.
41

Human-Timescale Adaptation in an Open-Ended Task Space
Table D.4 | No-op Ô¨Åltering hyperparameters.
Parameter
Value
Relative to trial duration
ùúñ0
0.01
Y
ùúñ1
1.1
Y
ùúñ2
0.4
Y
ùúñ3
5
N
ùúñ4
1
N
ùúñ5
3
N
ùúñ6
0.01
Y
PLR details.
Here we provide additional details for Prioritised Level Replay (PLR), used in training
AdA. PLR uses a Ô¨Åtness score that approximates the agent regret for a given task (Jiang et al., 2021a,b).
PLR maintains an archive P of tasks to replay with Ô¨Åxed maximum size. With probability ùëù(referred
to as the replay probability) a task is sampled from P while taking into account the Ô¨Åtness score and
staleness of each task (see Jiang et al. (2021b), Section 3) to train the learning agent. The staleness
represents how much time has passed since the task was last sampled, and ensures that all tasks in P
have accurate scores. The Ô¨Ånal probabilities are computed by combining the Ô¨Åtness and staleness
scores, with staleness weighted using the parameter ùë†‚àà[0, 1].
Tasks are added to P by Ô¨Årst sampling with probability 1 ‚àíùëùa proposal task from the training task
set and evaluating its Ô¨Åtness score. If the Ô¨Åtness score is greater than the minimum Ô¨Åtness score of all
tasks in P, the proposal task is added to P. If the new size of P exceeds the Ô¨Åxed maximum size, the
lowest Ô¨Åtness task is dropped. Note that in PLR a task can potentially be trained on indeÔ¨Ånitely, if it
never leaves P.
We found that using last-trial Ô¨Åtness led to better empirical performance and sample eÔ¨Éciency
than Ô¨Årst or average trial Ô¨Åtness. This is likely because in earlier trials, error-based Ô¨Åtness is higher
as the agent is pursuing exploratory behavior, which should not be taken as a sign that the policy is
sub-optimal. However, high error-based Ô¨Åtness in the last trial likely indicates a sub-optimal policy
when solving the task after time for adaptation, analogous to the regret approximation in the original
single-trial PLR.
In order to use the last-trial Ô¨Åtness as our objective we need to make a number of changes to the
original PLR framework, which was designed for single trials in a more homogeneous domain. We
denote the per-step Ô¨Åtness score at the ùëñth step of trial ùëòby ùëìùëñ,ùëò. First, to avoid adding a bias towards
longer trial durations we use the average per-trial Ô¨Åtness score Àúùëìùëò‚âú√ç
ùëñÀúùëìùëñ,ùëò/ùëÅùëòwhere ùëÅùëòis the number
of steps per trial. Next, to ensure we do not prioritise lower values of ùëò, which tend to have a higher
average last-trial Ô¨Åtness score, we then normalise Àúùëìùëòby ùëìùëò‚âú( Àúùëìùëò‚àíùúáùëò)/ùúéùëòwhere ùúáùëòand ùúéùëòare rolling
per-trial means and variances for each trial index, calculated from all evaluated tasks. Finally, we can
deÔ¨Åne the Ô¨Åtness for PLR to be ùëìùëò, the normalised last-trial Ô¨Åtness score.
As described in Jiang et al. (2021a,b), PLR contains the following hyper-parameters: replay
probability ùëù, maximum replay size ùëÅmax, minimum replay size ùëÅmin (we set ùëù= 0 if |P| < ùëÅmin),
ùëÅtrain the total number of trials for which to run a training task before re-sampling, and ùë†the staleness
coeÔ¨Écient. See Table D.5 for the hyper-parameters used. We conducted a grid search over the replay
probability ùëù‚àà{0.1, 0.2, 0.5}, size of the replay pool ùëÅmax ‚àà{1000, 10000, 50000} and staleness
coeÔ¨Écient ùë†‚àà{0.1, 0.2}, and in all cases set ùëÅmin to be 90% of ùëÅmax.
42

Human-Timescale Adaptation in an Open-Ended Task Space
Table D.5 | PLR hyperparameters.
Parameter
Value
ùëù
0.2
ùëÅmax
1000
ùëÅmin
900
ùëÅtrain
30
ùë†
0.2
PLR Ô¨Åtness metric.
It remains for us to deÔ¨Åne the per-step Ô¨Åtness score ùëìùëñ,ùëò. For this, we use the
simplest regret-like metric, the 1-step TD-error (Jiang et al., 2021b; Schaul et al., 2015). Concretely,
we estimate the TD-error Ô¨Åtness based on the immediate value-predictions of the model: |ùëüùë°+ùõæÀÜùë£ùë°+1
0
‚àíÀÜùë£ùë°
0|.
In some settings this may be undesirable, for example, TD-errors typically increase as the agent
achieves higher rewards. Therefore, we also propose to compute Ô¨Åtness metrics based on the Muesli
dynamics model. Rather than simply using the accuracy of the model prediction, we look at the
impact of the prediction on the value function and action predictions. We deÔ¨Åne the value-model
Ô¨Åtness as |ÀÜùë£ùë°+1
0
‚àíÀÜùë£ùë°
1|, the diÔ¨Äerence between the value estimate at the predicted next state and the true
next state. We also deÔ¨Åne a value-agnostic metric, the action-model Ô¨Åtness as follows: ùêΩùëÜ(ÀÜùúãùë°+1
0 , ÀÜùúãùë°
1), i.e.
the diÔ¨Äerence between the action predictions at the predicted next state and the actual next state,
where diÔ¨Äerence is measured with the Jensen-Shannon divergence (Farquhar et al., 2021; Filos et al.,
2022; Pislar et al., 2022; Xu et al., 2017).
In Figure D.1 we show training curves for TD-error Ô¨Åtness, value-model Ô¨Åtness, and action-model
Ô¨Åtness. Table D.6 shows the experimental setup for these experiments. We see that both TD-error
and action-model Ô¨Åtness metrics outperform the value-model Ô¨Åtness. We chose TD-error for our PLR
training runs because it has better asymptotic performance in both the zero-shot and the few-shot
setting, and because it was shown to perform well in previous work (Jiang et al., 2021a).
0G
5G
10G
15G
20G
25G
Steps
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Median Score
TD-error fitness
Action-model fitness
Value-model fitness
Few-shot
Zero-shot
Figure D.1 | PLR Ô¨Åtness metric comparison for zero-shot generalisation (ùëò= 1) and few-shot adaptation
(ùëò= 13). We compare the TD-error Ô¨Åtness used in our main agents against two approaches using
the Muesli dynamics model. We see that action-model Ô¨Åtness matches TD-error Ô¨Åtness in few-shot
performance, with weaker zero-shot performance.
43

Human-Timescale Adaptation in an Open-Ended Task Space
Table D.6 | Experimental setup for comparing diÔ¨Äerent PLR Ô¨Åtness functions.
Model parameters
Memory
Task pool
Curriculum
Teacher
Steps
Fitness function
23M TXL / 76M total
1800
25B
PLR
None
25B
TD error
Value model
Action model
0G
5G
10G
15G
20G
25G
Steps
0.0
0.2
0.4
0.6
0.8
1.0
Median Score
PLR
No-op
Uniform
(a)
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Estimated TFlops
1e8
0.0
0.2
0.4
0.6
0.8
1.0
Median Score
PLR
No-op
Uniform
(b)
Figure D.2 | (a) Sample eÔ¨Éciency in steps for diÔ¨Äerent choices of curricula. Both No-op and PLR
signiÔ¨Åcantly improves sample eÔ¨Éciency over uniform sampling of tasks. Few-shot denotes ùëò= 13
score and zero-shot denotes ùëò= 1 score. (b) Sample eÔ¨Éciency in FLOPs for diÔ¨Äerent choices of
curricula. No-op has an initial advantage over PLR, but PLR outperforms No-op later in training.
Curriculum eÔ¨Éciency.
Next, we compare training sample eÔ¨Éciency for baseline uniform sampling
and the diÔ¨Äerent curriculum methods, in units of both learner steps and FLOPS. Figure D.2 shows the
median last-trial scores for few-shot (ùëò= 13) and zero-shot evaluation tasks as a function of learner
steps. We see that both No-op Ô¨Åltering and PLR curricula strongly improve training sample eÔ¨Éciency
over uniform sampling, with PLR being more eÔ¨Écient early in the training process. When we plot
the same few-shot median performance as a function of FLOPS, we see that No-op has a slight early
advantage, but PLR outperforms No-op later in training. The initial advantage for No-op may be
because No-op expends more FLOPS (10 evaluations per task vs. 1 in PLR) for task evaluation, which
Ô¨Ånds higher quality training tasks at the start of training.
Emergent curricula.
In Figure D.3 we show task metrics analysing the tasks selected by PLR and
No-op Ô¨Åltering, expanding upon the results shown in Figure 11.
D.6. Distillation teacher for scaling experiments
In all of our scaling experiments (Sections 3.4 and 3.5), we distill the policy from an identical teacher
snapshot to ensure our experiments are comparable. Training details for the teacher are detailed in
Table D.7. This teacher is used to kickstart our agents for their Ô¨Årst 4B frames of training.
D.7. Scaling the network
Table D.8 shows the experimental setup for the model size scaling experiments in Section 3.4. More
details about the number of parameters for the various model sizes can be found in Table D.9.
44

Human-Timescale Adaptation in an Open-Ended Task Space
2.5
3.0
3.5
4.0
Num Rules
2.50
2.75
3.00
3.25
3.50
Num Trials
1.0
1.5
2.0
2.5
Min to Trigger
0.50
0.75
1.00
1.25
1.50
Num Hidden Critical Rules
0.6
0.8
1.0
1.2
Num Deadend Rules
0.4
0.6
0.8
Num Hidden Deadend Rules
8
10
12
14
Num Initial Objects
3.5
4.0
4.5
5.0
5.5
6.0
Num Distractor Objects
0G
10G
20G
30G
40G
Training Steps
0.8
1.0
1.2
1.4
1.6
Num Dangerous Rules
0G
10G
20G
30G
40G
Training Steps
0.5
0.6
0.7
0.8
0.9
Num Hidden Dangerous Rules
0G
10G
20G
30G
40G
Training Steps
0.0
0.1
0.2
0.3
Num Critical Dangerous Rules
0G
10G
20G
30G
40G
Training Steps
0.4
0.6
0.8
1.0
Max Rule Repetitions
No-op Filtering
PLR
Test
Hand-authored
Figure D.3 | Emergent curricula for No-op Ô¨Åltering and PLR. Plots show the full set of task metrics for
the dynamic training set, averaged over all tasks in the set, with standard error shaded. In all plots,
a higher metric value corresponds to greater task diÔ¨Éculty. Horizontal lines show the same metric
values averaged over the test (dashed) and hand-authored (dotted) evaluation task sets.
Table D.7 | Distillation teacher for scaling experiments.
Model parameters
Memory
Task pool
Curriculum
Teacher
Steps
23M TXL / 76M total
1800
200M
No-op
None
23B
Table D.8 | Experimental setup for model size scaling.
Model parameters
Memory
Task pool
Curriculum
Teacher
Steps
6M TXL / 41M total
1800
25B
No-op
Table D.7
75B
23M TXL / 76M total
42M TXL / 112M total
57M TXL / 141M total
75M TXL / 175M total
169M TXL / 353M total
265M TXL / 533M total
Transformer-XL memory is a cached memory of previous attention layer inputs, concatenated to
the keys and values during each forward pass. Inputs to intermediate layer are activations from the
previous layer, which in themselves contain information about the past. Caching ùëÄactivations this
way theoretically allows for an eÔ¨Äective memory horizon of ùëÄx ùêø, where ùêøis the number of attention
layers in the network. Therefore, to avoid implicitly scaling eÔ¨Äective Transformer-XL memory length,
in our model size scaling experiments, we Ô¨Åx the number of layers in the Transformer, and scale
45

Human-Timescale Adaptation in an Open-Ended Task Space
parameters only by altering the Transformer embedding size (ùëëmodel), with the feed-forward size
Ô¨Åxed at 4ùëëmodel, as is standard in Transformer architectures (Vaswani et al., 2017).
Table D.9 | Transformer hyperparameters for diÔ¨Äerent model sizes.
Model parameters
Embedding size
Blocks
Key size
Value size
Heads
FFW size
6M TXL / 41M total
288
6
48
48
6
1152
23M TXL / 76M total
576
6
48
48
12
2304
42M TXL / 112M total
768
6
32
32
24
3072
57M TXL / 141M total
896
6
32
32
28
3584
75M TXL / 175M total
1024
6
32
32
32
4096
169M TXL / 353M total
1536
6
48
48
32
6144
265M TXL / 533M total
1920
6
48
48
40
7680
D.8. Scaling the memory length
Table D.10 shows the details of the experimental setup for the memory length scaling experiments in
Section 3.4. We show the eÔ¨Äective memory timesteps for each experiment, computed as the number
of cached network activations times the number of transformer blocks (6).
Table D.10 | Experimental setup for scaling the memory length.
Model Parameters
Memory
Training task pool
Curriculum
Teacher
Training steps
23M TXL / 76M total
600
200M
No-op
Table D.7
25B
1800
3000
4200
D.9. Scaling the size of the task pool
Table D.11 shows the details of the experimental setup for scaling the size of the task pool in
Section 3.5.
Table D.11 | Experimental setup for scaling the task pool size.
Model parameters
Memory
Training task pool
Curriculum
Teacher
Training steps
23M TXL / 76M total
1800
200M
No-op
Table D.7
25B
25B
75M TXL / 175M total
200M
25B
D.10. Scaling the complexity of the task pool
Table D.12 shows the details of the experimental setup for scaling the complexity of the task pool in
Appendix E.3. In this experiment, the distillation teachers are diÔ¨Äerent for the two agents we compare.
Therefore we cannot disentangle the eÔ¨Äects of distillation and task complexity. Nevertheless, the
results remain indicative of the importance of task complexity. The teacher for the task distribution
46

Human-Timescale Adaptation in an Open-Ended Task Space
across multiple world topologies is trained as in Table D.7. The teacher for the task distribution in a
single room comes from a long lineage (> 6 generations) of distillation teachers starting with agents
trained on XLand 1.0 (OEL Team et al., 2021).
Table D.12 | Experimental setup for scaling the complexity of the task distribution.
Model parameters
Memory
Task pool
Curriculum
Steps
6M TXL / 41M total
1800
4k worlds √ó 50k games
No-op
23B
23M TXL / 76M total
42M TXL / 112M total
57M TXL / 141M total
75M TXL / 175M total
6M TXL / 41M total
1800
1 world √ó 5k inits √ó 50k games
No-op
23B
23M TXL / 76M total
42M TXL / 112M total
57M TXL / 141M total
D.11. Distillation enables scaling agents
Table D.13 shows the experimental setup for the distillation experiments in Section 3.6.
Table D.13 | Experimental setup for distillation experiments.
Model Parameters
Memory
Training task pool
Curriculum
Teacher
Steps
TXL 23M TXL / 76M total
1800
See Sec D.3
PLR (D.5)
Table D.2
22B
None
TXL 265M TXL / 533M total
Table D.2
None
D.12. Training on more trials with skip memory
Table D.14 shows the details of the experimental setup for the memory scaling experiments in Section
3.7. This is the same setup as in Table D.14, except for the number of training steps and the variation
of memory architecture and training trials discussed in the main text.
Table D.14 | Experimental setup for experiments training on more trials with skip memory.
Model Parameters
Memory
Training task pool
Curriculum
Teacher
Steps
23M TXL / 76M total
1800
200M
No-op
Table D.7
50B
1800 √ó 4 = 7200
47

Human-Timescale Adaptation in an Open-Ended Task Space
E. Additional Experiments
E.1. Multi-agent adaptation
Figure E.1 | We report the distribution of normalised task scores over the multi-agent task test set when
evaluated with various numbers of trials. All tasks are evaluated in cooperative self-play. On the ùë¶-axis
is the total last-trial reward relative to that of an agent Ô¨Åne-tuned on the test tasks (approximating
‚ÄúinÔ¨Ånite trials‚Äù performance). Curves moving further towards the top right corner indicate better
performance. When given more trials, the agent achieves higher scores in the last trial, showing
test-time adaptation across most of the task distribution (shaded regions).
2
1
3
REWARD
1
1 Trial
8 Trials
Figure E.2 | Average performance and representative behaviour of AdA on the probe task
Irreversible Production for Two when evaluated in self-play with various numbers of trials.
AdA‚Äôs performance increases when given more trials, showing test-time adaptation. The top-down
view images show representative last-trial trajectories when given diÔ¨Äerent numbers of total trials. A
corresponding video for the case ùëò= 8 shows the behaviour across all trials within one episode.
Figure E.1 demonstrates adaptation across a wide range of percentiles on a test-task set of multi-
agent tasks. Figure E.2 demonstrates last-trial performance of AdA in one particular probe task. To
generate these plots, AdA was trained as described in Section 3.1 (Multi-agent).
48

Human-Timescale Adaptation in an Open-Ended Task Space
Figure E.3 | A comparison of the Ô¨Årst-trial score in episodes with 1 trial and episodes with 8 trials.
The lines are almost perfectly overlapping, which indicates that our agent does not leverage number-
of-trials conditioning information to adjust its policy to a more exploratory one in early trials when
more trials are available.
E.2. Conditioning on number of shots doesn‚Äôt aÔ¨Äect agents‚Äô performance
Figure E.3 shows the score obtained by AdA for each percentile, in trial 1 of episodes with only 1
trial (ùëò= 1) and in trial 1 of episodes with 8 trials (ùëò= 8) in our held-out test set. The overlap of
these lines indicates that AdA does not use the trial conditioning information it observes to adjust
its behaviour in any way that aÔ¨Äects its score. If the agent were to follow a more exploratory policy
when it has more trials, we might expect the scores of trial 1 with ùëò= 8 to be lower than the score of
trial 1 with ùëò= 1.
This may be the optimal policy for our XLand 2.0 tasks, or it may reveal a limitation of our
training procedure. One can imagine a scenario in which, knowing that there are 8 trials in total, a
Bayes-optimal policy chooses to display a less rewarding and more exploratory behaviour in trial 1,
compared to how it would behave if told that there was only a single trial in which to collect reward.
For instance, an agent may be able to guarantee a deterministic reward later, having discovered some
key information, at the cost of foregoing an stochastic reward early on. We did not directly incentivise
this behaviour in our training process. In fact, we may have discouraged it, since AdA learns from all
rewards in an episode (not just in the last trial), and with a discount factor smaller than 1, which
could lead to myopic behaviour.
E.3. Scaling complexity of the task pool
One Ô¨Ånal axis along which it is possible to scale our method is the overall complexity of the task
distribution. We compare our main task distribution, as described in Section 2.1 and Appendix A.2 to
a subset which maintains the use of the same goals, production rules, and objects, but eliminates any
navigational complexity by having a single world topology: an empty room. Recall that we count the
number of tasks as the product of number of worlds and the number of games. To disentangle the
eÔ¨Äects of scaling complexity versus scaling the sheer number of tasks, we add 5,000 unique object
initialisation points for the empty room. These serve as the proxy ‚Äú4,000 worlds‚Äù and are, by design,
much less diverse and complex than the 4,000 worlds in the main training pool.7 For more details of
the experimental setup, see Appendix D.10 and Table D.12.
7Note that the distribution over world topologies we use here is smaller than the distribution used in the model scaling
experiments in Section 3.4, and results are therefore not comparable across these sections.
49

Human-Timescale Adaptation in an Open-Ended Task Space
(a) Task distribution with only empty room inhibits
scaling (median).
(b) Task distribution with only empty room inhibits
scaling (20th percentile).
(c) Task distribution with many world topologies
facilitates scaling (median).
(d) Task distribution with many world topologies
facilitates scaling (20th percentile).
Figure E.4 | The beneÔ¨Åt of scaling model size is bottlenecked if the distribution is not complex enough,
even if the total number of tasks is accounted for.
In Figure E.4, we show that low environment complexity can be a bottleneck to scaling, by
comparing the eÔ¨Äectiveness of model scaling between agents trained on the two distributions, and
each evaluated on their respective test sets. On both the median (Figure E.4a) and 20th (Figure
E.4b) percentiles in the empty room, we see that past a certain point (42M Transformer parameters),
scaling model size begins to reduce performance. By contrast, in the distribution with many world
topologies (Figures E.4c and E.4d), increased model size continues to improve performance far beyond
this, showing improvements through at least 75M Transformer parameters. Open-ended settings
with unbounded environment complexity, such as multi-agent systems, may therefore be particularly
important for scaling up adaptive agents.
E.4. Computational cost
In the scaling experiments (Sections 3.4 and 3.5), we compare agents after they have been trained for
an equivalent number of steps. While this controls for sample eÔ¨Éciency of models, here we provide
an analysis of the computational cost in FLOPs for a given experiment, and reproduce some of our
scaling results, controlling for for compute cost. We see that bigger is not always better from this
perspective. For each model size and memory length we use JAX (Bradbury et al., 2018) cost analysis
to estimate the number of FLOPs per frame of the learner step and actor step (Table E.1).
50

Human-Timescale Adaptation in an Open-Ended Task Space
(a)
(b)
Figure E.5 | Scaling Transformer model size controlling for the total number of FLOPs for the learner
and actors, including auto-curriculum evaluation actors.
(a)
(b)
Figure E.6 | Scaling Transformer-XL memory controlling for the total number of FLOPs for the learner
and actors, including auto-curriculum evaluation actors.
Table E.1 | FLOPs per frame for diÔ¨Äerent model sizes and memory lengths.
Model parameters
Memory
Learner FLOPs per frame
Actor FLOPs per frame
6M TXL / 41M total
1800
1,138,005,632
736,987,392
23M TXL / 76M total
1,380,255,078
2,580,445,440
42M TXL / 112M total
1,623,461,663
4,489,239,552
57M TXL / 141M total
1,821,422,230
6,063,742,976
75M TXL / 175M total
2,048,253,663
7,879,863,808
169M TXL / 353M total
3,243,189,525
17,560,326,144
265M TXL / 533M total
4,443,008,234
27,358,181,376
23M TXL / 76M total
600
1,278,491,404
963,739,136
3000
1,481,009,258
4,181,042,688
4200
1,582,270,213
5,789,702,144
We multiply the values in Table E.1 by the number of learner/actor steps for each experiment,
then, for a given comparison, we take the largest such value common to all experiments (usually
51

Human-Timescale Adaptation in an Open-Ended Task Space
associated with the smallest model) as the total FLOPs, and make the comparison of each model at
this number of FLOPs. The results for the FLOPs-matched model scaling experiments are shown in
Figure E.5. We see a reduction in performance as the model size grows beyond a ‚Äúsweet spot‚Äù around
57M Transformer parameters (141M total parameters). Results for the FLOPs-matched memory
scaling experiments in Figure E.6 show that there is still beneÔ¨Åt to increasing context lengths given
a Ô¨Åxed computational budget. Details of the compute used for these experiments can be found in
Tables E.2 (model size) and E.3 (memory).
We note that Table E.1 indicates poor scaling of actor step FLOPs with model size, and suspect
this could be due to poor optimisation of a single-step query of the Transformer on TPU, compared
to the operation batching achieved with a rollout length of 80 on the learner. In order to account
for this and for potential discrepancies in number of actor steps due to diÔ¨Äerences in curriculum
evaluation based on model quality, we also provide plots which only account for the learner step
FLOPs for each model: Figures E.7 (model size), and E.8 (memory length). These might be more
informative, and show that performance still increases as a function of model size and memory length,
albeit not as steeply as when controlling for sample eÔ¨Éciency directly. Details of the compute used
for these experiments is shown in Tables E.4 (model size) and E.5 (memory length).
(a)
(b)
Figure E.7 | Scaling Transformer-XL model size controlling for the number of learner FLOPs.
(a)
(b)
Figure E.8 | Scaling Transformer-XL memory length controlling for the number of learner FLOPs.
52

Human-Timescale Adaptation in an Open-Ended Task Space
Table E.2 | Corresponding learner steps given total FLOPs for each model size.
Model parameters
Memory
Total FLOPs
Learner steps
6M TXL / 41M total
1800
2.0 √ó 1020
97B
23M TXL / 76M total
44B
42M TXL / 112M total
27B
57M TXL / 141M total
23B
75M TXL / 175M total
18B
169M TXL / 353M total
9B
265M TXL / 533M total
5B
Table E.3 | Corresponding learner steps given total FLOPs for each memory length.
Model parameters
Memory
Total FLOPs
Learner steps
23M TXL / 76M total
600
7.9 √ó 1019
31B
1800
17B
3000
11B
4200
8B
Table E.4 | Corresponding learner steps given learner-step-only FLOPs for each model size.
Model parameters
Memory
Learner step FLOPs
Learner steps
6M TXL / 41M total
1800
1.0 √ó 1020
92B
23M TXL / 76M total
76B
42M TXL / 112M total
65B
57M TXL / 141M total
58B
75M TXL / 175M total
51B
169M TXL / 353M total
32B
265M TXL / 533M total
23B
Table E.5 | Corresponding learner steps given learner-step-only FLOPs for each memory length.
Model parameters
Memory
Learner step FLOPs
Learner steps
23M TXL / 76M total
600
3.9 √ó 1019
39B
1800
36B
3000
29B
4200
25B
E.5. Repeated distillation
In Section 3.6, we show that distilling an agent into an identical student can lead to large increases
in the agent‚Äôs performance. Here, we investigate the potential beneÔ¨Åts of applying the procedure
repeatedly. To this end, we continue the experiment shown in Figure 16 and add a third generation,
using a snapshot taken from the previous student after 25 billion frames, equivalent to 50 billion
frames of total experience when taking the teacher‚Äôs experience into account. Figure E.9 shows
that applying this procedure repeatedly can indeed lead to additional beneÔ¨Åts; however, we observe
diminishing returns with successive generations.
53

Human-Timescale Adaptation in an Open-Ended Task Space
0G
20G
40G
60G
80G 100G 120G 140G 160G 180G 200G
Training Steps
0.0
0.2
0.4
0.6
0.8
1.0
Median Score
Gen 1 Gen 2
Gen 3
(a)
0G
20G
40G
60G
80G 100G 120G 140G 160G 180G 200G
Training Steps
0.0
0.2
0.4
0.6
0.8
1.0
20th Percentile Score
Gen 1 Gen 2
Gen 3
(b)
Figure E.9 | Normalised few-shot score over three generations using the 23M parameter Transformer-
XL. The Ô¨Årst and second generations correspond to the agents shown in Figure 16. The third
generation is distilled from the second after it has been trained for 25 billion steps. The ùë•-axis counts
the combined amount of experience, starting from the experience collected by the original teacher.
The third generation shows additional gains over the second, but to a lesser degree than the gap
between the Ô¨Årst and second generations.
F. Human-Timescale Adaptation
In this section we provide more details regarding our claim of human-timescale adaptation.
F.1. Probe tasks
Tables F.1 and F.2 describe in detail the single-agent and multi-agent probe tasks respectively. Unless
noted otherwise, all probe tasks are set in complex worlds with many objects in them, and use multiple
production rules, which are fully hidden from the players.
Table F.1 | Single-agent probe tasks
Name
Description
Wrong pair
disappears
The player‚Äôs goal is to hold a black cube, which does not exist among the
initial objects. There are two (hidden) rules. The player needs to identify
the correct world state which triggers the rule that creates the cube and
not the one which destroys the necessary inputs. All this is embedded in
a challenging world layout with one-way drops and limited visibility.
Wrong pair
disappears, partial
hiding
‚ÄòWrong pair disappears‚Äô, but instead of hiding all rules completely we only
hide the input objects for both rules (outputs and conditions are fully
visible).
Irreversible
production
Similar to ‚ÄòWrong pair disappears‚Äô, but this task has multiple dead ends
(rules which create unsolvable states).
Irreversible
production, all rules
visible
‚ÄòIrreversible production‚Äô, but with all rules fully visible to the player.
54

Human-Timescale Adaptation in an Open-Ended Task Space
Push, don‚Äôt lift
The vast majority of training and evaluation tasks require lifting objects.
Here two hidden rules destroy any object when lifted. In order to create
the goal state, some ‚Äúlateral thinking‚Äù is necessary: the player needs to
identify that pushing the cubes with their body is possible.
Push, don‚Äôt lift, with
distractors
Similar to ‚ÄòPush, don‚Äôt lift‚Äô, but here a large number of distractor objects
in the world make this a much more challenging exploration task for any
player ignoring the objects mentioned in the goal.
Spacer tool
Two objects need to be brought close together, but lifting them or touching
them (with the avatar‚Äôs body) destroy them. The solution is to use another
object in the world as a tool to push them together.
Transform to
transport
Again, two objects need to be brought close to each other, but lifting and
touching them destroys them. The solution here is to exploit a set of rules
that can turn one of the objects into something that can be carried safely,
and then turn it back into the original object once it is in the right place.
Small workstation
A very hard object handling task. 8 objects near the player‚Äôs spawn
position need to be combined in diÔ¨Äerent ways and 5 rules need to be
triggered (some multiple times) to create the goal object. This task is set
on top of a tall plateau and it is very easy to fail by touching an object
too hard and throwing it oÔ¨Äthe edge of the plateau.
Small workstation, all
rules visible
The same as ‚ÄòSmall workstation‚Äô, but all rules are visible to the player.
Crafting pyramid
Eight objects need to be recursively combined Ô¨Årst into four, then two
and then ultimately one Ô¨Ånal object. This requires triggering a chain of 7
rules. This seems easy for humans. But the lack of intermediate reward
makes this a hard hierarchical credit assignment task for agents.
Crafting tree, all rules
visible
Similar to the crafting trees in video games like Minecraft, this multi-
step task requires triggering diÔ¨Äerent rules in a chain to create the goal
object. All objects exist in the world multiple times, making many diÔ¨Äerent
solutions viable.
Crafting tree, hidden
shortcut
Identical to ‚ÄòCrafting tree, all rules visible‚Äô, but one additional (hidden)
rule exists. This allows the player to take a shortcut that lets them Ô¨Ånish
the task faster than executing only the visible rules.
Antimatter
In a world full of yellow and black spheres, the goal is for no pair of black
and yellow spheres to ‚Äúsee each other‚Äù (no direct line of sight). Beyond
moving the objects and blocking line of sight with the avatar there exists
a production rule which destroys any yellow sphere and black sphere pair
which touch (similar in spirit to matter and antimatter particles). This is
all embedded in a world requiring advanced navigation skills.
Antimatter with
creation
Similar to ‚ÄòAntimatter‚Äô, only here a third object (purple pyramids) exists
which duplicates any sphere touching it. In addition, this task is set on
two plateaus, making it easier to break line of sight and reducing the
navigation challenge.
55

Human-Timescale Adaptation in an Open-Ended Task Space
Pyramid in a haystack
To create the necessary yellow pyramid, the player needs to Ô¨Ånd and hold
the purple cube. There are several distractor objects and distractor rules
in this world, requiring the player to deal not just with a hard exploration
challenge but also a very noisy environment.
Protect the egg
The player is tasked to hold the single yellow sphere in the world. A large
number of other spheres exist in the world. These destroy the yellow
sphere on collision. As these touch each other or get near the player they
get duplicated. This can lead to a constantly growing number of objects,
Ô¨Ålling up the world.
3 spheres jumbled
3 spheres exist in the world. Holding one of them creates the goal object.
Only one sphere can be reached within the 10-second time limit, meaning
that the optimal policy on the Ô¨Årst trial is to choose uniformly at random.
Two doors
The goal object is hidden behind one of the two large objects (the ‚Äúdoors‚Äù)
positioned at opposite ends of the world. Only one of them can be reached
in time, so the player needs to decide between exploring one of them per
trial.
Same signature:
match colours
All ‚ÄòSame signature‚Äô tasks have the exact same world layout, goal and
number of fully-hidden rules. This means they look exactly the same to a
player starting out. This one only requires two objects of matching colour
to be brought close together to create the goal object, using only one
production rule out of three.
Same signature: three
steps
This ‚ÄòSame signature‚Äô task variant requires the player to trigger all three
(hidden) production rules to create the goal object.
Same signature: two
dead ends
In this ‚ÄòSame signature‚Äô task variant, two of the tree rules are dead ends,
leading to an unsolvable world state. Only one rule is helpful (and in fact
required) to solve the task.
Same signature:
destroy to protect
To solve this ‚ÄòSame signature‚Äô task variant the player Ô¨Årst needs to destroy
a black sphere (by getting near it) before creating the goal object (a yellow
cube). Otherwise when the black sphere ‚Äúsees‚Äù the yellow cube, both
get destroyed. This is a very hard credit assignment challenge even for
human players (it is hard to notice what is going on).
Don‚Äôt act
This task is pre-solved: the player is getting reward from the very begin-
ning. If they lift or touch one of the two objects in the world, the object
gets destroyed and reward is now impossible.
Don‚Äôt peek
This task is ‚Äòpre-solved‚Äô: the player is getting reward from the very be-
ginning. However, the player will destroy any object they look at, at
which point reward is impossible. So the optimal strategy is to not look
at anything but the sky.
Navigation: Ô¨Ånd the
cube
This memory task is not using production rules. It uses a large world with
the goal object (a cube) hidden after a very winding path.
56

Human-Timescale Adaptation in an Open-Ended Task Space
Navigation: Ô¨Ånd the
cube with teaser
This memory task is not using production rules. It is set in a large world
with the goal object (a cube) hidden after a very winding path. The object
is visible from the spawn point but out of sight after starting to traverse
the terrain.
Navigation: hold up
high
This memory task is not using production rules. The goal object (a pyra-
mid) is hidden on top of a plateau and can only be seen when nearly
there.
Object permanence:
yellow cube
This memory task is not using production rules. The goal object (a yellow
cube) is visible from the spawn point. After moving for a bit, a decision
between two paths has to be made, with the correct path being to the
right. At this point the cube is no longer visible.
Object permanence:
black cube
This memory task is not using production rules. This has same world
layout as the task above, only here the player is asked to Ô¨Ånd the black
cube, which requires going to the left.
Table F.2 | Multi-agent probe tasks
Name
Description
Pass over the wall
The players are separated by an opaque wall. They cannot see each other,
only their half of the world. The solution requires ‚Äúpassing‚Äù the accessible
objects on either player‚Äôs side to other player to combine them into the
goal object. Then the players must make sure the correct player holds
this object.
Pass over the wall
repeatedly
Similar to ‚ÄòPass over the wall‚Äô but here 2 out of 3 initial objects are ‚Äòfrozen‚Äô
(cannot be moved). This prescribes a very speciÔ¨Åc solution strategy that
requires the players to pass 3 objects over the wall in a speciÔ¨Åc order.
Coordinated
production
This task requires each player to be near a sphere to turn this sphere
into a pyramid, and then for both pyramids to be touching each other
to create the goal object. The spheres are slightly hidden in a complex
world, requiring some exploration.
Coordinated
production with
deadends
Like ‚ÄòCoordinated production‚Äô but here each player will destroy one of
the initial objects if they get near it. These dead-end rules make this a
much harder exploration problem.
Coordinated exchange
This requires each player to create a new object by holding an existing
object, then to hold the object created by the other player to turn it into
another intermediate object and Ô¨Ånally for both objects to be combined
into the goal object. While the world is fully accessible to both players,
this can only be solved if both players actively participate.
Overcooked:
coordination ring
Inspired by the video game Overcooked (Carroll et al., 2019; Strouse
et al., 2021), in this task both players need to ‚Äúserve tomato soup to a
hungry patron‚Äù. This is implemented as a repeatable four-step production
rule chain which requires both players to traverse their shared space (a
circular layout) carefully in order to not block the other player.
57

Human-Timescale Adaptation in an Open-Ended Task Space
Overcooked:
coordination ring, all
rules visible
While in ‚ÄòOvercooked: Coordination ring‚Äô all production rules are hidden
from both players, here they are fully visible (to match the dynamics of
the original Overcooked game).
Overcooked: cramped
room
Similar to ‚ÄòOvercooked: coordination ring‚Äô but with a diÔ¨Äerent layout for
the shared space and a diÔ¨Äerent number of initial objects, using diÔ¨Äerent
shapes and colours.
Overcooked: cramped
room, all rules visible
While in ‚ÄòOvercooked: cramped room‚Äô all production rules are hidden
from both players, here they are fully visible (to match the dynamics of
the original Overcooked game).
Overcooked: forced
coordination
Similar to the other ‚ÄòOvercooked‚Äô task variants, but here both players
are restricted to only a certain part of the world and so are forced to
coordinate to solve this task. No player can solve this alone since they
cannot reach all initial objects.
Overcooked: forced
coordination, all rules
visible
While in ‚ÄòOvercooked: forced coordination‚Äô all production rules are hidden
from both players, here they are fully visible (to match the dynamics of
the original Overcooked game).
Kickball
This task is set in large world with two frozen pyramids on opposite
sides of the world. Both players want to bring all of the plentiful purple
spheres to the yellow pyramid. But lifting them destroys the pyramids so
they need to ‚Äúkick‚Äù them by bouncing them oÔ¨Äthe avatar. Think ‚Äúsoccer
practice‚Äù.
Lemon eater
The Ô¨Årst player destroys all yellow spheres (of which there are many)
when bumping into them. This is also the goal for both players. So they
need to cooperate to bring all yellow spheres to the Ô¨Årst player as quickly
as possible.
Careful lemon eater
Like ‚ÄòLemon eater‚Äô but any collision between two spheres turns them from
yellow to purple. Purple spheres are ‚Äúnot edible‚Äù and so the players need
to be careful to not create those, otherwise they will lose out on reward.
Lemon eater and
maker
Like ‚ÄòLemon eater‚Äô but we start out with only purple spheres. Only the sec-
ond player can turn purple into yellow spheres by lifting them, eÔ¨Äectively
having to ‚Äúcreate food‚Äù for the Ô¨Årst player.
Antimatter for two
Identical in nature to the single-player ‚ÄòAntimatter‚Äô task but set in a diÔ¨Äer-
ent world layout and with two players who share the same goal.
Antimatter with
creation for two
Identical in nature to the single-player ‚ÄòAntimatter with creation‚Äô task but
set in a diÔ¨Äerent world layout and with two players who share the same
goal.
Antimatter with
copies for two
Identical to ‚ÄòAntimatter for two‚Äô but here any two spheres of the same
colour colliding leads to the creation of another sphere of that colour.
This can set in motion runaway growth in the number of spheres, making
it very hard to solve the task.
Irreversible
production for two
Identical in nature to the single-player ‚ÄòIrreversible production‚Äô task but
set in a diÔ¨Äerent world layout and with two players who share the same
goal.
58

Human-Timescale Adaptation in an Open-Ended Task Space
Irreversible
production for two, all
rules visible
Like ‚ÄòIrreversible production for two‚Äô but with all production rules visible
to both players.
Wrong pair
disappears for two
Identical in nature to the single-player ‚ÄòWrong pair disappears‚Äô task but
set in a diÔ¨Äerent world layout and with two players who share the same
goal.
Wrong pair
disappears for two,
partial hiding
Like ‚ÄòWrong pair disappears for two‚Äô but with only the input objects of the
productions rules being hidden, the outputs and condition being visible.
Crafting pyramid for
two
Identical in nature to the single-player ‚ÄòCrafting pyramid‚Äô task but set in a
diÔ¨Äerent world layout and with two players who share the same goal.
Information
asymmetry
A simple world in which the players are asked to execute a two-step
production rule in the presence of multiple dead ends. While the Ô¨Årst
player knows all the rules, they are completely hidden from the second
player. This task is intended to measure a speciÔ¨Åc Ô¨Çavor of third-person
imitation.
Information
asymmetry with
repetition
While ‚ÄòInformation asymmetry‚Äô only allows for up to four completions,
this task variant is (given unlimited time) inÔ¨Ånitely repeatable, providing
more opportunities for imitation.
Combine outputs
Similar in nature to ‚ÄòCoordinated production‚Äô but set in a larger world and
(through the use of frozen objects) requiring both players to repeatedly
navigate quite far to create input objects for shared creation.
Two machines
Many objects of diÔ¨Äerent colours and shapes litter this world. The frozen
yellow pyramid on one end of the world transforms these objects into
an intermediary object. The players then need to bring the intermediary
object to the frozen black pyramid on the other end of the world to ‚Äúcash
it in‚Äù for instantaneous reward, at which point the intermediary object is
destroyed. Therefore to get more reward, the players must repeat this
process.
Two machines with
information
asymmetry
Like ‚ÄòTwo machines‚Äô, but all rules are visible to one player, and all rules
are hidden from the other player. This creates an information asymmetry
and thus an opportunity for third-person imitation.
F.2. Comparing human and agent scores on every probe task
In Figure F.1 we show the raw last-trial total reward for humans and our agent as a function of
number of trials across every one of the 30 evaluation probe tasks.
F.3. Quantifying stochasticity
Task variation.
Figure F.2a shows that there is fairly high variance in the score obtained by a
single agent over 50 repetitions of a single task. This is due to random spawn rotations of the avatar
following every environment reset and stochasticity in the agent‚Äôs policy. To address this we run 50
repetitions of every (task, ùëò) combination and average the score over these repetitions. This reduces
the standard deviation to that shown in Figure F.2b. Here, the standard deviation of the mean task
59

Human-Timescale Adaptation in an Open-Ended Task Space
Figure F.1 | Comparison of AdA against 19 human players on each of the 30 held-out single-agent
hand-authored tasks. The performance of a baseline agent trained to optimise zero-shot performance
is shown as a black dashed line and indicates that AdA does not sacriÔ¨Åce its zero-shot generalisation
to achieve adaptation. The reward of an agent Ô¨Åne-tuned on the hand-authored tasks is also shown
as a red dashed line to provide some indication of the maximum reward achievable in a trial of each
task.
(a)
(b)
Figure F.2 | (a) Mean single-repetition score and 95% conÔ¨Ådence intervals over 50 samples. (b) Mean
of the aggregated 50-repetition score and 95% conÔ¨Ådence intervals over 50 samples.
60

Human-Timescale Adaptation in an Open-Ended Task Space
score reduced from a maximum of 0.43 with 1 repetition, to a maximum of 0.06 with 50 repetitions.
Figure F.3 | 95% bootstrap conÔ¨Ådence intervals around the median and 20th percentile over our test
set of 5 agents trained with diÔ¨Äerent initialisation seeds. The maximum observed standard deviation
was 0.04 around the median and 0.02 around the 20th percentile.
Agent initialisation variation.
After accounting for task variation, Figure F.3 shows the variance
due to agent initialisation seed during training. We plot the score as a function of ùëòon our test set for
the 76M total parameter version of AdA with 5 diÔ¨Äerent initialisation seeds. The maximum standard
deviation observed for any number of trials in the median was 0.04 and for the 20th percentile was
0.02. This low initialisation seed variance led us to run our ablations with one initialisation seed to
save on compute. We note that the results shown in our ablation section have signiÔ¨Åcantly larger than
one standard deviation diÔ¨Äerences.
F.4. Prompting through Ô¨Årst-person demonstrations
Figure F.4 shows the performance of AdA prompted with a Ô¨Åne-tuned agent compared to an un-
prompted baseline on each of the 30 single agent hand-authored probe tasks. The Ô¨Ågure reveals a
set of tasks on which AdA is able to leverage information in the prompt, resulting in perfect or near
perfect scores. There are also tasks where AdA does not seem to be able to do this. In all but one case,
prompting does not hurt performance.
Analysing the tasks in Figure F.4 suggests that prompting is useful for short navigational tasks
such as Navigation:
hold up high in which the agent follows a short and simple path to reach
the goal object. Prompting does not, however, improve performance for longer and more complex
navigation tasks like Navigation:
find the cube, likely due to the full demonstration being
too long to Ô¨Åt in the agent‚Äôs memory context.
We observe a similar pattern in tasks involving production rules. For tasks with up to 2 production
rules in the solution path, such as Same signature:
match colors, we observe the unprompted
agent exploring diÔ¨Äerent objects to determine the correct rule to trigger. When prompted with a
demonstration it subsequently triggers the correct rule immediately and achieves a perfect score. An
exception to this is Same signature:
destroy to protect where one of the production rules
involves destroying an object, which the agent does not appear to remember from the demonstration.
For tasks using 3 or more production rules like Same signature:
three steps (3 production
rules in the solution path) and Small workstation (5 production rules), the agent tends to only
remember a subset of the rules to trigger and continues engaging in exploratory behaviour following
61

Human-Timescale Adaptation in an Open-Ended Task Space
Figure F.4 | A comparison of a prompted agent and unprompted baseline for our full set of 30
hand-authored single-agent tasks. The dashed red lines indicate the score obtained by the teacher
providing the Ô¨Årst-person demonstration to the prompted agent in the Ô¨Årst trial.
the demonstration. The performance on these tasks tend to match the unprompted baseline.
Another factor appearing to inÔ¨Çuence the eÔ¨Äectiveness of prompting is the topology and con-
Ô¨Åguration of objects in the world, as seen in the Small workstation tasks. While the teacher
demonstrations for these tasks present a clean trajectory, the agent subsequently knocks into and
displaces distractor objects, leading to environment states not observed during the demonstration (and
thus not recallable from memory). On the other hand, a favourable conÔ¨Åguration of objects appears
to make the demonstration easier to learn from, as observed in Irreversible production. Here
the objects required to trigger the last production rule are positioned close together. The agent shows
optimal behaviour here despite the task requiring 3 production rules on the solution path. The agent
also appears unable to infer from prompting certain more subtle requirements like relative positioning
of objects or tool use. This is observed in tasks like Antimatter in which prompted AdA is able to
trigger the destruction rule but we did not observe it immediately hiding objects from each other.
Prompting may also help eliminate biases that the agent may have acquired during training. This
is reÔ¨Çected in the lower score obtained by the unprompted agent for small ùëòin Object permanence:
yellow cube compared to Object permanence:
black cube. These tasks are identical except
for the goal being to navigate to a yellow cube on the right, or a black cube on the left respectively.
This suggests that the agent may have acquired a bias during training to either prefer black cubes
or to navigate towards objects on its left. The signiÔ¨Åcantly higher prompted scores on Object
62

Human-Timescale Adaptation in an Open-Ended Task Space
Figure F.5 | Performance of AdA on 6 hand-authored tasks when prompted with an expert Ô¨Årst-person
human demonstration, compared with an unprompted baseline.
Figure F.6 | Top-down views depicting the behaviour of AdA with and without a human expert prompt
on the task Object permanence:
yellow cube. On its own, AdA appears to have a bias of
navigating to the black cube which is a dead end in this task. When prompted with a human (or
Ô¨Åne-tuned) expert trajectory, AdA is able to overcome this bias and navigate to the yellow cube in the
second trial.
permanence:
yellow cube for small ùëòsuggest that a prompt may help the agent overcome these
biases.
Prompting with human demonstrations.
We prompted AdA with expert human demonstrations
in a small selection of 6 hand-authored tasks, depicted in Figure F.5. These tasks were chosen to
be a mixture of tasks where AdA excelled with Ô¨Åne-tuned teacher prompting, where it failed with
Ô¨Åne-tuned teacher prompting and where even the Ô¨Åne-tuned teacher failed.
63

Human-Timescale Adaptation in an Open-Ended Task Space
The results show the same pattern as those obtained when prompting with a Ô¨Åne-tuned teacher. In
both Navigation:
hold up high and Object permanence:
yellow cube, prompted AdA
achieved close to optimal performance, exceeding both the baseline and the human demonstration.
Figure F.6 depicts the latter behaviour in detail. AdA continues to fail to learn from a demonstration in
Navigation:
find the cube with teaser and in both Spacer tool and Transform to
transport, which our Ô¨Åne-tuned teacher also failed at. A successful human demonstration did not
unlock any capabilities AdA was previously not capable of demonstrating, suggesting that these tasks
are perhaps too far out-of-distribution with respect to AdA‚Äôs training tasks. The fact that prompting
with oÔ¨Ä-policy human demonstrations is partially successful is worthy of note, and opens an exciting
area of future research.
64
