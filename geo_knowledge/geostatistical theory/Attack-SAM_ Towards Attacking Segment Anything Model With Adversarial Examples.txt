ATTACK-SAM: TOWARDS ATTACKING SEGMENT ANYTHING
MODEL WITH ADVERSARIAL EXAMPLES
Chenshuang Zhang
KAIST
Chaoning Zhang∗
Kyung Hee University
Taegoo Kang
Kyung Hee University
Donghun Kim
Kyung Hee University
Sung-Ho Bae
Kyung Hee University
In So Kweon
KAIST
May 9, 2023
ABSTRACT
Segment Anything Model (SAM) has attracted signiﬁcant attention recently, due to its impressive
performance on various downstream tasks in a zero-short manner. Computer vision (CV) area might
follow the natural language processing (NLP) area to embark on a path from task-speciﬁc vision
models toward foundation models. However, deep vision models are widely recognized as vulnerable
to adversarial examples, which fool the model to make wrong predictions with imperceptible pertur-
bation. Such vulnerability to adversarial attacks causes serious concerns when applying deep models
to security-sensitive applications. Therefore, it is critical to know whether the vision foundation
model SAM can also be fooled by adversarial attacks. To the best of our knowledge, our work is the
ﬁrst of its kind to conduct a comprehensive investigation on how to attack SAM with adversarial
examples. With the basic attack goal set to mask removal, we investigate the adversarial robustness
of SAM in the full white-box setting and transfer-based black-box settings. Beyond the basic goal of
mask removal, we further investigate and ﬁnd that it is possible to generate any desired mask by the
adversarial attack.
1
Introduction
Foundation models [3] have made signiﬁcant breakthroughs in the NLP area, with the impressive zero-shot performance
of large language models [4, 27] on various downstream tasks. As a representative large language model, ChatGPT [41]
has revolutionized the perceptions of AI and also falls into the ﬁeld of generative AI [43, 46] for its ability to generate
text. The success of large language models also accelerates the development of multi-modality tasks, such as text-to-
image [44] and text-to-speech [45]. By contrast, the progress of foundation models for computer vision has somewhat
lagged behind. A widely known way to gain a vision foundation model is through contrastive language image
pretraining [15, 39], such as CLIP [29]. Without resorting to language, masked autoencoder [42] is another popular
approach for obtaining foundation models. However, those foundation models often need to be ﬁnetuned on the target
dataset before they can be used for the downstream tasks, which hinders their generic nature. Following NLP to exploit
prompt engineering for adapting the model to downstream tasks without ﬁnetuning, very recently, Segment Anything
Model (SAM) [19] has proposed a task termed promptable segmentation to train a model to cut out objects on the
images in the form of masks. Despite impressive performance in zero-shot transfer tasks, it is unclear whether SAM
can be robust against adversarial attack and our work is the ﬁrst to investigate how to attack SAM with adversarial
examples.
We follow the common practices of popular adversarial attack methods like FGSM attack [11] and PGD attack [25].
As the ﬁrst foundation model to perform promptable segmentation, SAM differs from traditional image recognition
(segmentation) models in two ways: (1) it outputs a mask without label prediction and (2) it relies on prompt inputs
(like points or boxes) . In other words, existing adversarial attacks mainly focus on how to manipulate their image-level
∗Correspondence Author: chaoningzhang1990@gmail.com
arXiv:2305.00866v2  [cs.CV]  8 May 2023

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples
(or pixel-level) label prediction without prompts. To this end, we propose a framework termed Attack-SAM, which is
formulated for attacking SAM on the prompt-based mask prediction task. Speciﬁcally, we propose a mask removal
problem as the basic attack, which is guided by a simple yet effective loss termed ClipMSE. Experimental results
show that adversarial attack can successfully remove the predicted mask, suggesting SAM is vulnerable to adversarial
examples. We further experiment with transfer-based attacks in two setups: cross-prompt transfer and cross-task transfer.
The success of cross-prompt transfer suggests that attackers do not need to know the given point prompt to guarantee a
successful attack. With cross-task transfer, we ﬁnd that adversarial examples generated with models for semantic label
prediction can realize partial success of attacking SAM for prompt-based mask prediction.
Beyond the basic goal of mask removal, we further investigate whether adversarial examples can attack SAM to generate
any desired mask. To this end, we design the desired task in three setups by setting it to (1) a manually designed mask
at a random position, (2) a mask generated on the same image under a different point prompt, (3) a mask generated on
another image with a random point prompt. Overall, we ﬁnd it is possible to generate the desired mask properly in
most cases, which further highlights the adversarial vulnerability of SAM. Overall, the contributions of this work are
summarized as follows.
• We conduct the ﬁrst yet comprehensive investigation on attacking SAM with adversarial examples. We propose
a framework for attacking SAM by setting the attack goal as mask removal and design a simple yet effective
ClipMSE loss to realize the goal.
• We reveal that SAM is vulnerable to adversarial attacks in full white-box setting. Moreover, we demonstrate
that the adversary can attack the model without knowing the prompt and that the SAM can be partially attacked
by adversarial examples in a cross-task setup.
• Beyond the basic attack goal of mask removal, we further investigate and successfully show that the SAM can
be attacked to generate any desired mask, which further highlights the vulnerability of SAM.
The rest of this work is organized as follows. Section 2 introduces related work by summarizing the progress of SAM
and various attack methods. Section 3 proposes the framework for Attack-SAM by formulating mask removal as the
attack goal. Section 4 reports the results of Attack-SAM in the white-box setting. Section 5 reports our investigation of
transfer-based attacks on SAM. In Section 6, we further experiment with how to attack SAM to generate any desired
mask. Section 7 discusses the relationship between attacking label prediction and attacking mask prediction, and the
limitation of our work.
2
Related work
2.1
Segment Anything Model (SAM)
Within less than one month after the advent of SAM, numerous projects and papers have investigated it from various
angles, which can be roughly divided into the following categories. A mainstream line of works has tested whether
SAM can really segment anything in real-world scenarios, such as medical images [24, 47], Camouﬂaged Object [33]
and glass (transparent object and mirror) [13]. Their ﬁndings show that SAM often fails to detect objects in those
challenging scenarios. A similar ﬁnding has also been found in [7], which proposed to use a task-speciﬁc adapter to
improve SAM in those challenging scenarios. SNA [16] is a pioneering work to extend the SAM to Non-Euclidean
Domains with multiple preliminary solutions proposed. Another mainstream line of work has attempted to augment
SAM for improving its utility. For example, Grounded SAM [14] realizes detecting and segmenting anything with text
inputs via combing Grounding DINO [22] with SAM. Given that the generated masks of SAM have no label predictions,
multiple works [6, 28] have attempted to combine SAM with BLIP [21] or CLIP [29]. Some works have also utilized
SAM for image editing [18, 10, 30] and inpainting [38]. Moreover, SAM has been applied in [37, 48] for tracking
objects in video and in [31, 17] for reconstructing 3D objects from a single image.
2.2
Adversarial attacks
Deep neural networks are widely known to be vulnerable to adversarial examples, including CNN [32, 11, 20] and vision
transformer(ViT) [9, 1, 2, 26]. This vulnerability has inspired numerous works to investigate the model robustness
under various types of adversarial attacks. Adversarial attack methods can be divided into white-box setting [11, 5, 25]
which allows full access to the target model and black-box attacks [8, 36, 23, 34, 35, 12, 40] that mainly rely on
transferability of adversarial examples. Another way to categorize the attacks is untargeted and targeted. In the context
of image recognition (classiﬁcation), an attack is considered as successful if the predicted label is different from the
ground-truth label under the untargeted setting. In the more strict targeted setting, the attack fails if the predicted label
is not the pre-determined target label. The above works have mainly focused on manipulating the image-level label
2

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples
prediction for image classiﬁcation tasks, while our work investigates how to attack SAM for the task of prompt-based
mask prediction. Since SAM is the ﬁrst work to achieve prompt-based mask prediction, it remains unclear whether SAM
can be robust against adversarial attacks. It is worth mentioning that attacking SAM is also different from attacking
semantic segmentation models, since the generated masks have no semantic labels.
3
Framework for Attack-SAM
3.1
Preliminaries
Prompotable mask prediction. The task SAM solves is promptable segmention, where the model generates masks as
the outputs by taking both images and prompts as the inputs. It is worth noting that the generated masks just cut out the
detected objects but has no semantic labels for each mask. Therefore, the data pair for SAM could be represented by (x,
prompt, Mask), where prompt is necessary for each prediction. For a certain image x, the prompt could be selected
randomly across the image, leading to multiple data pairs D = {(x, prompti, Maski) }.
The forward progress of SAM is given as follows:
y = SAM(promt, x; θ)
(1)
where y indicates the conﬁdence of being masked for each pixel. In the vanilla setting of SAM, y has the shape of
H ∗W where H, W indicate the height and width of input image, respectively. With i and j indicate the coordinates of
pixel in the image, the pixel xij is marked within the mask area if the predicted value yij for xij is positive (larger than
zero). Otherwise, it is marked as the background. The ﬁnal predicted masks are termed as Maskpred, which is a binary
matrix in the shape of H ∗W .
Common attack methods. Before introducing Attack-SAM, we ﬁrst revisit the widely applied attack methods in the
traditional classiﬁcation task. We deﬁne f(·, θ) as the to-be-attacked target model, parameterized by θ. With (xclean,
Y ) as data pairs from the original dataset, the adversarial image xadv is deﬁned as x + δ∗, where δ∗is optimized in
Equation 2. Speciﬁcally, the attack algorithm is designed to generate the optimal δ∗. In the classiﬁcation task, Y often
indicates the class label and loss is often cross-entropy function.
δ∗= max
δ∈S loss(f(xclean + δ; θ), Y )
(2)
Fast Gradient Sign Method (FGSM) [11] and Projected Gradient Descent (PGD) [25] are two widely used methods for
evaluating the model robustness for their simplicity and effectiveness. FGSM is a single-step attack method based on
the model gradient on the input image. PGD can be seen as an iterative version of FGSM, which is also termed I-FGSM
in some works. In this work, we stick to term it PGD for consistency. If the iteration number is N, the PGD attack is
denoted as PGD-N.
3.2
Attack-SAM
Task deﬁnition. For typical adversarial attacks on image recognition models, the goal is to change the image-wise or
pixel-wise predicted labels so that the model makes wrong predictions. Given that the generated masks from SAM have
no semantic labels, a straightforward way to successfully attack SAM is to make the model fail to detect the objects so
that the generated masks are removed after adding adversarial perturbation. In this work, we perceive mask removal
as the basic goal of adversarial attacks on SAM. According to Section 3.1, a pixel xij is predicted as masked if the
predicted value yij is positive. Therefore, the mask removal task is successful if the predicted values y in the target area
all become negative. In other words, we can attack SAM to remove masks by reducing y until being negative.
Loss design. To remove masks by attacking SAM, the loss design is expected to be made to reduce the predicted
values y until being negative. Moreover, for positive yij with large amplitude, the loss is expected to punish them more
than the positive ymn with small amplitude or already negative ones. To reduce the randomness effect, it is desired to
make the predicted values lower than a certain negative value instead of being slightly lower than zero. With these
three expectations, the Mean Squared Error (MSE) loss with a negative threshold is a natural choice. As shown in
Equation 3, the predicted value SAM(prompt, xclean + δ) is optimized to be close to a negative threshold Negth after
attack. In the extreme case of SAM(prompt, xclean + δ) = Negth for all predicted values y, the MSE loss achieves
its minimum: zero.
δ∗= min
δ∈S ||SAM(prompt, xclean + δ) −Negth||2
(3)
3

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples
The above MSE loss in Equation 3 could optimize δ to predict negative values and also meets the expectations discussed
above. For the predicted values already lower than Negth, however, it makes no sense to increase them to be close to
Negth. To mitigate this issue, we clip the predicted values smaller than Negth so that the loss leads to zero gradients
on those pixel values. We term this loss as ClipMSE, as shown in Equation 4. The superiority of ClipMSE over the
vanilla MSE is veriﬁed in Table 1.
δ∗= min
δ∈S ||Clip(SAM(prompt, xclean + δ), min = Negth) −Negth||2
(4)
Attack details. We adopt FGSM [11] attack and PGD attack [25] which are widely used for robustness evaluation
in prior works. Following prior works on attacking vision models in the white-box setting, we set the maximum
allowable perturbation magnitude to 8/255 by default, with a step size of 8/255 and 2/255 for FGSM and PGD attack,
respectively. If not speciﬁed, PGD-10 attack is adopted, which indicates that the iteration number is 10. The threshold
Negth is set to −10 in this work, since the predicted values in the background (non-mask) region are often around this
value.
4
Main results of Attack-SAM
In the vanilla version of SAM, a single point is chosen in the image and a single mask near the given point is then
predicted. As a basic setup, we ﬁrst attack SAM to remove a single mask as discussed in Section 3.2. Since each mask
is generated from an input prompt, the attack succeeds if SAM fails to predict the original mask based on that prompt.
Speciﬁcally, the attack succeeds in the mask removal task if Maskadv is empty or at least with a much smaller area than
Maskclean. To not lose generality, the (prompt, Maskclean) pair to be attacked is selected randomly from the image.
4.1
Qualitative results
For visualization examples, we randomly select clean images either from demo images of SAM project or SA-1B
dataset of SAM paper [19]. The visualization results of adversarial images and predicted masks are reported in Figure 1,
and more results are given in the appendix. With a randomly selected point as the prompt (marked as the green star in
Figure 1(a)), the model is able to generate adversarial images with imperceptible perturbations after FGSM and PGD
attack (see Figure 1(b) and Figure 1(c)). Although SAM is able to predict a high-quality Maskclean in Figure 1(d),
both FGSM and PGD attack are able to remove the area of Maskclean, especially the tiny white area of Maskpgd in
Figure 1(f). Figure 1 shows that SAM is vulnerable to adversarial attacks in the mask removal task, and the PGD attack
performs better than the FGSM attack.
(a) xclean
(b) xfgsm
(c) xpgd
(d) Maskclean
(e) Maskfgsm
(f) Maskpgd
Figure 1: Attack SAM to remove the masks. Figure (a) refers to the clean image with the location of point prompt
marked in green star. Figure (b) and (c) are adversarial images generated by FGSM and PGD attacks, respectively. The
white area in Figure (d) (e) (f) refer to masks predicted by SAM based on the given prompt and images in Figure (a) (b)
(c), respectively. The results in Figure (e) (f) show that SAM is vulnerable to adversarial attacks considering the reduce
white area compared to Figure (d).
4.2
Quantitative results
Evaluation metric. For quantitative evaluation of adversarial attack on SAM, we adopt the IoU metric widely used in
segmentation. To evaluate the change of masks after attack, we calculate IoU between the predicted masks of clean
image Maskclean and predicted masks of adversarial image Maskadv. As shown in Equation 5, an average of IoU
4

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples
from N data pairs is calculated.
mIoU = 1
N
N
X
i=1
IoU(Maskadv, Maskclean),
(5)
It should be noted that Mask in Equation 5 is a binary matrix indicating whether a pixel is predicted to be masked. The
maximum of mIoU equals 1 when perturbation δ is a zero vector, corresponding to the case of no attack.
We report mIoU results after different attacks in Table 1. With the proposed ClipMSE, the mIoU drops from 1 to
close to zero after PGD-attack. Although FGSM attack also achieves mIoU much smaller than 1, the result under
FGSM attack is worse than PGD attack due to weaker attack strength. This result is consistent with the visualization in
Figure 1. We also compare MSE loss and ClipMSE loss. Although the two losses achieve similar results under the
FGSM attack, ClipMSE loss signiﬁcantly outperforms MSE loss under the PGD attack. This is expected since the
advantage of ClipMSE loss is to remove the gradient of pixels with predicted value y smaller than Negth. Meanwhile, it
is hard for FGSM attack to make predicted y smaller than Negth within a single attack step. Therefore, the advantages
of ClipMSE loss are more signiﬁcant in multiple-iteration attacks, such as PGD-10.
Table 1: Results of mIoU in removing mask attack on SAM. Both FGSM and PGD-10 attack achieve much lower mIoU
than the setting of no attack, while PGD-10 with ClipMSE achieves the lowest mIoU, outperforming other settings.
Attacks
No Attack
MSE
ClipMSE
FGSM
PGD-10
FGSM
PGD-10
mIoU
1.0
0.4868
0.1639
0.4807
0.0487
4.3
Attack SAM under other forms of prompts
In this work, we mainly experiment with attacking SAM under the point prompt. However, SAM also accepts other
forms of prompts like bounding box. Interested readers can refer to the appendix for the relevant results.
5
Transfer-based attacks
Section 4 has shown that SAM is vulnerable to adversarial attacks in a full white-box setting. A natural question
arises: is SAM robust to transfer-based attacks? In the black-box setting, the attacker cannot access all the information
needed when attacking a certain target model. In this section, we evaluate the robustness of SAM by introducing two
transfer-based attacks: cross-prompt transfer and cross-task transfer.
5.1
Cross-prompt transfer
Task deﬁnition. In Section 4, a single point prompti is randomly selected as input prompt to generate the adversarial
image xadv. SAM then fails to predict the corresponding Maski with (prompti, xadv) as the input. The question is:
could this vulnerability transfer between point prompts? In other words, if we generate xadv by attacking the (prompti,
Maski) as in Section 4, could SAM predict valid masks for another point prompt promptj? We term this attack as
cross-prompt transfer attack. Speciﬁcally, the point prompt (above prompti) selected to generate xadv is termed as
source prompt (promptsource) , and the above promptj for mask prediction is termed target prompt (prompttarget) .
A preliminary study. To investigate whether the vulnerability could transfer between different point prompt s, we take
the adversarial images xpgd in Figure 1(c) as the input image of SAM, and attack SAM to predict masks with prompts
(prompttarget) different from promptsource marked as the green star in Figure 1(a) . The xpgd is shown in Figure 2(a) ,
with prompttarget marked as green star. It should be noted that the xpgd in Figure 1(a) is the same image as Figure 1(c)
. As shown in Figure 2(c) , SAM could predict masks based on the (prompttarget, xpgd) pair in Figure 2(a) . However,
the quality of generated masks in Figure 2(c) is lower than the masks generated on xclean in Figure 2(b) . These
observations show that generating xpgd from promptsource can cause vulnerability of other (prompt, mask) pairs to
some extent (see the reduced area of masks in Figure 2(c) compared to Figure 2(b) ) . However, there still exist valid
masks in Figure 2(c) with xpgd as input. The results in Figure 2 inspire us to investigate the possible reason for the
above phenomenon.
Enhancing cross-prompt transferability. We propose a possible explanation for the interesting phenomenon in
Figure 2 as follows. The reason is that SAM by default generates the mask around the given point prompt . When the
generated perturbation in xpgd mainly focuses on removing the mask around the selected promptsource, it can be less
5

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples
(a)
Input
xpgd
with
prompttarget
(b) Masks predicted by
(xclean,prompttarget)
(c) Masks predicted by
(xpgd,prompttarget)
Figure 2: Preliminary study for corss-prompt transfer. (a) The adversarial image xpgd is Figure 1(c) is adopted for
mask prediction with prompt (prompttarget) marked as a green star. Figure (b) and (c) are predicted masks based
on (prompttarget, xclean) and (prompttarget, xpgd) , respectively. Note that prompttarget in (a) is different from the
prompt (promptsource) when generating xpgd in Figure 1.
effective in other regions. To verify this hypothesis, we increase the number (K) of promptsource when generating
xpgd. The results in Table 2 and Figure 3 show that the mask predicted by (prompttarget, xpgd) could be removed
as K increases. The above example shows the setting when the number of prompttarget is 1, and we also report
the masks predicted on multiple (x, prompttarget) pairs in Figure 4. As shown in Figure 4(c), when the number of
promptsource (K) is 1, the number of predicted masks is less than that in Figure 4(b), but there are still valid masks
remaining in Figure 4(c). As the number of promptsource (K) increases, more masks could be removed in the multiple
prompttarget setting. This conclusion is consistent with that in Table 2 and Figure 3 where the number of prompttarget
is 1. These results show that the adversary can attack the model without knowing the prompt, and more masks could be
removed as the number of promptsource increases. More investigations and understandings of the cross-prompt task
are encouraged in future works.
Table 2: Results of cross-prompt transfer with different numbers of promptsource when generating adversarial images,
termed K. The mIoU decreases signiﬁcantly as K increases, indicating more successful attack.
K
1
100
400
mIoU
0.4234
0.1247
0.0971
(a) K=1
(b) K=100
(c) K=400
Figure 3: Masks predicted on (xpgd, prompttarget) with different numbers of promptsource (K) when generating
adversarial images. The masks in white shrink signiﬁcantly as K increases.
5.2
Cross-task transfer
Task deﬁnition. The task of SAM is to predict masks based on prompts. Therefore, a successful attack can mislead
SAM to make wrong decisions for mask prediction. With various deeply explored models on label classiﬁcation task,
6

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples
(a) xclean
(b) Maskclean
(c) Maskadv (K=1)
(d) Maskadv (K=100) (e) Maskadv (K=400)
Figure 4: Masks predicted on multiple (x, prompttarget) pairs in cross-prompt transfer task, with different numbers of
promptsource (K) when generating adversarial images. With each colored region referring to a mask, the valid masks
Maskadv reduce signiﬁcantly as K increases.
we raise the following question: could SAM predict valid masks based on the adversarial images xadv generated on
models from other tasks, such as classiﬁcation? Since the task to generate xadv (tasksource) is different from the mask
prediction task in SAM (tasktarget), we term this attack as cross-task transfer attack.
A preliminary investigation. We conduct a cross-task transfer attack by generating xadv with a ViT model trained
on ImageNet dataset for image classiﬁcation, and then apply SAM to perform mask predictions on xadv. As shown
in Figure 5, although xadv in Figure 5(b) is generated by a classiﬁcation task model, it can somehow remove several
masks, comparing Maskadv in Figure 5(d) to Maskclean in Figure 5(c) . However, it should be noted that there are
still many masks remaining in Maskadv (Figure 5(d) ) . More investigations on the cross-task transfer are encouraged
in the future.
(a) xclean
(b) xadv
(c) Maskclean
(d) Maskadv
Figure 5: Masks predicted on multiple (x, prompt) pairs in cross-task transfer task. The xadv is generated by a
pretrained ViT model for label classiﬁcation task, and then used for mask generation with SAM on multiple prompts.
Compared to Maskclean in (c) , several masks are removed in Maskadv in (d) but there are still masks remain.
7

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples
6
Beyond mask removal
In the above sections, we mainly target to remove the valid masks. Here, we further investigate a more intriguing
scenario to exploit adversarial example for generating any desired masks. Conceptually, it aims to create a new mask
instead of just removing an old mask as discussed above.
6.1
Mask enlargement
After investigating the mask removal attack, it is natural to ask whether it is possible to add new masks to a segmentation
map. As a preliminary investigation, we ﬁrst attempt to enlarge the mask area regardless of the shape or size of original
mask. Following a similar procedure in mask removal task, we replace the Negth in Equation 4 with a positive threshold
Posth, as shown in Equation 6.
δ∗= min
δ∈S ||Clip(SAM(prompt, xclean + δ), max = Posth) −Posth||2
(6)
We visualize the result of mask enlargement in Figure 6. The experimental results in Figure 6 show that the mask of
adversarial images Maskadv is much larger than Maskclean, comparing Figure 6(c) and Figure 6(d) . Speciﬁcally, the
entire image can be predicted as a single mask based on a randomly selected point prompt under adversarial attack.
This indicates that the adversarial attack is capable of not only shrinking the mask region but also enlarging it. In other
words, under the adversarial attack, the prediction can be changed from mask to background and from background to
mask. This motivates us to attack SAM for generating any desired mask.
(a) xclean
(b) xadv
(c) Maskclean
(d) Maskadv
Figure 6: Results of mask enlargement attack. The Maskclean and Maskadv in (c) (d) are generated on xclean and
xadv in (a) (b), respectively. Results show that the mask predicted by SAM could be enlarged by the adversarial attack.
6.2
Mask Manipulation
Here, we manipulate the original mask on the same image. Speciﬁcally, we experiment with two use cases: mask shift
and mask ﬂipping. In the following, we highlight the main results with technical details like the loss design reported in
the appendix.
Mask shift. For an existing mask in the image, mask shift attack changes its position by either duplicating or translating
the original mask to a new location. The experimental results in Figure 7 show that both duplicating or translating the
original mask could be achieved by the adversarial attack. It should be noted that translating the original mask (see
Figure 7(f) ) to a new location is more difﬁcult than duplicating it (see Figure 7(e) ) since the task in Figure 7(f) also
needs to remove its original mask.
Mask ﬂipping. Another manipulation attempt on the original mask is to ﬂip it. We follow the procedure of mask shift
by duplicating or ﬂipping the original mask, as shown in Figure 8. The results in Figure 8 show that the original mask
could be ﬂipped either in the horizontal or vertical direction, generating new tasks in both duplicating (Figure 8(e) ) and
ﬂipping Figure 8(f) ) tasks.
8

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples
(a) xclean
(b) xdup
adv
(c) xtrans
adv
(d) Maskclean
(e) Maskdup
adv
(f) Masktrans
adv
Figure 7: Results of mask shift attack. The Maskclean and Maskadv in (d) (e) (f) are generated on xclean and xadv in
(a) (b) (c) , respectively. With adversarial attack, the original mask in (d) could be shifted by either duplicating in (e) or
just translating in (f) to a new position.
(a) (a) xclean
(b) xflip
adv
(c) xdup
adv
(d) Maskclean
(e) Maskdup
adv
(f) Maskflip
adv
Figure 8: Results of mask ﬂipping attack. The Maskclean and Maskadv in (d) (e) (f) are generated on xclean and xadv
in (a) (b) (c) , respectively. With adversarial attack, the original mask in (d) could be ﬂipped by either duplicating in (e)
or just ﬂipped in (f) .
6.3
Towards generating any desired mask
Although mask manipulation in the above Section 6.2 succeeds in generating masks in different locations, they do
not change the mask shape of the detected objects. Here, we experiment with a more general goal to generate any
desired mask. Similar to the above investigation, we highlight the main results here and report the technical details in
the appendix. We experiment with setting the target mask in three setups: (1) a manually designed mask at a random
position; (2) a mask generated with a different point prompt on the same image; (3) a mask generated on a different
image (with a random point prompt).
Setting 1: a manually designed mask at a random position. Here, we investigate whether an adversarial attack
could generate manually desired masks at a random position. For not losing generality, we design masks of geometric
shapes including circle and square. Figure 9 shows that this goal could be achieved by setting the mask target to be a
circle and square at a random position when generating xadv in Figure 9(b) and Figure 9(c). Although the input prompt
in Figure 9(a) expects a mask of dog as in Maskclean of Figure 9(d), the desired circle or square masks can be obtained
in Maskadv. It is non-trivial to manually design more complex mask than circles or squares. Therefore, we further
exploit the real object masks generated by the SAM as the target (reference) masks to attack the SAM (see Setting 2
and Setting 3).
(a) xclean
(b) xcircle
adv
(c) xsquare
adv
(d) Maskclean
(e) Maskcircle
adv
(f) Masksquare
adv
Figure 9: Towards generating any desired masks (setting 1). The Maskclean and Maskadv in (d) (e) (f) are generated
on xclean and xadv in (a) (b) (c), respectively. With adversarial attack, manually designed mask in (e) (f) could be
generated at a random position.
9

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples
Setting 2: a mask generated with a different point prompt on the same image. In this setup, we set the target mask
to that generated on the same image with a different point prompt. The results are shown in Figure 10. Let us ﬁrst look
at the ﬁrst row in Figure 10. The input point prompt prompt1 in Figure 10(a) expects a mask of a dog as in Maskclean
(see the ﬁrst row of Figure 10(c) ). However, the Maskadv predicted based on xadv includes human legs as in the ﬁrst
row of Figure 10(d). This is achieved by taking the predicted mask of another prompt prompt2 as the target when
generating xadv. However, in the original dog region, the mask is not fully removed, which suggests that the attack
method can be further improved. We leave this investigation to future works. A similar phenomenon can be observed in
the second row of Figure 10.
(a) xclean
(b) xadv
(c) Maskclean
(d) Maskadv
Figure 10: Towards generating any desired masks (setting 2) . The Maskclean and Maskadv in (c) (d) are generated on
xclean and xadv in (a) (b) , respectively. With adversarial attack, the mask from same image but different point prompt
in (d) could be generated.
Setting 3: a mask generated on a different image with a random point prompt. We investigate this setting by two
example images in Figure 11. Take the ﬁrst row of Figure 11 for example, a dog mask and a cow mask in Figure 11(d)
and Figure 11(e) are predicted based on the clean images in Figure 11(a) and Figure 11(b), respectively. If we take
the cow mask in Figure 11(e) as Masktarget and attack the (xclean,prompt) pair of dog image in Figure 11(a) , the
adversarial image xadv of dog image is obtained in Figure 11(c) . Interestingly, a cow mask Maskadv is predicted in
Figure 11(f) based on xadv of dog image in Figure 11(c) . A similar observation can also be made in the second row of
Figure 11, predicting a dog mask in Maskadv of Figure 11(f) based on xadv of a cow image in Figure 11(c).
(a) xclean
(b) xreference
(c) xadv
(d) Maskclean
(e) Maskreference
(f) Maskadv
Figure 11: Towards generating any desired masks (setting 3) . The masks in (d) (e) (f) are generated on images in (a) (b)
(c) , respectively. With (b) as a reference image and (e) as a reference mask, the xadv in (c) could predict a Maskadv in
(f) that is similar to Maskreference in (e) .
7
Discussion
Attack goals: label prediction v.s. mask prediction. In contrast to existing works that mainly focus on attacking the
model to change the label prediction, our work investigates how to attack the SAM for changing the mask prediction.
Conceptually, mask removal can be interpreted as untargeted attack setting for making the adversarial mask/label
prediction different from the clean mask/label prediction. Our investigation in Section 6 to generate any desire mask is
conceptually similar to the targeted attack setting. In other words, the prediction after the attack needs to align with the
pre-determined target label/mask.
10

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples
Limitations. Overall, we demonstrate successful attack performance in various setups. However, in some challenging
setups, like cross-task attacks, the success is only partial, which is somewhat expected considering the task discrepancy
between label prediction and mask prediction. For the case of generating any desired task, we ﬁnd that the generated
mask is not always perfect with some small noise masks in the unintended region. Future works can investigate how to
improve the attack performance by removing those noise masks. Moreover, our choice of reference mask is still limited,
and thus more diverse mask types can be explored in future works.
8
Conclusion
Our work conducts the ﬁrst yet comprehensive investigation on how to attack SAM with adversarial examples. In
the full white-box setting, SAM is found to be vulnerable with successful removal of the original mask. When the
prompt is not given, the attacker can enhance the cross-prompt transferability by attacking multiple masks with different
prompts when generating the adversarial image. We also experiment with the cross-task transferability and ﬁnd that the
adversarial examples generated to attack the semantic label prediction can also be used to attack the mask prediction to
some extent. Beyond the basic attack goal of mask removal, we also attempt to generate any desired task with an overall
satisfactory attack performance. It is not our intention to ﬁnd the strongest method to attack SAM. Instead, we focus
on adapting the common attack methods from attacking label prediction to attacking mask prediction to investigate
whether SAM is robust against the attack of adversarial examples. Our ﬁnding that SAM is vulnerable to adversarial
examples suggests a need to examine the safety concerns of deploying SAM in safety-critical scenarios. Future works
are expected to identify stronger attack methods as well as enhance the robustness of SAM.
References
[1] Philipp Benz, Soomin Ham, Chaoning Zhang, Adil Karjauv, and In So Kweon. Adversarial robustness comparison
of vision transformer and mlp-mixer to cnns. arXiv preprint arXiv:2110.02797, 2021.
[2] Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and Andreas Veit.
Understanding robustness of transformers for image classiﬁcation. arXiv preprint arXiv:2103.14586, 2021.
[3] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation
models. arXiv preprint arXiv:2108.07258, 2021.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-
lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances
in neural information processing systems, 2020.
[5] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In SP, 2017.
[6] Jiaqi Chen, Zeyu Yang, and Li Zhang. Semantic-segment-anything, 2023. GitHub repository.
[7] Tianrun Chen, Lanyun Zhu, Chaotao Ding, Runlong Cao, Shangzhan Zhang, Yan Wang, Zejian Li, Lingyun Sun,
Papa Mao, and Ying Zang. Sam fails to segment anything?–sam-adapter: Adapting sam in underperformed scenes:
Camouﬂage, shadow, and more. arXiv preprint arXiv:2304.09148, 2023.
[8] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial
attacks with momentum. In CVPR, 2018.
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
[10] Feizc. Iea, 2023. GitHub repository.
[11] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In
ICLR, 2015.
[12] Yiwen Guo, Qizhang Li, and Hao Chen. Backpropagating linearly improves transferability of adversarial examples.
arXiv preprint arXiv:2012.03528, 2020.
[13] Dongsheng Han, Chaoning Zhang, Yu Qiao, Maryam Qamar, Yuna Jung, SeungKyu Lee, Sung-Ho Bae, and
Choong Seon Hong. Segment anything model (sam) meets glass: Mirror and transparent objects cannot be easily
detected. arXiv preprint, 2023.
[14] IDEA-Research. Grounded segment anything, 2023. GitHub repository.
11

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples
[15] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,
and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In
ICML, 2021.
[16] Yongcheng Jing, Xinchao Wang, and Dacheng Tao. Segment anything in non-euclidean domains: Challenges and
opportunities. arXiv preprint arXiv:2304.11595, 2023.
[17] Minki Kang, Dongchan Min, and Sung Ju Hwang. Any-speaker adaptive text-to-speech synthesis with diffusion
models. arXiv preprint arXiv:2211.09383, 2022.
[18] Kevmo. magic-copy, 2023. GitHub repository.
[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer
Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.
[20] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In ICLR, 2017.
[21] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for
uniﬁed vision-language understanding and generation. In ICML, pages 12888–12900. PMLR, 2022.
[22] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su,
Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv
preprint arXiv:2303.05499, 2023.
[23] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and
black-box attacks. ICLR, 2017.
[24] Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.
[25] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks. In ICLR, 2018.
[26] Kaleel Mahmood, Rigel Mahmood, and Marten Van Dijk. On the robustness of vision transformers to adversarial
examples. arXiv preprint arXiv:2104.02610, 2021.
[27] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.
arXiv preprint arXiv:2203.02155, 2022.
[28] Curt Park. segment anything with clip, 2023. GitHub repository.
[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language
supervision. In ICML, 2021.
[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 10684–10695, 2022.
[31] Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-3d: Towards single-view anything reconstruction in
the wild. arXiv preprint arXiv:2304.10261, 2023.
[32] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
[33] Lv Tang, Haoke Xiao, and Bo Li. Can sam segment anything? when sam meets camouﬂaged object detection.
arXiv preprint arXiv:2304.04709, 2023.
[34] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble
adversarial training: Attacks and defenses. ICLR, 2018.
[35] Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma. Skip connections matter: On the
transferability of adversarial examples generated with resnets. arXiv preprint arXiv:2002.05990, 2020.
[36] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille. Improving
transferability of adversarial examples with input diversity. In CVPR, 2019.
[37] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything
meets videos. arXiv preprint arXiv:2304.11968, 2023.
[38] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything:
Segment anything meets image inpainting. arXiv preprint arXiv:2304.06790, 2023.
12

Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples
[39] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong
Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint
arXiv:2111.11432, 2021.
[40] Chaoning Zhang, Philipp Benz, Adil Karjauv, Jae Won Cho, Kang Zhang, and In So Kweon. Investigating top-k
white-box and transferable black-box attack. In CVPR, 2022.
[41] Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun
Zhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, et al. One small step for generative ai, one giant leap for agi:
A complete survey on chatgpt in aigc era. arXiv preprint arXiv:2304.06488, 2023.
[42] Chaoning Zhang, Chenshuang Zhang, Junha Song, John Seon Keun Yi, Kang Zhang, and In So Kweon. A survey
on masked autoencoder for self-supervised learning in vision and beyond. arXiv preprint arXiv:2208.00173, 2022.
[43] Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar
Dam, Chu Myaet Thwal, Ye Lin Tun, Le Luang Huy, et al. A complete survey on generative ai (aigc): Is chatgpt
from gpt-4 to gpt-5 all you need? arXiv preprint arXiv:2303.11717, 2023.
[44] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion models in
generative ai: A survey. arXiv preprint arXiv:2303.07909, 2023.
[45] Chenshuang Zhang, Chaoning Zhang, Sheng Zheng, Mengchun Zhang, Maryam Qamar, Sung-Ho Bae, and In So
Kweon. A survey on audio diffusion models: Text to speech synthesis and enhancement in generative ai. arXiv
preprint arXiv:2303.13336, 2023.
[46] Mengchun Zhang, Maryam Qamar, Taegoo Kang, Yuna Jung, Chenshuang Zhang, Sung-Ho Bae, and Chaoning
Zhang. A survey on graph diffusion models: Generative ai in science for molecule, protein and material. arXiv
preprint arXiv:2304.01565, 2023.
[47] Yizhe Zhang, Tao Zhou, Peixian Liang, and Danny Z Chen. Input augmentation with sam: Boosting medical
image segmentation with segmentation foundation model. arXiv preprint arXiv:2304.11332, 2023.
[48] Zxyang. Segment and track anything, 2023. GitHub repository.
13
