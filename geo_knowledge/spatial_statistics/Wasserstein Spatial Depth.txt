Wasserstein Spatial Depth
FranÂ¸cois Bachoc1, Alberto GonzÂ´alez-Sanz2, Jean-Michel Loubes3,
Yisha Yao4
1IMT, UniversitÂ´e de Toulouse, Institut universitaire de France (IUF), France, e-mail:
francois.bachoc@math.univ-toulouse.fr
2Department of Statistics, Columbia University, New York, USA, e-mail:
ag4855@columbia.edu
3INRIA, UniversitÂ´e de Toulouse, France, e-mail: loubes@math.univ-toulouse.fr
4Department of Statistics, Columbia University, New York, USA, e-mail:
yy3381@columbia.edu
Abstract: Modeling observations as random distributions embedded within
Wasserstein spaces is becoming increasingly popular across scientific fields,
as it captures the variability and geometric structure of the data more effec-
tively. However, the distinct geometry and unique properties of Wasserstein
space pose challenges to the application of conventional statistical tools,
which are primarily designed for Euclidean spaces. Consequently, adapting
and developing new methodologies for analysis within Wasserstein spaces
has become essential. The space of distributions on Rd with d > 1 is not lin-
ear, and â€œmimicâ€ the geometry of a Riemannian manifold. In this paper, we
extend the concept of statistical depth to distribution-valued data, intro-
ducing the notion of Wasserstein spatial depth. This new measure provides
a way to rank and order distributions, enabling the development of order-
based clustering techniques and inferential tools. We show that Wasserstein
spatial depth (WSD) preserves critical properties of conventional statistical
depths, notably, ranging within [0, 1], transformation invariance, vanishing
at infinity, reaching a maximum at the geometric median, and continuity.
Additionally, the population WSD has a straightforward plug-in estima-
tor based on sampled empirical distributions. We establish the estimatorâ€™s
consistency and asymptotic normality. Extensive simulation and real-data
application showcase the practical efficacy of WSD.
MSC2020 subject classifications: Primary 62R10, 62G30; secondary
62G35.
Keywords and phrases: Distributional data analysis, High dimensional
data, Order statistic, Outlier detection, Statistical depths, Wasserstein dis-
tance.
1. Introduction
Contemporary data collected in various disciplines is complex and multifaceted.
Traditional statistical tools, which model data objects as samples from a Eu-
clidean space or vector space, are inadequate to capture the variation and ge-
ometry of the data objects. Random objects lying in general metric spaces, in-
cluding spaces of functions [60], Wasserstein spaces [44], and hyperbolic spaces
[61], are gaining increasing favor in the scientific community. For instances, lon-
gitudinal images are treated as functions [60]; texts and media are modeled as
1
arXiv:2411.10646v2  [math.ST]  6 Mar 2025

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
2
distributions in modern AI training models [13, 29]; certain trees and graphs
are embedded into hyperbolic spaces [12]. It is widely recognized that statistical
efficiency can be gained by utilizing special properties of the above metric spaces
[6].
In this paper, we focus on modeling distribution-valued data objects within
Wasserstein spaces. There are several advantages to model certain data objects
as distributions or probability measures. Firstly, it captures the hierarchical
variations in the data by simulating a two-stage data-generating process: initially
sampling multiple distributions from a Wasserstein space, followed by drawing
data points from each sampled distribution. Secondly, it captures variations of
the data along geodesics of the distribution space that are not straight lines
as in the Euclidean setting and thus are closer to the observations. Thirdly,
it often provides a low-dimensional embedding that effectively represents high-
dimensional data, enabling better statistical inference without the curse of the
dimension. Since the Wasserstein space has different structure and property from
the Euclidean space, conventional analytic tools cannot apply to distribution-
valued data objects. Therefore, new methods specifically designed for analyzing
such data are essential.
There have been some efforts in this line of research, including but not lim-
ited to histogram regression [9], Wasserstein regression [2, 15], geodesic PCA [7],
template estimation [8], and Wasserstein clustering [24, 62]. Despite the above
developments, there is limited effort in agnostic exploratory analysis for data
objects in Wasserstein spaces [22, 25, 28, 59]. Still, exploratory analysis and de-
scriptional statistics are critical to overview the properties of the data distribu-
tion before modeling. In particular, a notion of â€œorderingâ€ for distribution-valued
objects in Wasserstein spaces will be of fundamental utility. Besides exploratory
analysis, it will also facilitate nonparametric methods for distribution-valued
data.
Quantiles, ranks, and signs are pivotal tools of semiparametric and nonpara-
metric statistics. Due to the lack of canonical ordering in multi-dimensional Eu-
clidean space, quantile or rank based tools have been limited to one-dimensional
data before the creation of statistical depths. The notion of statistical depths
fills this gap, extending the notion of order to higher dimension. Given a distri-
bution P on Rd, the depth of any data point x âˆˆRd is a non-negative value that
measures the â€œcentralityâ€ of x with respect to P. A larger value of depth indi-
cates the data point is more central within the distribution, while data points
with small depths are considered outliers or less typical within the distribution,
worthy of investigation. Several different types of depths have been proposed,
including Tukey depth [47, 54], simplicial depth [34], spatial depth [14, 56],
Monge-Kantorovich depth [16, 31] and lens depth [35]. Via endowing multi-
variate data points with â€œcenter-outwardâ€ orderings, depths allow extension of
order statistics, robust inference [36, 63] and classification to multivariate data
[43, 64].
Statistical depth theory is one of the main research areas of functional data
analysis (FDA). Most of the Euclidean depth functions extend naturally to
Hilbert-space-valued data, see [20, 38, 45, 46]. For instance, this is the case for

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
3
the h-depth [19], the Tukey depth [26] and its random version [18], the spatial
depth [11], the integrated depth [20] and the Monge-Kantorovich depth [30]. For
Banach spaces, some examples are the integrated depth [20], the band depth
and its modified version [38], the half-region depth and its modified version [39],
the Lâˆždepth [37] and the infimal depth [42].
While it may be tempting to embed the Wasserstein space into a function
space, for instance a reproducing kernel Hilbert space (RKHS) [52], and ap-
ply existing functional depth measures, this approach neglects the intricate
geodesic structure of the Wasserstein space. There is no linear representation of
the Wasserstein distance between distributions on Rd with d > 1 [5]. Existing
depths do not generalize well to nonlinear spaces. Besides the nonlinearity, the
Wasserstein distance is computationally expensive even for empirical measures
[49], which essentially rules out practical implementation of Tukey depth [54]
and Monge-Kantorovich depth [16, 31]. The computational complexity of these
two depths grows exponentially with the sample size.
In conclusion, conventional depth measures cannot be directly extended to
Wasserstein spaces due to the unique properties and structure discussed above.
This requires the development of a new notion of depth tailored specifically for
Wasserstein spaces.
1.1. Contributions
In this paper we develop a new notion of depth to order or rank distributions. It
is inspired by spatial depth, one of the simplest and most widely used notions of
statistical depths. Recall that the spatial depth of a point x âˆˆRd with respect
to a probability measure P over Rd is defined as
SD(x; P) = 1 âˆ’
E
 X âˆ’x
âˆ¥X âˆ’xâˆ¥
 ,
X âˆ¼P.
(1.1)
The spatial depth has been generalized to Hilbert spaces by following exactly
the same definition [51, 56, 59]. However, the lack of linear structure of the
Wasserstein space prevents a straightforward adaptation of the spatial depth.
Nevertheless, the Wasserstein metric endows the space of probability measures
with a structure of geodesic metric space (see [1]). For absolutely continuous
probability measures Q and P, the constant speed geodesic joining Q and P is
given by the curve of probability measures
[0, 1] âˆ‹Î» 7â†’((1 âˆ’Î»)I + Î» TQ,P )#Q,
where # denotes the push-forward operator (see Section 3.3 for its definition).
The definition of spatial depth for manifold-valued data motivates us to de-
fine the depth of a probability measure Q âˆˆPa.c
2 (Rd) (where Pa.c
2 (Rd) is the
set of absolutely continuous measures on Rd with finite second moments, see
Section 3.3) with respect to a probability measure over the Wasserstein space

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
4
P âˆˆP(P2(Rd)) (where P(P2(Rd)) is formally defined in Section 3.3) as
SD(Q; P) := 1 âˆ’
 Z EP âˆ¼P
x âˆ’TQ,P (x)
W2(P, Q)

2
dQ(x)
! 1
2
.
Above, TQ,P is the optimal transport map from Q to P and W2(P, Q) is the
Wasserstein distance between P and Q, both being formally defined in Sec-
tion 3.3. We will show that SD(Q; P) satisfies the same properties as its Eu-
clidean counterpart, namely transformation invariance, taking values in [0, 1],
decreasing at infinity and attaining maximum at the median. Moreover, we show
that Q 7â†’SD(Q; P) is continuous and P 7â†’SD(Q; P) is continuous. Next, we
will propose a finite-sample estimator under the so-called two-stage and one-
stage sampling models. In the two-stage sampling model, such an estimator can
be computed in polynomial time. Moreover, we will show that in both models,
the estimator is consistent, meaning that it approximates the true population
depth function as the sample size increases. We also prove asymptotic normality.
Finally, we will provide numerical simulations for real and synthetic datasets. In
particular, we highlight that our suggested depth is more informative than depth
methods designed for linear spaces and applied to mappings of distributions to
these linear spaces.
In conclusion, Wasserstein spatial depth (WSD) serves as a valid measure for
ordering objects within Wasserstein spaces, adhering to the axiomatic properties
of depth [64] and being computationally feasible. This concept facilitates the
extension of depth-based analytic tools to Wasserstein spaces, paving the way
for future research.
1.2. Organization
General notations are provided in Section 2. The definition of WSD is given in
Section 3 with illustrating examples in Section 4. In Section 5, we show that
WSD shares the desirable properties of conventional statistical depths [64]. In
Section 6, we tackle consistent estimation with asymptotic normality. In Sec-
tion 7, we compare WSD to several depths in general metric spaces [22, 28, 59]
adapted to Wasserstein spaces. We advocate WSD over the other depths in
terms of computational feasibility and assumption flexibility, while possessing
all desirable properties of a depth. In section 8, extensive numerical simulations
are shown to demonstrate the empirical validity and merits of WSD. Finally, in
Section 9, we apply it to explore real-world data and make informative discov-
eries. All the proofs are provided in the Appendix.
2. Notation
The space of Borel probability measures on a Polish space (K, d) is denoted as
P(K). For P âˆˆP(K), its support is written supp(P). The space of Borel finite
(signed) measures is denoted as M(K) and the space of finite (signed) measures

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
5
with 0 mass as M0(K), meaning that h âˆˆM0(K) if h âˆˆM(K) and h(K) = 0.
The integral of a measurable function f : K â†’R with respect to P âˆˆP(K) is
denoted as
Z
f(x)dP(x) =
Z
fdP = P(f).
Set P âˆˆP(K) and f : K â†’R be measurable. Then
âˆ¥fâˆ¥L2(P ) :=
Z
f 2dP
1/2
denotes the L2(P)-norm of f. The Hilbert space of measurable functions with
finite L2(P)-norm is denoted as L2(P) with inner product âŸ¨Â·, Â·âŸ©L2(P ). We also
extend the definition of the Hilbert space L2(P) and the associated notation to
vector-valued functions, with for f, g : K â†’Rk,
âŸ¨f, gâŸ©L2(P ) :=
Z
âŸ¨f, gâŸ©dP.
We say that a sequence {Âµn}nâˆˆN âŠ‚P(K) converges weakly to Âµ âˆˆP(K) if
Z
f dÂµn âˆ’â†’
Z
f dÂµ
for every bounded and continuous function f : K â†’R. In such a case we
write Âµn
P(K)
âˆ’âˆ’âˆ’â†’Âµ and also say that Âµn â†’Âµ in the weak sense of P(K). For
Zn âˆ¼Âµn and Z âˆ¼Âµ we write similarly Zn
P(K)
âˆ’âˆ’âˆ’â†’Z and we may also write
simply Zn
w
âˆ’â†’Z. Such a convergence is metrizable by means of the so-called
bounded Lipschitz metric [55, p. 73]
dBL(Âµ, Î½) = sup
 Z
f(x)d(Âµ âˆ’Î½)(x) :
|f(x)| â‰¤1 and |f(x) âˆ’f(y)| â‰¤d(x, y), âˆ€x, y âˆˆK

.
3. From Euclidean to Wasserstein spatial depth
In this section we define our notion of Wasserstein spatial depth. In Section 3.1
we recall the definition of Euclidean spatial depth and its main properties. In
Section 3.2 we provide our interpretation of spatial depth in terms of geodesics,
which allows for its generalization to the Wasserstein space of measures (see
Section 3.3). For readers interested in a more comprehensive understanding of
the mathematical concepts discussed in Sections 3.2 and 3.3, we recommend
consulting the monograph [1] for an in-depth exposition.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
6
3.1. Euclidean spatial depth
In Rd, for d > 1, the spatial depth of a point x with respect to a random variable
X âˆ¼P is defined as
SD(x; P) = 1 âˆ’
E
 X âˆ’x
âˆ¥X âˆ’xâˆ¥
 .
Throughout the paper, we use the convention 0/0 = 0. The spatial depth
shares the following properties with the univariate canonical depth function
2 min(F(x), 1 âˆ’F(x)). First, the statistical depth SD(x; P) belongs to the in-
terval [0, 1]. Second, the geometric median, defined as
mX âˆˆarg min
m
E[âˆ¥X âˆ’mâˆ¥],
satisfies SD(mX; P) = 1. Third, as âˆ¥xâˆ¥â†’âˆž, we have that SD(x; P) â†’0.
Finally, for an isometric transformation T : Rd â†’Rd, it holds that
SD(T(x); T#P) = SD(x; P),
where again the push-forward operator # is defined in Section 3.3.
3.2. Geodesic interpretations of the spatial depth
Let (M, d) be a metric space. A curve {Î³xâ†’y
t
}tâˆˆ[0,1] valued in M is a (constant
speed) geodesic joining x âˆˆM to y âˆˆM if
d(Î³xâ†’y
t
, Î³xâ†’y
s
) = (t âˆ’s)d(x, y),
for all 0 â‰¤s â‰¤t â‰¤1.
The space (M, d) is said to be geodesic if any two points are joined by at least
one geodesic. The length of a curve {Î³t}tâˆˆ[0,1] with values in M (not necessarily
a geodesic) is defined as L(Î³) =
R 1
0 |Î³â€²
t|dt, where |Î³â€²
t| = limsâ†’t
d(Î³t,Î³s)
|tâˆ’s| . Assume
now that M âŠ‚Rd is a Riemannian manifold with metric tensor {gx}xâˆˆM. Then
it holds that
L(Î³) =
Z 1
0
q
gÎ³t(âˆ‚tÎ³t, âˆ‚tÎ³t)dt,
where {âˆ‚tÎ³t}tâˆˆ[0,1] denotes the velocity (standard time derivative) of the curve
{Î³t}tâˆˆ[0,1].
In Rd, a geodesic joining x and y is just the segment Î³xâ†’y
t
= (1 âˆ’t)x + ty,
t âˆˆ[0, 1]. Therefore, the spatial depth of x can be seen as the spatial depth of
the velocities at time 0
SD(x; P) = 1 âˆ’
E
 âˆ‚t|t=0Î³xâ†’X
t
âˆ¥âˆ‚t|t=0Î³xâ†’X
t
âˆ¥
 .
This allows for the following Riemannian generalization of the spatial depth
SD(x; P) = 1 âˆ’
s
gx

E
 âˆ‚t|t=0Î³xâ†’X
t
âˆ¥âˆ‚t|t=0Î³xâ†’X
t
âˆ¥

, E
 âˆ‚t|t=0Î³xâ†’X
t
âˆ¥âˆ‚t|t=0Î³xâ†’X
t
âˆ¥

.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
7
3.3. Geodesic spatial depth over the space of measures
Let Pp(Rd) be the space of Borel probability measures over Rd with finite pth
order moment. The optimal transport cost between two probability measures
P, Q âˆˆPp(Rd) is defined as
OTp(P, Q) =
inf
Ï€âˆˆÎ (P,Q)
1
2
Z
âˆ¥x âˆ’yâˆ¥pdÏ€(x, y),
(3.1)
where Î (P, Q) âŠ‚Pp(Rd Ã— Rd) stands for the set of probability measures with
marginals P and Q, i.e., (X, Y) âˆ¼Ï€ âˆˆÎ (P, Q) if X âˆ¼P and Y âˆ¼Q. For p â‰¥1,
the mapping (P, Q) 7â†’Wp(P, Q) = (OTp(P, Q))
1
p defines a distance over the
space Pp(Rd) such that
Wp(Pn, P) â†’0
â‡â‡’
Pn
P(Rd)
âˆ’âˆ’âˆ’âˆ’â†’P
and
Z
âˆ¥xâˆ¥pdPn(x) â†’
Z
âˆ¥xâˆ¥pdP(x).
We focus now on the case p = 2. We define Pa.c
2 (Rd) as the subset of P2(Rd)
composed of absolutely continuous measures. If P belongs to Pa.c
2 (Rd), there
exists a unique minimizer Ï€P,Q of (3.1), for p = 2. Moreover, there exists a
unique gradient of a convex function TP,Q = âˆ‡Ï•P,Q such that Ï€P,Q = (I Ã—
âˆ‡Ï•P,Q)#P. The map TP,Q is called an optimal transport map. Here, for a
probability measure Âµ and a Borel mapping T, T#Âµ denotes the push forward
measure, which is the distribution of T(X), for X âˆ¼Âµ.
In [48], the author demonstrated that W2 serves as the natural metric for
P2(Rd), aimed at describing the long-term behavior of solutions to the porous
medium equation. This metric also imparts a geodesic metric space structure to
P2(Rd).
It is natural in the following sense. If {Xt}tâˆˆ[0,1] is a curve of random vec-
tors with âˆ‚tXt = vt(X0), then its associated curve of distributions {Pt}tâˆˆ[0,1]
satisfies the so-called transport/continuity equation
âˆ‚tPt + div(vtPt) = 0
(3.2)
in an appropriate weak sense. The continuity equation is commonly used in
fluid mechanics, where vt represents the flow velocity vector field. However,
given the curve {Xt}tâˆˆ[0,1] there could exist several curves of velocity fields
{vt}tâˆˆ[0,1] solving (3.2), i.e., generating the same flow. Among all of them, there
exists only one belonging to
arg min
Z 1
0
âˆ¥vtâˆ¥2
L2(Pt)dt :
âˆ‚tPt + div(vtPt) = 0

.
(3.3)
The tangent bundle of (P2(Rd), W2) is
TP (P2(Rd)) = {âˆ‡Ï• :
Ï• âˆˆCâˆž
c (Rd)}
L2(P ),
P âˆˆP2(Rd)

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
8
where Câˆž
c (Rd) denotes the set of infinitely differentiable functions with com-
pact support. Above, A
L2(P ) denotes the closure of a subset A in the Hilbert
space L2(P). Given two probability measures P and Q, a geodesic is any curve
{Î³P â†’Q
t
}tâˆˆ[0,1] with endpoints Î³P â†’Q
0
= P and Î³P â†’Q
1
= Q with minimal velocity,
i.e., any element of
arg min
Z 1
0
âˆ¥vtâˆ¥2
L2(Î³t)dt :
âˆ‚tÎ³t + div(vtÎ³t) = 0, Î³0 = P and Î³1 = Q

.
(3.4)
If P belongs to Pa.c
2 (Rd), there exists a unique geodesic given by the relation
Î³P â†’Q
t
= ((1 âˆ’t)I + tTP,Q)#P.
Its velocity field at t = 0 is vP â†’Q
0
= TP,Q âˆ’I and the Riemannian inner product
in TP (P2(Rd)) is âŸ¨Â·, Â·âŸ©L2(P ). Therefore, the WSD of a probability measure Q âˆˆ
Pa.c.
2
(Rd) with respect to a a probability measure P over P2(Rd) is defined as
SD(Q; P) := 1 âˆ’
EP âˆ¼P
"
vQâ†’P
0
âˆ¥vQâ†’P
0
âˆ¥L2(Q)
#
L2(Q)
.
Since vQâ†’P
0
= TQ,P âˆ’I we get the following definition of spatial depth.
Definition 3.1. The Wasserstein spatial depth of a distribution Q âˆˆPa.c
2 (Rd)
with respect to a distribution of distributions P âˆˆP(P2(Rd)) is defined as
SD(Q; P) := 1 âˆ’
 Z EP âˆ¼P
x âˆ’TQ,P (x)
W2(P, Q)

2
dQ(x)
! 1
2
.
When PP âˆ¼P(W2(P, Q) = 0) Ì¸= 0, we set xâˆ’TQ,P (x)
W2(P,Q)
= 0 for all x when W2(P, Q) =
0.
Note that the definition of SD(Q; P) is focused on absolutely continuous
distributions Q, while the distributions that P samples can be arbitrary (for
instance, absolutely continuous, discrete, or a mixture of both). We also refer
to the discussion in Section 10 on this point.
4. Examples
In this section we give several examples where the WSD can be computed ex-
plicitly.
4.1. Univariate case
In the case of univariate distributions, WSD reduces to quantile spatial depth.
The univariate Wasserstein distance has a flat structure since there is an iso-
metric homeomorphism between distributions and the corresponding generalized

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
9
quantile functions. Consequently, the Wasserstein distance between univariate
distributions P and Q has a simple form
W2
2(P, Q) =
Z 1
0
(F âˆ’1
P (u) âˆ’F âˆ’1
Q (u))2du,
where F âˆ’1
P (u) = inf{x âˆˆR : u â‰¤P((âˆ’âˆž, x])}. Moreover, the univariate case
is the unique case where the composition of optimal transport maps (here non-
decreasing functions) is still an optimal transport map. Therefore, the spatial
depth is just
SD(Q; P) := 1 âˆ’
ï£«
ï£¬
ï£­
Z 1
0
ï£«
ï£¬
ï£­EP âˆ¼P
ï£®
ï£¯ï£°
F âˆ’1
P (u) âˆ’F âˆ’1
Q (u)
R 1
0 (F âˆ’1
P (u) âˆ’F âˆ’1
Q (u))2du
 1
2
ï£¹
ï£ºï£»
ï£¶
ï£·
ï£¸
2
du
ï£¶
ï£·
ï£¸
1
2
,
which in short notation stands
SD(Q; P) := 1 âˆ’

EP âˆ¼P
ï£®
ï£¯ï£°
F âˆ’1
P
âˆ’F âˆ’1
Q
F âˆ’1
P (u) âˆ’F âˆ’1
Q

L2([0,1])
ï£¹
ï£ºï£»

L2([0,1])
,
which is the spatial depth of the quantile functions in the Hilbert space L2([0, 1])
(see [51, 56, 59]).
4.2. Location families
Consider that P is supported on a location family, a set of shifted distribu-
tions indexed by the location parameter Î¸. In this case, P coincides with the
distribution of the location parameter. And the WSD reduces to
SD(Q; P) = 1 âˆ’
EP âˆ¼P
 
Î¸P âˆ’Î¸Q
âˆ¥Î¸P âˆ’Î¸Qâˆ¥
! = 1 âˆ’
EÎ¸
 
Î¸ âˆ’Î¸Q
âˆ¥Î¸ âˆ’Î¸Qâˆ¥
! ,
(4.1)
which is the Euclidean spatial depth of Î¸Q with respect to the distribution of
Î¸. This also includes the Gaussian location family (see below).
4.3. Gaussian families
It is well known that the optimal transport problem between Gaussian prob-
ability measures admits a closed form (see [17]). In particular if Q and P are
non degenerated Gaussian with means ÂµQ and ÂµP and (invertible) covariance
matrices Î£Q and Î£P , respectively, the optimal transport map TQ,P is
ÂµP + AQ,P (x âˆ’ÂµQ)

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
10
with
AQ,P = Î£
âˆ’1
2
Q

Î£
1
2
QÎ£P Î£
1
2
Q
 1
2 Î£
âˆ’1
2
Q .
Therefore, if supp(P) is a set of Gaussian probability measures and Q is a non-
degenerated Gaussian, then the WSD can be equivalently formulated as
SD(Q; P) =
1âˆ’
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Z

EP âˆ¼P
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°
x âˆ’ÂµP âˆ’AQ,P (x âˆ’ÂµQ)

âˆ¥ÂµP âˆ’ÂµQâˆ¥2 + Tr

Î£P + Î£Q âˆ’2

Î£
1
2
P Î£QÎ£
1
2
P
 1
2  1
2
ï£¹
ï£ºï£ºï£ºï£ºï£»

2
dQ(x)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
1
2
.
In the special case of a common Î£ for all P âˆˆsupp(P), and when Î£Q = Î£, the
above formula reduces to
SD(Q; P) = 1 âˆ’
EP âˆ¼P
 
ÂµP âˆ’ÂµQ
âˆ¥ÂµP âˆ’ÂµQâˆ¥
! ,
which is the Euclidean spatial depth function of ÂµQ. When P = 1
n
Pn
i=1 Î´Pi, we
obtain
SD(Q; P) :=
1âˆ’
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Z

1
n
n
X
i=1
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°
x âˆ’ÂµPi âˆ’AQ,Pi(x âˆ’ÂµQ)

âˆ¥ÂµPi âˆ’ÂµQâˆ¥2 + Tr

Î£Pi + Î£Q âˆ’2

Î£
1
2
PiÎ£QÎ£
1
2
Pi
 1
2  1
2
ï£¹
ï£ºï£ºï£ºï£ºï£»

2
dQ(x)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
1
2
.
Furthermore, if Î£Pi = Î£Q for all i = 1, . . . , n, the WSD is
1 âˆ’
ï£«
ï£­
Z 
1
n
n
X
i=1
ÂµPi âˆ’ÂµQ
âˆ¥ÂµPi âˆ’ÂµQâˆ¥

2
dQ(x)
ï£¶
ï£¸
1
2
= 1 âˆ’

1
n
n
X
i=1
ÂµPi âˆ’ÂµQ
âˆ¥ÂµPi âˆ’ÂµQâˆ¥
 .
5. Properties of Wasserstein spatial depth
Zuo and Serfling postulated in [64] the main four properties that a data depth
should satisfy in Euclidean spaces. Those properties are affine invariance, mean-
ing that the data depth function is invariant to affine transformations; center-
outward monotonicity, meaning that the depth function decreases along rays
arising from the deepest point; vanishing at infinity, meaning that the depth
function tends to 0 as the distance to the deepest point tends to infinity; maxi-
mality at the center, meaning that for elliptic distributions, its geometric center

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
11
is the unique deepest point. The Euclidean spatial depth satisfies some of these
properties. In particular, it is invariant to isometric transformations, it van-
ishes at infinity and, if the spatial median is unique it is the unique maximizer
of the spatial depth. As Rd is trivially embedded on P2(Rd) by means of the
mapping x 7â†’Î´x, we cannot expect better properties for the Wasserstein space
adaptation.
5.1. General properties
In this section we prove that the WSD shares the main properties of the Eu-
clidean spatial depth, i.e., it belongs to the interval [0, 1], it decreases at infinity
and it is transformation invariant.
Theorem 5.1. Set P âˆˆP(P2(Rd)). Then the following properties hold:
1. (Values in [0, 1].) SD(Q; P) âˆˆ[0, 1] for all Q âˆˆPa.c
2 (Rd).
2. (Transformation invariance.) Assume that d â‰¥2. Then for any isometry
F : P2(Rd) â†’P2(Rd), it holds that
SD(F(Q); F#P) = SD(Q; P),
for all Q âˆˆPa.c
2 (Rd).
3. (Vanishing at infinity.) Let {Qn}nâˆˆN âŠ‚Pa.c
2 (Rd) be a sequence such that
W2(Qn, Q) â†’+âˆž, for one Q âˆˆP2(Rd), then SD(Qn; P) â†’0.
Recall from [5] that there are tree types of isometries in (P2(Rd), W2). Let
F : P2(Rd) â†’P2(Rd) be an isometry, i.e.,
W2(F(P), F(Q)) = W2(P, Q)
for all P, Q âˆˆP2(Rd).
Then F is called trivial if there exists an isometry f : Rd â†’Rd such that
F(P) = f#P for all P âˆˆP2(Rd); F is said to preserve shapes if for all P âˆˆ
P2(Rd) there exists an isometry f = fP : Rd â†’Rd such that F(P) = f#P; and
if F does not preserve shapes, it is said to be exotic. An example of nontrivial
isometry on (P2(Rd), W2) that preserves shapes is given by the mapping Î¦(Ï†) :
P 7â†’Î¦(Ï†)(P) where Ï† : Rd â†’Rd is a linear isometry and Î¦(Ï†)(P) is the law
of the random variable
Ï†(X âˆ’E[X]) + E[X],
for X âˆ¼P.
Theorems 1.1 and 1.2 in [5] prove that (P2(Rd), W2) admits exotic isometries
if and only if d = 1, which is the reason for which the invariance of the WSD
holds for d â‰¥2.
5.2. Maximality at the center
The set of spatial medians of P is defined as
arg min
QâˆˆP2(Rd)
EP âˆ¼P[W2(P, Q)].

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
12
The following result shows that, under some assumptions, the set of spatial
medians which are absolutely continuous with respect to Lebesgue measure has
maximum depth.
Theorem 5.2. Set P âˆˆP(P2(K)) for a compact set K âŠ‚Rd. Assume that P
is supported on a finite set P1, . . . , Pn. Then any
Q âˆˆPa.c
2 (K) âˆ©arg min
Qâ€²âˆˆP(K)
EP âˆ¼P[W2(P, Qâ€²)]
such that Q Ì¸= Pi for all i = 1, . . . , n satisfies SD(Q; P) = 1.
Remark 5.3. We do not know if the set of spatial medians which are absolutely
continuous with respect to Lebesgue measure is nonempty. It is known that,
under the setting of Theorem 5.2, if we assume that Pi âˆˆPa.c
2 (K), the set of
geometric means (or barycenters) is a singleton and its unique element belongs
to Pa.c
2 (K) (see [65]). However, the proof of [65], based on a fixed point argument
which exploits the strict convexity of the squared Wasserstein distance, does not
apply to our setting.
5.3. Continuity
In this section we investigate some topological properties of the WSD. We ana-
lyze separately the function Q 7â†’SD(Q; P) and P 7â†’SD(Q; P). The following
result shows that the function Pa.c.
2
(Rd) âˆ‹Q 7â†’SD(Q; P) is continuous.
Theorem 5.4. Let P âˆˆP(P2(Rd)) be atomless and {Qn}nâˆˆN âŠ‚Pa.c.
2
(Rd) be a
sequence such that W2(Qn, Q) â†’0 for some Q âˆˆPa.c.
2
(Rd). Then
lim
nâ†’âˆžSD(Qn; P) = SD(Q; P).
Next we show the continuity of P 7â†’SD(Q; P) for fixed Q. As an intermediate
step we need to show that for each Q âˆˆPa.c.
2
(Rd) the function
T Q : P2(Rd) âˆ‹P 7â†’TQ,P âˆˆL2(Q)
is continuous. Recall that TQ,P is the optimal transport map from Q to P.
Lemma 5.5 (Continuity of T Q). Set Q âˆˆPa.c
2 (Rd). Let {Pn}n âŠ‚P2(Rd) be a
sequence of probability measures such that W2(Pn, P) â†’0 for some P âˆˆP2(Rd).
Then
âˆ¥TQ,Pn âˆ’TQ,P âˆ¥L2(Q) â†’0.
In words, T Q : P2(Rd) â†’L2(Q) is continuous.
Fix Q âˆˆPa.c
2 (Rd). Lemma 5.5 implies that the function
P2(Rd) âˆ‹P 7â†’I âˆ’TQ,P
W2(P, Q) âˆˆL2(Q)
is continuous around all P Ì¸= Q. This observation enables to derive the conti-
nuity of the function P(P2(Rd)) âˆ‹P 7â†’SD(Q; P) around atomless probability
measures.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
13
Theorem 5.6. Let P âˆˆP(P2(Rd)) be atomless and let Q âˆˆPa.c
2 (Rd). Then
lim
nâ†’âˆžSD(Q; Pn) = SD(Q; P)
for every sequence {Pn}nâˆˆN âŠ‚P(P2(Rd)) such that Pn â†’P weakly in P(P2(Rd)).
6. Consistent estimation
In practice, we only observe sample datasets instead of knowing the true P
or even the true P1, P2, . . . , Pn âˆ¼P. Two common scenarios in the literature
of distributional data learning [3, 41, 53] will be considered, namely, one-stage
sampling model and two-stage sampling model. One-stage sampling model as-
sumes the observation of an i.i.d. sample P1, . . . , Pn of P. Two-stage sampling
model assumes the observation of a data array
ï£«
ï£¬
ï£­
X1,1
. . .
X1,m
...
...
...
Xn,1
. . .
Xn,m
ï£¶
ï£·
ï£¸,
(6.1)
where Xi,1, . . . , Xi,m âˆˆRd is an i.i.d. sample from the distribution Pi for each
i = 1, . . . , n, and P1, . . . , Pn are i.i.d. drawn from P. The difference between the
two models is that the sampled distributions P1, . . . , Pn are known in one-stage
sampling model, but unknown and to be estimated by the empirical distributions
in two-stage sampling model.
In each scenario, we give the empirical counterpart to the population WSD
in Definition 3.1. We also establish a point-wise central limit theorem for the
empirical WSD under the one-stage sampling model and a consistency result for
the two-stage sampling model.
6.1. One-stage sampling
We describe the asymptotic behavior of the empirical WSD
SD(Q; Pn) := 1 âˆ’
ï£«
ï£­
Z 
1
n
n
X
i=1
x âˆ’TQ,Pi(x)
W2(Q, Pi)

2
dQ(x)
ï£¶
ï£¸
1
2
,
Pn = 1
n
n
X
i=1
Î´Pi,
(6.2)
where Pn is the empirical counterpart to P. The WSD is associated with the
spatial distribution process
SP : Pa.c
2 (Rd) âˆ‹Q 7â†’SP,Q = EP âˆ¼P
 I âˆ’TQ,P
W2(P, Q)

âˆˆL2(Q).
The representation
SP,Q = EP âˆ¼P

I âˆ’TQ,P
âˆ¥I âˆ’TQ,P âˆ¥L2(Q)


Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
14
allows to use standard techniques to obtain the point-wise strong law of large
numbers and a central limit theorem for the empirical spatial distribution pro-
cess (SPn,Q âˆ’SP,Q) âˆˆL2(Q), after showing that the random function TQ,P in
L2(Q) is tight if P âˆ¼P for a tight P. In other words, we need that for each
Q âˆˆPa.c.
2
(Rd) the function
T Q : P2(Rd) âˆ‹P 7â†’TQ,P âˆˆL2(Q)
pushes forward tight probability measures over P2(Rd) to tight probabilities in
L2(Q). Recall that a probability measure Âµ âˆˆP(X) over a separable topological
space (X, d) is said to be tight if for every Ïµ > 0 there exists a compact (in the
metric topology) set K such that Âµ(K) â‰¥1 âˆ’Ïµ. A random variable is tight if
its distribution is tight. Therefore, Lemma 5.5 implies that if P âˆ¼P is tight
in P2(Rd), then TQ,P is tight in L2(Q). As a consequence, if P âˆˆP(P2(Rd)),
then
n
Iâˆ’TQ,Pi
âˆ¥Iâˆ’TQ,Piâˆ¥L2(Q)
on
i=1 is an i.i.d. sequence of tight random elements in the
separable Hilbert space L2(Q), with finite second order moments. The strong
law of large numbers and the central limit theorem in separable Hilbert spaces
(cf. [33, Corollary 10.9]) yield the following result. Note that a random element
Z of a Hilbert space H is defined to be Gaussian when h(Z) follows a (univariate)
Gaussian distribution for all linear continuous mappings h : H â†’R.
Theorem 6.1. Set P âˆˆP(P2(Rd)), Q âˆˆPa.c
2 (Rd). Then
âˆ¥SPn,Q âˆ’SP,Qâˆ¥L2(Q)
a.s
âˆ’âˆ’â†’0
and
âˆšn(SPn,Q âˆ’SP,Q)
P(L2(Q))
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’GP,Q,
for some centered Gaussian element GP,Q âˆˆL2(Q). As a consequence, it holds
that
SD(Q; Pn)
a.s
âˆ’âˆ’â†’SD(Q; P)
and, if SD(Q; P) < 1, also
âˆšn(SD(Q; Pn) âˆ’SD(Q; P))
P(R)
âˆ’âˆ’âˆ’â†’âŸ¨GP,Q, SP,QâŸ©L2(Q)
SD(Q; P) âˆ’1
.
The last two statements of Theorem 6.1 are a mere application of the delta
method.
6.2. Two-stage sampling
Now we deal with the scenario where only the data array (6.1) is available. Recall
that, in this case, the i.i.d. samples P1, . . . , Pn of P are no longer observed but a
sample {Xi,j}n,m
i,j=1 is, where Xi,j âˆ¼Pi for j âˆˆ{1, . . . , m} and each i âˆˆ{1, . . . , n}
. We denote
Pn,m := 1
n
n
X
i=1
Î´Pi,m,
with
Pi,m := 1
m
m
X
j=1
Î´Xi,j
for each
i âˆˆ{1, . . . , n}.
(6.3)

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
15
Correspondingly, the empirical WSD is formulated as
SD(Qm; Pn,m) = 1 âˆ’
v
u
u
t 1
m
m
X
j=1

1
n
n
X
i=1
Xq,j âˆ’TQm,Pi,m(Xq,j)
W2(Qm, Pi,m)

2
,
where Qm =
1
m
Pm
j=1 Î´Xq,j with Xq,1, . . . , Xq,m
i.i.d.
âˆ¼
Q, and the convention
0
0 = 0 remains. Now we show that, as n, m â†’âˆž, Pn,m converges in probability
in P(Pp(Rd)) for all p â‰¥1. We endow P(Pp(Rd)) with the metric
dBL(p)(P, Q)
= sup
 Z
f(P)d(Pâˆ’Q)(P) : |f(P)| â‰¤1 and |f(P)âˆ’f(Q)| â‰¤Wp(P, Q), âˆ€P, Q âˆˆPp(Rd)

.
Lemma 6.2. Let {Pn,m}nâˆˆN be as in (6.3) where m = m(n) is such that
m â†’âˆžas n â†’âˆž. Assume that P âˆˆP(Pp(Rd)) for p â‰¥1. Then
E[dBL(p)(Pn,m, P)] âˆ’â†’0
as n â†’âˆž.
A combination of Lemma 6.2, Theorem 5.6 and the continuous mapping theo-
rem yields the following consistency result for the two-stage sampling estimator.
Theorem 6.3. Set P âˆˆP(P2(Rd)) be atomless. Let {Pn,m}nâˆˆN be as in (6.3)
where m = m(n) is such that m â†’âˆžas n â†’âˆž. Then, for every Q âˆˆPa.c
2 (Rd),
SD(Q; Pn,m)
P
âˆ’â†’SD(Q; P)
as n â†’âˆž.
Theorems 6.1 and 6.3 state that the empirical WSD converges to the pop-
ulation version asymptotically. Given enough sample size, the empirical WSD
is informative of the truth and has practical values. The simulation results in
Section 8.1 also verify the above theorems.
7. Comparison with other possible depth notions
In the field of nonparametric statistics, the concept of depth presents significant
challenges when attempting generalization to non-Euclidean spaces, a topic that
has garnered considerable attention in advanced statistical research. Within
the confines of linear functional spaces, such as Banach or Hilbert spaces, the
application of Euclidean methodologies remains largely successful, attributed
primarily to their inherent vectorial structures. Contrastingly, the landscape
becomes markedly more complex when venturing into the domain of infinite-
dimensional spaces devoid of a vectorial framework.
The statistical literature identifies a mere trio of propositions capable of ad-
dressing this complexity: lens depth, Tukey depth, and a novel approach of
metric spatial depth, different from our proposal. Here we delve into a metic-
ulous exploration of these methodologies, with a particular emphasis on their
adaptability to Wasserstein space framework.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
16
We shall demonstrate that these methodologies do not possess all the favor-
able theoretical and computational properties that we have established for the
WSD. The WSD is thus most beneficial in broad, complex statistical contexts,
thereby yielding a significant advancement in the field of machine learning and
statistical analysis.
7.1. Tukey depth
The next definition is a natural adaptation of the metric Tukey (or halfspace)
depth proposed by Dai and Lopez-Pintado [22] to the Wasserstein space.
Definition 7.1 (Adapted from [22]). The Wasserstein halfspace depth of a
distribution Q âˆˆP2(Rd) with respect to a probability measure P âˆˆP(P2(Rd)) is
the value
HSD(Q; P) =
inf
P1,P2âˆˆP2(Rd)
W2(Q,P1)â‰¤W2(Q,P2)
PP âˆ¼P(W2(P, P1) â‰¤W2(P, P2)).
According to [22], the Wasserstein halphspace depth is transformation in-
variant and vanishes at infinity. Moreover, center-outward monotonicity (the
function t 7â†’HSD(Î³(t); P) is monotone decreasing for any geodesic Î³(t) with
HSD(Î³(0); P) = 1/2) holds if for any constant speed geodesic Î³ of (P2(Rd), W2),
the following geometric condition holds:
there exists t âˆˆ[0, 1] such that W2
2(Î³(t), P) â‰¤W2
2(Î³(t), Q)
=â‡’
 W2
2(Î³(0), P) â‰¤W2
2(Î³(0), Q)

or
 W2
2(Î³(1), P) â‰¤W2
2(Î³(1), Q)

.
(7.1)
Recall (Section 3.2) that a constant speed geodesic in a metric space (M, d) is
a curve Î³ : [0, 1] â†’M such that d(Î³(t), Î³(s)) = |t âˆ’s|d(Î³(0), Î³(1)) for all s, t âˆˆ
[0, 1]. In (P2(Rd), W2), a constant speed geodesic corresponds to interpolations
obtained from optimal transport plans [1, p. 158]. More precisely, any constant
speed geodesic connecting two absolutely continuous distributions P1 and P2 is
of the form Î³(t) = ((1 âˆ’t)I + tTP1,P2)#P1, where TP1,P2 is the unique optimal
map pushing P1 to P2 (see also Section 3.3).
Center-outward monotonicity is widely regarded as a favorable attribute
within the scope of statistical depth measures. However, it is an attribute not
typically anticipated in the context of spatial depths, particularly within Eu-
clidean spaces. Notably, neither the transport-based depth nor the lens depth
exhibit this property. Moreover, for the Wasserstein halphspace depth it is not
clear if the geometric condition (7.1), and a fortiori the center-outward mono-
tonicity, holds in general.
Despite the ostensibly advantageous traits of Tukey depths, they are en-
cumbered by significant computational demands, particularly evident as the
dimensionality of the data increases. This computational intensity escalates to
the point of impracticality for exact calculations in dimensions exceeding five,
already in the Euclidean case. Within the confines of Wasserstein spaces, which
are characterized by infinite dimensions, approximating Tukey depths poses a
substantial challenge, much more so than for the WSD.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
17
7.2. Lens depth
Let us now turn our attention to the adaptation of the metric lens depth, pre-
sented by Geenens, Nieto-Reyes and Francisci in [28], to the Wasserstein space.
Definition 7.2 (Adapted from [28]). The Wasserstein lens depth of a distribu-
tion Q âˆˆP2(Rd) with respect to P âˆˆP(P2(Rd)) is defined as
LD(Q; P) = P(P â€²,P )âˆ¼PâŠ—P[W2(P, P â€²) â‰¥max(W2(P, Q), W2(P â€², Q))].
The Wasserstein lens depth is transformation invariant and vanishes at in-
finity. The two-stage plug-in estimator of LD(Q; P) can be computed exactly
for a discrete distribution Q within polynomial time. Nevertheless, as indicated
in [28], the lens depth fails to exhibit center-outward monotonicity in the linear
case. Similarly, this property would possibly be absent in Wasserstein spaces.
7.3. Metric spatial Wasserstein depth
Virta [59] gave a definition of spatial depth for general metric spaces that does
not agree with our definition in the particular case of Wasserstein space. To
avoid confusion in terminology, the proposal from [59] will be referred to as
metric spatial Wasserstein depth.
Definition 7.3 (Adapted from [59]). The metric spatial Wasserstein depth of
Q âˆˆP2(Rd) with respect to P âˆˆP(P2(Rd)) is defined as
MSD(Q; P) = 1 âˆ’1
2E(P â€²,P )âˆ¼PâŠ—P
W2
2(P, Q) + W2
2(P â€², Q) âˆ’W2
2(P, P â€²)
W2(Q, P)W2(Q, P â€²)

.
The function Q 7â†’MSD(Q; P) takes values in the interval [0, 2]. It is trans-
formation invariant and vanishing at infinity. The metric spatial depth presents
a remarkably viable and effective solution that is widely applicable to general
metric spaces. Nevertheless, when specialized to the Wasserstein space, it falls
short of fulfilling all the desirable properties that we have established for the
WSD. In particular, also pointed out in [59], the question of the inclusion of spa-
tial medians within the set of deepest points in terms of the metric spatial depth
remains overall open. Note that taking a directional derivative of MSD(Q; P)
with respect to Q, in the aim of studying deepest points, does not seem par-
ticularly fruitful. This leads us to conjecture that, in general, spatial medians
have no relation to the maximizers of MSD(Q; P). In contrast, our Theorem 5.2
establishes that spatial medians maximize the WSD, in more general situations.
Another natural question remaining overall open in [59] is whether the max-
imal possible value 2 for the metric spatial depth can be reached. In particular
Theorem 3 there states that this value 2 is attained by any non-atomic Q such
that, given two independent realizations P1 and P2 from P, Q always falls be-
tween these two points on a geodesic going through all three. As noted in [59],
this condition is very strict and only satisfied in arguably pathological cases.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
18
8. Numerical simulations
In this section, we carry out extensive numerical simulations to validate our
notion of Wasserstein spatial depth and support its theoretical properties and
practical utility. Specifically, we confirm the consistency of the empirical WSD,
examine its relationship with conventional spatial depth in certain cases, eval-
uate its effectiveness in outlier detection and show its benefit compared to ap-
plying functional depths to distributions. Throughout this section, we use the R
package transport to compute all the Wasserstein distances and optimal trans-
port maps from data clouds. Based on the two stage sampling model in (6.1), the
empirical WSDs are calculated via the formula below. For Qm = 1
m
Pm
j=1 Î´Xq,j
with Xq,1, . . . , Xq,m
i.i.d.
âˆ¼
Q,
SD(Qm; Pn,m) = 1 âˆ’
v
u
u
t 1
m
m
X
j=1

1
nq
X
iÌ¸=q
Xq,j âˆ’TQm,Pi,m(Xq,j)
W2(Qm, Pi,m)

2
,
(8.1)
where Qm could be outside (with the convention q = n + 1 and nq = n) or
within (with the convention q âˆˆ{1, . . . , n} and nq = n âˆ’1) the sampled dis-
tributions {P1,m, . . . , Pn,m}, and where we recall the convention 0/0 = 0. The
code for all simulations is publicly available at https://github.com/YishaYao/
Wasserstein-Spatial-Depth/tree/main.
Since computing the optimal transport map between any pair of empirical dis-
tributions costs O(m2) [49], and once the optimal transport map between a pair
of empirical distributions is available, the corresponding Wasserstein distance
immediately follows with almost zero extra cost, the computational complexity
of SD(Qm, P n,m) is of order O(nm2).
8.1. Consistency of the empirical Wasserstein spatial depth
The simulation results below support the theoretical results in Section 6. That
is, the empirical WSD, formulated in (8.1), is close to the theoretical value
SD(Q; P) in Definition 3.1. Hence, the WSD can be inferred accurately from
sample data and has practical value. Four cases are considered and described
below.
â€¢ Case 1: P is supported on a family of exponential distributions indexed by
the rate parameter Î» which follows a Beta(2, 2) distribution. The theoret-
ical WSD of the exponential distribution with rate parameter Î»Q âˆˆ(0, 1],

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
19
denoted as exp(Î»Q), with respect to P is
SD(Q; P) = 1 âˆ’
sZ âˆž
0

EÎ»âˆ¼Beta(2,2)
x âˆ’(Î»Q/Î»)x
W2(FÎ»Q, FÎ»)
2
Î»Qeâˆ’Î»Qxdx
= 1 âˆ’
sZ âˆž
0
Î»2
Qx2
2

EÎ»âˆ¼Beta(2,2)
1/Î»Q âˆ’1/Î»
|1/Î»Q âˆ’1/Î»|
2
Î»Qeâˆ’Î»Qxdx
= 1 âˆ’
sZ âˆž
0
Î»2
Q
2

4Î»3
Q âˆ’6Î»2
Q + 1
2
x2Î»Qeâˆ’Î»Qxdx
= 1 âˆ’
1 + 4Î»3
Q âˆ’6Î»2
Q
,
where FÎ» is the CDF of the exponential distribution with rate parameter
Î», the optimal map from exp(Î»Q) to exp(Î») is
TÎ»Q,Î»(x) = F âˆ’1
Î»
â—¦FÎ»Q(x) = Î»Qx
Î» ,
and W2(FÎ»Q, FÎ») is derived by
W2(FÎ»Q, FÎ») =
sZ âˆž
0

x âˆ’(Î»Q/Î»)x
2
Î»Qeâˆ’Î»Qxdx =
âˆš
2
 1
Î»Q
âˆ’1
Î»
.
Note that the WSD is equal to 1 (maximal) for Î»Q = 1/2 which is the
mean of the Beta(2, 2) distribution.
â€¢ Case 2: P is supported on a family of Weibull distributions with fixed
scale parameter 1 and varying shape parameter k. This family of Weibull
distributions is indexed by the shape parameter k which takes value either
1 or 2 with equal probabilities, i.e., k âˆ¼Unif({1, 2}). Let Q be the Weibull
distribution with shape parameter kQ (kQ equating either 1 or 2). Its
theoretical WSD with respect to P is
SD(Q; P) = 1 âˆ’
sZ âˆž
0

Ekâˆ¼Unif({1,2})
x âˆ’xkQ/k
W2(kQ, k)
2
kQxkQâˆ’1eâˆ’xkQ dx
= 1 âˆ’
sZ âˆž
0
 x âˆ’xkQ/kQ
2W2(kQ, kQ)
2
kQxkQâˆ’1eâˆ’xkQ dx
= 1 âˆ’
1
2W2(kQ, kQ)
sZ âˆž
0
 x âˆ’xkQ/kQ2kQxkQâˆ’1eâˆ’xkQ dx
= 1/2,
where the optimal map from Weibull(kQ) to Weibull(k) is
TkQ,k(x) = F âˆ’1
k
â—¦FkQ(x) = xkQ/k,

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
20
using kQ = 3 âˆ’kQ, the convention 0/0 = 0, and where W2(kQ, kQ) is
derived by
W2(kQ, kQ) =
sZ âˆž
0
 x âˆ’xkQ/kQ2kQxkQâˆ’1eâˆ’xkQ dx.
â€¢ Case 3: P is supported on a family of isotropic bivariate Gaussian dis-
tributions with varying centers. The distribution of the Gaussian cen-
ters is supported on four points {Âµ1 = (1, 0)âŠ¤, Âµ2 = (âˆ’1, 0)âŠ¤, Âµ3 =
(0, 1)âŠ¤, Âµ4 = (0, âˆ’1)âŠ¤} with equal probabilities 1/4. Let Q be N(Âµq, I)
for q âˆˆ{1, . . . , 4}. The theoretical WSD is computed as, see Section 4.3,
SD(Q; P) = 1 âˆ’

1
4
X
kÌ¸=q
Âµq âˆ’Âµk
âˆ¥Âµq âˆ’Âµkâˆ¥
 = 3 âˆ’
âˆš
2
4
.
â€¢ Case 4: P is supported on a family of bivariate uniform distributions
Unif
 [0, c]2
with c âˆ¼Unif([1, 2]). Let Q be Unif
 [0, cq]2
. Its theoreti-
cal WSD with respect to P is
SD(Q; P) = 1 âˆ’
sZ
[0,cq]2
Ecâˆ¼Unif([1,2])
x âˆ’(c/cq)x
W2(cq, c)

2 1
c2q
dx
= 1 âˆ’
sZ
[0,cq]2

p
3/2xEcâˆ¼Unif([1,2])
1 âˆ’(c/cq)
|cq âˆ’c|

2 1
c2q
dx
= 1 âˆ’
s
3(2 âˆ’3/cq)2/2
Z
[0,cq]2 âˆ¥xâˆ¥2 1
c2q
dx
= 1 âˆ’|2cq âˆ’3|,
where the optimal map from Unif
 [0, cq]2
to Unif
 [0, c]2
is the dilation
Tcq,c(x) = c
cq
x,
and W2(cq, c) is computed as
W2(cq, c) =
sZ
[0,cq]2 âˆ¥x âˆ’(c/cq)xâˆ¥2(1/c2q)dx =
p
2/3|cq âˆ’c|.
For each of the above cases, we repeat independently the following experiment
for 100 times: generate the data array X via the two-stage sampling procedure
in Section 6.2; then compute the empirical WSDs of the sampled distributions;
finally, compare the theoretical WSD and the ensemble of 100 empirical WSDs.
We choose m = 1000, n = 2000. As shown in Figure 1, the empirical estimates
are gathering tightly around the corresponding theoretical values.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
21
Fig 1: The green solid lines depict the change of theoretical WSD along the
parameter indexing P. The black circles represent the distribution of empirical
WSDs, with error bars indicating one standard deviation above and below the
mean.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
22
8.2. Wasserstein spatial depth vs. conventional spatial depth
As discussed in Section 4, when P is supported on a location family, the WSD
coincides with the spatial depth of the location parameter. We verify the equiv-
alence between WSD and spatial depth in the four cases described below.
â€¢ Case 1: P is supported on a set of d = 10-dimensional Gaussian distribu-
tions with identity covariance matrix. The Gaussian centers are i.i.d. from
Unif([âˆ’2, 2]d).
â€¢ Case 2: P is supported on a set of d = 10-dimensional Gaussian distribu-
tions with a common covariance matrix. The common covariance matrix
is chosen as Î£i,j = 0.2|iâˆ’j|. The Gaussian centers are drawn in the same
way as in Case 1.
â€¢ Case 3: The support of P is a set of uniform distributions on d = 10-
dimensional unit cubes with varying centers. The centers of the cubes are
identically independently drawn from N(0, I).
â€¢ Case 4: P is supported on a set of univariate double exponential distribu-
tions with fixed rate equaling 1 and varying locations. The locations are
identically independently drawn from N(0, 1).
The simulation procedure is as follows. First, n = 500 distributions are drawn
as described above in each case. Second, m = 500 data points are randomly
drawn for each sampled distribution. Third, the empirical WSD of each empirical
distribution is computed according to (8.1), and the empirical spatial depths of
the locations are computed as in (4.1). Finally, we check whether the empirical
WSDs and spatial depths are approximately equal. As shown in Figure 2, there
are nice equality relationships between the two depths.
8.3. Outlier detection
Like conventional statistical depth, WSD can be used to detect outlier distri-
butions. We demonstrate its utility for outlier detection in two cases. In each
case, we draw n = 500 distributions from a population P and six outlier dis-
tributions which are relatively far away from the population. For each sampled
distribution, we draw m = 500 data points. All the distributions are on Rd with
d = 10.
â€¢ Case 1: the population is a collection of Gaussian distributions with com-
mon identity covariance matrix and random centers, where the centers
follow i.i.d. N(0, I); the six outlier distributions are
N((5, . . . , 5)âŠ¤, I),
N((5, . . . , 5)âŠ¤, Î£) with Î£i,j = 0.5|iâˆ’j|,

Gamma(3, 2)
d,

Unif([âˆ’6, 6])
d,

Beta(0.1, 0.1)
d,
Multinomial
 2d, (0.25, 0.25, 0.15, 0.15, 0.15, 0.01, 0.01, 0.01, 0.01, 0.01)

.
Here for a distribution Âµ, [Âµ]d is the distribution such that for Z =
(Z1, . . . , Zd) âˆ¼[Âµ]d we have Z1, . . . , Zd âˆ¼i.i.d. Âµ.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
23
Fig 2: The relationships between the WSD and conventional spatial depth in
the four cases of Section 8.2.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
24
â€¢ Case 2: the population is a collection of uniform distributions

Unif([0, u])
d
with u âˆ¼Unif([1, 2]); the outlier distributions are
N
 (3, . . . , 3)âŠ¤, I

,
N
 (âˆ’1, . . . , âˆ’1)âŠ¤, Î£

with Î£i,j = 0.5|iâˆ’j|,

Poisson(3)
d,

Binomial(d, 0.2)
d,

Ï‡2
10
d,
Multinomial
 2d, (0.25, 0.15, 0.1, 0.1, 0.15, 0.05, 0.05, 0.05, 0.05, 0.05)

.
For each case, we repeat similar experiments for 20 times. The experimental
procedure is as follows: draw the data array X according to Case 1 or Case
2; compute their empirical WSD according to (8.1); detect the outlier distribu-
tions whose empirical WSDs are smaller than the 1% quantile of all the empirical
WSDs. In each of these 20 repetitions, all the outlier distributions can be de-
tected for both Case 1 and Case 2. Figure 3 shows the result of one experiment,
where the black and orange dots represent distributions from P and the outlier
distributions, respectively.
Fig 3: Left panel: the distributions are drawn according to Case 1. Right panel:
the distributions are drawn according to Case 2. The green dots represent regular
distributions from the population P, and the orange dots represent the outlier
distributions.
8.4. Wasserstein spatial depth vs. functional depth
Hilbertian embedding of probability measures (into a RKHS) is a powerful tech-
nique in machine learning and statistics that allows for a functional representa-
tion of probability measures [52]. This approach maps a probability distribution
Âµ âˆˆP(Rd) to an element fÂµ in the RKHS HK via kernel mean embedding,
fÂµ(t) =
Z
Rd K(x, t)dÂµ(x).

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
25
Here K : Rd Ã— Rd â†’R is a kernel on Rd, yielding the Hilbert space HK (the
RKHS) of functions from Rd to R [4]. Given this embedding machinery and an
available notion of depth for functional data [27, 38], one could first transform
a distribution into a functional data point and then compute its functional
depth. However, such an approach neglects the rich geodesic structure of the
Wasserstein space. The relative distance and â€œorderingâ€ of pairs of distributions
are probably distorted after Hilbertian embedding. The simulation results in this
subsection support the above point of view.
We consider two cases here. In each case, n = 100 similar distributions (de-
noted as regular distributions) and four exotic distributions are drawn. By â€œsim-
ilarâ€ we mean that these n distributions are of the same parametric family and
are close to each other in terms of Wasserstein distance. All the distributions
are on R3 so that visualization is possible. We draw m = 300 data points for
each distribution.
â€¢ Case 1: the regular distributions are from a collection of spherical Gaussian
distributions N(Âµ, Ïƒ2I) with varying centers Âµ
i.i.d.
âˆ¼N(0, I) and varying
variances Ïƒ
i.i.d.
âˆ¼Unif([0.8, 1]); the four exotic distributions are

Gamma(3, 2)
d,

Weibull(2, 1) Â· 3Bernoulli(âˆ’1, 1, 1/2)
d,

Unif({âˆ’3.5, âˆ’2.5, 2.5, 3.5})
d,
N((âˆ’3, 3, âˆ’3)âŠ¤, Î£) with Î£i,j = 0.5|iâˆ’j|.
â€¢ Case 2: the regular distributions are from a collection of uniform distri-
butions

Unif([0, u])
d with u âˆ¼Beta(2, 2) + 1; the exotic distributions
are

Poisson(1)
d,

Exponential(2) Â· Bernoulli(âˆ’1, 1, 1/2)
d,

Unif({1, 2, 3})
d,
Multinomial
 2d, (0.1, 0.2, 0.7)

.
Here Bernoulli(âˆ’1, 1, 1/2) means an independent Bernoulli random variable tak-
ing value âˆ’1 or +1 with probability 1/2. In each case, the regular distributions
are close to each other in the Wasserstein space because
W2

N(Âµ1, Ïƒ2
1I), N(Âµ2, Ïƒ2
2I)

=
p
âˆ¥Âµ1 âˆ’Âµ2âˆ¥2 + d(Ïƒ1 âˆ’Ïƒ2)2 â‰²1.5
âˆš
d,
W2

Unif[0, u1]
d,

Unif[0, u2]
d
=
sZ
[0,u1]d
âˆ¥x âˆ’(u2/u1)xâˆ¥2
ud
1
dx
=
p
d/3 |u2 âˆ’u1| â‰¤
p
d/3.
Also shown in Figure 4 (a) and Figure 5 (a), the regular distributions (rep-
resented by green triangles) tend to form a data cloud and are not visually
distinguishable, while the exotic distributions are visually distant to the regular
distributions.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
26
We compare the WSD and two types of functional depth, Modified Band
Depth (MBD) [38] and Functional Spatial Depth (FSD) [11] in terms of detecting
those exotic distributions. To compute the functional depth of a distribution,
we first embed the distribution into a RKHS via a Gaussian kernel, and then
compute the functional depth of the embedded function. The MBD and FSD are
computed, respectively, by the R packages depthTools and fda.usc. As shown
in Figures 4 and 5, the WSD is able to detect the exotic distributions in both
cases, while the functional depths are not informative on the â€œorderingâ€ of the
distributions. This numerical experiment shows the superiority of the proposed
WSD when applied to distribution-valued data objects, which is expected since
the WSD is specially designed for distribution-valued data objects and adapts
to the geometry of the Wasserstein space.
9. Application
Nowadays, climate change is a major concern across the society. Considerable
amount of information can be extracted from longitudinal series of daily tem-
peratures. We apply the notion of WSD to explore a dataset recording European
daily temperatures during the past two centuries.
The data is collected from the public database â€œEuropean Climate Assess-
ment and Datasetâ€1. It contains the daily average temperatures collected at
40 meteorological stations located across Europe, including Austria, Croatia,
Czech Republic, Denmark, Finland, Germany, Sweden, and United Kingdom,
from year 1874 to 2023. These 40 meteorological stations cover a broad range
of Europe and are representative of the region. The goal is to explore the tem-
perature change over the years.
We consider monthly temperatures obtained by averaging daily temperatures
per month. For each weather station, we obtain a 12 monthly-average tempera-
ture curve, represented by a vector in R12. Hence, the monthly temperatures of
each year correspond to one distribution on R12. For a particular year, the 12
monthly temperatures (forming a vector in R12) collected at each station act as
a sample point drawn from this distribution. Finally, we gather 150 distributions
(from year 1874 to year 2023) with each distribution associated to 40 sample
points (for the 40 meteorological stations), and where each sample point is a
12-dimensional real vector. In the following we assume that the distributions
are drawn each year independently.
Contrary to other work, we do not consider the annual evolution of the tem-
peratures for a particular place but rather analyze the different temperature
curves at all locations at the same time. We aim at understanding weather
change at a global scale by considering the 40 different locations as representa-
tives of the European climate.
Within this framework, we compute the empirical WSDs of these 150 distri-
butions as in (8.1). Several outlier years are identified based on their excessively
1https://www.ecad.eu/dailydata/index.php.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
27
Fig 4: (a): The data points are drawn from the distributions of Case 1. The
green triangles represent data points from the regular distributions, while orange
triangles represent data points from the exotic distributions. (b): The green dots
represent the WSD values of the regular distributions, while the orange dots
represent the WSD values of the exotic distributions. (c): Each dot represents
the MBD of a distribution. The coloring pattern is the same as before. (d):
Each dot represents the FSD of a distribution. The coloring pattern remains
the same.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
28
Fig 5: (a): The data points are drawn from the distributions of Case 2. The
green triangles represent data points from the regular distributions, while orange
triangles represent data points from the exotic distributions. (b): The green dots
represent the WSD values of the regular distributions, while the orange dots
represent the WSD values of the exotic distributions. (c): Each dot represents
the MBD of a distribution. The coloring pattern is the same as before. (d):
Each dot represents the FSD of a distribution. The coloring pattern remains
the same.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
29
small WSDs. As we discuss next, these identified â€œabnormal yearsâ€ are consis-
tent with historical records, which further validate the practical utility of the
WSD.
For the reproducibility of our research, the code for data analysis is publicly
available at https://github.com/YishaYao/Wasserstein-Spatial-Depth/tree/
main.
Fig 6: The green dots represent regular/representative/central distributions,
while the red dots correspond to the distributions near the outskirt and â€œfarâ€
from the center.
The values of the 150 empirical WSDs are shown in Figure 6. The lowest 5%
values, which we consider as outliers, are colored red, and the corresponding
years are also marked. Based on empirical WSDs, the temperatures at years
1879, 1929, 1940, 1942, 1947, 1956, 1963, and 2018 are more â€œexoticâ€ or near
outskirt. After searching among historical documentations, we indeed fond evi-
dences to support this discovery. Year 1879 was an extremely cold year, featured
with a unusually snowy winter (November and December). The first two months
of 1929 were recorded as one of the coldest winters in Europe during the past
century with temperature reaching down to -30Â°C in central Europe. Both year
1940 and year 1942 were marked by severe winters with dramatic ice storms,
and year 1942 had a cool summer. The weather in year 1947 was unusually cold
in winter and record-breaking hot in summer. Europe experienced severe cold
waves in both winters of 1956 and 1963. The well-known 2018 European drought
and heat wave led to record-breaking temperatures and wildfires in many parts
of Europe.
To get a better view on how these yearsâ€™ temperatures differ from other
regular yearsâ€™, we compare the four most â€œexoticâ€ years with the most regular
years. We pick the two years with the largest WSDs as our â€œregular yearsâ€,
year 1935 and year 1960. In each plot of Figure 7, the bundle of green curves
represents the temperature trends of the 40 locations in the regular years (1935

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
30
and 1960), while the bundle of red curves corresponds to one particular outlier
year. The green bundle and red bundle do exhibit clear visual differences in
temperature trends over the months.
(a) outlier year 1929
(b) outlier year 1940
(c) outlier year 1956
(d) outlier year 2018
Fig 7: Comparisons between the most regular years 1935 and 1960 (the two
years with the largest WSDs) and four outlier years. In each plot, the bundle
of green curves represents the temperature trends in years 1935 and 1960 at 40
locations in Europe (totally 80 green curves); the bundle of red curves represents
the temperature trends in an outlier year at the same 40 locations (totally 40
red curves).
10. Further directions and future work
In this work, we propose a new notion of depth on the Wasserstein space.
We demonstrate that it preserves critical properties of conventional statistical
depths. Additionally, it has a straightforward empirical counterpart that can be
easily computed from sample data and is asymptotically consistent. Numerical
simulations and real data analysis further support its practical utility. Impor-
tantly, in Section 8.4, we demonstrate that simply embedding distributions into
linear Hilbert spaces, and relying on existing FDA methods, is not satisfying.
In contrast, the WSD proves to be very informative in this section.
Note that we have defined the new notion of WSD SD(Q; P) for absolutely
continuous distributions Q and where P can be arbitrary. This is because our

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
31
approach exploits the definition of the geodesics in the Wasserstein space (see
Section 3.3).
When Q is not absolutely continuous, the geodesic between Q and another
distribution P might be not unique. In this case, the set of geodesics is given
by the laws of the random vectors (1 âˆ’t)X + tY where the law of the random
vector (X, Y), namely Ï€P,Q, is an optimal transport plan, as in (3.1) with p = 2.
Hence, uniqueness of the geodesics is equivalent to uniqueness of the transport
plans.
Thus, if with P-probability one P âˆ¼P is absolutely continuous, even if Q is
not absolutely continuous, the geodesics are unique and, following the route of
Section 3.3, we can still define a notion of depth as follows:
SDdiscr(Q; P) := 1âˆ’

E
(P,P â€²)âˆ¼PâŠ—P
Z 
x âˆ’y
W2(P, Q),
x âˆ’yâ€²
W2(P â€², Q)

dÏ€Q,P,P â€²(x, y, yâ€²)
1/2
,
(10.1)
where Ï€Q,P,P â€²(x, y, yâ€²) is the distribution of a vector (X, Y, Yâ€²) with (X, Y) âˆ¼
Ï€Q,P , (X, Yâ€²) âˆ¼Ï€Q,P â€² and Y and Yâ€² are independent given X. Here Ï€Q,P
(resp. Ï€Q,P â€²) is the unique optimal transport plan from Q to P (resp. P â€²).
This provides a definition of WSD for any distribution Q when P samples a.s.
absolutely continuous distributions. It can be seen, similarly as the proof of
Theorem 5.1, that SDdiscr(Q; P) would be [0, 1]-valued (and the quantity in the
square root being non-negative).
We leave for future exploration the practical utility of this complementary
WSD, along with the task of establishing analogous favorable mathematical
properties as those demonstrated in this paper. Note that the depth in (10.1)
coincides with SD(Q; P) in the special case where both Q and (a.s.) the samples
from P are absolutely continuous. This can be seen from the arguments leading
to (A.2) in the Appendix.
Finally, for computational reasons, the statistics and machine learning com-
munity has also focused on regularized optimal transport [21, 49]. It is an in-
teresting prospect as well to extend the WSD to regularized optimal transport.
Appendix A: Proof of Theorem 5.1
A.1. Values in [0, 1]
Here we prove that SD(Q; P) âˆˆ[0, 1] for all Q âˆˆPa.c
2 (Rd), which is probably
the easiest statement to prove. To prove the upper bound we realize that
 Z EP âˆ¼P
x âˆ’TQ,P (x)
W2(P, Q)

2
dQ(x)
! 1
2
â‰¥0,
so that
SD(Q; P) = 1 âˆ’
 Z EP âˆ¼P
x âˆ’TQ,P (x)
W2(P, Q)

2
dQ(x)
! 1
2
â‰¤1.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
32
To prove the lower bound we observe that
 Z EP âˆ¼P
x âˆ’TQ,P (x)
W2(P, Q)

2
dQ(x)
! 1
2
=
sup
âˆ¥Gâˆ¥L2(Q)â‰¤1
Z 
EP âˆ¼P
x âˆ’TQ,P (x)
W2(P, Q)

, G(x)

dQ(x)

=
sup
âˆ¥Gâˆ¥L2(Q)â‰¤1
Z
EP âˆ¼P
âŸ¨x âˆ’TQ,P (x), G(x)âŸ©
W2(P, Q)

dQ(x)

â‰¤EP âˆ¼P
"supâˆ¥Gâˆ¥L2(Q)â‰¤1
R
âŸ¨x âˆ’TQ,P (x), G(x)âŸ©dQ(x)
W2(P, Q)
#
= EP âˆ¼P
âˆ¥I âˆ’TQ,P âˆ¥L2(Q)
W2(P, Q)

= EP âˆ¼P
W2(P, Q)
W2(P, Q)

= 1,
so that
SD(Q; P) = 1 âˆ’
 Z EP âˆ¼P
x âˆ’TQ,P (x)
W2(P, Q)

2
dQ(x)
! 1
2
â‰¥0.
A.2. Transformation invariance
Theorem 1.2 in [32] describes the group of isometries of (P2(Rd), W2) for d â‰¥2.
Any isometry F can be written as the composition of Î¦(Ï†) and a trivial isometry.
Recall that Î¦(Ï†) : P 7â†’Î¦(Ï†)(P) where Ï† : Rd â†’Rd is a linear isometry and
Î¦(Ï†)(P) is the law of the random variable
Ï†(X âˆ’E[X]) + E[X],
for X âˆ¼P.
Therefore, it is enough to show that the WSD is invariant with respect to
trivial isometries and isometries of type Î¦(Ï†) for some linear isometry Ï† : Rd â†’
Rd.
Invariance under trivial isometries. Let A be a d Ã— d orthogonal matrix and
b âˆˆRd. We write
fA,b(x) = Ax + b.
The mapping
SP = fA,b â—¦TQ,P â—¦(fA,b)âˆ’1 : x 7â†’A TQ,P (AT (x âˆ’b)) + b
is the a.s. defined gradient of a convex function and (by construction) pushes
(fA,b)#Q forward to (fA,b)#P. Therefore, SP is the optimal transport map
from (fA,b)#Q forward to (fA,b)#P (cf. [40]). Hence, the following holds for

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
33
the induced isometry F : P 7â†’F(P) = (fA,b)#P:
SD(F(Q); F#P) = 1 âˆ’
 Z EP
x âˆ’SP (x)
W2(P, Q)

2
d((fA,b)#Q)(x)
! 1
2
= 1 âˆ’
 Z EP
fA,b(x) âˆ’fA,b â—¦TQ,P (x)
W2(P, Q)

2
dQ(x)
! 1
2
= 1 âˆ’
 Z EP
A(x âˆ’TQ,P (x))
W2(P, Q)

2
dQ(x)
! 1
2
= 1 âˆ’
 Z A EP
x âˆ’TQ,P (x)
W2(P, Q)

2
dQ(x)
! 1
2
= 1 âˆ’
 Z EP
x âˆ’TQ,P (x)
W2(P, Q)

2
dQ(x)
! 1
2
= SD(Q; P).
This proves the invariance under trivial isometries.
Invariance under isometries of type Î¦(Ï†). Let Ï† be a linear isometry. Then the
mapping SP solving
SP (Ï†(x âˆ’EXâˆ¼Q[X]) + EXâˆ¼Q[X]) = Ï†(TQ,P (x) âˆ’EYâˆ¼P [Y]) + EYâˆ¼P [Y]
is, as in the previous case, the optimal transport map from Î¦(Ï†)(Q) to Î¦(Ï†)(P).
Then it holds that
SD(Î¦(Ï†)(Q); (Î¦(Ï†))#P) = 1 âˆ’
 Z EP âˆ¼P
x âˆ’SP (x)
W2(P, Q)

2
dÎ¦(Ï†)(Q))(x)
! 1
2
= 1 âˆ’
 Z EP âˆ¼P
Ï†(x âˆ’EXâˆ¼Q[X]) + EXâˆ¼Q[X]
W2(P, Q)
âˆ’Ï†(TQ,P (x) âˆ’EYâˆ¼P [Y]) + EYâˆ¼P [Y]
W2(P, Q)

2
dQ(x)
 1
2
.
As Ï† is linear, we get the equality
SD(Î¦(Ï†)(Q); (Î¦(Ï†))#P) =1 âˆ’
 Z EP âˆ¼P
Ï†(x âˆ’TQ,P (x) âˆ’EXâˆ¼Q[X] + EYâˆ¼P [Y])
W2(P, Q)
+ EXâˆ¼Q[X] âˆ’EYâˆ¼P [Y]
W2(P, Q)

2
dQ(x)
 1
2
=1 âˆ’
 Z Ï†

EP âˆ¼P
x âˆ’TQ,P (x) âˆ’EXâˆ¼Q[X] + EYâˆ¼P [Y])
W2(P, Q)

+ EP âˆ¼P
EXâˆ¼Q[X] âˆ’EYâˆ¼P [Y]
W2(P, Q)

2
dQ(x)
 1
2
.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
34
Develop the squares and use the fact that Ï† is an isometry to obtain
SD(Î¦(Ï†)(Q); (Î¦(Ï†))#P)
=1 âˆ’
 Z EP âˆ¼P
x âˆ’TQ,P (x)
W2(P, Q)

2
+ 2
EP âˆ¼P
EXâˆ¼Q[X] âˆ’EYâˆ¼P [Y]
W2(P, Q)

2
+ 2

EP âˆ¼P
x âˆ’TQ,P (x)
W2(P, Q)

, EP âˆ¼P
EYâˆ¼P [Y] âˆ’EXâˆ¼Q[X]
W2(P, Q)

âˆ’2

Ï†

EP âˆ¼P
x âˆ’TQ,P (x)
W2(P, Q)

, EP âˆ¼P
EYâˆ¼P [Y] âˆ’EXâˆ¼Q[X]
W2(P, Q)

âˆ’2

Ï†

EP âˆ¼P
EYâˆ¼P [Y] âˆ’EXâˆ¼Q[X]
W2(P, Q)

, EP âˆ¼P
EYâˆ¼P [Y] âˆ’EXâˆ¼Q[X]
W2(P, Q)

dQ(x)
 1
2
.
The second term of the sum cancels with the third and the fourth with the last
one as a consequence of Fubiniâ€™s theorem, the linearity of Ï† and the fact that
(TQ,P )#Q = P. Therefore, the result follows.
A.3. Vanishing at infinity
The goal of this section is to prove that SD(Qn; P) â†’0 as W2(Qn, P) â†’âˆžfor
one P âˆˆP2(Rd).
Remark A.1. Note that W2(Qn, P) â†’âˆžimplies that for any other P â€² âˆˆ
P2(Rd),
W2(Qn, P â€²) â‰¥W2(Qn, P) âˆ’W2(P â€², P) â†’+âˆž.
Moreover, for any compact set K,
inf
P âˆˆK W2(Qn, P) â†’+âˆž.
Let {Qn}nâˆˆN âŠ‚Pa.c
2 (Rd) be such that W2(Qn, P) â†’âˆžfor all P âˆˆP2(Rd).
Recall that
SD(Qn; P) := 1 âˆ’
 Z EP âˆ¼P
x âˆ’TQn,P (x)
W2(P, Qn)

2
dQn(x)
! 1
2
with the convention xâˆ’TQn,P (x)
W2(P,Qn)
= 0 if W2(P, Qn) = 0. First we want to get rid
of this last pathological case. Let
An :=
Z EP âˆ¼P
x âˆ’TQn,P (x)
W2(P, Qn)

2
dQn(x).
(A.1)
Let En = {Qn}. Note that when P âˆˆEn, a 0 appears in the expression of An
(recall the convention 0/0 = 0). For each n, we modify P = P1 + P2, where P1
is a measure on P2(Rd)\En and P2 is a measure on En, by Pâ€² = P1 + eP2, where

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
35
eP2 is an arbitrary measure on P2(Rd)\En such that eP2(P2(Rd)) = P(En). Note
that Pâ€² is also a probability measure.
Since the measure P is tight and Qn diverges, it is clear that P(En) â†’0 as
n â†’âˆž. Moreover,

 Z EP âˆ¼Pâ€²
x âˆ’TQn,P (x)
W2(P, Qn)

2
dQn(x)
!1/2
âˆ’(An)1/2

â‰¤P(En)
 Z EP âˆ¼
e
P2
P(En)
x âˆ’TQn,P (x)
W2(P, Qn)

2
dQn(x)
!1/2
.
Since the spatial depth lies in [0, 1], we can upper bound this quantity by P(En)
and obtain that the limit of SD(Qn; P) is that of SD(Qn; Pâ€²). Therefore, we can
feel free to assume that W2(P, Qn) = 0 does not happen for n big enough and
for P âˆ¼P.
We prove that An â†’1, where An is defined in (A.1). To do so, let P â€² be an
independent copy of P, so that
An =
Z 
EP âˆ¼P
x âˆ’TQn,P (x)
W2(P, Qn)

, EP â€²âˆ¼P
x âˆ’TQn,P â€²(x)
W2(P â€², Qn)

dQn(x)
=
Z
EP,P â€²âˆ¼P
âŸ¨x âˆ’TQn,P (x), x âˆ’TQn,P â€²(x)âŸ©
W2(P, Qn)W2(P â€², Qn)

dQn(x).
(A.2)
In order to reduce the size of the formulas we call BP,n(x) = x âˆ’TQn,P (x) and
BP â€²,n(x) = x âˆ’TQn,P â€²(x). Then
An =
Z
EP,P â€²âˆ¼P

âŸ¨BP,n(x), BP â€²,n(x)âŸ©
âˆ¥BP,nâˆ¥L2(Qn)âˆ¥BP â€²,nâˆ¥L2(Qn)

dQn(x),
and, via Fubiniâ€™s theorem,
An = EP,P â€²âˆ¼P
"
âŸ¨BP,n, BP â€²,nâŸ©L2(Qn)
âˆ¥BP,nâˆ¥L2(Qn)âˆ¥BP â€²,nâˆ¥L2(Qn)
#
.
Since
|Cn(P, P â€²)| :=

âŸ¨BP,n, BP â€²,nâŸ©L2(Qn)
âˆ¥BP,nâˆ¥L2(Qn)âˆ¥BP â€²,nâˆ¥L2(Qn)
 â‰¤âˆ¥BP,nâˆ¥L2(Qn)âˆ¥BP â€²,nâˆ¥L2(Qn)
âˆ¥BP,nâˆ¥L2(Qn)âˆ¥BP â€²,nâˆ¥L2(Qn)
= 1,
the dominated convergence theorem can be applied and we only need to show
that
Cn(P, P â€²) âˆ’â†’1,
for P âŠ—P âˆ’a.e. (P, P â€²).
(A.3)
We decompose Cn(P, P â€²) in two terms: Cn(P, P â€²) = Cn,1(P, P â€²) + Cn,2(P, P â€²)
with
Cn,1(P, P â€²) =
âˆ¥BP,nâˆ¥2
L2(Qn)
âˆ¥BP,nâˆ¥2
L2(Qn)
= 1,

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
36
and
Cn,2(P, P â€²) =

BP,n
âˆ¥BP,nâˆ¥L2(Qn)
,
BP â€²,n
âˆ¥BP â€²,nâˆ¥L2(Qn)
âˆ’
BP,n
âˆ¥BP,nâˆ¥L2(Qn)

L2(Qn)
.
The goal, of course, is to show that Cn,2(P, P â€²) â†’0, for P âŠ—P-a.e. (P, P â€²).
Since
Cn,2(P, P â€²)
=

BP,n
âˆ¥BP,nâˆ¥L2(Qn)
, BP â€²,n âˆ’BP,n
âˆ¥BP â€²,nâˆ¥L2(Qn)
+ BP,n

1
âˆ¥BP â€²,nâˆ¥L2(Qn)
âˆ’
1
âˆ¥BP,nâˆ¥L2(Qn)

L2(Qn)
=

BP,n
âˆ¥BP,nâˆ¥L2(Qn)
, TQn,P âˆ’TQn,P â€²
âˆ¥BP â€²,nâˆ¥L2(Qn)
+ BP,n

1
âˆ¥BP â€²,nâˆ¥L2(Qn)
âˆ’
1
âˆ¥BP,nâˆ¥L2(Qn)

L2(Qn)
,
we can upper bound |Cn,2(P, P â€²)| by
âˆ¥BP,nâˆ¥L2(Qn)
âˆ¥BP,nâˆ¥L2(Qn)
âˆ¥TQn,P âˆ¥L2(Qn) + âˆ¥TQn,P â€²âˆ¥L2(Qn)
âˆ¥BP â€²,nâˆ¥L2(Qn)
+


BP,n
âˆ¥BP,nâˆ¥L2(Qn)
, BP,n

1
âˆ¥BP,nâˆ¥L2(Qn)
âˆ’
1
âˆ¥BP â€²,nâˆ¥L2(Qn)

L2(Qn)
 .
(A.4)
The first term of (A.4) tends to 0 for P âŠ—P-a.e. (P, P â€²). Indeed, using the
equality âˆ¥TQn,P âˆ¥2
L2(Qn) =
R
âˆ¥xâˆ¥2dP(x), the first term of (A.4) is equal to
qR
âˆ¥xâˆ¥2dP(x) +
qR
âˆ¥xâˆ¥2dP â€²(x)
âˆ¥BP â€²,nâˆ¥L2(Qn)
.
(A.5)
The latter clearly tends to 0 since âˆ¥BP â€²,nâˆ¥L2(Qn) = W2(Qn, P â€²).
To show that the second term of (A.4) also tends to 0 we use the bound


BP,n
âˆ¥BP,nâˆ¥L2(Qn)
, BP,n

1
âˆ¥BP,nâˆ¥L2(Qn)
âˆ’
1
âˆ¥BP â€²,nâˆ¥L2(Qn)

L2(Qn)

â‰¤
âˆ¥BP,nâˆ¥
âˆ¥BP,nâˆ¥L2(Qn) âˆ’âˆ¥BP â€²,nâˆ¥L2(Qn)
âˆ¥BP,nâˆ¥L2(Qn)âˆ¥BP â€²,nâˆ¥L2(Qn)

followed by the triangle inequality
âˆ¥BP,nâˆ¥
âˆ¥BP,nâˆ¥L2(Qn) âˆ’âˆ¥BP â€²,nâˆ¥L2(Qn)
âˆ¥BP,nâˆ¥L2(Qn)âˆ¥BP â€²,nâˆ¥L2(Qn)
 =

âˆ¥BP,nâˆ¥L2(Qn) âˆ’âˆ¥BP â€²,nâˆ¥L2(Qn)
âˆ¥BP â€²,nâˆ¥L2(Qn)

â‰¤
âˆ¥BP,n âˆ’BP â€²,nâˆ¥L2(Qn)
âˆ¥BP â€²,nâˆ¥L2(Qn)

=
âˆ¥TQn,P âˆ’TQn,P â€²âˆ¥L2(Qn)
âˆ¥BP â€²,nâˆ¥L2(Qn)

.
The latter can be upper bounded by (A.5), so that the second term of (A.4)
also tends to 0 for P âŠ—P-a.e. (P, P â€²). This implies Cn,2(P, P â€²) tends to 0 for
P âŠ—P-a.e. (P, P â€²). Hence, (A.3) holds.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
37
Appendix B: Proof of Theorem 5.2
Consider P âˆˆ{P1, . . . , Pn} and Q âˆˆPa.c.
2
(Rd) such that Q Ì¸âˆˆ{P1, . . . , Pn}. We
recall from [57] that
W2
2(P, Q) =
inf
Ï€âˆˆÎ (P,Q)
1
2
Z
âˆ¥x âˆ’yâˆ¥2dÏ€(x, y)
(B.1)
admits a dual formulation
W2
2(P, Q) =
sup
(f,g)âˆˆÎ¦
Z
f(x) dQ(x) +
Z
g(y) dP(y)

,
(B.2)
where Î¦ = {(f, g) âˆˆC(Rd) Ã— C(Rd) : f(x) + g(y) â‰¤1
2âˆ¥x âˆ’yâˆ¥2}. Here C(Rd) is
the set of continuous functions on Rd. We denote as (fQ,P , fP,Q) the solutions
of (B.2). It is well-known that âˆ‡fQ,P (x) = x âˆ’TQ,P (x). Now we argue by
contradiction. We assume first that there exists
Q âˆˆPa.c
2 (Rd) âˆ©arg min
Qâ€²
EP âˆ¼P[W2(P, Qâ€²)]
with Q Ì¸âˆˆ{P1, . . . , Pn} and we assume that the set Kâ€² of all x such that
s(x) := EP âˆ¼P
x âˆ’TQ,P (x)
W2(P, Q)

Ì¸= 0
has positive measure Q(Kâ€²) > 0. As TQ,P is the gradient of a lower semi
continuous convex function, it is continuous Q-a.e., so that s is also contin-
uous Q-a.e. Therefore, there exists a compact convex set with non-empty in-
terior U such that U âŠ‚Kâ€². Consider the signed measure h such that
dh
dQ =
âˆ’1U

fQ,P âˆ’
1
Q(U)
R
U fQ,P (z)dQ(z)

, where 1U is the indicator function of the
set U.
Note that h(Rd) = 0 and Q + th is a probability measure with finite second
order moment for all t in a neighborhood of zero. Since (Â·)1/2 is concave,
W2(P, Q + th) â‰¤W2(P, Q) + W2
2(P, Q + th) âˆ’W2
2(P, Q)
2W2(P, Q)
.
Using the dual formulation (B.2) we obtain for t in a neighborhood of zero,
W2(P, Q + th) âˆ’W2(P, Q)
t
â‰¤
R
fQ+th,P (x) dh(x)
2W2(P, Q)
.
Since h(Rd) = 0, we have for t in a neighborhood of zero
W2(P, Q + th) âˆ’W2(P, Q)
t
â‰¤âˆ’
R
U

fQ,P (x) âˆ’
1
Q(U)
R
U fQ,P (z)dQ(z)
 
fQ+th,P (x) âˆ’
1
Q(U)
R
U fQ+th,P (z)dQ(z)

dQ(x)
2W2(P, Q)
.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
38
Set
M(P) := 1
2
R
U

fQ,P (x) âˆ’
1
Q(U)
R
U fQ,P (z)dQ(z)
2
dQ(x)
W2(P, Q)
and the norm
âˆ¥Ï•âˆ¥U :=
 Z
U

Ï•(x) âˆ’
1
Q(U)
Z
U
Ï•(z)dQ(z)
2
dQ(x)
! 1
2
.
Then
W2(P, Q + th) âˆ’W2(P, Q)
t
â‰¤âˆ’M(P) + âˆ¥fQ,P âˆ¥Uâˆ¥fQ,P âˆ’fQ+th,P âˆ¥U
2W2(P, Q)
.
Since s(x) Ì¸= 0 for x âˆˆU, the function U âˆ‹x 7â†’EP âˆ¼P[fQ,P (x)] is non constant,
which implies that
EP âˆ¼P[M(P)] := 1
2EP âˆ¼P
ï£®
ï£¯ï£°
R
U

fQ,P (x) âˆ’
1
Q(U)
R
U fQ,P (z)dQ(z)
2
dQ(x)
W2(P, Q)
ï£¹
ï£ºï£»> 0.
The theorem follows upon showing that
EP âˆ¼P
âˆ¥fQ,P âˆ¥Uâˆ¥fQ,P âˆ’fQ+th,P âˆ¥U
W2(P, Q)

â†’0
as t â†’0,
(B.3)
which is a trivial consequence of the main result of [50] and the assumption
Q Ì¸âˆˆ{P1, . . . , Pn}.
Appendix C: Proofs of Section 5.3
Proof of Theorem 5.4. As P âˆˆP(P2(Rd)) is atomless there exists an open
Wasserstein ball
BW2(Q, Î²) = {P âˆˆP2(Rd) : W2(P, Q) < Î²}
with P(BW2(Q, Î²)) â‰¤Ïµ/2. Since P âˆˆP(P2(Rd)) is tight, there exists a compact
set K âŠ‚P2(Rd) such that P(P2(Rd) \ K) â‰¤Ïµ/2. Set VÎ² = K âˆ©(P2(Rd) \
BW2(Q, Î²)) and Vc
Î² = P2(Rd) \ VÎ². In summary, it holds that
P(Vc
Î²) â‰¤Ïµ.
(C.1)
Moreover, as W2(Qn, Q) â†’0, we can assume that n is large enough such that
W2(Qn, Q) â‰¤Î²/2, which implies that
W2(Qn, P) â‰¥W2(P, Q) âˆ’W2(Qn, Q) â‰¥Î²/2,
(C.2)

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
39
for all P âˆˆVÎ². Next, call
A2
n =
Z EP âˆ¼P
x âˆ’TQn,P (x)
W2(P, Qn)

2
dQn(x)
and
A2 =
Z EP âˆ¼P
x âˆ’TQ,P (x)
W2(P, Q)

2
dQ(x).
The result follows by showing that A2
n â†’A2. Triangle inequality implies that

An âˆ’
ï£«
ï£¬
ï£¬
ï£¬
ï£­
Z EP âˆ¼P

1VÎ²(P)x âˆ’TQn,P (x)
W2(P, Qn)

2
dQn(x)
|
{z
}
=:B2n
ï£¶
ï£·
ï£·
ï£·
ï£¸
1
2 
â‰¤
 Z EP âˆ¼P

1Vc
Î²(P)x âˆ’TQn,P (x)
W2(P, Qn)

2
dQn(x)
! 1
2
,
so that, arguing as in Subsection A.1 and using (C.1), we derive the bound
|An âˆ’Bn| â‰¤Ïµ for all n âˆˆN. By the same means |A âˆ’B| â‰¤Ïµ where
B2 =
Z EP âˆ¼P

1VÎ²(P)x âˆ’TQ,P (x)
W2(P, Q)

2
dQ(x).
Therefore, since Ïµ is arbitrary, the result follows after showing that Bn â†’B.
To do so, we set Xn âˆ¼Qn for n âˆˆN, X âˆ¼Q and P, P â€² âˆˆP2(Rd). Arguing as in
the proof of Theorem 2.1 in [23] we get for every P, P â€² âˆˆP2(Rd),
(Xn, TQn,P (Xn), TQn,P â€²(Xn))
w
âˆ’â†’(X, TQ,P (X), TQ,P â€²(X)).
(C.3)
Indeed a straightforward adaptation of the arguments there shows first that
there is a limit in distribution which is the distribution of the random vector
(Z1, Z2, Z3),
where of course we have Z1 âˆ¼Q. Then the arguments there show that (Xn, TQn,P (Xn))
w
âˆ’â†’
(X, TQ,P (X)) and thus a.s.
Z2 = TQ,P (Z1).
Similarly,
Z3 = TQ,P â€²(Z1)
and thus (C.3) holds. The continuous mapping theorem with the function (x, y, z) 7â†’
(y âˆ’x, z âˆ’x) implies that
TQn,P (Xn) âˆ’Xn
TQn,P â€²(Xn) âˆ’Xn

w
âˆ’â†’
TQ,P (X) âˆ’X
TQ,P â€²(X) âˆ’X

.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
40
Since for all P âˆˆP2(Rd), it holds that W2(Qn, P) â†’W2(Q, P), Slutskyâ€™s
theorem yields
 TQn,P (Xn)âˆ’Xn
W2(Qn,P )
TQn,P â€²(Xn)âˆ’Xn
W2(Qn,P â€²)
!
w
âˆ’â†’
 TQ,P (X)âˆ’X
W2(Q,P )
TQ,P â€²(X)âˆ’X
W2(Q,P â€²)
!
(C.4)
for all P, P â€² such that W2(Q, P) > 0 and W2(Q, P â€²) > 0. As a consequence,
(C.4) holds for P-a.e. P, P â€².
Let PÎ² be the probability measure A 7â†’PÎ²(A) = P(VÎ²âˆ©A)
P(VÎ²) . Therefore, for
(P, P â€²) âˆ¼PÎ² âŠ—PÎ² with (P, P â€²) independent of {Xn}nâˆˆN, we obtain
Yn := âŸ¨Xn âˆ’TQn,P (Xn), Xn âˆ’TQn,P â€²(Xn)âŸ©
W2(P, Qn)W2(P â€², Qn)
w
âˆ’â†’Y := âŸ¨X âˆ’TQ,P (X), X âˆ’TQ,P â€²(X)âŸ©
W2(P, Q)W2(P â€², Q)
.
Indeed, for a bounded continous function F : R â†’R,
E [F(Yn)] = E [E [F(Yn)| P, P â€²]]
=
Z Z
E
h
F(Yn)| P = ËœP, P â€² = ËœP â€²i
dPÎ²( ËœP)dPÎ²( ËœP â€²)
=
Z Z
E
ï£®
ï£°F
ï£«
ï£­
D
Xn âˆ’TQn, Ëœ
P (Xn), Xn âˆ’TQn, Ëœ
P â€²(Xn)
E
W2( ËœP, Qn)W2( ËœP â€², Qn)
ï£¶
ï£¸
ï£¹
ï£»dPÎ²( ËœP)dPÎ²( ËœP â€²)
âˆ’â†’
nâ†’âˆž
Z Z
E
ï£®
ï£°F
ï£«
ï£­
D
X âˆ’TQ, Ëœ
P (X), X âˆ’TQ, Ëœ
P â€²(X)
E
W2( ËœP, Q)W2( ËœP â€², Q)
ï£¶
ï£¸
ï£¹
ï£»dPÎ²( ËœP)dPÎ²( ËœP â€²)
= E[F(Y)],
where the above limit holds due to dominated convergence.
Skorokhodâ€™s representation theorem yields the existence of a sequence of ran-
dom variables { ËœYn} defined on a common probability space (â„¦â€², Aâ€², Pâ€²) taking
values in R with ËœYn
d= Yn converging Pâ€²-a.e. to a random variable ËœY : â„¦â€² â†’Rd
with ËœY
d= Y. Since
B2
n = P(VÎ²)2E
âŸ¨Xn âˆ’TQn,P (Xn), Xn âˆ’TQn,P â€²(Xn)âŸ©
W2(P, Qn)W2(P â€², Qn)

= P(VÎ²)2E[ ËœYn]
and
B2 = P(VÎ²)2E
âŸ¨X âˆ’TQ,P (X), X âˆ’TQ,P â€²(X)âŸ©
W2(P, Q)W2(P â€², Q)

= P(VÎ²)2E[ ËœY],
we only need to prove that Yn is uniformly integrable. The bound (C.2) implies
that it is enough to show that each of the terms of the right hand side of
| âŸ¨Xn âˆ’TQn,P (Xn), Xn âˆ’TQn,P â€²(Xn)âŸ©|
â‰¤âˆ¥Xnâˆ¥2+âˆ¥TQn,P (Xn)âˆ¥âˆ¥Xnâˆ¥+âˆ¥TQn,P â€²(Xn)âˆ¥âˆ¥Xnâˆ¥+âˆ¥TQn,P (Xn)âˆ¥âˆ¥TQn,P â€²(Xn)âˆ¥
(C.5)

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
41
are uniformly integrable. Recall that a set S of random variables is uniformly
integrable if
lim
Râ†’+âˆžsup
UâˆˆS
E[|U|1|U|>R] = 0.
Since VÎ² and {Qn}nâˆˆN are relatively compact subsets in the 2-Wasserstein topol-
ogy, Theorem 7.12 in [57] implies that
lim
Râ†’+âˆžsup
P âˆˆVÎ²
Z
âˆ¥xâˆ¥2>R
âˆ¥xâˆ¥2dP(x) = 0
(C.6)
and
lim
Râ†’+âˆžsup
nâˆˆN
Z
âˆ¥xâˆ¥2>R
âˆ¥xâˆ¥2dQn(x) = 0.
(C.7)
The last limit (C.7) implies that the sequence {âˆ¥Xnâˆ¥2}nâˆˆN is uniformly in-
tegrable, so that the first term of the right-hand-side of (C.5) is uniformly
integrable. For the second, we observe that
E[âˆ¥TQn,P (Xn)âˆ¥âˆ¥Xnâˆ¥1âˆ¥TQn,P (Xn)âˆ¥âˆ¥Xnâˆ¥>R]
â‰¤E
h
âˆ¥TQn,P (Xn)âˆ¥âˆ¥Xnâˆ¥1âˆ¥Xnâˆ¥>R
1
2
i
+ E
h
âˆ¥TQn,P (Xn)âˆ¥âˆ¥Xnâˆ¥1âˆ¥TQn,P (Xn)âˆ¥>R
1
2
i
â‰¤

E

âˆ¥TQn,P (Xn)âˆ¥2
E
h
âˆ¥Xnâˆ¥21âˆ¥Xnâˆ¥>R
1
2
i 1
2
+

E
h
âˆ¥TQn,P (Xn)âˆ¥21âˆ¥TQn,P (Xn)âˆ¥>R
1
2
i
E

âˆ¥Xnâˆ¥2 1
2
â‰¤
 
sup
P âˆˆVÎ²
Z
âˆ¥xâˆ¥2dP(x)
Z
âˆ¥xâˆ¥2>R
âˆ¥xâˆ¥2dQn(x)
! 1
2
+
 
sup
P âˆˆVÎ²
Z
âˆ¥xâˆ¥2>R
âˆ¥xâˆ¥2dP(x)
Z
âˆ¥xâˆ¥2dQn(x)
! 1
2
,
where we used the fact that TQn,P (Xn) âˆ¼P for all n âˆˆN. Since, supnâˆˆN
R
âˆ¥xâˆ¥2dQn(x)
and supP âˆˆVÎ²
R
âˆ¥xâˆ¥2dP(x) are bounded, the previous display, (C.6) and (C.7)
imply that the second term of (C.5) is uniformly integrable. Since P and P â€² are
exchangeable, the same holds for the third term. The uniform integrability of
the last one follows directly from (C.6).
Proof of Lemma 5.5. From [58, Corollary 5.23], for every Ïµ > 0, it holds that
Q(âˆ¥TQ,Pn âˆ’TQ,P âˆ¥â‰¥Ïµ) â†’0. As âˆ¥TQ,Pn âˆ’TQ,P âˆ¥L2(Q) is uniformly bounded,
the sequence {TQ,Pn âˆ’TQ,P }nâˆˆN is compact w.r.t. the weak topology of L2(Q)
by the Banach-Alaogluâ€“Bourbaki theorem (cf. [10, Theorem 3.16]). Therefore,
for each subsequence {TQ,Pnk âˆ’TQ,P }kâˆˆN there exists a further subsequence
{TQ,Pnkâ„“âˆ’TQ,P }â„“âˆˆN such that
âŸ¨TQ,Pnkâ„“âˆ’TQ,P , hâŸ©L2(Q) â†’âŸ¨L, hâŸ©L2(Q)

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
42
for some L âˆˆL2(Q) and all h âˆˆL2(Q). We prove now that L = 0, irrespective of
the subsequences. To improve readability, we write {TQ,Pn âˆ’TQ,P }nâˆˆN instead
of {TQ,Pnkâ„“âˆ’TQ,P }â„“âˆˆN. Since Q(âˆ¥TQ,Pn âˆ’TQ,P âˆ¥â‰¥Ïµ) â†’0 and
âˆ¥TQ,Pnâˆ¥2
L2(Q) =
Z
âˆ¥xâˆ¥2dPn(x) â†’
Z
âˆ¥xâˆ¥2dP(x) = âˆ¥TQ,P âˆ¥2
L2(Q) < +âˆž,
Vitali convergence theorem implies that {TQ,Pn âˆ’TQ,P }nâˆˆN converges to zero
in the reflexive space L
3
2 (Q). Therefore, 0 is also the weak limit of TQ,Pn âˆ’TQ,P
in L
3
2 (Q), i.e.,
Z
âŸ¨TQ,Pn âˆ’TQ,P , hâŸ©dQ â†’0
for all h âˆˆL3(Q). As a consequence, L = 0, Q-a.e. Moreover,
âˆ¥TQ,Pn âˆ’TQ,P âˆ¥2
L2(Q) = âˆ¥TQ,Pnâˆ¥2
L2(Q) + âˆ¥TQ,P âˆ¥2
L2(Q) âˆ’2âŸ¨TQ,Pn, TQ,P âŸ©L2(Q)
â†’2âˆ¥TQ,P âˆ¥2 âˆ’2âŸ¨TQ,P , TQ,P âŸ©L2(Q)
= 0.
This concludes the proof.
Proof of Theorem 5.6. Fix Ïµ > 0. As P âˆˆP(P2(Rd)) is atomless there exists an
open Wasserstein ball
BW2(Q, Î²) = {P âˆˆP2(Rd) : W2(P, Q) < Î²}
with P(BW2(Q, Î²)) â‰¤Ïµ/8. Since Pn
w
âˆ’â†’P in P(P2(Rd)) and the closure of
BW2(Q, Î²/2) under the W2-metric, is contained in BW2(Q, Î²), there exists n0 âˆˆ
N such that
Pn(BW2(Q, Î²/2)) â‰¤Ïµ/4
for all n â‰¥n0.
As {Pn}nâˆˆN âŠ‚P(P2(Rd)) is tight, there exists a compact set K âŠ‚P2(Rd) such
that
Pn(P2(Rd) \ K) â‰¤Ïµ/4
for all n â‰¥n0.
Call V = K âˆ©(P2(Rd) \ BW2(Q, Î²/2)) and V c = P2(Rd) \ V . Then
P(V c) + Pn(V c) â‰¤Ïµ
for all n â‰¥n0.
We call
A :=


Z
I âˆ’TQ,P
W2(P, Q)dPn(P)

L2(Q)
âˆ’

Z
I âˆ’TQ,P
W2(P, Q)dP(P)

L2(Q)
 .
The triangle inequality yields
A â‰¤

Z
I âˆ’TQ,P
W2(P, Q)d(Pn âˆ’P)(P)

L2(Q)
â‰¤

Z
V
I âˆ’TQ,P
W2(P, Q)d(Pn âˆ’P)(P)

L2(Q)
+

Z
V c
I âˆ’TQ,P
W2(P, Q)dP(P)

L2(Q)
+

Z
V c
I âˆ’TQ,P
W2(P, Q)dPn(P)

L2(Q)
.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
43
Arguing as in Section A.1 we get that, for n â‰¥n0,

Z
V c
I âˆ’TQ,P
W2(P, Q)dP(P)

L2(Q)
+

Z
V c
I âˆ’TQ,P
W2(P, Q)dPn(P)

L2(Q)
â‰¤P(V c)+Pn(V c) â‰¤Ïµ.
Moreover, as the function
V âˆ‹P 7â†’I âˆ’TQ,P
W2(P, Q) âˆˆL2(Q)
is continuous and bounded (Lemma 5.5), for every h âˆˆL2(Q) it holds that
Z
V
 I âˆ’TQ,P
W2(P, Q), h

L2(Q)
d(Pn âˆ’P)(P) â†’0,
meaning that
R
V
Iâˆ’TQ,P
W2(P,Q)d(Pn âˆ’P)(P) converges to zero in the weak topology
of L2(Q). However, as the set
 I âˆ’TQ,P
W2(P, Q) : P âˆˆV

âˆª{0}
is compact (note that V is compact in P2(Rd) and P2(Rd)\{Q} âˆ‹P 7â†’
Iâˆ’TQ,P
W2(P,Q)
is continuous, see Lemma 5.5), its closed convex hull, namely C, is compact
as well. Since
R
V
Iâˆ’TQ,P
W2(P,Q)dPn lies in C for all n âˆˆN, the convergence of
R
V
Iâˆ’TQ,P
W2(P,Q)d(Pn âˆ’P)(P) towards zero holds in the strong topology of L2(Q).
We have proven that A â‰¤2Ïµ for n big enough. Since Ïµ was arbitrarily chosen,
the result follows.
Appendix D: Proof of Lemma 6.2
Let S âŠ‚Pp(Rd) be a closed set and define
BL1(S) = {f : S â†’R : |f(P)| â‰¤1 and |f(P) âˆ’f(Q)| â‰¤Wp(P, Q), âˆ€P, Q âˆˆS} .
Fix f âˆˆBL1(Pp(Rd)). Then

Z
f(P)d(Pn,m âˆ’P)(P)
 â‰¤

Z
f(P)d(Pn,m âˆ’Pn)(P)

|
{z
}
An,m(f)
+

Z
f(P)d(Pn âˆ’P)(P)

|
{z
}
Bn(f)
,
where Pn = 1
n
Pn
i=1 Î´Pi. It can be proved by standard means that
E
"
sup
fâˆˆBL1(Pp(Rd))
Bn(f)
#
â†’0

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
44
as n â†’âˆž. Since f âˆˆBL1(Pp(Rd)), it holds that
An,m(f) =

1
n
n
X
i=1
f(Pi,m) âˆ’f(Pi)
 â‰¤1
n
n
X
i=1
min(2, Wp(Pi,m, Pi))
which, by taking expectations, implies
E
"
sup
fâˆˆBL1(Pp(Rd))
An,m(f)
#
â‰¤1
n
n
X
i=1
E[min(2, Wp(Pi,m, Pi))].
Since the sequence {Wp(Pi,m, Pi)}n
i=1 is exchangeable, it holds that
E
"
sup
fâˆˆBL1(Pp(Rd))
An,m(f)
#
â‰¤E[min(2, Wp(P1,m, P1))].
The latter tends to zero by Glivenkoâ€“Cantelli theorem and the fact that, con-
ditionally to P1,
1
m
m
X
j=1
Xp
1,j
a.s.
âˆ’âˆ’â†’
Z
âˆ¥xâˆ¥pdP1(x)
as m â†’âˆž.
Funding
FranÂ¸cois Bachoc was supported by the Project GAP (ANR-21-CE40-0007) of
the French National Research Agency (ANR) and by the Chair UQPhysAI of
the Toulouse ANITI AI Cluster.
References
[1] Ambrosio, L., Gigli, N. and Savare, G. (2005). Gradient Flows in Met-
ric Spaces and in the Space of Probability Measures. BirkhÂ¨auser Basel.
[2] Bachoc, F., BÂ´ethune, L., Gonzalez-Sanz, A. and Loubes, J.-M.
(2023a). Gaussian processes on distributions based on regularized optimal
transport. In International Conference on Artificial Intelligence and Statis-
tics 26 4986â€“5010.
[3] Bachoc, F., BÂ´ethune, L., GonzÂ´alez-Sanz, A. and Loubes, J.-M.
(2023b). Improved learning theory for kernel distribution regression with
two-stage sampling. arXiv:2308.14335.
[4] Berlinet, A. and Thomas-Agnan, C. (2011). Reproducing Kernel
Hilbert Spaces in Probability and Statistics. Springer Science & Business
Media.
[5] Bertrand, J. and Kloeckner, B. (2012). A geometric study of Wasser-
stein spaces: Hadamard spaces. Journal of Topology and Analysis 4 515â€“
542.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
45
[6] Bigot, J. (2020). Statistical data analysis in the Wasserstein space.
ESAIM: Proceedings and Surveys 68 1â€“19.
[7] Bigot, J., Gouet, R., Klein, T. and LÂ´opez, A. (2017). Geodesic PCA
in the Wasserstein space by convex PCA. Annales de lâ€™Institut Henri
PoincarÂ´e, ProbabilitÂ´es et Statistiques 53 1 â€“ 26.
[8] Boissard, E., Le Gouic, T. and Loubes, J.-M. (2015). Distributionâ€™s
template estimate with Wasserstein metrics. Bernoulli 21 740â€“759.
[9] Bonneel, N., PeyrÂ´e, G. and Cuturi, M. (2016). Wasserstein barycen-
tric coordinates: histogram regression using optimal transport. ACM Trans-
actions on Graphics 35 71â€“1.
[10] Brezis, H. (2010). Functional Analysis, Sobolev Spaces and Partial Dif-
ferential Equations. New York: Springer.
[11] Chakraborty, A. and Chaudhuri, P. (2014). The spatial distribution
in infinite dimensional spaces and related quantiles and depths. The Annals
of Statistics 42 1203 â€“ 1231.
[12] Chami, I., Gu, A., Chatziafratis, V. and RÂ´e, C. (2020). From trees to
continuous embeddings and back: Hyperbolic hierarchical clustering. Ad-
vances in Neural Information Processing Systems 33 15065â€“15076.
[13] Chan,
S., Santoro,
A., Lampinen,
A., Wang,
J., Singh,
A.,
Richemond, P., McClelland, J. and Hill, F. (2022). Data distribu-
tional properties drive emergent in-context learning in transformers. Ad-
vances in Neural Information Processing Systems 35 18878â€“18891.
[14] Chaudhuri, P. (1996). On a geometric notion of quantiles for multivariate
data. Journal of the American Statistical Association 91 862â€“872.
[15] Chen, Y., Lin, Z. and MÂ¨uller, H.-G. (2023). Wasserstein regression.
Journal of the American Statistical Association 118 869â€“882.
[16] Chernozhukov, V., Galichon, A., Hallin, M. and Henry, M. (2017).
Monge-Kantorovich depth, quantiles, ranks and signs. The Annals of Statis-
tics 45 223â€“256.
[17] Cuesta-Albertos, J. A., MatrÂ´an-Bea, C. and Tuero-DiÂ´az, A.
(1996). On lower bounds for the L2-Wasserstein metric in a Hilbert space.
Journal of Theoretical Probability 9 263-283.
[18] Cuesta-Albertos, J. A. and Nieto-Reyes, A. (2008). The random
Tukey depth. Computational Statistics and Data Analysis 52 4979â€“4988.
[19] Cuevas, A., Febrero, M. and Fraiman, R. (2007). Robust estimation
and classification for functional data via projection-based depth notions.
Computational Statistics 22 481â€“496.
[20] Cuevas, A. and Fraiman, R. (2009). On depth measures and dual statis-
tics. A methodology for dealing with general data. Journal of Multivariate
Analysis 100 753-766.
[21] Cuturi, M. (2013). Sinkhorn distances: Lightspeed computation of op-
timal transport. Advances in Neural Information Processing Systems 27
2292-2300.
[22] Dai, X. and Lopez-Pintado, S. (2023). Tukeyâ€™s depth for object data.
Journal of the American Statistical Association 118 1760-1772.
[23] Deb, N. and Sen, B. (2023). Multivariate rank-based distribution-free

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
46
nonparametric testing using measure transportation. Journal of the Amer-
ican Statistical Association 118 192â€“207.
[24] Del Barrio, E., Inouzhe, H., Loubes, J.-M., MatrÂ´an, C. and Mayo-
Â´Iscar, A. (2020). optimalFlow: optimal transport approach to flow cytom-
etry gating and population matching. BMC Bioinformatics 21 1â€“25.
[25] Dubey, P., Chen, Y. and MÂ¨uller, H.-G. (2024). Metric statistics: Ex-
ploration and inference for random objects with distance profiles. The An-
nals of Statistics 52 757â€“792.
[26] Dutta, S., Ghosh, A. K. and Chaudhuri, P. (2011). Some intriguing
properties of Tukeyâ€™s half-space depth. Bernoulli 17.
[27] Fraiman, R. and Muniz, G. (2001). Trimmed means for functional data.
Test 10 419â€“440.
[28] Geenens, G., Nieto-Reyes, A. and Francisci, G. (2023). Statistical
depth in abstract metric spaces. Statistics and Computing 33.
[29] Ghorbani, A., Kim, M. and Zou, J. (2020). A distributional framework
for data valuation. In International Conference on Machine Learning 37
3535â€“3544.
[30] GonzÂ´alez-Sanz, A., Hallin, M. and Sen, B. (2023). Monotone
measure-preserving maps in Hilbert spaces: existence, uniqueness, and sta-
bility. arXiv:2305.11751.
[31] Hallin, M., del Barrio, E., Cuesta-Albertos, J. and MatrÂ´an, C.
(2021). Distribution and quantile functions, ranks and signs in dimension
d: A measure transportation approach. The Annals of Statistics 49 1139 â€“
1165.
[32] Kloeckner, B. (2010). A geometric study of Wasserstein spaces: Eu-
clidean spaces. Annali della Scuola Normale Superiore di Pisa - Classe di
Scienze 9 297â€“323.
[33] Ledoux, M. and Talagrand, M. (1991). Probability in Banach Spaces.
Springer Berlin Heidelberg.
[34] Liu, R. Y. (1990). On a notion of data depth based on random simplices.
The Annals of Statistics 405â€“414.
[35] Liu, Z. and Modarres, R. (2011). Lens data depth and median. Journal
of Nonparametric Statistics 23 1063â€“1074.
[36] Liu, R. Y. and Singh, K. (1993). A quality index based on data depth and
multivariate rank tests. Journal of the American Statistical Association 88
252â€“260.
[37] Long, J. P. and Huang, J. Z. (2015). A study of functional depths.
arXiv:1506.01332.
[38] LÂ´opez-Pintado, S. and Romo, J. (2009). On the concept of depth for
functional data. Journal of the American Statistical Association 104 718â€“
734.
[39] LÂ´opez-Pintado, S. and Romo, J. (2011). A half-region depth for func-
tional data. Computational Statistics & Data Analysis 55 1679-1695.
[40] McCann, R. J. (1995). Existence and uniqueness of monotone measure-
preserving maps. Duke Mathematical Journal 80 309 â€“ 323.
[41] Meunier, D., Pontil, M. and Ciliberto, C. (2022). Distribution re-

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
47
gression with sliced Wasserstein kernels. In International Conference on
Machine Learning 39 15501â€“15523.
[42] Mosler, K. (2013). Depth statistics. Robustness and Complex Data Struc-
tures: Festschrift in Honour of Ursula Gather 17â€“34. Springer Berlin Hei-
delberg.
[43] Mosler, K. and Mozharovskyi, P. (2022). Choosing among notions of
multivariate depth statistics. Statistical Science 37 348â€“368.
[44] Muzellec, B. and Cuturi, M. (2018). Generalizing point embeddings
using the Wasserstein space of elliptical distributions. Advances in Neural
Information Processing Systems 31 10258 - 10269.
[45] Nagy, S. (2017). Monotonicity properties of spatial depth. Statistics and
Probability Letters 129 373-378.
[46] Nieto-Reyes, A. and Battey, H. (2016). A topologically valid definition
of depth for functional data. Statistical Science 31 61 â€“ 79.
[47] Oja, H. (1983). Descriptive statistics for multivariate distributions. Statis-
tics & Probability Letters 1 327â€“332.
[48] Otto, F. (2001). The geometry of dissipative evolution equations: The
porous medium equation. Communications in Partial Differential Equa-
tions 26 101â€“174.
[49] PeyrÂ´e, G. and Cuturi, M. (2019). Computational optimal transport:
With applications to data science. Foundations and TrendsÂ® in Machine
Learning 11 355â€“607.
[50] Segers, J. (2022). Graphical and uniform consistency of estimated optimal
transport plans. arXiv:2208.02508.
[51] Serfling, R. (2002). A depth function and a scale curve based on spatial
quantiles. In Statistical Data Analysis Based on the L1-Norm and Related
Methods 25â€“38. Springer.
[52] Sriperumbudur, B. K., Gretton, A., Fukumizu, K., SchÂ¨olkopf, B.
and Lanckriet, G. R. (2010). Hilbert space embeddings and metrics on
probability measures. The Journal of Machine Learning Research 11 1517â€“
1561.
[53] SzabÂ´o, Z., Sriperumbudur, B. K., PÂ´oczos, B. and Gretton, A.
(2016). Learning theory for distribution regression. Journal of Machine
Learning Research 17 1â€“40.
[54] Tukey, J. W. (1975). Mathematics and the picturing of data. In Proceed-
ings of the International Congress of Mathematicians 2 523â€“531. Vancou-
ver.
[55] van der Vaart, A. W. and Wellner, J. A. (1996). Weak Convergence
and Empirical Processes. Springer New York.
[56] Vardi, Y. and Zhang, C.-H. (2000). The multivariate L1-median and
associated data depth. Proceedings of the National Academy of Sciences 97
1423â€“1426.
[57] Villani, C. (2003). Topics in Optimal Transportation. Graduate Studies
in Mathematics 58. American Mathematical Society, Providence, RI.
[58] Villani, C. (2009). Optimal Transport: Old and New. Springer-Verlag,
Berlin.

Bachoc, GonzÂ´alez-Sanz, Loubes, and Yao/Wasserstein Spatial Depth
48
[59] Virta,
J.
(2023).
Spatial
depth
for
data
in
metric
spaces.
arXiv:2306.09740.
[60] Wang, J.-L., Chiou, J.-M. and MÂ¨uller, H.-G. (2016). Functional data
analysis. Annual Review of Statistics and its Application 3 257â€“295.
[61] Zhou, Y. and Sharpee, T. O. (2021). Hyperbolic geometry of gene ex-
pression. Iscience 24.
[62] Zhuang, Y., Chen, X. and Yang, Y. (2022). Wasserstein K-means for
clustering probability distributions. Advances in Neural Information Pro-
cessing Systems 35 11382â€“11395.
[63] Zuo, Y. and He, X. (2006). On the limiting distributions of multivariate
depth-based rank sum statistics and related tests. The Annals of Statistics
34 2879 â€“ 2896.
[64] Zuo, Y. and Serfling, R. (2000). General notions of statistical depth
function. Annals of Statistics 461â€“482.
[65] Â´Alvarez Esteban, P. C., del Barrio, E., Cuesta-Albertos, J. A.
and MatrÂ´an, C. (2016). A fixed-point approach to barycenters in Wasser-
stein space. Journal of Mathematical Analysis and Applications 441
744â€“762.
