Consistent Bayesian Spatial Domain Partitioning Using Predictive
Spanning Tree Methods
Kun Huang1 and Huiyan Sang2
1Department of Statistics, Texas A&M University, College Station, US. Email:
k-huang@tamu.edu
2Department of Statistics, Texas A&M University, College Station, US. Email:
huiyan@stat.tamu.edu
Abstract
Bayesian model-based spatial clustering methods are widely used for their flexibility in
estimating latent clusters with an unknown number of clusters while accounting for spatial
proximity. Many existing methods are designed for clustering finite spatial units, limiting their
ability to make predictions, or may impose restrictive geometric constraints on the shapes of
subregions. Furthermore, the posterior clustering consistency theory of spatial clustering models
remains largely unexplored in the literature. In this study, we propose a Spatial Domain Random
Partition Model (Spat-RPM) and demonstrate its application for spatially clustered regression,
which extends spanning tree-based Bayesian spatial clustering by partitioning the spatial domain
into disjoint blocks and using spanning tree cuts to induce contiguous domain partitions. Under
an infill-domain asymptotic framework, we introduce a new distance metric to study the posterior
concentration of domain partitions.
We show that Spat-RPM achieves a consistent estimation
of domain partitions, including the number of clusters, and derive posterior concentration rates
for partition, parameter, and prediction. We also establish conditions on the hyperparameters of
priors and the number of blocks, offering important practical guidance for hyperparameter selection.
Finally, we examine the asymptotic properties of our model through simulation studies and apply
it to Atlantic Ocean data.
1
arXiv:2508.08324v1  [stat.ME]  9 Aug 2025

1
Introduction
Spatial clustering models
[6, 10, 4, 31] are essential for identifying and characterizing spatial
heterogeneity, enabling a deep understanding of spatial patterns and the underlying differences in
physical, social, or biological driving factors. Bayesian spatial clustering models have gained great
popularity due to their flexibility in modeling latent clustered variables within Bayesian hierarchical
frameworks while accounting for spatial information. These models specify a prior distribution over
the partition space and fit probabilistic models to the data to estimate underlying cluster memberships
and other model parameters. One concrete example is the spatially clustered regression model (see
[22, 33, 23, 48]), where the goal is to study the spatial heterogeneity in the latent relationship between
covariates and spatial response. Let [{si, x(si), y(si)}n
i=1 be the spatial data observed at locations
s1, . . . , sn âˆˆD âŠ‚R2, where x(si) is d-dimensional covariate and y(si) is the response variable at si.
Conditional on {x(si)}n
i=1, we write the likelihood of {y(si)}n
i=1 as Qn
i=1 PÎ¸(si){y(si) | x(si)}, where
PÎ¸(si){Â· | x(si)} is the conditional probability density function of y(si) with unknown parameter Î¸(si).
To account for spatial heterogeneity, we assume Î¸(si) = Î¸(siâ€²) if si and siâ€² belong to the same cluster,
and Î¸(si) Ì¸= Î¸(siâ€²) otherwise.
Most existing Bayesian spatial clustering methods consider a finite random partition prior model
to cluster the n observed locations into k0 disjoint sub-clusters [32, 37, 35, 17], but they cannot be used
to predict cluster memberships, regression parameters, or responses at new locations. Alternatively,
a domain partition model considers a random partition of the entire domain D into k0 disjoint sub-
domains, say {Dl,0}k0
l=1, such that âˆªk0
l=1Dl,0 = D, making it suitable for prediction tasks. Popular
domain partition models include binary decision trees [7, 15], which recursively split the domain into
non-overlapping hyper-rectangular regions, and Voronoi tessellation models [19, 18, 9], which partition
the domain into convex polygons. However, these shape constraints might be too restrictive for some
applications.
While spatial clustering methods have been widely studied and applied, their theoretical
development remains relatively limited.
Existing theoretical work of spatially clustered regression
model mostly focuses on showing that the posterior is a consistent estimate of the true regression
parameter or data-generating density [27, 30], while the more relevant problem of clustering consistency
has not been well studied due to its theoretical challenges. Many existing work on Bayesian posterior
clustering (see, e.g., [16, 28, 46, 2]) consistency focuses on exchangeable random set partition
models [13].
Nevertheless, these exchangeable random partition priors differ fundamentally from
those used in spatial clustering models, making some of the existing theoretical tools unsuitable for
directly analyzing spatial clustering. Most recently, [47] establishes clustering consistency under a
mixture model framework, assuming that the data are generated dependently from a disjoint union
of component graphs.
[33] establishes clustering consistency for spatial panel data assuming the
2

number of repeated measurements goes to infinity. However, in spatial statistics, it is more common
and reasonable to assume either an infill-domain asymptotic or an increasing-domain asymptotic
framework, where the number of spatial locations goes to infinity.
We propose a spatial domain random partition model (Spat-RPM) in the context of spatially
clustered regression for data [{si, x(si), y(si)}n
i=1. The model extends the finite spanning tree prior
[40, 26, 21] by modeling latent domain partition {Dl,0}k0
l=1. In Spat-RPM, we first discretize the domain
into small disjoint blocks. We construct spanning trees on blocks, based on which a contiguous domain
partition is induced after removing some edges and assigning locations within the same block to the
same cluster. We assign priors on spanning trees, the number of clusters, and the induced partitions.
We design an efficient Bayesian inference algorithm to draw posterior samples of domain partitions
and Î¸(Â·). We show in our numerical examples that Spat-RPM produces spatially contiguous clusters
with more flexible shapes while enabling spatial predictions.
The blocking technique in Spat-RPM reduces the infinite domain partition space to the finite
blocking partition space, making the estimation practical. The blocking also enables Spat-RPM to
handle larger-scale data by reducing the computational burden. Under a mild assumption on the
Minkowski dimension of the true partition boundary set and the shape of each subregion, we study
the approximation error between the blocking partition space and the true domain partition. In our
theoretical analysis, we establish conditions on the asymptotic rate of the number of blocks, balancing
the trade-off between approximation error and partition space dimensionality to achieve partition
consistency.
We conduct Bayesian posterior theoretical analysis for Spat-RPM, assuming an infill-domain
asymptotic framework. We establish the domain partition consistency theory from the ground up.
To study partition consistency, we formally define a valid distance metric for comparing two domain
partitions. We show that under the defined metric, the posterior domain partition converges to the
true partition, given certain conditions including a finite number of clusters, regularity constraints
on the domain boundary, and rates of hyperparameters. The clustering consistency of the observed
locations follows directly from the domain partition consistency result. We also show that the number
of clusters can be consistently estimated. Since the blocking partition prior model involves spanning
trees with a diverging number of nodes as n increases, we derive several original graph-theoretical
results related to spanning trees to establish partition consistency, which may be of independent
interest for future research. Furthermore, based on the partition consistency, we show the Bayesian
posterior contraction rate of Î¸(Â·) and prediction error. To the best of our knowledge, our work is
among the first to develop Bayesian spatial domain partitioning consistency under the spatial infill
domain asymptotic framework.
The rest of the paper is organized as follows. We introduce Spat-RPM in Section 2 and present
theoretical results in Section 3. In Section 4, we conduct a numerical simulation on U-shape to examine
3

the asymptotic properties of Spat-RPM. In Section 5, we apply our model to real data to demonstrate
results. Technical proofs are contained in Section 6 and Supplementary Material.
2
Methodology
2.1
Background of graphs and spanning trees
We start by introducing some concepts and notations of graphs. Let V = {v1, . . . , vnâˆ—} be nâˆ—vertices
and G = (V, E) be an undirected graph, where the edge set E is a subset of {(vi, viâ€²) : vi, viâ€² âˆˆ
V, vi Ì¸= viâ€²}. We call a sequence of edges {(vi0, vi1), (vi1, vi2), . . . , (vitâˆ’1, vit)} âŠ†E as a path of length
t between vi0 and vit, if all {vij}t
j=0 are distinct. A path is called a cycle if vi0 = vit and all other
vertices are distinct. A subgraph (V0, E0), where V0 âŠ†V and E0 âŠ†E, is called a connected component
of G if there is a path between any two vertices and there is no path between any vertex in V0 and
any vertex in V \V0, the difference between V and V0. Given an undirected graph G = (V, E), a subset
V0 âŠ†V is a contiguous cluster if there exists a connected subgraph G0 = (V0, E0), where E0 âŠ†E. We
say Ï€(V) = {V1, . . . , Vk} is a contiguous partition of V with respect to G, if Vj âŠ†V is a contiguous
cluster for j = 1, . . . , k, âˆªk
j=1Vj = V, Vj âˆ©Vjâ€² = âˆ…for j Ì¸= jâ€². For simplicity, we refer to contiguous
partitions (clusters) simply as partitions (clusters) in the following context.
A spanning tree of G is defined as a subgraph T = (V, ET ), where the edge set ET âŠ†E has no
cycle and connects all vertices. Hence, a spanning tree has nâˆ—vertices and nâˆ—âˆ’1 edges. See Figure 1
for an example of a spanning tree of a lattice graph. A well-known property of the spanning tree is
that we obtain k connected components of T , if k âˆ’1 edges are deleted from T . This property has
motivated the development of hierarchical generative prior models of spatial clusters. These models
begin with the construction of a spatial graph G, based on which a prior is defined over the spanning
tree space of G. Conditional on the spanning tree, prior models are assumed for the number of clusters
and a partition of the spanning tree. The likelihood of {y(si)}n
i=1 can be derived afterward given the
partition. Following this path, we describe below a domain partition prior model based on spanning
trees.
4

0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Cluster
1
2
3
(a)
(b)
(c)
Figure 1: Illustration of our partition model with K = 5. (a) Graph G = (V, E), where V is the set of
blocks, and edges in E are denoted by the red lines between adjacent blocks. (b) One spanning tree
obtained by cutting some edges of graph G in (a). (c) Domain partition Ï€âˆ—(D) induced by cutting
two edges (dashed lines) of the spanning tree in (b). Each cluster (denoted by different colors) is a
connected component. Locations within the same block have the same cluster membership.
2.2
A prior model for partitions
Without loss of generality, we assume D = [0, 1]2. Note that our method and theoretical results can be
easily extended to a more general domain that is homeomorphic to [0, 1]2 with the Euclidean metric
and a bi-Lipschitz homeomorphism. We first select an integer K, and segment D into K2 disjoint
blocks, say {Bm}K2
m=1, where each block Bm is a Kâˆ’1 Ã— Kâˆ’1 rectangle. We construct G = (V, E) as
a mesh grid graph of blocks, where V = {Bm}K2
m=1 and E is the set of edges connecting only adjacent
blocks (see Figure 1(a) for the constructed G with K = 5). Given graph G, write âˆ†as the space
of spanning trees induced from G. The following describes two popular approaches in the existing
literature to assign priors on spanning trees.
The first approach is the uniform spanning tree prior (UST, [1, 36, 40]), which assumes
T âˆI(T âˆˆâˆ†),
(1)
where I(Â·) is the indicator function. [3] establishes foundational principles for understanding UST.
The second approach is the random minimum spanning tree prior (RST, [12, 8, 26]). Given graph G,
RST assigns a weight we for âˆ€e âˆˆE with a uniform prior, and obtains the minimum spanning tree
(MST), defined as the spanning tree with the minimal Î£eâˆˆET we over all spanning trees induced from
G, i.e.,
T = MST({we}eâˆˆE), we
i.i.d.
âˆ¼Unif(0, 1),
(2)
where Unif(Â·, Â·) is the uniform distribution.
Either of priors (1) and (2) can be used in our model. It has been shown in Proposition 7 of [26]
that the MST algorithm (2) generates T with a strictly positive probability, for âˆ€T âˆˆâˆ†. Thus, priors
5

(1) and (2) have the same spanning tree support (see Figure 1(b) for an example of obtaining T from
G). Next, we assume the number of clusters k follows a truncated Poisson distribution with mean
parameter Î»:
k âˆ¼Poisson(Î») Â· I(1 â©½k â©½kmax),
(3)
where kmax is a pre-specified maximum number of clusters. Conditional on T and k, we assume a
uniform distribution on all possible partitions induced by T :
P{Ï€(V)|k, T } âˆI {Ï€(V) is induced from T and has k clusters} .
(4)
Note that Ï€(V) is a partition of blocks. Conditional on Ï€(V), the partition of D, say Ï€âˆ—(D), is obtained
immediately by assigning locations within the same block to the same cluster. Note that we use the
notation Â·âˆ—to emphasize that Ï€âˆ—(D) is induced from Ï€(V). See Figure 1(c) for an example of obtaining
Ï€âˆ—(D) from T .
Let S = {si}n
i=1 be the set of observed locations. Conditional on Ï€âˆ—(D), the partition of S, say
Ï€(S), is obtained immediately by assigning locations in S the same cluster memberships as in Ï€âˆ—(D).
2.3
The proposed Spat-RPM
Let Ï€âˆ—(D) = {Dâˆ—
1, . . . , Dâˆ—
k} be the domain partition with k clusters. Given a cluster Dâˆ—
j âˆˆÏ€âˆ—(D), let
Î¸j be the regression coefficient within it and write Î¸ = {Î¸j}k
j=1. Conditional on Ï€âˆ—(D), we write our
hierarchical model as
Î¸ | Ï€âˆ—(D), k, T , s, x âˆ¼
k
Y
j=1
P(Î¸j), and
(5)
y | Ï€âˆ—(D), k, T , s, x, Î¸ âˆ¼
k
Y
j=1
Y
siâˆˆDâˆ—
j
PÎ¸j{y(si) | x(si)},
(6)
where s = {si}n
i=1, x = {x(si)}n
i=1, y = {y(si)}n
i=1 and P(Î¸j) is the prior model we assume for the
unknown parameter Î¸j.
2.4
Prediction at new locations
Following priors in Section 2.2 and the hierarchical model in Section 2.3, we obtain the posterior
distribution of {Î¸, Ï€âˆ—(D)}, based on which we predict the distribution of the response variable y(s) |
s, x(s) for a new location s and covariate x(s). Given {Î¸, Ï€âˆ—(D)}, write Î¸(s) = Pk
j=1 Î¸jI(s âˆˆDâˆ—
j) as the
predicted regression coefficient at s. We predict the distribution of y(s) | s, x(s) by PÎ¸(s){Â· | x(s)}.
2.5
Bayesian Computations
We use Markov chain Monte Carlo (MCMC) algorithm to draw samples from the posterior
distribution P{Ï€âˆ—(D), k, T , Î¸ | s, x, y}.
Given Ï€âˆ—(D), we draw sample of Î¸ from distribution
6

P{Î¸ | Ï€âˆ—(D), k, T , s, x, y}.
This step is standard if we take a conjugate prior of Î¸ in (5), so we
omit the details. To sample Ï€âˆ—(D), we analytically integrate Î¸ out and sample from the collapsed
conditional distribution of Ï€âˆ—(D), k, T | s, x, y. Borrowing ideas from [26], at each MCMC iteration,
we propose one of four moves from the current state: birth, death, change, and hyper, with probabilities
rb(k), rd(k), rc(k) and rh(k), respectively. For the birth move, we split one cluster in the current Ï€âˆ—(D)
into two clusters by randomly removing an edge connecting two blocks in the same cluster in T . The
Metropolis-Hastings (M-H) acceptance ratio is
min

1,
Î»
k + 1 Ã— rd(k + 1)
rb(k)
Ã— P{y | Ï€âˆ—
new(D), k + 1, T , s, x}
P{y | Ï€âˆ—(D), k, T , s, x}

,
where Ï€âˆ—
new(D) is the new partition after removing the edge, and P{y | Ï€âˆ—(D), k, T , s, x} is the
integrated likelihood with Î¸ marginalized out. We derive the closed form of P{y | Ï€âˆ—(D), k, T , s, x} in
Equation (29) under a linear regression setting. For a death move, we randomly merge two adjacent
clusters in Ï€âˆ—(D). Specifically, an edge in T that connects two distinct clusters in Ï€âˆ—(D) is selected
uniformly, and the two clusters are then combined into a single cluster. The M-H acceptance ratio is
computed by
min

1, k
Î» Ã— rb(k âˆ’1)
rd(k)
Ã— P{y | Ï€âˆ—
new(D), k âˆ’1, T , s, x}
P{y | Ï€âˆ—(D), k, T , s, x}

.
For a change move, we first perform a birth move, then a death move. The purpose of the change move
is to encourage a better mixing of the sampler. The cluster number is unchanged after the change
move.
Finally, for a hyper move, we update the spanning tree T . For every edge e âˆˆE, we sample
a weight we âˆ¼Unif(0, 1/2), if e connects blocks within the same cluster, and we âˆ¼Unif(1/2, 1)
otherwise. Based on {we}eâˆˆE, Primâ€™s MST algorithm [34] is used to construct a new spanning tree
Tnew with a computational complexity of O(K2 log K). It has been shown that Tnew is guaranteed to
induce current partition Ï€âˆ—(D) [39]. The M-H acceptance rate is 1. Note that when applying RST
prior (2), this is an exact sampler [26]. When using UST prior (1), this is an approximated sampler
used in [40].
3
Main result
In this section, we introduce the theoretical result of the proposed Spat-RPM. Note that priors
specified in Sections 2.2 and 2.3 assume a general likelihood PÎ¸(si){y(si) | x(si)}.
For example,
PÎ¸(si){y(si) | x(si)} can be a Poisson, Gaussian or multinomial distribution.
In this section, to
simplify the theoretical analysis, we focus on the Gaussian distribution case, where we write our
model specifically as a linear regression form, based on which the theoretical analysis is conducted.
7

3.1
Linear regression setting
Under the linear regression setting, we write our model as
y(si) = Âµ(si) + Ïµ(si),
(7)
where Âµ(si) = xT (si)Î¸(si) is the regression mean of y(si), and {Ïµ(si)}n
i=1 are identically and
independently distributed (i.i.d.) mean zero Gaussian noises. Based on model (7), we assume an
independent conjugate Zellnerâ€™s g-prior on each Î¸j. Let Î¸ = {Î¸j}k
j=1. We modify models (5) and (6)
as
Î¸ | Ï€âˆ—(D), k, T , s, x âˆ¼
k
Y
j=1
PGaussian

Î¸j; 0, Î³nÏƒ2
 X
siâˆˆDâˆ—
j
x(si)xT (si)
â€ 
, and
(8)
y | Ï€âˆ—(D), k, T , s, x, Î¸ âˆ¼
k
Y
j=1
Y
siâˆˆDâˆ—
j
PGaussian{y(si); xT (si)Î¸j, Ïƒ2},
(9)
where PGaussian(Â·; a, Î£) denotes the probability density function of the Gaussian distribution with
mean a and covariance Î£, Î³ > 0 is a hyperparameter controlling the prior variance, Â·â€  denotes the
pseudoinverse. Ïƒ2 is the variance of Ïµ(si) which is often assumed unknown and assigned an inverse
Gamma prior in practice. For simplicity, we assume Ïƒ2 is a fixed value and drop the notation Ïƒ2 in
the left hand side of (8) and (9). However, we shall show that our theoretical results hold even when
Ïƒ2 is not fixed at its true value.
Under the linear regression setting, we predict the regression mean for a new observed location s
and covariate x(s). Following notations in Section 2.4, we predict the regression mean by Âµ{s, x(s)} =
xT (s)Î¸(s), for a given {Î¸, Ï€âˆ—(D)}.
3.2
Notations
We introduce some notations in this section. Denote âˆ¥Â· âˆ¥2 and âˆ¥Â· âˆ¥âˆas L2 norm and Lâˆnorm,
respectively. For two locations s1, s2, let d(s1, s2) = âˆ¥s1 âˆ’s2âˆ¥2 be the Euclidean distance. For two
spatial domains D1 and D2, we write d(D1, D2) = infs1âˆˆD1,s2âˆˆD2 d(s1, s2). For a constant Î´ > 0 and
a spatial domain D, we define the Î´-neighborhood of D as N(D, Î´) = {s âˆˆR2 : d(s, D) â‰¤Î´}. The
notation |D| is used as two ways: if D is a spatial domain, |D| represents its area; if D is a set, |D|
denotes its cardinality. Furthermore, for a spatial domain D, we denote âˆ¥Dâˆ¥as the number of observed
locations within it.
We use c and C to denote some constants independent of partitions Ï€âˆ—(D) (hence Ï€(S)). The
values of c and C may change from line to line.
For two positive series an and bn, we say an â‰«bn if an/bn â†’âˆ, and an â‰ªbn if an/bn â†’0. We
write an âˆ¼bn if there exist constants c, C > 0, such that c < an/bn < C, for all n â©¾1. We write
an = O(bn), if there exits a constant c > 0, such that an/bn â©½c for all n â©¾1. We write an = o(bn) if
an â‰ªbn.
8

For a positive integer a, we use Ia to denote an a Ã— a identity matrix. For a matrix A and a
constant c, we say A > c, if Î»min(A) > c, and A < c if Î»max(A) < c, where Î»min(A) and Î»max(A) are
the minimum and maximum eigenvalues of A, respectively.
3.3
Assumptions and main theorems
Before stating our main theorems, we introduce the required assumptions.
We first make an
assumption on the distribution of the observed locations {si}n
i=1.
Assumption 1. The observed locations {si}n
i=1 are i.i.d. on spatial domain D with a probability
density function PD(Â·) satisfying 0 < infsâˆˆD PD(s) â©½supsâˆˆD PD(s) < âˆ.
Assumption 1 is standard in spatial literature [45, 44]. Under Assumption 1, the observed locations
are randomly scattered over spatial domain D.
Recall that we assume D can be partitioned into k0 heterogeneous sub-domains {Dl,0}k0
l=1. We
define the partition boundary set of {Dl,0}k0
l=1 as
B = {s âˆˆD : there exist l Ì¸= lâ€², such that Dl,0 âˆ©N(s, Î´) Ì¸= âˆ…, and Dlâ€²,0 âˆ©N(s, Î´) Ì¸= âˆ…hold for âˆ€Î´ > 0}.
We make the following assumption on B and {Dl,0}k0
l=1.
Assumption 2. We assume the boundary set B and {Dl,0}k0
l=1 satisfy
2.1 B has a Î½-covering number N(B, Î½, âˆ¥Â· âˆ¥2) â©½cÎ½âˆ’1 for some constant c > 0.
2.2 For a given l âˆˆ{1, . . . , k0} and any two locations s, sâ€² âˆˆDl,0, there exists a path connecting s
and sâ€², say P(s, sâ€²), such that P(s, sâ€²) is contained in Dl,0 and
d{P(s, sâ€²), B} â©¾min{d(s, B), d(sâ€², B), C},
(10)
where C > 0 is some constant dependent on Dl,0.
The covering number condition in Assumption 2.1 is the same as that in [43, 25]. Specifically, if
the boundary set B is a curve with finite length, Assumption 2.1 is satisfied. Assumption 2.2 assumes
each sub-domain Dl,0 to be connected. Equation (10) is a mild condition for the shape of Dl,0, and it
is satisfied for some common shapes (e.g., finite union of regular polygons and circles).
Under our model, the domain partition space consists of all partitions induced from the random
minimum spanning tree model in Section 2.2, which may not include the true partition. There may
exist some blocks intersecting with B and containing misclustered locations, as our model assigns the
same cluster membership for locations within the same block. The following proposition is established
under Assumption 2.
9

Proposition
1. Under Assumption 2, there exists a contiguous domain partition Ï€âˆ—
0(D)
=
{Dâˆ—
1,0, . . . Dâˆ—
k0,0} in our partition model space, such that
|WÏ€âˆ—
0| =
k0
X
j=1
|{Bm : Bm âŠ†Dâˆ—
j,0, Bm âŠŠDj,0}| â©½cK
(11)
for some constant c, where WÏ€âˆ—
0 = âˆªk0
j=1{Bm : Bm âŠ†Dâˆ—
j,0, Bm âŠŠDj,0} is the set of blocks containing
misclustered locations.
We defer the proof of Proposition 1 to Section S.4. Proposition 1 states that there exists a Ï€âˆ—
0(D)
in our modelâ€™s partition space, such that |WÏ€âˆ—
0|, which is the number of blocks containing misclustered
locations, is upper bounded by cK. In the following context, we refer to â€œapproximation errorâ€ as the
area of blocks in WÏ€âˆ—
0. Note that each blockâ€™s area is Kâˆ’2. By Proposition 1, the approximation error
is bounded by Kâˆ’2 Ã— cK = O(Kâˆ’1). Hence, the larger K, the smaller the approximation error.
For the sub-domain Dl,0, l = 1, . . . , k0, let Î¸l,0 be the corresponding true regression coefficient. We
make following assumptions on {Î¸l,0}k0
l=1 and {x(si)}n
i=1, respectively.
Assumption 3. There exists a constant c, such that minlÌ¸=lâ€² âˆ¥Î¸l,0 âˆ’Î¸lâ€²,0âˆ¥2 > c > 0.
Assumption 4. Conditional on {si}n
i=1, we assume {x(si)}n
i=1 are independent d-dimensional
bounded random variables. Furthermore, we assume that there exist constants c and C, such that
0 < c < E{x(si)xT (si) | si} < C holds for 1 â©½i â©½n.
Assumption 3 ensures sufficient separation between clusters for the identification of sub-domains
{Dl,0}k0
l=1. We consider a random covariate design, and Assumption 4 is to avoid the colinearity of
covariates, and is similar to Assumption (A2) in [44]. The boundness conditions of the covariate x(si)
and E{x(si)xT (si) | si} are standard in linear regression setting, see also Assumption (C1) in [26] and
Assumption (A2) in [29].
The next assumption is on the orders of hyperparameters, which provide important practical
guidance for selecting K and Î» in our prior model.
Assumption 5. We assume the number of blocks hyperparameter K and the Poisson hyperparameter
Î» in (3) satisfy
5.1 K âˆ¼
q
n
log1+Î±b(n) for some Î±b > 0.
5.2 Î» = o(1), and log(Î»âˆ’1) âˆ¼
n
logÎ±p(n) for some 0 < Î±p < Î±b.
Recall that after Proposition 1, we illustrate that a larger K indicates a smaller approximation
error of Ï€âˆ—
0(D). However, since the number of all the possible block partitions grows exponentially
with K2, a larger K also increases the difficulty in obtaining the correct partition due to the curse of
dimensionality. Thus, a trade-off analysis is necessary when choosing the value of K. Assumption 5.1
10

provides a rate condition for K, under which the partition consistency can be achieved while keeping
the approximation error relatively small. Note that K should satisfy K â‰¤n1/2, otherwise, some blocks
do not contain any observed locations. The order of K by Assumption 5.1 is marginally smaller than
n1/2.
A similar trade-off exists for the rate of Î».
Since the model with a larger number of clusters
provides enhanced flexibility for data fitting, the data likelihood typically prefers a larger number
of clusters. To avoid such â€œoverfittingâ€, the Poisson hyperparameter Î» serves as a penalty for the
number of clusters, which is the rationale of Î» = o(1) in Assumption 5.2. On the other hand, the rate
of Î» going to zero must not be too rapid, otherwise, the number of clusters can be underestimated.
Assumption 5.2 provides the rate condition for Î» to obtain partition consistency.
Recall that we use âˆ†to denote the space of spanning trees induced from G. The next assumption
is on the priors of our model.
Assumption 6. We make the following assumptions on the priors of our model.
6.1 The true number of clusters k0 satisfies k0 â‰¤kmax, for kmax specified in (3).
6.2 For Ï€âˆ—
0(D) in Proposition 1, we assume
sup
T1âˆˆâˆ†,T2âˆˆ{T âˆˆâˆ†:Ï€âˆ—
0(D) can be induced from T }
P(T1)
P(T2) = O[exp{cK log(K)}]
for some constant c.
Assumption 6.1 ensures the true number of clusters is within our prior upper bound on k.
Assumption 6.2 assumes that the spanning treeâ€™s probability of inducing Ï€âˆ—
0(D) is not excessively
small. For the UST prior specified in (1), Assumption 6.2 is satisfied immediately since the prior ratio
of any two spanning trees is 1.
We next define a distance measure of two spatial domain partitions, say Ï€1(D) = {D11, . . . , D1k1}
and Ï€2(D) = {D21, . . . , D2k2}, where k1 and k2 are their respective number of clusters. We define the
â€œdistanceâ€ between Ï€1(D) and Ï€2(D) as
Ïµ{Ï€1(D), Ï€2(D)} = 2 âˆ’|D|âˆ’1
 k1
X
j=1
max
lâˆˆ{1,...,k2} |D1j âˆ©D2l| +
k2
X
l=1
max
jâˆˆ{1,...,k1} |D1j âˆ©D2l|

.
(12)
Similarly, for two partitions of S, say Ï€1(S) = {S11, . . . , S1k1} and Ï€2(S) = {S21, . . . , S2k2}, where k1
and k2 are the number of clusters in each partition, respectively, we define the â€œdistanceâ€ between
Ï€1(S) and Ï€2(S) as
Ïµn{Ï€1(S), Ï€2(S)} = 2 âˆ’nâˆ’1
 k1
X
j=1
max
lâˆˆ{1,...,k2} |S1j âˆ©S2l| +
k2
X
l=1
max
jâˆˆ{1,...,k1} |S1j âˆ©S2l|

.
(13)
We can consider Ïµn(Â·, Â·) as a discrete version of Ïµ(Â·, Â·). [41] first introduces the same distance measure
as (13) (differing by a normalization term) for comparing two discrete set partitions. We extend Ïµn(Â·, Â·)
11

to Ïµ(Â·, Â·) in this paper for comparing two spatial domain partitions. The idea of Ïµ(Â·, Â·) (and Ïµn(Â·, Â·)) is
based on set matching. For each sub-domain D1j in Ï€1(D), we find a â€œbest matchedâ€ sub-domain in
Ï€2(D), defined as the one sharing the largest intersection area with D1j. The corresponding intersection
area is then computed by maxlâˆˆ{1,...,k2} |D1j âˆ©D2l|. If two partitions are close, the summation of the
â€œbest matchedâ€ areas (i.e., Pk1
j=1 maxlâˆˆ{1,...,k2} |D1j âˆ©D2l|) is expected to be close to |D|, leading to
a small Ïµ(Â·, Â·). The same rationale applies to Pk2
l=1 maxjâˆˆ{1,...,k1} |D1j âˆ©D2l|. Through the definition,
we can see Ïµ(Â·, Â·) takes values in [0, 2) and equals 0 when Ï€1(D) = Ï€2(D). The same property holds
for Ïµn(Â·, Â·). Roughly speaking, we can consider Ïµ{Ï€1(D), Ï€2(D)}/2 (Ïµn{Ï€1(S), Ï€2(S)}/2) as the â€œmis-
matchedâ€ percentage for Ï€1(D) and Ï€2(D) (Ï€1(S) and Ï€2(S)). Furthermore, we have the following
result for Ïµ(Â·, Â·) and Ïµn(Â·, Â·).
Proposition 2. Ïµ(Â·, Â·) and Ïµn(Â·, Â·) defined in (12) and (13) are distances, in the sense that they satisfy
the axioms for a distance, i.e., non-negativity, the identity of indiscernibles, symmetry, and triangle
inequality.
We defer the proof of Proposition 2 to Section S.5. Denote {Sl,0 = {si âˆˆDl,0 : 1 â©½i â©½n}}k0
l=1 as
the true partition of S. Write D = [{si, x(si), y(si)}n
i=1 as the observed data. The following Theorem 1
establishes the partition consistency of the posterior domain partition Ï€âˆ—(D) and the observed location
partition Ï€(S), with respect to distance Ïµ(Â·, Â·) and Ïµn(Â·, Â·), respectively.
Theorem 1. Let Î±0 be a positive value. Under Assumptions 1, 2, 3, 4, 5 and 6, there exist some
positive constants c1, c2 and c3 (which are dependent on Î±0), such that with probability tending to 1,
we have
P(|Ï€âˆ—(D)| = k0 | D) â©¾1 âˆ’c1 exp{âˆ’c2n1/2 logÎ±0+(1+Î±b)/2(n)},
(14)
P(Ïµ[Ï€âˆ—(D), {Dl,0}k0
l=1] â©½c3nâˆ’1/2 logÎ±0+(1+Î±b)/2(n) | D) â©¾1 âˆ’c1 exp{âˆ’c2n1/2 logÎ±0+(1+Î±b)/2(n)} (15)
and
P(Ïµn[Ï€(S), {Sl,0}k0
l=1] â©½c3nâˆ’1/2 logÎ±0+(1+Î±b)/2(n) | D) â©¾1 âˆ’c1 exp{âˆ’c2n1/2 logÎ±0+(1+Î±b)/2(n)}. (16)
Theorem 1 states the partition consistency of our model: with probability tending to 1, the
posterior partition achieves the correct cluster number, and Ïµ[Ï€âˆ—(D), {Dl,0}k0
l=1] and Ïµn[Ï€(S), {Sl,0}k0
l=1]
are of order nâˆ’1/2 logÎ±0+(1+Î±b)/2(n) with a high probability. Since no existing literature on spatial
domain partition consistency is available for a direct comparison with Theorem 1, we refer to a
change point detection result in a one-dimensional space. If the spatial domain degenerates to one
dimension, estimating a contiguous partition of it is equivalent to detecting change points in one-
dimensional space. According to Theorem 3 in [11], the number of change points in a one-dimensional
space can be consistently estimated, which aligns with Equation (14). However, Theorem 7 in [11]
indicates that Ïµn(Â·, Â·) of one-dimensional change point detection result is of order log(n)/n, which
12

is smaller than our result. This difference arises because partitioning in a spatial domain involves
a substantially larger partition space than the one-dimensional case, increasing the complexity of
achieving partition consistency. See also [24] for a similar one-dimensional change point detection
result under the Bayesian context.
Recall that Ï€âˆ—
0(D) = {Dâˆ—
1,0, . . . , Dâˆ—
k0,0} in Proposition 1. For a given Ï€âˆ—(D) = {Dâˆ—
1, . . . , Dâˆ—
k}, we
write M(Dâˆ—
j) = argmaxlâˆˆ{1,...,k0}|Dâˆ—
j âˆ©Dâˆ—
l,0| as the index of the sub-domain in {Dâˆ—
l,0}k0
l=1 with the largest
intersection area with Dâˆ—
j. Dâˆ—
M(Dâˆ—
j ),0 is considered as the â€œbest matchedâ€ sub-domain in {Dâˆ—
l,0}k0
l=1 for
Dâˆ—
j. Thus, roughly speaking, we consider Î¸M(Dâˆ—
j ),0 as the â€œtrueâ€ regression coefficient in Dâˆ—
j. For a
new observed location s and covariate x(s), write Âµ0{s, x(s)} = xT (s)Î¸0(s) as the true regression mean,
where Î¸0(s) = Pk0
l=1 Î¸l,0I(s âˆˆDl,0) is the true regression coefficient. Recall the definition of Âµ{s, x(s)}
in Section 3.1. The following Theorem 2 states the posterior contraction rate of Î¸ and the prediction
error of Âµ{s, x(s)}.
Theorem 2. Under Assumptions 1, 2, 3, 4, 5 and 6 and for the same Î±0 in Theorem 1, with probability
tending to 1, we have
P[{Î¸M(Dâˆ—
j ),0}k
j=1 = {Î¸l,0}k0
l=1 | D] â†’1,
(17)
P{ max
1â©½jâ©½k âˆ¥Î¸j âˆ’Î¸M(Dâˆ—
j ),0âˆ¥2 > Mnnâˆ’1/2 logÎ±0+(1+Î±b)/2(n) | D} â†’0,
(18)
P
Z
D
âˆ¥Î¸(s) âˆ’Î¸0(s)âˆ¥2
2P(s)ds > Mâ€²
nnâˆ’1/2 logÎ±0+(1+Î±b)/2(n) | D

â†’0, and
(19)
P
ZZ
[Âµ{s, x(s)} âˆ’Âµ0{s, x(s)}]2P(s, x)dsdx > Mâ€²â€²
nnâˆ’1/2 logÎ±0+(1+Î±b)/2(n) | D

â†’0
(20)
for any sequences Mn, Mâ€²
n, Mâ€²â€²
n â†’âˆ.
Equation (17) states that with probability tending to 1, the set {Î¸M(Dâˆ—
j ),0}k
j=1 is the same as the
set {Î¸l,0}k0
l=1. Through the interpretation of Î¸M(Dâˆ—
j ),0, âˆ¥Î¸j âˆ’Î¸M(Dâˆ—
j ),0âˆ¥2 specified in (18) is considered
as the distance between Î¸j and its â€œtrueâ€ value.
From Equation (18), the posterior contraction
rate of Î¸j is of the order nâˆ’1/2 logÎ±0+(1+Î±b)/2(n), which is slightly slower than the classic parametric
contraction rate nâˆ’1/2. This is non-trivial, as it indicates that the unknown spatial partition caused by
spatial heterogeneity impacts the posterior contraction rate of Î¸j solely at the order of some power of
log(n). Equations (19) - (20) provide the posterior predictive contraction rates of Î¸(s) and Âµ{s, x(s)},
respectively. The rate given by (20) is the same as that in Corollary 6 of [26] (if ignoring logarithmic
terms), who considers the posterior contraction rate for the regression mean at {si}n
i=1.
4
Simulation studies
In this section, we conduct simulation studies to assess the asymptotic property of the proposed Spat-
RPM model and make a comparison with the Bayesian spatially clustered varying coefficient (BSCC)
model proposed in [26]. We generate data in a U-shape domain as shown in Figure 2(a). The U-shape
13

domain is partitioned into three sub-domains, {Dl,0}3
l=1, as indicated by different colors in Figure 2(a):
D1,0 is the upper arm, D2,0 is the lower arm, and D3,0 is the middle circle. Let n be the sample size,
the data is generated by
y(si) = xT (si)Î¸(si) + Ïµ(si), 1 â©½i â©½n,
where x(si) = {x1(si), x2(si)}T is a two dimensional vector, with x1(si) â‰¡1 and {x2(si)}n
i=1
being i.i.d. Unif(âˆ’1, 1), Î¸(si) = Î¸1,0, Î¸2,0 and Î¸3,0, for si âˆˆD1,0, D2,0 and D3,0, respectively,
Î¸1,0 = (0, 1)T , Î¸2,0 = (1, 0)T , Î¸3,0 = (2, âˆ’1)T , and {Ïµ(si)}n
i=1 are i.i.d. Gaussian noise with mean 0
and variance 9. The sampling locations {si}n
i=1 are uniformly distributed within the U-shape.
(a)
(b)
(c)
Figure 2: (a) U-shape domain with {Dl,0}3
l=1 represented by different colors.
(b) One randomly
chosen posterior partition sample with different colors representing different clusters. (c) The spatial
distribution of absolute prediction errors. The darker the color, the larger the absolute prediction
error.
Given an integer K, to construct graph G, we start from a mesh grid graph of K2 square blocks
with side length Kâˆ’1 in [0, 1]2. Then we remove blocks from the graph without observed locations. We
set kmax = 5, and noise variance Ïƒ2 = 1. Note that the noise variance we set is different from the true
variance (which is 9). We will see from the result in Section 4.1 that this noise variance misspecification
doesnâ€™t affect the partition consistency of our model, which aligns with our theoretical result.
We obtain posterior samples following strategies in Section 2.5. We run 20000 MCMC iterations
with a burn-in period 5000 and a thinning parameter 5.
Finally, we obtain posterior samples
{ks, Ï€âˆ—
s, {Î¸s,j}ks
j=1}M
s=1, where M = 3000, and ks, Ï€âˆ—
s, {Î¸s,j}ks
j=1 are the posterior cluster number, domain
partition and regression coefficients, respectively, at the s-th MCMC sample. The asymptotic analysis
and comparison with the BSCC model are conducted in Sections 4.1 and 4.2, respectively.
4.1
Asymptotic analysis
To investigate the asymptotic property of our model, we compare the posterior samples with
n = (100, 500, 1000, 2000, 3000, 4000).
We select the number of blocks hyperparameter K and
14

Poisson hyperparameter Î» following Assumption 5, i.e., K = cbn1/2 logâˆ’(1+Î±b)/2(n) and log(Î»âˆ’1) =
cpn logâˆ’Î±p(n) with cb = 5, Î±b = 1, cp = 0.1 and Î±p = 0.5.
Figure 3(a) shows boxplots of {ks}M
s=1 under different sample sizes n. We can see that when n is
small, the number of clusters is overestimated. However, when n is larger than 2000, the posterior
number of clusters concentrates at the true value. This aligns with Equation (14) in Theorem 1.
100
500
1000
2000
3000
4000
3.0
3.5
4.0
4.5
5.0
n
Number of clusters
100
500
1000
2000
3000
4000
0.4
0.6
0.8
1.0
1.2
n
Normalized cluster error
100
500
1000
2000
3000
4000
0
10
20
30
40
n
Normalized error of Î¸
(a) Boxplots of {ks}M
s=1
(b) Boxplots of {en,s,1}M
s=1
(c) Boxplots of {en,s,2}M
s=1
2000
3000
4000
0.5
1.0
1.5
2.0
2.5
n
Normalized error of Î¸
100
500
1000
2000
3000
4000
0
5
10
15
20
25
30
n
Normalized error of Âµ
2000
3000
4000
0.5
0.7
0.9
1.1
n
Normalized error of Âµ
(d) Zoomed-in of (c)
(e) Boxplots of {en,s,3}M
s=1
(f) Zoomed-in of (e)
Figure 3: Fitting results under different sample sizes n
Next, we evaluate the asymptotic property of other metrics. For a given n and the corresponding
posterior samples {ks, Ï€âˆ—
s, {Î¸s,j}ks
j=1}M
s=1, we compute the following normalized errors according to
Theorems 1 and 2:
en,s,1 = n1/2Ïµ[Ï€âˆ—
s, {Dl,0}3
l=1]
logÎ±0+(1+Î±b)/2(n)
, and en,s,2 = n1/2 max1â©½jâ©½ks âˆ¥Î¸s,j âˆ’Î¸Ms,j,0âˆ¥2
logÎ±0+(1+Î±b)/2(n)
,
where Î±0 = 0.1, Ms,j is the index of the sub-domain in {Dl,0}3
l=1 with the largest intersection area
with the j-th cluster, at the s-th MCMC sample. To study the prediction error of our model, we
randomly generate 5000 locations uniformly distributed in the U-shape domain, and the corresponding
covariates. Following the procedure described in Section 3.1, we write Âµs as the prediction vector of
15

the regression mean at the sampled 5000 locations, given a posterior sample [ks, Ï€âˆ—
s, {Î¸s,j}ks
j=1]. Let Âµ0
be the true value of Âµs. We define the normalized prediction error as
en,s,3 = n1/2 Ã— 5000âˆ’1âˆ¥Âµs âˆ’Âµ0âˆ¥2
2
logÎ±0+(1+Î±b)/2(n)
.
According to Theorems 1 and 2, the above three normalized errors are bounded by constants with
a high probability when n is large. Figures 3(b) - 3(f) show boxplots of {en,s,1}M
s=1, {en,s,2}M
s=1, and
{en,s,3}M
s=1 under different n. We can see that the three normalized errors first decrease fast as n
increases, then stay stable when n is larger than 2000, which is consistent with our theory. When n
is small, large normalized errors may result from the misspecification of Ïƒ2 and having few locations
within each block.
Specifically, we focus on posterior samples when n = 4000 to illustrate the effectiveness of our
model. Figures 2(b) - (c) show a randomly chosen posterior sample of the partition and the spatial
distribution of the absolute prediction errors, computed as the absolute value of entries in Âµsâˆ’Âµ0. We
can see that the posterior partition result recovers the true partition well, except for some locations near
boundaries as expected. The values of {Î¸s,j}3
j=1 under this partition are Î¸s,1 = (0.022, 1.068)T , Î¸s,2 =
(0.915, âˆ’0.095)T and Î¸s,3 = (1.969, âˆ’0.874)T , close to the true values. We can also see from Figure 2(c)
that the absolute prediction errors are relatively small, except for those â€œwronglyâ€ clustered locations
near boundaries.
4.2
Comparison with the BSCC model
We compare the performance of our model with BSCC in [26] in this section. One key difference
between these two models is that BSCC doesnâ€™t use the blocking technique and only provides clustering
of the observed locations. We conduct the comparison for n = 4000 and repeat the simulation for 100
times. For each repeat, we fit BSCC and our model to the same simulated data, respectively. The
hyperparameters in our model are chosen in the same way as in Section 4.1, and we use the default
hyperparameter setting for the BSCC model as specified in [26]. We compare the posterior number of
clusters, mean absolute error (MAE), and continuous ranked probability scores (CRPS, [14]) for the
regression mean prediction, and computing time between two models.
Figures 4(a) - (b) show the posterior cluster number comparison between the two models. We can
see from Figures 4(a) - (b) that the number of clusters in our model concentrates at the true value for
all MCMC samples and repeats. On the contrary, BSCC tends to overestimate the number of clusters
according to Figure 4(a). Figure 4(b) shows that for BSCC, the posterior probability of the correct
cluster number is smaller than 0.2 in most of repeats. The result indicates that our model shows
significant improvement in terms of estimating the number of clusters, attributable to the blocking
technique and hyperparameter selection guidelines in Assumption 5.
Since the prediction method of a new location is not provided in [26], we compare prediction errors
16

Spatâˆ’RPM
BSCC
Number of clusters
1
3
5
7
9
13
Spatâˆ’RPM
BSCC
0.0
0.2
0.4
0.6
0.8
1.0
Probability of correct k
âˆ’0.08
âˆ’0.04
0.00
0.04
Difference of CRPS
âˆ’0.12
âˆ’0.08
âˆ’0.04
0.00
Difference of MAE
(a)
(b)
(c)
(d)
Figure 4: (a) Boxplots of the posterior number of clusters for different MCMC samples, from one
randomly selected repeat. (b) Boxplots of the posterior probability of correct cluster number. 100
repeats are taken. (c) Boxplot of {CRPSr,1 âˆ’CRPSr,2}100
r=1. (d) Boxplot of {MAEr,1 âˆ’MAEr,2}100
r=1
.
of the observed locations. Let Âµ be the vector of the regression mean of the observed locations. Denote
Âµr,s,1 and Âµr,s,2 as the predictions of Âµ based on the s-th posterior sample from the r-th simulation
repeat for our model and BSCC, respectively. We compute CRPS values for two models at r-th repeat,
say CRPSr,1 and CRPSr,2, based on {Âµr,s,1}M
s=1 and {Âµr,s,2}M
s=1, respectively. We also compute two
modelsâ€™ MAE as
MAEr,1 = nâˆ’1Mâˆ’1
M
X
s=1
(Âµr,s,1 âˆ’Âµr,0)

1, and MAEr,2 = nâˆ’1Mâˆ’1
M
X
s=1
(Âµr,s,2 âˆ’Âµr,0)

1,
where âˆ¥Â·âˆ¥1 is L1 norm and Âµr,0 is the true value. We compute the difference of CRPS (MAE) between
two models as CRPSr,1 âˆ’CRPSr,2 (MAEr,1 âˆ’MAEr,2) and the results are shown in Figures 4(c) - (d).
By definition, a negative difference indicates that our model performs better. From Figures 4(c)
and (d), we observe that more than half of the simulation repeats show a negative CRPS difference,
while nearly all repeats show a negative MAE difference. To further evaluate this, we perform a one-
sample t-test to determine whether the mean difference is significantly negative. For CRPS (MAE),
the t-test yields a mean difference of âˆ’0.00875 (âˆ’0.03) with a p-value < 0.001 (< 0.001). These results
demonstrate that our model achieves higher prediction accuracy than BSCC, likely due to the more
accurate partition estimation.
Finally, we compare the computing times for MCMC sampling of the posterior distribution between
the two models. Using the â€œmicrobenchmarkâ€ package in R, we obtain the average computing time
over 10 runs of the MCMC procedure. The average computing times for our and BSCC models are
15 seconds and 52 seconds, respectively. This demonstrates a significant reduction in computing time,
which is attributed to the reduced partition space achieved through the blocking technique.
17

5
Real data analysis
In this section, we apply Spat-RPM to study the temperature-salinity (T-S) relationship of seawater
in the Atlantic Ocean.
The study aims to identify the Antarctic Intermediate Water (AAIW),
characterized by a negative T-S relationship [38]. The data on temperature and salinity is obtained
from National Oceanographic Data Center (https://www.nodc.noaa.gov/OC5/woa13/), and the
detailed data description can be found in [26]. The n = 1936 observed locations are shown in Figure
5(a).
0.0
0.1
0.2
0.3
0.4
âˆ’0.50
âˆ’0.45
âˆ’0.40
âˆ’0.35
âˆ’0.30
âˆ’0.25
Sh
Sv
34.0
34.5
35.0
35.5
36.0
y
(34.66 , 0.03)
(33.57 , 0.12)
(34.03 , âˆ’0.05)
(34.68 , âˆ’0.11)
0.4
0.3
0.2
0.1
0
âˆ’0.50 âˆ’0.45 âˆ’0.40 âˆ’0.35 âˆ’0.30 âˆ’0.25
Sh
Sv
Cluster
1
2
3
4
(a)
(b)
(c)
Figure 5: (a) The observed locations in salinity data. (b) Partition result from our model. Four
clusters are obtained and represented by different colors. The number annotations within clusters are
the corresponding posterior sample of {Î¸(si)}n
i=1. AAIW is identified as the area of clusters 1 and 4.
(c) The point identified by BSCC with negative Î¸2(si) (hence as AAIW).
Let si = (sh,i, sv,i) be the location of the i-th observation. Write y(si) as the salinity at si and
x(si) = (1, Temp(si))T , where Temp(si) is the temperature at si. We model the T-S relationship
as y(si) = xT (si)Î¸(si) + Ïµ(si), where Î¸(si) = (Î¸1(si), Î¸2(si))T is the unknown piecewise constant
regression coefficient and Ïµ(si) is a Gaussian noise.
We apply our model described in Section 2. We use the sample variance of {y(si)}n
i=1 as the value
of Ïƒ2 in our model. We use the same values of Î³ and kmax as in simulation studies, as well as the
same formulas to select hyperparameters K and Î», i.e., K = cbn1/2 logâˆ’(1+Î±b)/2(n) and log(Î»âˆ’1) =
cpn logâˆ’Î±p(n) with Î±b = 1 and Î±p = 0.5. To decide the values of cb and cp, we adopt Watanabe-Akaike
information criterion (WAIC, [42]) and select (cb, cp) âˆˆ(1, 3, 5, 7) Ã— (0.01, 0.1, 0.5) as the values that
minimize WAIC. We use MCMC to sample from the posterior distribution of partitions and {Î¸(si)}n
i=1.
The number of MCMC iterations, the burn-in period, and thinning parameter are the same as in the
simulation setting. We use the â€œsalsoâ€ package in R [5] to obtain a point estimation of the partition
from posterior samples, which is shown in Figure 5(b). We display the partition result obtained from
the BSCC model by [26] in Figure 5(c) for comparison.
As shown in Figures 5(b) - (c), among the four clusters identified by our model, two clusters
18

(clusters 1 and 4) have negative Î¸2(si). Thus, the AAIW identified by our model corresponds to the
areas of clusters 1 and 4. Compared to Figure 5(c), we can see that the majority of the AAIW region
is the same for both models, while our model includes some areas with Sh > âˆ’0.3 as part of the
AAIW. Specifically, we observe that clusters 1 and 4 correspond to shallow water and deep water
areas, respectively. It is reasonable to observe some differences in the T-S relationship between these
two areas.
Although the AAIW identified by Spat-RPM and BSCC is consistent, these two models have
fundamental differences. The BSCC model conducts partitioning exclusively at the observed locations,
restricting its ability to identify the cluster memberships of unobserved locations. Moreover, due to the
lack of partition consistency, AAIW in the BSCC model is estimated by checking signs of posterior
samples {Î¸2(si)}n
i=1, which may be inconsistent with the partition result.
In contrast, our model
identifies the AAIW directly from the posterior domain partition, offering a simpler and more natural
interpretation.
6
Proof
6.1
Additional notations and definitions
Throughout the proof, in general, subscripts i, (j, l), m denote the index for locations, clusters in a
partition, and blocks, respectively. k is used as the number of clusters, and k0 is the true number of
clusters.
To simplify notations, we use Ï€âˆ—(Ï€) as the shorthand of Ï€âˆ—(D) (Ï€(S)). For two block partitions
Ï€âˆ—
1 and Ï€âˆ—
2, we write Ï€âˆ—
1 âˆ©Ï€âˆ—
2 as the intersection partition, such that two locations are within the same
cluster if and only if they are within the same cluster by both Ï€âˆ—
1 and Ï€âˆ—
2. We say a block Bm is a
â€œboundary blockâ€ if Bm intersects with the boundary set B. Let Îâˆ—= {Ï€âˆ—: P(Ï€âˆ—) > 0} be the space
of partitions under priors in Section 2.2, and we write ËœÎâˆ—= {Ï€âˆ—: Ï€âˆ—= Ï€âˆ—
1 âˆ©Ï€âˆ—
2 for some Ï€âˆ—
1, Ï€âˆ—
2 âˆˆÎâˆ—}.
It is easy to see that Îâˆ—âŠ†ËœÎâˆ—. Recall the contiguous partition Ï€âˆ—
0 in Propsition 1. Under Assumption
6, we have Ï€âˆ—
0 âˆˆÎâˆ—.
For two domain partitions Ï€1(D) = {D11, . . . , D1k1} and Ï€2(D) = {D21, . . . , D2k2} with k1
and k2 denoting the number of clusters, we decompose Ïµ{Ï€1(D), Ï€2(D)} = Ïµ1{Ï€1(D), Ï€2(D)} +
Ïµ2{Ï€1(D), Ï€2(D)}, where
Ïµ1{Ï€1(D), Ï€2(D)} = 1 âˆ’|D|âˆ’1
 k1
X
j=1
max
lâˆˆ{1,...,k2} |D1j âˆ©D2l|

, and
(21)
Ïµ2{Ï€1(D), Ï€2(D)} = 1 âˆ’|D|âˆ’1
 k2
X
l=1
max
jâˆˆ{1,...,k1} |D1j âˆ©D2l|

.
(22)
Table 1 summarizes main notations used throughout the proof.
19

Table 1: Commonly used notations throughout the proof
Notation
Meaning
Ï€âˆ—= {Dâˆ—
1, . . . , Dâˆ—
k}
Shorthand of Ï€âˆ—(D), which is a domain partition under our model
Ï€ = {S1, . . . , Sk}
Shorthand of Ï€(S), which is a partition of {si}n
i=1 and can be induced from Ï€âˆ—
{Dl,0}k0
l=1
True domain partition
{Sl,0}k0
l=1
True partition of {si}n
i=1
Ï€âˆ—
0 = {Dâˆ—
1,0, . . . , Dâˆ—
k0,0}
Domain partition defined in Proposition 1
M(Dâˆ—
j)
argmaxlâˆˆ{1,...,k0}|Dâˆ—
j âˆ©Dâˆ—
l,0|
6.2
Proof framework
For a given Ï€âˆ—, we decompose Ïµ[Ï€âˆ—, {Dl,0}k0
l=1] as
Ïµ[Ï€âˆ—, {Dl,0}k0
l=1]
Triangle inequality of Ïµ(Â·, Â·)
â©½
Ïµ(Ï€âˆ—, Ï€âˆ—
0) + Ïµ[Ï€âˆ—
0, {Dl,0}k0
l=1].
(23)
According to Proposition 1, it is easy to see that Ïµ[Ï€âˆ—
0, {Dl,0}k0
l=1] â©½cK Ã— Kâˆ’2 = cKâˆ’1 Assumption 5
âˆ¼
nâˆ’1/2 log(1+Î±b)/2(n).
To study Ïµ[Ï€âˆ—, {Dl,0}k0
l=1], it then remains to study Ïµ(Ï€âˆ—, Ï€âˆ—
0) = Ïµ1(Ï€âˆ—, Ï€âˆ—
0) +
Ïµ2(Ï€âˆ—, Ï€âˆ—
0). We will focus on the bound of Ïµ1(Ï€âˆ—, Ï€âˆ—
0), from which the bound of Ïµ2(Ï€âˆ—, Ï€âˆ—
0) can be derived
similarly. Note that by definition, Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2 takes only integer values, and 0 â©½Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2 â©½K2.
Recall the definition of M(Dâˆ—
j) after Theorem 1. Based on {M(Dâˆ—
j)}|Ï€âˆ—|
j=1 and the number of clusters,
we divide the prior block partition space, Îâˆ—, into five categories:
Î âˆ—
1 = {Ï€âˆ—âˆˆÎâˆ—: |Ï€âˆ—| < k0}, Î âˆ—
2 = {Ï€âˆ—âˆˆÎâˆ—: |Ï€âˆ—| > k0},
Î âˆ—
3 =
n
Ï€âˆ—âˆˆÎâˆ—: |Ï€âˆ—| = k0 and {M(Dâˆ—
j)}|Ï€âˆ—|
j=1 âŠŠ{1, . . . , k0}
o
,
Î âˆ—
4 = âˆªK2
q=âŒŠK logÎ±0(n)âŒ‹Î âˆ—
Ïµ,q, and Î âˆ—
5 = âˆªâŒŠK logÎ±0(n)âŒ‹âˆ’1
q=0
Î âˆ—
Ïµ,q, where
Î âˆ—
Ïµ,q =
n
Ï€âˆ—âˆˆÎâˆ—: |Ï€âˆ—| = k0, {M(Dâˆ—
j)}|Ï€âˆ—|
j=1 = {1, . . . , k0} and Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2 = q
o
and âŒŠaâŒ‹denotes the largest integer less than or equal to a. Both Î âˆ—
1 and Î âˆ—
3 are sets of â€œunderfittedâ€
partitions, in the sense that |{M(Dâˆ—
j)}|Ï€âˆ—|
j=1| < k0 for Ï€âˆ—âˆˆÎ âˆ—
1 âˆªÎ âˆ—
3.
Î âˆ—
2 is the set of â€œoverfittedâ€
partitions, in the sense that |Ï€âˆ—| > k0 for Ï€âˆ—âˆˆÎ âˆ—
2. Î âˆ—
4 is the set of partitions with relatively large
Ïµ1(Ï€âˆ—, Ï€âˆ—
0), although having the correct number of clusters. Î âˆ—
5 is the set of â€œgoodâ€ partitions, in the
sense that the number of clusters is correct and Ïµ1(Ï€âˆ—, Ï€âˆ—
0) is small.
Ïµ1(Ï€âˆ—, Ï€âˆ—
0) takes different values when Ï€âˆ—belongs to different categories. We will study the posterior
probability of Ï€âˆ—in Î âˆ—
1, Î âˆ—
2, Î âˆ—
3, Î âˆ—
4 and Î âˆ—
5 respectively, based on which we derive the posterior
distribution of Ïµ1(Ï€âˆ—, Ï€âˆ—
0). To study the posterior distribution of Ï€âˆ—, we first write out P(Ï€âˆ—|D)
P(Ï€âˆ—
0|D). Given
a partition Ï€âˆ—, by Bayesian rule, we have
P(Ï€âˆ—| D)
âˆ
X
T
P(Ï€âˆ—, x, s, y, T ) âˆ
X
T
P(y | Ï€âˆ—, x, s)P(T )P(Ï€âˆ—| T ).
20

We then write the posterior ratio P(Ï€âˆ—|D)
P(Ï€âˆ—
0|D) as
P
T P(y | Ï€âˆ—, x, s)P(T )P(Ï€âˆ—| T )
P
T P(y | Ï€âˆ—
0, x, s)P(T )P(Ï€âˆ—
0 | T )
Assumption 6
â©½
c exp{cK log(K)}P(y | Ï€âˆ—, x, s) P
T P(Ï€âˆ—| T )
P(y | Ï€âˆ—
0, x, s) P
T P(Ï€âˆ—
0 | T )
=c exp{cK log(K)}P(y | Ï€âˆ—, x, s)
P(y | Ï€âˆ—
0, x, s) Ã—
P
T P(Ï€âˆ—| T )I(T can induce Ï€âˆ—)
P
T P(Ï€âˆ—
0 | T )I(T can induce Ï€âˆ—
0)
(i)
=c exp{cK log(K)}P(y | Ï€âˆ—, x, s)Î»|Ï€âˆ—|
P(y | Ï€âˆ—
0, x, s)Î»|Ï€âˆ—
0| Ã— |Ï€âˆ—
0|!
|Ï€âˆ—|!
ï£«
ï£¬
ï£­
K2 âˆ’1
|Ï€âˆ—
0| âˆ’1
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
K2 âˆ’1
|Ï€âˆ—| âˆ’1
ï£¶
ï£·
ï£¸
|{T : T can induce Ï€âˆ—}|
|{T : T can induce Ï€âˆ—
0}|,
(24)
where (i) uses Equations (3) and (4).
From Equation (24), we can see that
P(y|Ï€âˆ—,x,s)Î»|Ï€âˆ—|
P(y|Ï€âˆ—
0,x,s)Î»|Ï€âˆ—
0| and
|{T :T can induce Ï€âˆ—}|
|{T :T can induce Ï€âˆ—
0}| play essential roles in P(Ï€âˆ—|D)
P(Ï€âˆ—
0|D).
To study P(y|Ï€âˆ—,x,s)Î»|Ï€âˆ—|
P(y|Ï€âˆ—
0,x,s)Î»|Ï€âˆ—
0| , we first define two events, say An and En, and show that P(An âˆ©En) â†’1.
The definition of An and En are deferred to Section 6.3. Conditional on events An âˆ©En, we give the
bound of P(y|Ï€âˆ—,x,s)Î»|Ï€âˆ—|
P(y|Ï€âˆ—
0,x,s)Î»|Ï€âˆ—
0| by the following proposition.
Proposition 3. Under events An âˆ©En and Assumptions 1, 2, 3, 4, 5 and 6, there exists a constant
c, such that
P(y | Ï€âˆ—, x, s)Î»|Ï€âˆ—|
P(y | Ï€âˆ—
0, x, s)Î»|Ï€âˆ—
0| â©½
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
exp(âˆ’cn), if Ï€âˆ—âˆˆÎ âˆ—
1
exp{âˆ’cn logâˆ’Î±p(n)}, if Ï€âˆ—âˆˆÎ âˆ—
2
exp(âˆ’cn), if Ï€âˆ—âˆˆÎ âˆ—
3
exp{âˆ’cÏµ1(Ï€âˆ—, Ï€âˆ—
0)K2 log1+Î±b(n)}, if Ï€âˆ—âˆˆÎ âˆ—
4,
,
where Î±0 is defined in Theorem 1.
We defer the proof of Proposition 3 to Section S.6. We can see that the ratio bounds for partitions
in Î âˆ—
1 and Î âˆ—
3 share the same rate, since they are both sets of â€œunderfittedâ€ partitions. As discussed
after Assumption 5, we control the overfitted probability by Poisson hyperparameter Î». Hence, there
is a term Î±p (which controls the order of Î») in the ratio bound for the â€œoverfittedâ€ Ï€âˆ—âˆˆÎ âˆ—
2. For
Ï€âˆ—âˆˆÎ âˆ—
4, we can see that the larger Ïµ1(Ï€âˆ—, Ï€âˆ—
0), the smaller P(y|Ï€âˆ—,x,s)Î»|Ï€âˆ—|
P(y|Ï€âˆ—
0,x,s)Î»|Ï€âˆ—
0| . However, for Ï€âˆ—âˆˆâˆªK
q=0Î âˆ—
Ïµ,q,
a subset of Î âˆ—
5, it is not necessary that P(y|Ï€âˆ—,x,s)Î»|Ï€âˆ—|
P(y|Ï€âˆ—
0,x,s)Î»|Ï€âˆ—
0| converges to 0. The reason is caused by the
â€œapproximation errorâ€ between Ï€âˆ—
0 and the true partition {Dl,0}k0
l=1. More details are in the proof in
Section S.6. The next proposition gives a bound of |{T :T can induce Ï€âˆ—}|
|{T :T can induce Ï€âˆ—
0}|.
Proposition 4. Under our model and Assumption 2, there exists a constant C > 0, such that
|{T : T can induce Ï€âˆ—}|
|{T : T can induce Ï€âˆ—
0}| â©½exp{CK log(K)}
holds for all Ï€âˆ—âˆˆÎâˆ—.
21

We defer the proof of Proposition 4 to Section S.7. Proposition 4 shows that for a K Ã— K mesh
grid graph, the ratio of the numbers of spanning trees inducing two partitions is upper bounded by
exp{CK log(K)}, with our assumptions. Although this is a result of the mesh grid graph, it is worth
mentioning that it has the potential to be extended to a more general graph, with some higher-level
assumptions on graphs. This upper bound provides a reference potentially useful for studying other
spanning-tree-based approaches.
The remaining proof consists of several parts. In Section 6.3, we detail the definitions of events
An and En. In Sections 6.4 and 6.5, making use of Propositions 1 - 4, we prove Theorems 1 and 2,
respectively. Other proofs are deferred to the Supplementary Material. In Section S.1, we introduce
some basic lemmas regarding distribution and probability inequalities, which are useful for theoretical
analysis. Sections S.2 and S.3 prove P(An) â†’1 and P(En) â†’1, respectively, under our assumptions.
In Sections S.4 - S.7, we give the proof of Propositions 1 - 4, respectively.
6.3
Definition of An and En
Recall that we use {Bm}K2
m=1 to denote blocks and âˆ¥Bmâˆ¥to denote the number of locations in the
m-th block. We define An = A1n âˆ©A2n as events with respect to s and x, where
A1n =
n
c < min1â©½mâ©½K2 âˆ¥Bmâˆ¥
log1+Î±b(n)
â©½max1â©½mâ©½K2 âˆ¥Bmâˆ¥
log1+Î±b(n)
< C
o
, and
(25)
A2n =
n
0 < c < âˆ¥Bmâˆ¥âˆ’1
X
siâˆˆBm
x(si)xT (si) < C < âˆ, m = 1, 2, . . . , K2o
(26)
for some constants c, C > 0 to be determined later. We have the following result.
Lemma 1. Under Assumptions 1, 4 and 5, there exist constants c, C > 0, such that P(An) â†’1.
Proof. The proof is deferred to Section S.2.
In what follows, we assume the constants c and C in (25) - (26) are chosen such that P(An) â†’1.
Under An, we can replace the pseudoinverse notation in (8) with inverse. For the simplicity of proof,
we take the value of Î³ in (8) to be 1, and it is trivial to extend the proof to the case for any fixed
Î³ > 0. For a given Ï€âˆ—(and Ï€ = {S1, . . . , Sk} induced from Ï€âˆ—), following priors given by (8) and (9),
and after replacing the pseudoinverse notation with inverse, we have
Î¸ | Ï€âˆ—, k, T , s, x âˆ¼
k
Y
j=1
PGaussian{Î¸j; 0, nÏƒ2(xT
j xj)âˆ’1}, and
(27)
y | Ï€âˆ—, k, T , s, x, Î¸ âˆ¼
k
Y
j=1
PGaussian(yj; xjÎ¸j, Ïƒ2I|Sj|),
(28)
where xj is a |Sj|Ã—d design matrix of covariates in Sj, and yj = {y(si) : si âˆˆSj}. Following Equations
(27) and (28), yj | Ï€âˆ—, k, T , s, x âˆ¼Gaussian[0, Ïƒ2{nxj(xT
j xj)âˆ’1xT
j + I|Sj|}]. We can thus write
P(y | Ï€âˆ—, x, s)
=
P(y | Ï€âˆ—, k, T , s, x) =
k
Y
j=1
PGaussian[yj; 0, Ïƒ2{nÏ•j + I|Sj|}]
22

=
(2Ï€Ïƒ2)âˆ’n/2 exp
 
âˆ’yT y
2Ïƒ2
!
(n + 1)âˆ’kd/2 exp

n
2Ïƒ2(n + 1)yT Ï•Ï€âˆ—y

,
(29)
where Ï•j = xj(xT
j xj)âˆ’1xT
j is a projection matrix, Ï•Ï€âˆ—= P T
Ï€âˆ—diag(Ï•1, . . . Ï•k)P Ï€âˆ—, and P Ï€âˆ—ia a
permutation matrix such that P Ï€âˆ—y = (yT
1 , . . . , yT
k )T .
Recall the definitions of ËœÎâˆ—and âˆ©in Section 6.1. Let Ïƒ2
0 be the true variance of Ïµ(si). We define
En = E1n âˆ©E2n âˆ©E3n âˆ©E4n as events with respect to Ïµ, where
E1n = { sup
Ï€âˆ—âˆˆËœÎâˆ—ÏµT Ï•Ï€âˆ—Ïµ â©½5Ïƒ2
0K2 log(n)},
(30)
E2n = [ÏµT (Ï•Ï€âˆ—âˆ©Ï€âˆ—
0 + Ï•Ï€âˆ—)Ïµ â©½2Ïƒ2
0{1 + Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2} log1+Î±b/2(n), âˆ€Ï€âˆ—âˆˆÎ âˆ—
4 âˆªÎ âˆ—
5],
(31)
E3n =
n
sup
1â©½mâ©½K2

X
siâˆˆBm
x(si)Ïµ(si)

2 â©½C log(2+Î±b)/2(n)
o
, and
(32)
E4n =
n
sup
1â©½lâ©½k0

X
siâˆˆDâˆ—
l,0
x(si)Ïµ(si)

2 â©½n1/2 logÎ±0(n)
o
(33)
for Î±0 in Theorem 1 and some constant C > 0 to be determined later. We have the following result.
Lemma 2. Under our model and Assumptions 4 and 5, there exists a constant C > 0, such that
P(En) â†’1.
Proof. The proof is deferred to Section S.3
In what follows, we assume that the constant C in (32) are chosen such that P(En) â†’1. Combining
Lemmas 1 and 2, we conclude that under Assumptions 1, 4 and 5, P(Anâˆ©En) â†’1. In the later context,
probabilities are considered while conditional on the events An âˆ©En by default.
6.4
Proof of Theroem 1
Recall that we partition Îâˆ—= Î âˆ—
1 âˆªÎ âˆ—
2 âˆªÎ âˆ—
3 âˆªÎ âˆ—
4 âˆªÎ âˆ—
5. We first study the probability of Ï€âˆ—belonging
to Î âˆ—
1, Î âˆ—
2, Î âˆ—
3 and Î âˆ—
4, which is given by the following lemma.
Lemma 3. Under events An âˆ©En and Assumptions 1, 2, 3, 4, 5 and 6, there exists a constant c, such
that
P(Ï€âˆ—âˆˆÎ âˆ—
1 âˆªÎ âˆ—
2 âˆªÎ âˆ—
3 | D) â©½c exp{âˆ’cn logâˆ’Î±p(n)}, and
(34)
P (Ï€âˆ—âˆˆÎ âˆ—
4 | D) â©½c exp{âˆ’cn1/2 logÎ±0+(1+Î±b)/2(n)}.
(35)
Proof. Recall the expression of P(Ï€âˆ—|D)
P(Ï€âˆ—
0|D) in (24). In Proposition 4, we give a bound of |{T :T can induce Ï€âˆ—}|
|{T :T can induce Ï€âˆ—
0}|.
Combining with the fact that |Ï€âˆ—
0|!
|Ï€âˆ—|!
ï£«
ï£¬
ï£­
n âˆ’1
|Ï€âˆ—
0| âˆ’1
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
n âˆ’1
|Ï€âˆ—| âˆ’1
ï£¶
ï£·
ï£¸
âˆ’1
â©½cnkmaxâˆ’1, we have
P(Ï€âˆ—| D)
P(Ï€âˆ—
0 | D) â©½cP(y | Ï€âˆ—, x, s)Î»|Ï€âˆ—|
P(y | Ï€âˆ—
0, x, s)Î»|Ï€âˆ—
0| nkmaxâˆ’1 Ã— exp{CK log(K)}.
(36)
23

Next, under events An âˆ©En, we will make use of Proposition 3 to bound P(Ï€âˆ—âˆˆÎ âˆ—
1 âˆªÎ âˆ—
2 âˆªÎ âˆ—
3 | D),
and P (Ï€âˆ—âˆˆÎ âˆ—
4 | D), respectively.
Proof of (34):
Under events An âˆ©En, according to Proposition 3, for Ï€âˆ—âˆˆÎ âˆ—
1 âˆªÎ âˆ—
2 âˆªÎ âˆ—
3, we have P(y|Ï€âˆ—,x,s)Î»|Ï€âˆ—|
P(y|Ï€âˆ—
0,x,s)Î»|Ï€âˆ—
0| â©½
exp{âˆ’cn logâˆ’Î±p(n)}, thus, we write P(Ï€âˆ—âˆˆÎ âˆ—
1 âˆªÎ âˆ—
2 âˆªÎ âˆ—
3 | D) as
X
Ï€âˆ—âˆˆÎ âˆ—
1âˆªÎ âˆ—
2âˆªÎ âˆ—
3
P(Ï€âˆ—| D)
Equation (36)
â©½
X
Ï€âˆ—âˆˆÎ âˆ—
1âˆªÎ âˆ—
2âˆªÎ âˆ—
3
c exp{âˆ’cn logâˆ’Î±p(n) + CK log(K)}nkmaxâˆ’1
(i)
â©½
c exp{âˆ’cn logâˆ’Î±p(n) + CK2}
Assumption 5
â©½
c exp{âˆ’cn logâˆ’Î±p(n)},
where (i) uses the fact that the maximum number of partitions is no larger than kK2
max. The proof of
(34) is completed.
Proof of (35):
Under events An âˆ©En, according to Proposition 3, for Ï€âˆ—âˆˆÎ âˆ—
4, we have
P(y|Ï€âˆ—,x,s)Î»|Ï€âˆ—|
P(y|Ï€âˆ—
0,x,s)Î»|Ï€âˆ—
0| â©½
exp{âˆ’cÏµ1(Ï€âˆ—, Ï€âˆ—
0)K2 log1+Î±b(n)}. Thus, we have
P (Ï€âˆ—âˆˆÎ âˆ—
4 | D) =
PK2
q=âŒŠK logÎ±0(n)âŒ‹
P
Ï€âˆ—âˆˆÎ âˆ—Ïµ,q P(Ï€âˆ—| D)
Equation (36)
â©½
PK2
q=âŒŠK logÎ±0(n)âŒ‹
P
Ï€âˆ—âˆˆÎ âˆ—Ïµ,q c exp{âˆ’cq log1+Î±b(n) + CK log(K)}nkmaxâˆ’1
(i)
â©½
PK2
q=âŒŠK logÎ±0(n)âŒ‹
P
Ï€âˆ—âˆˆÎ âˆ—Ïµ,q c exp{âˆ’cq log1+Î±b(n)}nkmaxâˆ’1
(ii)
â©½
PK2
q=âŒŠK logÎ±0(n)âŒ‹c exp{âˆ’cq log1+Î±b(n) + Cq log(K)}nkmaxâˆ’1
Assumption 5
â©½
PK2
q=âŒŠK logÎ±0(n)âŒ‹c exp{âˆ’cq log1+Î±b(n)}nkmaxâˆ’1
(iii)
â©½
cnkmaxâˆ’1 exp{âˆ’cK logÎ±0+1+Î±b(n)} â©½c exp{âˆ’cn1/2 logÎ±0+(1+Î±b)/2(n)},
where (i) uses the fact that
K log(K)
q log1+Î±b(n) = o(1) for q â©¾âŒŠK logÎ±0(n)âŒ‹, (ii) uses the fact that |Î âˆ—
Ïµ,q| â©½
c exp{Cq log(K)}, and (iii) uses the property of summation of an geometric sequence. The proof of
(35) is completed.
Following Lemma 3, we conclude that under events An âˆ©En,
P(Ï€âˆ—âˆˆÎ âˆ—
5) â©¾1 âˆ’c exp{âˆ’cn1/2 logÎ±0+(1+Î±b)/2(n)}.
(37)
We next derive some property for Ï€âˆ—âˆˆÎ âˆ—
5.
Lemma 4. For âˆ€Ï€âˆ—= {Dâˆ—
j}k0
j=1 âˆˆÎ âˆ—
5, there exists a uniform constant c, such that
min
1â©½jâ©½k0 |Dâˆ—
j âˆ©Dâˆ—
M(Dâˆ—
j ),0| â©¾c.
(38)
Furthermore, for l = 1, . . . , k0, let
j1(l) = argumentjâˆˆ{1,...,k0}{M(Dâˆ—
j) = l}, and
(39)
j2(l)= argmaxjâˆˆ{1,...,k0}|Dâˆ—
j âˆ©Dâˆ—
l,0|.
(40)
We conclude j1(l) â‰¡j2(l).
24

Proof. Firstly,
from
the
definition
of
Î âˆ—
Ïµ,q,
we
have
Ïµ1(Ï€âˆ—, Ï€âˆ—
0)
â©½
K logÎ±0(n) Ã— Kâˆ’2
âˆ¼
nâˆ’1/2 logÎ±0+(1+Î±b)/2(n) for âˆ€Ï€âˆ—âˆˆÎ âˆ—
5.
After simple algebra, we can rewrite Ïµ1(Ï€âˆ—, Ï€âˆ—
0) defined in
(21) as
Ïµ1(Ï€âˆ—, Ï€âˆ—
0)
=
|D|âˆ’1
 k0
X
l=1
|Dâˆ—
l,0| âˆ’
k0
X
j=1
|Dâˆ—
j âˆ©Dâˆ—
M(Dâˆ—
j ),0|

=
|D|âˆ’1
 k0
X
j=1
{|Dâˆ—
M(Dâˆ—
j ),0| âˆ’|Dâˆ—
j âˆ©Dâˆ—
M(Dâˆ—
j ),0|}

â©½cnâˆ’1/2 logÎ±0+(1+Î±b)/2(n),
(41)
where we use the fact that |Ï€âˆ—| = k0 and {M(Dâˆ—
j)}|Ï€âˆ—|
j=1 = {1, . . . , k0} (since Ï€âˆ—âˆˆÎ âˆ—
5). On the other
hand, note that minlâˆˆ{1,...,k0}|Dâˆ—
l,0| â©¾c, we conclude that min1â©½jâ©½k0 |Dâˆ—
j âˆ©Dâˆ—
M(Dâˆ—
j ),0| â©¾c for some
constant c following (41).
Next, note that for a given l, if j1(l) Ì¸= j2(l), we have
Dâˆ—
j2(l) âˆ©Dâˆ—
M(Dâˆ—
j2(l)),0
 â©¾|Dâˆ—
j2(l) âˆ©Dâˆ—
l,0| â©¾|Dâˆ—
j1(l) âˆ©Dâˆ—
l,0| â©¾c,
and M(Dâˆ—
j2(l)) Ì¸= l since M(Dâˆ—
j1(l)) = l and j1(l) Ì¸= j2(l). Thus,
Ïµ1(Ï€âˆ—, Ï€âˆ—
0) â©¾|D|âˆ’1 min
Dâˆ—
j2(l) âˆ©Dâˆ—
M(Dâˆ—
j2(l)),0
, |Dâˆ—
j2(l) âˆ©Dâˆ—
l,0|
	 â©¾c,
which is contradictory to (41). We thus conclude j1(l) â‰¡j2(l). The lemma is proved.
We next give the proof of Theorem 1.
Proof.
Proof of (14): Following the definition of Î âˆ—
5 and Equation (37), together with the fact that
P(An âˆ©En) â†’1, we obtain (14) immediately.
Proof of (15): Recall the decomposition (23) of Ïµ[Ï€âˆ—, {Dl,0}k0
l=1]. Proposition 1 has shown that
Ïµ[Ï€âˆ—
0, {Dl,0}k0
l=1] â©½cnâˆ’1/2 log(1+Î±b)/2(n). We next show that under events An âˆ©En, for Ï€âˆ—âˆˆÎ âˆ—
5, we
have Ïµ(Ï€âˆ—, Ï€âˆ—
0) â©½cnâˆ’1/2logÎ±0+(1+Î±b)/2(n) for some constant c, which leads to (15) together with the
result in (37).
Firstly, for Ï€âˆ—âˆˆÎ âˆ—
5, it is easy to see Ïµ1(Ï€âˆ—, Ï€âˆ—
0) â©½K logÎ±0(n) Ã— Kâˆ’2 âˆ¼nâˆ’1/2 logÎ±0+(1+Î±b)/2(n). It
thus suffices to bound Ïµ2(Ï€âˆ—, Ï€âˆ—
0). We write
Ïµ2(Ï€âˆ—, Ï€âˆ—
0)
=
|D|âˆ’1
 k0
X
l=1
|Dâˆ—
l,0| âˆ’
k0
X
l=1
|Dâˆ—
j2(l) âˆ©Dâˆ—
l,0|

Lemma 4
=
|D|âˆ’1
 k0
X
l=1
|Dâˆ—
l,0| âˆ’
k0
X
l=1
|Dâˆ—
j1(l) âˆ©Dâˆ—
l,0|

= Ïµ1(Ï€âˆ—, Ï€âˆ—
0) â©½cnâˆ’1/2 logÎ±0+(1+Î±b)/2(n).(42)
Combining (41) - (42), we conclude Ïµ(Ï€âˆ—, Ï€âˆ—
0) â©½cnâˆ’1/2logÎ±0+(1+Î±b)/2(n) for some constant c and
Equation (15) is proved.
Proof of (16): Denote Ï€0 as the partition of {si}n
i=1 induced by Ï€âˆ—
0. Recall that we write {Sl,0}k0
l=1
as the true partition of {si}n
i=1. We decompose Ïµn[Ï€, {Sl,0}k0
l=1] â©½Ïµn(Ï€, Ï€0) + Ïµn[Ï€0, {Sl,0}k0
l=1]. Under
An âˆ©En, it is easy to see that
Ïµn[Ï€0, {Sl,0}k0
l=1] â©½cK Ã— log1+Î±b(n)nâˆ’1 â©½cnâˆ’1/2 log(1+Î±b)/2(n), and
25

Ïµn(Ï€, Ï€0) â©½cÏµ(Ï€âˆ—, Ï€âˆ—
0)K2 log1+Î±b(n)nâˆ’1 Equation (42)
â©½
cnâˆ’1/2logÎ±0+(1+Î±b)/2(n)
for Ï€âˆ—âˆˆÎ âˆ—
5. (16) is thus proved following (37).
6.5
Proof of Theorem 2
Note that Equation (17) is obtained immediately from Equation (37). We thus focus on the proof of
Equations (18)-(20) in this section. We first prove (18), based on which (19) and (20) can be obtained.
6.5.1
Proof of Equation (18)
To begin with, write
â„¦n = {Î¸ = {Î¸j}k
j=1 : max
1â©½jâ©½k âˆ¥Î¸j âˆ’Î¸M(Dâˆ—
j ),0âˆ¥2 â©½Mnnâˆ’1/2 logÎ±0+(1+Î±b)/2(n)},
for Mn in Theorem 2. To prove (18), it suffices to show that with probability tending to 1, we have
P(Î¸ âˆˆâ„¦c
n | D)
P(Î¸ âˆˆâ„¦n | D) â†’0.
(43)
We write out the ratio above as
P(Î¸ âˆˆâ„¦c
n | D)
P(Î¸ âˆˆâ„¦n | D)
=
P
Ï€âˆ—âˆˆÎ âˆ—
5 P(Î¸ âˆˆâ„¦c
n, Ï€âˆ—| D) + P
Ï€âˆ—âˆˆÎ âˆ—c
5 P(Î¸ âˆˆâ„¦c
n, Ï€âˆ—| D)
P
Ï€âˆ—âˆˆÎ âˆ—
5 P(Î¸ âˆˆâ„¦n, Ï€âˆ—| D) + P
Ï€âˆ—âˆˆÎ âˆ—c
5 P(Î¸ âˆˆâ„¦n, Ï€âˆ—| D)
â©½
P
Ï€âˆ—âˆˆÎ âˆ—
5 P(Î¸ âˆˆâ„¦c
n, Ï€âˆ—| D) + P(Ï€âˆ—âˆˆÎ âˆ—c
5 | D)
P
Ï€âˆ—âˆˆÎ âˆ—
5 P(Î¸ âˆˆâ„¦n, Ï€âˆ—| D)
.
(44)
In the proof of Theorem 1, we have already shown that under events An âˆ©En, P(Ï€âˆ—âˆˆÎ âˆ—c
5 | D) â†’0.
To prove (43), it thus suffices to show that under An âˆ©En, we have
sup
Ï€âˆ—âˆˆÎ âˆ—
5
P(Î¸ âˆˆâ„¦c
n, Ï€âˆ—| D)
P(Î¸ âˆˆâ„¦n, Ï€âˆ—| D) â†’0.
(45)
Recall that we use PGaussian(Â·; Âµ, Î£) to denote the density function of the Gaussian variable with mean
Âµ and covariance Î£. For a partition Ï€âˆ—(and Ï€ induced by Ï€âˆ—) we write yj = {y(si) : si âˆˆSj} and xj
as the |Sj| Ã— d design matrix for covariates in the j-th cluster Sj âˆˆÏ€. We have following lemmas.
Lemma 5. Given Ï€âˆ—âˆˆÎ âˆ—
5 and under events An âˆ©En, we have
P(Î¸ âˆˆâ„¦c
n, Ï€âˆ—| D)
P(Î¸ âˆˆâ„¦n, Ï€âˆ—| D) =
R
Î¸âˆˆâ„¦cn
nQk0
j=1 PGaussian(Î¸j; Â¯Î¸j, Î£j)
o
dÎ¸
1 âˆ’
R
Î¸âˆˆâ„¦cn
nQk0
j=1 PGaussian(Î¸j; Â¯Î¸j, Î£j)
o
dÎ¸
,
where Â¯Î¸j =
n
n+1(xT
j xj)âˆ’1xT
j yj, and Î£j = nÏƒ2
n+1(xT
j xj)âˆ’1.
Proof. By the Bayesian rule, we have
P(Î¸ âˆˆâ„¦c
n, Ï€âˆ—| D)
P(Î¸ âˆˆâ„¦n, Ï€âˆ—| D) =
R
Î¸âˆˆâ„¦cn P(Î¸ | Ï€âˆ—, x, s)P(y | Î¸, Ï€âˆ—, x, s)dÎ¸
R
Î¸âˆˆâ„¦n P(Î¸ | Ï€âˆ—, x, s)P(y | Î¸, Ï€âˆ—, x, s)dÎ¸.
(46)
26

Since Ï€âˆ—âˆˆÎ âˆ—
5, we have |Ï€âˆ—| = k0. Following priors (27) - (28), we have
P(Î¸ | Ï€âˆ—, x, s)P(y | Î¸, Ï€âˆ—, x, s) âˆ
k0
Y
j=1
PGaussian(Î¸j; Â¯Î¸j, Î£j),
where âˆmeans we do not care about those constants independent of Î¸, since they are contained in
both numerator and denominator of (46) and can be canceled out. Note that Qk0
j=1 PGaussian(Î¸j; Â¯Î¸j, Î£j)
is a probability density function of a multivariate Gaussian variable, we can thus write (46) as
P(Î¸ âˆˆâ„¦c
n, Ï€âˆ—| D)
P(Î¸ âˆˆâ„¦n, Ï€âˆ—| D)
=
R
Î¸âˆˆâ„¦cn
nQk0
j=1 PGaussian(Î¸j; Â¯Î¸j, Î£j)
o
dÎ¸
R
Î¸âˆˆâ„¦n
nQk0
j=1 PGaussian(Î¸j; Â¯Î¸j, Î£j)
o
dÎ¸
,
which proves the lemma.
The next lemma studies the property of Â¯Î¸j âˆ’Î¸M(Dâˆ—
j ),0.
Lemma 6. Under An âˆ©En and Assumption 4, we have
sup
Ï€âˆ—âˆˆÎ âˆ—
5
sup
1â©½jâ©½k
âˆ¥Â¯Î¸j âˆ’Î¸M(Dâˆ—
j ),0âˆ¥2 â©½cM1/2
n
nâˆ’1/2 logÎ±0+(1+Î±b)/2(n)
for some constant c.
Proof. For a given Ï€âˆ—= {Dâˆ—
j}k0
j=1 âˆˆÎ âˆ—
5, let Âµj,0 = E(yj | sj, xj) and Ïµj = {Ïµ(si)}siâˆˆDâˆ—
j . We write
Â¯Î¸j âˆ’Î¸M(Dâˆ—
j ),0
=
n
n + 1(xT
j xj)âˆ’1xT
j (Âµj,0 + Ïµj) âˆ’Î¸M(Dâˆ—
j ),0 = I1 + I2 + I3, where
I1 =
n
n + 1(xT
j xj)âˆ’1xT
j (Âµj,0 âˆ’xjÎ¸M(Dâˆ—
j ),0),
I2 = âˆ’
1
n + 1Î¸M(Dâˆ—
j ),0, and I3 =
n
n + 1(xT
j xj)âˆ’1xT
j Ïµj.
For I1, note that for Ï€ induced by Ï€âˆ—
âˆˆÎ âˆ—
5, we have shown in the proof of Theorem
1 that Ïµn[Ï€, {Sl,0}k0
l=1] â©½c3nâˆ’1/2 logÎ±0+(1+Î±b)/2(n), hence (Âµj,0 âˆ’xjÎ¸M(Dâˆ—
j ),0) has no more than
c3n1/2 logÎ±0+(1+Î±b)/2(n) non-zero elements.
On the other hand, by event An and Equation (38),
we have (xT
j xj)âˆ’1 â©½cnâˆ’1. Together with the boundness assumption on x(si) in Assumption 4, we
have
sup
Ï€âˆ—âˆˆÎ âˆ—
5
sup
1â©½jâ©½k
âˆ¥I1âˆ¥2 â©½cnâˆ’1 Ã— n1/2 logÎ±0+(1+Î±b)/2(n) = cnâˆ’1/2 logÎ±0+(1+Î±b)/2(n).
It is easy to see that supÏ€âˆ—âˆˆÎ âˆ—
5 sup1â©½jâ©½k âˆ¥I2âˆ¥2 â©½cnâˆ’1/2 logÎ±0+(1+Î±b)/2(n). It thus remains to bound I3.
We further decompose n+1
n I3 = (xT
j xj)âˆ’1(I31 + I32 + I33), where
I31 =
X
siâˆˆDâˆ—
M(Dâˆ—
j ),0
x(si)Ïµ(si),
I32 =
X
siâˆˆDâˆ—
j \Dâˆ—
M(Dâˆ—
j ),0
x(si)Ïµ(si), and I33 =
X
siâˆˆDâˆ—
M(Dâˆ—
j ),0\Dâˆ—
j
x(si)Ïµ(si).
27

From E4n in (33), we have
sup
Ï€âˆ—âˆˆÎ âˆ—
5
sup
1â©½jâ©½k
âˆ¥(xT
j xj)âˆ’1I31âˆ¥2 â©½cnâˆ’1 sup
1â©½jâ©½k

X
siâˆˆDâˆ—
M(Dâˆ—
j ),0
x(si)Ïµ(si)

2 â©½cnâˆ’1/2 logÎ±0(n).
Since Ï€âˆ—âˆˆÎ âˆ—
5, the number of blocks in Dâˆ—
j \ Dâˆ—
M(Dâˆ—
j ),0 is smaller than âŒŠK logÎ±0(n)âŒ‹. Combining the
result given by An and E3n, we have
sup
Ï€âˆ—âˆˆÎ âˆ—
5
sup
1â©½jâ©½k
âˆ¥(xT
j xj)âˆ’1I32âˆ¥2 â©½cnâˆ’1 Ã— âŒŠK logÎ±0(n)âŒ‹log(2+Î±b)/2(n) â©½cnâˆ’1/2 logÎ±0+1/2(n).
Similarly, it can be shown that supÏ€âˆ—âˆˆÎ âˆ—
5 sup1â©½jâ©½k âˆ¥(xT
j xj)âˆ’1I33âˆ¥2 â©½cnâˆ’1/2 logÎ±0+1/2(n). Putting the
result together, the lemma is proved.
We next prove (18).
Proof. By the result of Lemmas 5 and 6, to prove inequality (45), it suffices to show that
under events An âˆ©En, supÏ€âˆ—âˆˆÎ âˆ—
5
R
Î¸âˆˆâ„¦cn
nQk0
j=1 PGaussian(Î¸j; Â¯Î¸j, Î£j)
o
dÎ¸
â†’
0.
Let {Zj}k0
j=1 be
independent random variables with probability density function PGaussian(Â·; Â¯Î¸j, Î£j), we compute
supÏ€âˆ—âˆˆÎ âˆ—
5
R
Î¸âˆˆâ„¦cn
nQk0
j=1 PGaussian(Î¸j; Â¯Î¸j, Î£j)
o
dÎ¸ as
sup
Ï€âˆ—âˆˆÎ âˆ—
5
P{ max
1â©½jâ©½k0 âˆ¥Zj âˆ’Î¸M(Dâˆ—
j ),0âˆ¥2 > Mnnâˆ’1/2 logÎ±0+(1+Î±b)/2(n)}
Lemma 6
â©½
sup
Ï€âˆ—âˆˆÎ âˆ—
5
 k0
X
j=1
P

âˆ¥Zj âˆ’Â¯Î¸jâˆ¥2 > Mnnâˆ’1/2 logÎ±0+(1+Î±b)/2(n)
2

.
On the other hand, under An, we have Î»max(Î£j) = nÏƒ2
n+1Î»max{(xT
j xj)âˆ’1} â©½
cnÏƒ2
(n+1)|Sj| for some constant
c. Applying Lemma S.5, we have
k0
X
j=1
P

âˆ¥Zj âˆ’Â¯Î¸jâˆ¥2 > Mnnâˆ’1/2 logÎ±0+(1+Î±b)/2(n)
2

â©½
k0
X
j=1
2
âˆš
2c1/2Ïƒd3/2n
Ï€1/2(n + 1)1/2|Sj|1/2Mn logÎ±0+(1+Î±b)/2(n)
exp

âˆ’M2
n log2Î±0+(1+Î±b)(n)(n + 1)|Sj|
8cdn2Ïƒ2

Since Ï€âˆ—âˆˆÎ âˆ—
5, from (38), there exists a constant C > 0, such that |Sj| > Cn holds for all Ï€âˆ—âˆˆÎ âˆ—
5
and 1 â©½j â©½k0. Thus, the right hand side of inequality is bounded by a Ï€âˆ—-independent series going
to 0. Putting results together, (18) is proved.
6.5.2
Proof of Equation (19)
Proof. Take Mn in â„¦n to be log1/2(n), we write
â„¦â€²
n = {Î¸ = {Î¸j}k
j=1 : max
1â©½jâ©½k âˆ¥Î¸j âˆ’Î¸M(Dâˆ—
j ),0âˆ¥2 â©½nâˆ’1/2 logÎ±0+1+Î±b/2(n)}.
Note that we have already shown that P(Ï€âˆ—âˆˆÎ âˆ—
5 | D) â†’1, and P(Î¸ âˆˆâ„¦â€²
n | D) â†’1. Together with
Assumption 1, it thus suffices to show that
P
Z
D
âˆ¥Î¸(s) âˆ’Î¸0(s)âˆ¥2
2ds > Mâ€²
nnâˆ’1/2 logÎ±0+(1+Î±b)/2(n) | D, Ï€âˆ—âˆˆÎ âˆ—
5, Î¸ âˆˆâ„¦â€²
n

â†’0.
28

Given a Ï€âˆ—âˆˆÎ âˆ—
5 and Î¸ âˆˆâ„¦â€²
n, we next write (recall j1(l) and j2(l) in (39) - (40))
Î¸(s) âˆ’Î¸0(s)
=
k0
X
j=1
Î¸jI(s âˆˆDâˆ—
j) âˆ’
k0
X
l=1
Î¸0(s)I(s âˆˆDl,0)
=
k0
X
l=1
{Î¸j1(l)I(s âˆˆDâˆ—
j1(l)) âˆ’Î¸l,0I(s âˆˆDl,0)} = I1(s) + I2(s), where
I1(s) =
k0
X
l=1
{(Î¸j1(l) âˆ’Î¸l,0)I(s âˆˆDâˆ—
j1(l))}, and I2(s) =
k0
X
l=1
Î¸l,0{I(s âˆˆDâˆ—
j1(l)) âˆ’I(s âˆˆDl,0)}.
Since Î¸ âˆˆâ„¦â€²
n, we have
sup
sâˆˆD
âˆ¥I1(s)âˆ¥2 â©½k0 max
1â©½jâ©½k0 âˆ¥Î¸j âˆ’Î¸M(Dâˆ—
j ),0âˆ¥2 â©½cnâˆ’1/2 logÎ±0+1+Î±b/2(n).
(47)
On the other hand,
Z
D
âˆ¥I2(s)âˆ¥2
2ds
â©½
k0
k0
X
l=1
Z
D
âˆ¥Î¸l,0{I(s âˆˆDâˆ—
j1(l)) âˆ’I(s âˆˆDl,0)}âˆ¥2
2ds
=
k0
k0
X
l=1
âˆ¥Î¸l,0âˆ¥2
2{|Dâˆ—
j1(l)\Dl,0| + |Dl,0\Dâˆ—
j1(l)|}
=
k0
k0
X
l=1
âˆ¥Î¸l,0âˆ¥2
2{|Dâˆ—
j1(l)\Dl,0| + |Dl,0\Dâˆ—
j2(l)|} â©½cnâˆ’1/2 logÎ±0+(1+Î±b)/2(n),
(48)
where the last inequality is from (42). Combining result of (47) - (48), we conclude
P
nR
D âˆ¥Î¸(s) âˆ’Î¸0(s)âˆ¥2
2ds > Mâ€²
nnâˆ’1/2 logÎ±0+(1+Î±b)/2(n) | D, Ï€âˆ—âˆˆÎ âˆ—
5, Î¸ âˆˆâ„¦â€²
n
o
â©½
P
n
2
R
D âˆ¥I1(s)âˆ¥2
2ds + 2
R
D âˆ¥I2(s)âˆ¥2
2ds > Mâ€²
nnâˆ’1/2 logÎ±0+(1+Î±b)/2(n) | D, Ï€âˆ—âˆˆÎ âˆ—
5, Î¸ âˆˆâ„¦â€²
n
o
,
which converges to 0 according to (47) - (48). Equation (19) is proved.
6.5.3
Proof of Equation (20)
Proof. Under Assumption 4, x(s) is bounded.
Together with Equation (19), Equation (20) is
established immediately.
29

Supplementary Material for â€œConsistent Bayesian Spatial Domain Partitioning
Using Predictive Spanning Tree Methodsâ€
This supplement contains all the remaining proofs for theorems in the main paper.
S.1
Preliminary lemmas
Lemma S.1. (Bernsteinâ€™s inequality) Let X1, . . . , Xn be independent zero-mean real-valued random
variables and let Sn = Pn
i=1 Xi. If there exists a constant c > 0, such that Cramerâ€™s condition
E|Xi|k â©½ckâˆ’2k!EX2
i < âˆ, i = 1, 2, . . . , n; k = 3, 4, . . . .
(S.1)
holds, then
P(|Sn| â©¾t) â©½2 exp
 
âˆ’
t2
4 Pn
i=1 EX2
i + 2ct
!
, t > 0.
Lemma S.2. (Lemma 1 of [20]) Let Ï‡2
r be a chi-square distribution with a degree of freedom r. The
following concentration inequalities hold for any x > 0:
P

Ï‡2
r > r + 2x + 2âˆšrx

â‰¤exp(âˆ’x),
and
P

Ï‡2
r < r âˆ’2âˆšrx

â‰¤exp(âˆ’x).
From Lemma S.2, we can see that for x > r, we have
P(Ï‡2
r > 5x) â©½P

Ï‡2
r > r + 2x + 2âˆšrx

â‰¤exp(âˆ’x).
(S.2)
Lemma S.3. The binomial coefficient satisfies
n
k
k
â©½
ï£«
ï£¬
ï£­
n
k
ï£¶
ï£·
ï£¸â©½
en
k
k
and
k
X
i=0
ï£«
ï£¬
ï£­
n
i
ï£¶
ï£·
ï£¸â©½(n + 1)k
for k=0,...,n.
Proof. The proof is trivial and omitted.
Lemma S.4. For a real symmetric matrix A satisfying A2 = A and a standard Gaussian random
vector e, we have
eT Ae âˆ¼Ï‡2
r,
where r is the number of positive eigenvalues of A.
S1

Proof. The proof is trivial by performing eigen-decomposition of A and noting that the eigenvalues
of A equal either 0 or 1.
Lemma S.5. Denote Z as a d-dimensional Gaussian random variable with mean 0 and positive
definite covariance Î£. For any t > 0, we have
P(âˆ¥Zâˆ¥2 > t) â©½
r 2
Ï€d3/2Î»1/2
max(Î£)tâˆ’1 exp
(
âˆ’
t2
2dÎ»max(Î£)
)
.
Proof. First, for a 1-dimensional Gaussian variable Z âˆ¼Gaussian(0, 1), we have
P(|Z| > t)
=
P(Z > t) + P(Z < t)
=
2
âˆš
2Ï€
Z +âˆ
t
exp

âˆ’1
2z2

dz â©½
r 2
Ï€
Z +âˆ
t
z
t exp

âˆ’1
2z2

dz
=
tâˆ’1
r 2
Ï€ exp
 
âˆ’t2
2
!
.
(S.3)
Next, for d â©¾1, we write ËœZ = Î£âˆ’1/2Z = ( ËœZ1, . . . , ËœZd)T . It is easy to see that ËœZ âˆ¼Gaussian(0, Id).
Note that âˆ¥Zâˆ¥2 = âˆ¥Î£1/2 ËœZâˆ¥2 â©½Î»1/2
max(Î£)âˆ¥ËœZâˆ¥2, we thus have
P(âˆ¥Zâˆ¥2 > t)
â©½
P{âˆ¥ËœZâˆ¥2 > Î»âˆ’1/2
max (Î£)t}
â©½
d
X
i=1
P{| ËœZi| > dâˆ’1/2Î»âˆ’1/2
max (Î£)t}
Equation ( S.3)
â©½
r 2
Ï€d3/2Î»1/2
max(Î£)tâˆ’1 exp
(
âˆ’
t2
2dÎ»max(Î£)
)
.
S.2
Proof of Lemma 1
To prove Lemma 1, we will show that under Assumptions 1, 4 and 5, P(A1n) â†’0, and P(A2n) â†’0
for some constants c, C > 0. The proofs are given by Lemmas S.6 and S.7, respectively.
Lemma S.6. Under Assumptions 1 and 5, there exit positive constants c and C, such that for A1n
defined in (25), we have P(A1n) â†’0.
Proof. We prove the last inequality in (25), and the first inequality is similar. Given a block Bm,
denote Î´m,i as the binary variable indicating whether si is within Bm or not. We can see that
âˆ¥Bmâˆ¥=
n
X
i=1
Î´m,i.
Under Assumption 1, {Î´m,i}n
i=1 is a series of i.i.d. binary random variable with success probability
pm =
R
Bm PD(s)ds. It is easy to see that c = 1 satisfies condition (S.1) in Lemma S.1 for random
S2

variables {Î´m,iâˆ’pm}n
i=1, and E(Î´m,iâˆ’pm)2 = pm(1âˆ’pm). Thus, by taking t = a1
p
npm(1 âˆ’pm) log(n)
in Lemma S.1, where a1 is a constant to be determined later, we have
P

|âˆ¥Bmâˆ¥âˆ’npm| â©¾a1
q
npm(1 âˆ’pm) log(n)

â©½
2 exp
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
âˆ’
a2
1 log(n)
4 + 2a1
r
log(n)
npm(1âˆ’pm)
ï£¼
ï£´
ï£´
ï£½
ï£´
ï£´
ï£¾
.
Next, from Assumptions 1 and 5, we can see
inf
1â©½mâ©½K2 pm âˆ¼
sup
1â©½mâ©½K2 pm âˆ¼Kâˆ’2 âˆ¼nâˆ’1 log1+Î±b(n),
leading to
sup
1â©½mâ©½K2
q
npm(1 âˆ’pm) log(n) âˆ¼log1+Î±b/2(n).
Thus, for a given a1, we can then find an a2, such that a2 log1+Î±b(n)âˆ’npm > a1
p
npm(1 âˆ’pm) log(n),
m = 1, 2, . . . , K2. Therefore,
P
(
max1â©½mâ©½K2 âˆ¥Bmâˆ¥
log1+Î±b(n)
â©¾a2
)
â©½
K2
X
m=1
P
(
âˆ¥Bmâˆ¥
log1+Î±b(n) â©¾a2
)
â©½
K2
X
m=1
P{|âˆ¥Bmâˆ¥âˆ’npm| â©¾a2 log1+Î±b(n) âˆ’npm}
â©½
K2
X
m=1
P

|âˆ¥Bmâˆ¥âˆ’npm| â©¾a1
q
npm(1 âˆ’pm) log(n)

â©½
2K2 exp
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
âˆ’
a2
1 log(n)
4 + 2a1
r
log(n)
npm(1âˆ’pm)
ï£¼
ï£´
ï£´
ï£½
ï£´
ï£´
ï£¾
.
On the other hand, using the fact that
log(n)
npm(1âˆ’pm)
=
o(1), we can find a a1, such that
2K2 exp
ï£±
ï£²
ï£³âˆ’
a2
1 log(n)
4+2a1
q
log(n)
npm(1âˆ’pm)
ï£¼
ï£½
ï£¾< nâˆ’1 holds when n is large.
Thus, the last inequality of (25) is
proved. The proof of the first inequality of (25) is similar and is thus omitted.
Conditional on A1n, we have the following result.
Lemma S.7. Under Assumptions 1, 4 and 5, there exists a constant C > 0, such that
P
ï£±
ï£²
ï£³
sup
1â©½mâ©½K2 âˆ¥âˆ¥Bmâˆ¥âˆ’1
X
siâˆˆBm
[x(si)xT (si) âˆ’E{x(si)xT (si) | si}]âˆ¥âˆâ©½C logâˆ’Î±b/2(n) | A1n
ï£¼
ï£½
ï£¾â†’1.
Proof. Let xp(si) be the p-th entry of x(si). Since the dimension of x(si) is finite, it suffices to show
that there exists Cppâ€² > 0, such that
P
ï£±
ï£²
ï£³
sup
1â©½mâ©½K2 |âˆ¥Bmâˆ¥âˆ’1
X
siâˆˆBm
[xp(si)xpâ€²(si) âˆ’E{xp(si)xpâ€²(si) | si}]| â©½Cppâ€² logâˆ’Î±b/2(n) | A1n
ï£¼
ï£½
ï£¾â†’1
for âˆ€p, pâ€² = 1, 2, . . . , d. Assumption 4 entails that Cramerâ€™s condition (S.1) holds for xp(s1)xpâ€²(s1), we
can thus apply Lemma S.1 and obtain
P
n
sup1â©½mâ©½K2 |âˆ¥Bmâˆ¥âˆ’1 P
siâˆˆBm[xp(si)xpâ€²(si) âˆ’E{xp(si)xpâ€²(si) | si}]| > Cppâ€² logâˆ’Î±b/2(n) | A1n
o
S3

â©½
PK2
m=1 P
n
|âˆ¥Bmâˆ¥âˆ’1 P
siâˆˆBm[xp(si)xpâ€²(si) âˆ’E{xp(si)xpâ€²(si) | si}]| > Cppâ€² logâˆ’Î±b/2(n) | A1n
o
Lemma S.1
â©½
2 PK2
m=1 exp
"
âˆ’
âˆ¥Bmâˆ¥C2
ppâ€² logâˆ’Î±b(n)
4âˆ¥Bmâˆ¥âˆ’1 P
siâˆˆBm E{x2p(si)x2
pâ€²(si)|si}+2cCppâ€² logâˆ’Î±b/2(n)
#
Under A1n
â©½
2K2 exp
"
âˆ’
cC2
ppâ€² log(n)
4âˆ¥Bmâˆ¥âˆ’1 P
siâˆˆBm E{x2p(si)x2
pâ€²(si)|si}+2cCppâ€² logâˆ’Î±b/2(n)
#
.
Since K2 âˆ¼
n
log1+Î±b(n) according to Assumption 5, we can find a Cppâ€² > 0, such that the the above
inequality is smaller than nâˆ’1 = o(1). Iterating over p and pâ€², the lemma is proved.
Together with Assumption 4, an immediate result from Lemma S.7 is that there exist constants
c, C > 0, such that P(A2n) â†’1, for A2n defined in (26). Combining the results, Lemma 1 is proved.
S.3
Proof of Lemma 2
To prove Lemma 2, we will show that under our model and Assumption 5, P(E1n) â†’0, P(E2n) â†’0,
P(E3n) â†’0 and P(E4n) â†’0, for some constants c, C > 0. The proofs are given by Lemmas S.8, S.9,
S.10 and S.11, respectively.
Lemma S.8. Under our model, we have P(E1n) â†’0 for E1n defined in (30).
Proof. It is easy to see that Ï•Ï€âˆ—is a real symmetric matrix with Ï•2
Ï€âˆ—= Ï•Ï€âˆ—, and the number of
positive eigenvalues of Ï•Ï€âˆ—is smaller than kmaxd. Thus, by applying Lemma S.4, Ïƒâˆ’2
0 ÏµT Ï•Ï€âˆ—Ïµ follows
a chi-square distribution with degree of freedom no larger than k2
maxd. Applying Equation (S.2), we
have
P{ sup
Ï€âˆ—âˆˆÎâˆ—ÏµT Ï•Ï€âˆ—Ïµ > 5Ïƒ2
0K2 log(n)}
â©½
X
Ï€âˆ—âˆˆËœÎâˆ—
P{ÏµT Ï•Ï€âˆ—Ïµ > 5Ïƒ2
0K2 log(n)}
â©½
X
Ï€âˆ—âˆˆËœÎâˆ—
k2
maxd
X
r=1
P{Ï‡2
r > 5K2 log(n)}
Equation (S.2)
â©½
c|ËœÎâˆ—| exp{âˆ’K2 log(n)}.
(i)
â©½
c exp{2K2 log(kmax) âˆ’K2 log(n)} â†’0,
where (i) uses the fact that any Ï€âˆ—âˆˆËœÎâˆ—contains at most k2
max clusters, hence |ËœÎâˆ—| â©½k2K2
max. The
lemma is proved.
Lemma S.9. Under our model and Assumption 5, we have P(E2n) â†’1, for E2n defined in (31).
Proof. It suffices to show that
P[ÏµT Ï•Ï€âˆ—âˆ©Ï€âˆ—
0Ïµ â©½Ïƒ2
0{1 + Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2} log1+Î±b/2(n), âˆ€Ï€âˆ—âˆˆÎ âˆ—
4 âˆªÎ âˆ—
5] â†’1,
(S.4)
and
P[ÏµT Ï•Ï€âˆ—Ïµ â©½Ïƒ2
0{1 + Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2} log1+Î±b/2(n), âˆ€Ï€âˆ—âˆˆÎ âˆ—
4 âˆªÎ âˆ—
5] â†’1.
(S.5)
S4

We give proof of Equation (S.4), and the proof of Equation (S.5) is similar. Following similar arguments
in Lemma S.8, Ïƒâˆ’2
0 ÏµT Ï•Ï€âˆ—âˆ©Ï€âˆ—
0Ïµ follows a chi-square distribution with degree of freedom smaller than
k2
0d. Thus,
P[ÏµT Ï•Ï€âˆ—âˆ©Ï€âˆ—
0Ïµ > Ïƒ2
0{1 + Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2} log1+Î±b/2(n), âˆƒÏ€âˆ—âˆˆÎ âˆ—
4 âˆªÎ âˆ—
5]
â©½
P
Ï€âˆ—âˆˆÎ âˆ—
4âˆªÎ âˆ—
5 P[ÏµT Ï•Ï€âˆ—âˆ©Ï€âˆ—
0Ïµ > Ïƒ2
0{1 + Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2} log1+Î±b/2(n)]
â©½
PK2
q=0
P
Ï€âˆ—âˆˆÎ âˆ—Ïµ,q P[ÏµT Ï•Ï€âˆ—âˆ©Ï€âˆ—
0Ïµ > Ïƒ2
0(1 + q) log1+Î±b/2(n)]
Equation (S.2)
â©½
PK2
q=0
P
Ï€âˆ—âˆˆÎ âˆ—Ïµ,q c exp{âˆ’C(1 + q) log1+Î±b/2(n)}
(i)
â©½
PK2
q=0 c exp{cq log(K) âˆ’C(1 + q) log1+Î±b/2(n)},
where (i) uses the fact that
|Î âˆ—
Ïµ,q| â©½
ï£«
ï£¬
ï£­
K2
q
ï£¶
ï£·
ï£¸kq
max
Lemma S.3
â©½
eK2qkq
max.
Note that Assumption 5 entails that log(K) = O{log(n)}, thus
P[ÏµT Ï•Ï€âˆ—âˆ©Ï€âˆ—
0Ïµ > Ïƒ2
0{1 + Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2} log1+Î±b/2(n), âˆƒÏ€âˆ—âˆˆÎ âˆ—
4 âˆªÎ âˆ—
5]
â©½
exp{âˆ’C log1+Î±b/2(n)} PK2
q=0 c exp{âˆ’Cq log1+Î±b/2(n)}
â©½
c exp{âˆ’C log1+Î±b/2(n)} â†’0.
Equation (S.4) is thus proved. The proof of Equation (S.5) is omitted.
Lemma S.10. Under our model and Assumptions 4 and 5, there exists a constant C > 0, such that
for E3n defined in (32), we have P(E3n) â†’1.
Proof. Let xp(si) be the p-th entry of x(si). Since the dimension of x(si) is finite, it suffices to show
that
P
ï£±
ï£²
ï£³
sup
1â©½mâ©½K2

X
siâˆˆBm
xp(si)Ïµ(si)

â©½C log(2+Î±b)/2(n)
ï£¼
ï£½
ï£¾â†’1
Note that
P
n
sup1â©½mâ©½K2

P
siâˆˆBm xp(si)Ïµ(si)
 â©½C log(2+Î±b)/2(n)
o
â©¾
P
n
sup1â©½mâ©½K2
P
siâˆˆBm xp(si)Ïµ(si)
 â©½C log(2+Î±b)/2(n) | An
o
P(An).
Since we have shown that P(An) â†’1 in Lemma 1, it suffices to show that
P
ï£±
ï£²
ï£³
sup
1â©½mâ©½K2

X
siâˆˆBm
xp(si)Ïµ(si)

â©½C log(2+Î±b)/2(n) | An
ï£¼
ï£½
ï£¾P(An) â†’1.
We write
P
ï£±
ï£²
ï£³
sup
1â©½mâ©½K2

X
siâˆˆBm
xp(si)Ïµ(si)

> C log(2+Î±b)/2(n) | An
ï£¼
ï£½
ï£¾
S5

â©½
K2
X
m=1
P
ï£±
ï£²
ï£³

X
siâˆˆBm
xp(si)Ïµ(si)

> C log(2+Î±b)/2(n) | An
ï£¼
ï£½
ï£¾
(i)
â©½
K2 Ã— 2 exp
(
âˆ’
C2 log(2+Î±b)(n)
4c log1+Î±b(n) + 2cC log(2+Î±b)/2(n)
)
Assumption (5)
â©½
cn
log1+Î±b(n) Ã— exp
(
âˆ’
C2 log(2+Î±b)(n)
4c log1+Î±b(n) + 2cC log(2+Î±b)/2(n)
)
,
where we apply Lemma S.1 in (i). Taking C = 4âˆšc, we can see the right-hand side of inequality goes
to 0. The lemma is proved.
Lemma S.11. Under our model and Assumptions 4 and 5, we have P(E4n) â†’1, for E4n defined in
(33).
Proof. Since the dimension of x(si) and k0 is finite, the proof is trivial by applying Lemma S.1.
Combining the results of Lemmas S.8 - S.11, we finish the proof of Lemma 2.
S.4
Proof of Proposition 1
Recall that in our model, the domain partition Ï€âˆ—is induced from the partition of blocks V = {Bm}K2
m=1.
In Section 2.2, the partition of blocks is induced from the mesh grid graph G = {V, E}. Thus, to
prove Proposition 1, it suffices to show that there exists a contiguous partition of V, say Ï€0(V) =
{V1,0, . . . , Vk0,0}, such that
k0
X
j=1
|{Bm : Bm âˆˆVj,0, Bm âŠŠDj,0}| â©½cK.
(S.6)
The proof is given as follows.
Proof. We conduct the proof by constructing Ï€0(V). Recall the definition of â€œboundary blockâ€ in
Section 6.1. Let B be the set of boundary blocks. For each block Bm âˆˆB, we write Neighbour(Bm)
as the set of blocks surrounding Bm (see Figure S.1 for an illustration). Define set
b = {Bm : there exists a Bmâ€² âˆˆB, such that Bm âˆˆNeighbour(Bmâ€²)} âˆªB
as the set of blocks that are â€œnearâ€ boundary set B. We use Bc and bc to denote complements of B
and b, respectively. We will first construct a partition Ï€1 = {V1
1, . . . V1
k0} for blocks in Bc. Then we
filter Ï€1 to obtain a partition of a subset of blocks in Bc, say Ï€2 = {V2
1, . . . V2
k0}. After filtration, we
will show that each V2
j is connected under the mesh grid G. We then construct Ï€0(V) by extending Ï€2
to a partition of all blocks. Finally, we verify that Ï€0(V) is a contiguous partition and (S.6) is satisfied
under Ï€0(V).
S6

âˆ—
âˆ—
âˆ—
âˆ—
Bm
âˆ—
âˆ—
âˆ—
âˆ—
Figure S.1: Illustration of Neighbour(Bm). Blocks in Neighbour(Bm) are denoted by âˆ—.
To begin with, note that for a given block Bm âˆˆBc, it is fully containd in some sub-domain of
{Dl,0}k0
l=1. We construct Ï€1 = {V1
1, . . . V1
k0} as
V1
j = {Bm âˆˆBc : Bm âŠ†Dj,0}, 1 â©½j â©½k0.
For a given j and two blocks Bm, Bmâ€² âˆˆ{V1
j âˆ©bc} âŠ†{V1
j âˆ©Bc}, through the definition of bc, we can
see the center points of Bm and Bmâ€², say cm and cmâ€², satisfy
d(cm, B) â©¾1.5Kâˆ’1, and d(cmâ€², B) â©¾1.5Kâˆ’1.
According to Assumption 2, there exists a path P(cm, cmâ€²), such that
d{P(cm, cmâ€²), B} â©¾1.5Kâˆ’1.
Since each block is a Kâˆ’1 Ã—Kâˆ’1 rectangle, the distance of any two points within one block is no larger
than
âˆš
2Kâˆ’1. We thus conclude that for any block Bmâ€²â€² intersecting with P(cm, cmâ€²), we have
Bmâ€²â€² âˆˆV1
j âˆ©Bc.
Hence, there exists a path from cmto cmâ€² and intersects with only blocks in V1
j âˆ©Bc. Together with
the fact that {V1
j âˆ©bc} âŠ†{V1
j âˆ©Bc}, we conclude that for each 1 â©½j â©½k0, there exists a connected
component of V1
j âˆ©Bc, say V2
j , such that {V1
j âˆ©bc} âŠ†V2
j .
We then obtain our second partition
Ï€2 = {V2
1, . . . , V2
k0}.
Now that Ï€2 is partition of subset of {Bm}K2
m=1. We next expand V2
j to Vj,0, such that Vj,0 âŠ‡V2
j ,
and {V1,0, . . . , Vk0,0} is a partition of all the blocks {Bm}k0
m=1. Since the mesh grid G and each V2
j are
connected, it is easy to see that there exists an expansion of {V2
1, . . . , V2
k0}, say {V1,0, . . . , Vk0,0}, such
that {V1,0, . . . , Vk0,0} is a contiguous partition. On the other hand, we have
k0
X
j=1
|{Bm : Bm âˆˆVj,0, Bm âŠŠDj,0}|
â©½
| âˆªk0
j=1 {Vj,0 \ V2
j }| â©½|{âˆªk0
j=1V2
j }c|
(i)
â©½
|b|
(ii)
â©½9|B|
(iii)
â©½cK,
where (i) uses the fact that {V1
j âˆ©bc} âŠ†V2
j , (ii) uses |Neighbour(Bm) âˆªBm| â©½9, and (iii) uses the
fact that |B| â©½cK from Assumption (2). Equation (S.6) is thus established and the proposition if
proved.
S7

S.5
Proof of Proposition 2
We give the proof for Ïµ(Â·, Â·) in this section. The proof of Ïµn(Â·, Â·) is similar and thus omitted. Note that
the proof of non-negativity, identity of indiscernibles and symmetry axioms are trivial, we thus only
show the triangle inequality.
Recall that for two domain partition Ï€1(D) and Ï€2(D), we decompose Ïµ{Ï€1(D), Ï€2(D)} =
Ïµ1{Ï€1(D), Ï€2(D)} + Ïµ2{Ï€1(D), Ï€2(D)}, where Ïµ1{Ï€1(D), Ï€2(D)} and Ïµ2{Ï€1(D), Ï€2(D)} are defined in
(21) and (22), respectively. It is easy to see that Ïµ1{Ï€1(D), Ï€2(D)} = Ïµ2{Ï€2(D), Ï€1(D)}. Thus, to
obtain the triangle inequality for Ïµ(Â·, Â·), it suffices to show the triangle inequality for Ïµ1(Â·, Â·). Before
that, we first study some properties of Ïµ1(Â·, Â·).
Lemma S.12. For any domain partitions Ï€1(D) = {D11, . . . , D1k1}, Ï€2(D) = {D21, . . . , D2k2} and
Ï€3(D) = {D31, . . . , D3k3}, where k1, k2 and k3 are the number of clusters in Ï€1(D), Ï€2(D) and Ï€3(D),
respectively, we have
Ïµ1{Ï€1(D), Ï€2(D)} âˆ’Ïµ1{eÏ€(D), Ï€2(D)} â©¾0,
(S.7)
where eÏ€(D) = Ï€1(D) âˆ©Ï€3(D) = { ËœD1, . . . ËœDËœk}, with Ëœk denoting the corresponding number of clusters.
Furthermore, we have
Ïµ1{Ï€1(D), Ï€3(D)} âˆ’Ïµ1{eÏ€(D), Ï€3(D)} â©¾Ïµ1{Ï€1(D), Ï€2(D)} âˆ’Ïµ1{eÏ€(D), Ï€2(D)}.
(S.8)
Proof. Following (21), we have
Ïµ1{Ï€1(D), Ï€2(D)} = 1 âˆ’|D|âˆ’1
k1
X
j=1
max
lâˆˆ{1,...,k2} |D1j âˆ©D2l|,
and
Ïµ1{eÏ€(D), Ï€2(D)} = 1 âˆ’|D|âˆ’1
Ëœk
X
j=1
max
lâˆˆ{1,...,k2} | ËœDj âˆ©D2l|.
Since ËœÏ€(D) is nested in Ï€1(D), for each j = 1, . . . , k1, there exists an index set I(j), such that
âˆªjâ€²âˆˆI(j) ËœDjâ€² = D1j. Thus, we write
Ïµ1{Ï€1(D), Ï€2(D)}âˆ’Ïµ1{eÏ€(D), Ï€2(D)} = |D|âˆ’1
k1
X
j=1
ï£®
ï£°
ï£±
ï£²
ï£³
X
jâ€²âˆˆI(j)
max
lâˆˆ{1,...,k2} | ËœDjâ€² âˆ©D2l|
ï£¼
ï£½
ï£¾âˆ’
max
lâˆˆ{1,...,k2} |D1j âˆ©D2l|
ï£¹
ï£»,
(S.9)
from which it is easy to see that Ïµ1{Ï€1(D), Ï€2(D)} âˆ’Ïµ1{eÏ€(D), Ï€2(D)}
â©¾|D|âˆ’1
k1
X
j=1
ï£®
ï£°
ï£±
ï£²
ï£³
max
lâˆˆ{1,...,k2}
X
jâ€²âˆˆI(j)
| ËœDjâ€² âˆ©D2l|
ï£¼
ï£½
ï£¾âˆ’
max
lâˆˆ{1,...,k2} |D1j âˆ©D2l|
ï£¹
ï£»= 0.
(S.10)
Equation (S.7) is thus proved.
Next, substituting Ï€2(D) in (S.9) with Ï€3(D), we have
Ïµ1{Ï€1(D), Ï€3(D)}âˆ’Ïµ1{eÏ€(D), Ï€3(D)} = |D|âˆ’1
k1
X
j=1
ï£®
ï£°
ï£±
ï£²
ï£³
X
jâ€²âˆˆI(j)
max
lâˆˆ{1,...,k3} | ËœDjâ€² âˆ©D3l|
ï£¼
ï£½
ï£¾âˆ’
max
lâˆˆ{1,...,k3} |D1j âˆ©D3l|
ï£¹
ï£»
(S.11)
S8

for the same I(j) as define above. Since eÏ€(D) = Ï€1(D) âˆ©Ï€3(D), we can see that maxlâˆˆ{1,...,k3} | ËœDjâ€² âˆ©
D3l| = | ËœDjâ€²|. Thus, we rewrite (S.11) as
Ïµ1{Ï€1(D), Ï€3(D)} âˆ’Ïµ1{eÏ€(D), Ï€3(D)} = |D|âˆ’1
k1
X
j=1
ï£®
ï£°
ï£±
ï£²
ï£³
X
jâ€²âˆˆI(j)
| ËœDjâ€²|
ï£¼
ï£½
ï£¾âˆ’
max
lâˆˆ{1,...,k3} |D1j âˆ©D3l|
ï£¹
ï£».
(S.12)
On the other hand, let Ëœjâ€²(j) = argmaxjâ€²âˆˆI(j)| ËœDjâ€²|, we can see that maxlâˆˆ{1,...,k3} |D1j âˆ©D3l| = | ËœDËœjâ€²(j)|.
Thus, we rewrite (S.12) and (S.9) as
Ïµ1{Ï€1(D), Ï€3(D)} âˆ’Ïµ1{eÏ€(D), Ï€3(D)} = |D|âˆ’1
k1
X
j=1
ï£®
ï£°
ï£±
ï£²
ï£³
X
jâ€²âˆˆI(j)\Ëœjâ€²(j)
| ËœDjâ€²|
ï£¼
ï£½
ï£¾
ï£¹
ï£»,
and
Ïµ1{Ï€1(D), Ï€2(D)} âˆ’Ïµ1{eÏ€(D), Ï€2(D)}
=
|D|âˆ’1
k1
X
j=1
ï£®
ï£°
ï£±
ï£²
ï£³
X
jâ€²âˆˆI(j)\Ëœjâ€²(j)
max
lâˆˆ{1,...,k2} | ËœDjâ€² âˆ©D2l|
ï£¼
ï£½
ï£¾
ï£¹
ï£»
+|D|âˆ’1
k1
X
j=1
{
max
lâˆˆ{1,...,k2} | ËœDËœjâ€²(j)l âˆ©D2l| âˆ’
max
lâˆˆ{1,...,k2} |D1j âˆ©D2l|}
(i)
â©½
|D|âˆ’1
k1
X
j=1
ï£®
ï£°
ï£±
ï£²
ï£³
X
jâ€²âˆˆI(j)\Ëœjâ€²(j)
max
lâˆˆ{1,...,k2} | ËœDjâ€² âˆ©D2l|
ï£¼
ï£½
ï£¾
ï£¹
ï£»,
where (i) is because ËœDËœjâ€²(j) âŠ†D1j.
Note that | ËœDjâ€²| â©¾maxlâˆˆ{1,...,k2} | ËœDjâ€² âˆ©D2l|, Equation (S.8) is
proved.
Making use of Lemma S.12, we prove the triangle inequality for Ïµ1(Â·, Â·) as follows.
Proof. For any domain partitions Ï€1(D), Ï€2(D) and Ï€3(D), let eÏ€(D) = Ï€1(D) âˆ©Ï€3(D). We have
Ïµ1{Ï€1(D), Ï€3(D)}
=
Ïµ1{Ï€1(D), Ï€3(D)} âˆ’Ïµ1{eÏ€(D), Ï€3(D)}
Equation (S.8)
â©¾
Ïµ1{Ï€1(D), Ï€2(D)} âˆ’Ïµ1{eÏ€(D), Ï€2(D)}
(i)
â©¾
Ïµ1{Ï€1(D), Ï€2(D)} âˆ’Ïµ1 {Ï€3(D), Ï€2(D)} ,
where (i) is due to Ïµ1{eÏ€(D), Ï€2(D)} â©½Ïµ1 {Ï€3(D), Ï€2(D)} from Equation (S.7). The triangle inequality
for Ïµ1(Â·, Â·) is thus proved.
Combining the result above, the proposition is proved.
S.6
Proof of Proposition 3
This section gives proof of Proposition 3. We first give some technical lemmas. Write Âµ0 = E(y | s, x)
as the true regression mean.
Lemma S.13. Under the event An and Assumptions 2 and 5, there exists a constant c, such that
âˆ¥(Ï•Ï€âˆ—
0 âˆ’In)Âµ0âˆ¥2
2 â©½cn1/2 log(1+Î±b)/2(n).
S9

Proof. Recall the definition of P Ï€âˆ—under (29), let XÏ€âˆ—
0 = P T
Ï€âˆ—
0diag(x1, . . . , xk0). We can see that
Ï•Ï€âˆ—
0Âµ0 = XÏ€âˆ—
0 bÎ¸,
where
bÎ¸ = argminÎ¸âˆˆRk0dâˆ¥XÏ€âˆ—
0Î¸ âˆ’Âµ0âˆ¥2
2.
From (11), for Î¸0 = (Î¸T
1,0, . . . , Î¸T
k0,0)T , where {Î¸l,0}k0
l=1 are the true values of {Î¸l}k0
l=1, we can see
âˆ¥XÏ€âˆ—
0Î¸0 âˆ’Âµ0âˆ¥2
2 â©½CK Ã— log1+Î±b(n) â©½Cn1/2 log(1+Î±b)/2(n)
under An. Therefore, we conclude
âˆ¥(Ï•Ï€âˆ—
0 âˆ’In)Âµ0âˆ¥2
2 â©½âˆ¥XÏ€âˆ—
0Î¸0 âˆ’Âµ0âˆ¥2
2 â©½Cn1/2 log(1+Î±b)/2(n),
which proves the lemma.
Recall that we write Ï€âˆ—
0 = {Dâˆ—
l,0}k0
l=1 and M(Dâˆ—
j) = argmaxlâˆˆ{1,...,k0}|Dâˆ—
j âˆ©Dâˆ—
l,0| as the index of the
sub-domain in {Dâˆ—
l,0}k0
l=1 with the largest intersection area with Dâˆ—
j. After simple algebra, it can be
shown that Ïµ1(Ï€âˆ—, Ï€âˆ—
0) defined in (21) can be re-written as
Ïµ1(Ï€âˆ—, Ï€âˆ—
0) =
k
X
j=1
ï£±
ï£´
ï£²
ï£´
ï£³
X
lâˆˆ{1,...,k0}\M(Dâˆ—
j )
|Dâˆ—
j âˆ©Dâˆ—
l,0|
ï£¼
ï£´
ï£½
ï£´
ï£¾
,
where we use the fact |D| = 1 since D = [0, 1]2. Note that the number of summation terms is no larger
than k0kmax, we conclude that
max
1â©½jâ©½k
max
lâˆˆ{1,...,k0}\M(Dâˆ—
j ) |Dâˆ—
j âˆ©Dâˆ—
l,0| â©¾Ïµ1(Ï€âˆ—, Ï€âˆ—
0)
k0kmax
.
(S.13)
Recall the definition of Î±0 in Theorem 1. The next lemma gives a lower bound of âˆ¥(In âˆ’Ï•Ï€âˆ—)Âµ0âˆ¥2
2
by using (S.13).
Lemma S.14. Under the event An and Assumption 3, there exists a uniform constant c > 0, such
that
âˆ¥(In âˆ’Ï•Ï€âˆ—)Âµ0âˆ¥2
2 â©¾cÏµ1(Ï€âˆ—, Ï€âˆ—
0)K2 log1+Î±b(n)
holds for all Ï€âˆ—âˆˆÎâˆ—with Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2 â©¾âŒŠK logÎ±0(n)âŒ‹.
Proof. From the definition of Ïµ1(Ï€âˆ—, Ï€âˆ—
0) and (S.13), we can see that there exists a cluster Dâˆ—
j âˆˆÏ€âˆ—,
such that Dâˆ—
j contains âŒŠÏµ1(Ï€âˆ—,Ï€âˆ—
0)
k0kmax K2âŒ‹blocks in Dâˆ—
l,0, and another âŒŠÏµ1(Ï€âˆ—,Ï€âˆ—
0)
k0kmax K2âŒ‹blocks in Dâˆ—
lâ€²,0, for some
l Ì¸= lâ€². Recall the definition of WÏ€âˆ—
0 in Proposition 1. Since |WÏ€âˆ—
0| â©½cK, there exists another constant
c, such that for Ï€âˆ—âˆˆÎâˆ—with Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2 â©¾âŒŠK logÎ±0(n)âŒ‹, Dâˆ—
j contains cÏµ1(Ï€âˆ—, Ï€âˆ—
0)K2 blocks fully
contained in Dl,0, and another cÏµ1(Ï€âˆ—, Ï€âˆ—
0)K2 blocks fully contained in Dlâ€²,0, for some l Ì¸= lâ€². We denote
these two sets of blocks as S1 and S2, respectively. Accordingly, we write X1 and X2 as the design
S10

matrix constructed from S1 and S2, respectively, X = (XT
1 , XT
2 )T , and n1 and n2 as the number of
locations in S1 and S2, respectively. Under An, we can see that there exits a constant c, such that
min{n1, n2} â©¾cÏµ1(Ï€âˆ—, Ï€âˆ—
0)K2 Ã— log1+Î±b(n). Without loss of generality, we assume n1 â©¾n2.
We write Âµ0 | S1 as the sub-vector of Âµ0 containing only the locations within S1, and write Âµ0 | S2
and Âµ0 | S1âˆªS2 similarly. Since S1 and S2 belong to two different Dl,0 and Dlâ€²,0, we can write Î¸1 = Î¸l,0
and Î¸2 = Î¸lâ€²,0 with âˆ¥Î¸1 âˆ’Î¸2âˆ¥â©¾c (Assumption 3) and
Âµ0 | S1 = X1Î¸1, and Âµ0 | S2 = X2Î¸2.
On the other hand, since locations in S1 and S2 belong to the same cluster under Ï€âˆ—, there exists a
common Î¸, such that (Ï•Ï€âˆ—Âµ0) | S1 âˆªS2 = XÎ¸. Thus,
âˆ¥(In âˆ’Ï•Ï€âˆ—)Âµ0âˆ¥2
2
â©¾
âˆ¥{(In âˆ’Ï•Ï€âˆ—)Âµ0} | S1 âˆªS2âˆ¥2
2
=
âˆ¥X1Î¸1 âˆ’X1Î¸âˆ¥2
2 + âˆ¥X2Î¸2 âˆ’X2Î¸âˆ¥2
2
=
(Î¸1 âˆ’Î¸)T XT
1 X1(Î¸1 âˆ’Î¸) + (Î¸2 âˆ’Î¸)T XT
2 X2(Î¸2 âˆ’Î¸)
(i)
â©¾
cn1âˆ¥Î¸1 âˆ’Î¸âˆ¥2
2 + cn2âˆ¥Î¸2 âˆ’Î¸âˆ¥2
2
â©¾
cn2{âˆ¥Î¸1 âˆ’Î¸âˆ¥2
2 + âˆ¥Î¸2 âˆ’Î¸âˆ¥2
2} â©¾cn2âˆ¥Î¸1 âˆ’Î¸2âˆ¥2
2 â©¾cÏµ1(Ï€âˆ—, Ï€âˆ—
0)K2 Ã— log1+Î±b(n),
where (i) uses the fact that under event An, XT
1 X1 â©¾cn1 and XT
2 X2 â©¾cn2. From the derivation,
we can see the constant c in the last inequality doesnâ€™t depend on Ï€âˆ—. The lemma is thus proved.
Next, under events An âˆ©En, we give the proof of Proposition 3 as follows.
Proof. We first consider cases when Ï€âˆ—âˆˆÎ âˆ—
1 âˆªÎ âˆ—
2 âˆªÎ âˆ—
3. Following Equation (29), we have
P(y | Ï€âˆ—, x, s)Î»|Ï€âˆ—|
P(y | Ï€âˆ—
0, x, s)Î»|Ï€âˆ—
0| = exp
"
(|Ï€âˆ—
0| âˆ’|Ï€âˆ—|)

log(Î»âˆ’1) + d log(n + 1)
2

+
nyT (Ï•Ï€âˆ—âˆ’Ï•Ï€âˆ—
0)y
2Ïƒ2(n + 1)
.
#
(S.14)
Write ËœÏ€âˆ—= Ï€âˆ—âˆ©Ï€âˆ—
0, we bound yT (Ï•Ï€âˆ—âˆ’Ï•Ï€âˆ—
0)y by
yT (Ï•Ï€âˆ—âˆ’Ï•Ï€âˆ—
0)y
=
yT (Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—
0)y + yT (Ï•Ï€âˆ—âˆ’Ï•ËœÏ€âˆ—)y
(i)
=
âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—
0)yâˆ¥2
2 âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)yâˆ¥2
2
â©½
2âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—
0)Âµ0âˆ¥2
2 + 2âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—
0)Ïµâˆ¥2
2 âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)yâˆ¥2
2
(ii)
â©½
8âˆ¥(In âˆ’Ï•Ï€âˆ—
0)Âµ0âˆ¥2
2 + 4âˆ¥Ï•ËœÏ€âˆ—Ïµâˆ¥2 + 4âˆ¥Ï•Ï€âˆ—
0Ïµâˆ¥2
2 âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)yâˆ¥2
2
(iii)
â©½
cn1/2 log(1+Î±b)/2(n) + CK2 log(n) âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)yâˆ¥2
2,
(iv)
â©½
cn1/2 log(1+Î±b)/2(n) + Cn logâˆ’Î±b(n) âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)yâˆ¥2
2
â©½
cn logâˆ’Î±b(n) âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)yâˆ¥2
2,
(S.15)
where (i) uses the fact that (Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—
0)2 = Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—
0 and (Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)2 = Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—since ËœÏ€âˆ—in
nested in Ï€âˆ—and Ï€âˆ—
0, (ii) is because
âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—
0)Âµ0âˆ¥2
2 â©½2âˆ¥(Ï•ËœÏ€âˆ—âˆ’In)Âµ0âˆ¥2
2 + 2âˆ¥(In âˆ’Ï•Ï€âˆ—
0)Âµ0âˆ¥2
2 â©½4âˆ¥(In âˆ’Ï•Ï€âˆ—
0)Âµ0âˆ¥2
2,
S11

(iii) uses result from Lemma S.13 and E1n, and (iv) uses Assumption 5.
We next discuss Ï€âˆ—âˆˆ
Î âˆ—
1, Î âˆ—
2 and Î âˆ—
3, respectively.
Cases when Ï€âˆ—âˆˆÎ âˆ—
1:
Since each block has an area Kâˆ’2, we can see for each Dl,0, l = 1, . . . , k0, the number of blocks
intersecting with it is larger than cK2 for some constant c. Since |WÏ€âˆ—
0| â©½cK and K â†’âˆ, we
conclude that the number of blocks within each Dâˆ—
l,0, l = 1, . . . , k0 is larger than cK2. Similarly, it
is easy to derive that the number of blocks within each Dâˆ—
l,0, l = 1, . . . , k0 is smaller than CK2 for
another constant C.
Based on the above result, we can see that there exists a constant c, such that Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2 > cK2
holds for Ï€âˆ—âˆˆÎ âˆ—
1. Since Assumption 5 guarantees that K2 â‰«K logÎ±0(n), we can apply Lemma S.14
and obtain
âˆ¥(In âˆ’Ï•Ï€âˆ—)Âµ0âˆ¥2
2 â©¾cn, âˆ€Ï€âˆ—âˆˆÎ âˆ—
1.
(S.16)
Next, applying Equation (S.15), we have
yT (Ï•Ï€âˆ—âˆ’Ï•Ï€âˆ—
0)y
â©½
cn logâˆ’Î±b(n) âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)yâˆ¥2
2
â©½
cn logâˆ’Î±b(n) âˆ’{âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)Âµ0âˆ¥2 âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)Ïµâˆ¥2}2.
Note that
âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)Âµ0âˆ¥2
â©¾
âˆ¥(In âˆ’Ï•Ï€âˆ—)Âµ0âˆ¥2 âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’In)Âµ0âˆ¥2
(i)
â©¾
cn1/2 âˆ’Cn1/4 log(1+Î±b)/4(n) â©¾cn1/2,
(S.17)
where (i) uses Equation (S.16) and Lemma S.13. Besides, under E1n, we have
âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)Ïµâˆ¥2 â©½2
âˆš
5Ïƒ0K log1/2(n) âˆ¼n1/2 logâˆ’Î±b/2(n).
(S.18)
Putting the result together, we conclude
yT (Ï•Ï€âˆ—âˆ’Ï•Ï€âˆ—
0)y
â©½
cn logâˆ’Î±b(n) âˆ’cn â©½âˆ’cn.
Plugging the result into Equation (S.14), we have
P(y | Ï€âˆ—, x, s)Î»|Ï€âˆ—|
P(y | Ï€âˆ—
0, x, s)Î»|Ï€âˆ—
0|
â©½
exp

(|Ï€âˆ—
0| âˆ’|Ï€âˆ—|)

log(Î»âˆ’1) + d log(n + 1)
2

âˆ’cn

Assumption 5
â©½
exp[ck0n logâˆ’Î±p(n) âˆ’cn] â©½exp(âˆ’cn).
Cases when Ï€âˆ—âˆˆÎ âˆ—
2:
From Equation (S.15), we can see that
yT (Ï•Ï€âˆ—âˆ’Ï•Ï€âˆ—
0)y â©½cn logâˆ’Î±b(n).
Thus, together with Assumption 5, we have
P(y | Ï€âˆ—, x, s)Î»|Ï€âˆ—|
P(y | Ï€âˆ—
0, x, s)Î»|Ï€âˆ—
0|
â©½
exp

âˆ’

log(Î»âˆ’1) + d log(n + 1)
2

+ cn logâˆ’Î±b(n)

S12

â©½
exp[âˆ’cn logâˆ’Î±p(n) + cn logâˆ’Î±b(n)]
Assumption 5
â©½
exp[âˆ’cn logâˆ’Î±p(n)].
Cases when Ï€âˆ—âˆˆÎ âˆ—
3:
Using the similar arguments as the cases of Î âˆ—
1, it is easy to see that there exists a constant c,
such that Ïµ1(Ï€âˆ—, Ï€âˆ—
0) > cK2 holds for any Ï€âˆ—âˆˆÎ âˆ—
3. The proof is then similar to the cases of Î âˆ—
1 and is
omitted.
Next, we discuss cases when Ï€âˆ—âˆˆÎ âˆ—
4. Write ËœÏ€âˆ—= Ï€âˆ—âˆ©Ï€âˆ—
0, following the same arguments in (S.15),
we have
yT (Ï•Ï€âˆ—âˆ’Ï•Ï€âˆ—
0)y
â©½
8âˆ¥(In âˆ’Ï•Ï€âˆ—
0)Âµ0âˆ¥2
2 + 4âˆ¥Ï•ËœÏ€âˆ—Ïµâˆ¥2 + 4âˆ¥Ï•Ï€âˆ—
0Ïµâˆ¥2
2 âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)yâˆ¥2
2
(i)
â©½
cn1/2 log(1+Î±b)/2(n) + 16Ïƒ2
0{1 + Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2} log1+Î±b/2(n) âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)yâˆ¥2
2
â©½
c{n1/2 + Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2} log(1+Î±b)/2(n) âˆ’{âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)Âµ0âˆ¥2 âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)Ïµâˆ¥}2,
where (i) uses Lemma S.13 and E2n. On the other hand, from Lemmas S.13 and S.14, we can see
âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)Âµ0âˆ¥2
â©¾
âˆ¥(In âˆ’Ï•Ï€âˆ—)Âµ0âˆ¥2 âˆ’âˆ¥(Ï•ËœÏ€âˆ—
0 âˆ’In)Âµ0âˆ¥2
â©¾
cÏµ1/2
1
(Ï€âˆ—, Ï€âˆ—
0)K log(1+Î±b)/2(n) âˆ’Cn1/4 log(1+Î±b)/4(n),
and
âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)Ïµâˆ¥2 â©½âˆ¥Ï•ËœÏ€âˆ—Ïµâˆ¥2 + âˆ¥Ï•Ï€âˆ—Ïµâˆ¥2 â©½cÏµ1/2
1
(Ï€âˆ—, Ï€âˆ—
0)K log1/2+Î±b/4(n)
under E2n. Since Ï€âˆ—âˆˆÎ âˆ—
4 and K âˆ¼n1/2 logâˆ’(1+Î±b)/2(n) from Assumption 5, we have
Ïµ1/2
1
(Ï€âˆ—, Ï€âˆ—
0)K log(1+Î±b)/2(n) â‰«Cn1/4 log(1+Î±b)/4(n) + Ïµ1/2
1
(Ï€âˆ—, Ï€âˆ—
0)K log1/2+Î±b/4(n),
leading to
{âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)Âµ0âˆ¥2 âˆ’âˆ¥(Ï•ËœÏ€âˆ—âˆ’Ï•Ï€âˆ—)Ïµâˆ¥2}2 â©¾cÏµ1(Ï€âˆ—, Ï€âˆ—
0)K2 log(1+Î±b)(n).
Combining the result, we obtain
yT (Ï•Ï€âˆ—âˆ’Ï•Ï€âˆ—
0)y
â©½
c{n1/2 + Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2} log(1+Î±b)/2(n) âˆ’cÏµ1(Ï€âˆ—, Ï€âˆ—
0)K2 log(1+Î±b)(n)
(ii)
â©½
âˆ’cÏµ1(Ï€âˆ—, Ï€âˆ—
0)K2 log(1+Î±b)(n),
where (ii) uses the fact that Ïµ1(Ï€âˆ—, Ï€âˆ—
0)K2 â©¾âŒŠK logÎ±0(n)âŒ‹âˆ¼n1/2 logÎ±0âˆ’(1+Î±b)/2(n).
Plugging this
result into Equation (S.14), we have
P(y | Ï€âˆ—, x, s)Î»|Ï€âˆ—|
P(y | Ï€âˆ—
0, x, s)Î»|Ï€âˆ—
0|
=
exp
(nyT (Ï•Ï€âˆ—âˆ’Ï•Ï€âˆ—
0)y
2Ïƒ2(n + 1)
)
â©½exp{âˆ’cÏµ1(Ï€âˆ—, Ï€âˆ—
0)K2 log1+Î±b(n)}.
Combining the result, the proposition is proved.
S.7
Graph results and theory
This section derives some general graph results and gives proof of Proposition 4. Section S.7.1 provides
a general result on the number of spanning trees, given a graph G. Section S.7.2 applies result of Section
S.7.1 to our model setting to prove Proposition 4.
S13

S.7.1
General result of graph
The result in this section is a general graph result, which is independent of our model. For generality,
we take some new notations, which may be different from the main paper. Let G = (V0, E) be a spatial
graph, where V0 is a vertex set, and the edge set E is a subset of {(vi, viâ€²) : vi, viâ€² âˆˆV, vi Ì¸= viâ€²}. We
say a vertex set V âŠ†V0 is connected under G, if there exists a path from v1 to v2 with all the vertexes
in path contained in V, for any two vertexes v1, v2 âˆˆV. Based on graph G, we define the distance of
two vertexes v1, v2 âˆˆV0 as
dG(v1, v2) =
min
PâˆˆPG(v1,v2) |P|,
where PG(v1, v2) is the set containing all paths from v1 to v2 under graph G. For two vertex sets V1
and V2, we define their distance as
dG(V1, V2) =
min
v1âˆˆV1,v2âˆˆV2 dG(v1, v2).
Given a connected set V âŠ†V0, we write S(V) as the set of spanning trees of V under G. For a general
vertex set V (which is not necessary to be connected), it is easy to see that there exists a unique
decomposition V = âˆªjVj, such that each Vj is connected, and dG(Vj, Vjâ€²) > 1 for j Ì¸= jâ€². Based on the
decomposition, we define an operator of V, say H(V), as H(V) = Q
j H(Vj), where H(Vj) = |S(Vj)|.
For a set of m disjoint vertex sets {Vj}m
j=1, we write b({Vj}m
j=1) = Pm
j=1
Pm
jâ€²=1 I(jâ€² > j)b({Vj, Vjâ€²}),
where b({Vj, Vjâ€²}) is the number of edges (under graph G) connecting Vj and Vjâ€².
Figure S.2: One graph example. In this example, the graph G is a 5 Ã— 5 mesh grid. We denote V1,
V2 and V3 by vertexes with red, blue and gray colors, respectively. We can see {Vj}3
j=1 are three
connected vertex sets and dG(V1, V2) = 1, dG(V1, V3) = 2 and dG(V2, V3) = 1.
According to the
definition, b({Vj}3
j=1) = b({V1, V2})+b({V1, V3})+b({V2, V3}), where b({V1, V2}) = 6, b({V1, V3}) = 0
and b({V2, V3}) = 7 since the number of edges connecting {V1, V2}, {V1, V3} and {V2, V3} are 6, 0 and
7, respectively.
Note that H(V) defined above is the number of spanning trees of V, if V is connected. We will
study the property of H(Â·), because H(Â·) has a close relationship with the number of spanning trees
inducing a particular partition. We have the following lemma.
S14

Lemma S.15. Let V1 and V2 be two connected vertex sets, and V1 âˆ©V2 = âˆ…. Write V = V1 âˆªV2, we
have
H(V1)H(V2) â©½H(V) â©½H(V1)H(V2) Ã— exp[b({V1, V2}){log(|V1|) + log(|V2|) + log(2)}].
Proof. If dG(V1, V2) > 1, it is easy to see that b({V1, V2}) = 0 and H(V) = H(V1)H(V2) by the
definition. The inequality holds immediately. We next consider the case when dG(V1, V2) = 1, hence
V is also connected and S(V) is well defined.
If dG(V1, V2) = 1, there exists a pair (s1, s2), with s1 âˆˆV1, s2 âˆˆV2, such that s1 is connected to s2
by an edge e âˆˆE. For any T1 âˆˆS(V1), T2 âˆˆS(V2), we can thus construct a spanning tree, say T (T1, T2)
by connecting T1 and T2 with e. It is easy to see that T (T1, T2) âˆˆS(V) and T (T1, T2) Ì¸= T (T â€²
1, T â€²
2) if
(T1, T2) Ì¸= (T â€²
1, T â€²
2). Thus, we have H(V1)H(V2) = |S(V1)||S(V2)| â©½|S(V)| = H(V).
We next prove the second inequality. For any spanning tree T âˆˆS(V), we split it by removing all
the edges connecting (s1, s2), where s1 âˆˆV1 and s2 âˆˆV2. We can see that the number of removed
edges is smaller than b({V1, V2}). After the removal, we will obtain some â€œsubâ€ spanning trees, the
vertexes of which are either subset of V1 or subset of V2. Write ËœSi(T ) as the set of â€œsubâ€ spanning
trees obtained from T with vertexes belonging to Vi, i = 1, 2. Let A = {{ ËœS1(T ), ËœS2(T )} : T âˆˆS(V)}
be the space of { ËœS1(T ), ËœS2(T )}. We can see that each T âˆˆS(V) corresponds to an element a âˆˆA.
We will then bound H(V) = |S(V)| as
H(V)
â©½
|A| Ã— max
aâˆˆA [|{T âˆˆS(V) : T can induce a }|]
â©½
|{ ËœS1(T ) : T âˆˆS(V)}| Ã— |{ ËœS2(T ) : T âˆˆS(V)}| Ã— max
aâˆˆA [|{T âˆˆS(V) : T can induce a}|].
Since Vi is connected, we can always add some edges between those sub spanning trees in ËœSi(T ) to
join them into a spanning tree consisting all the vertexes of Vi. Thus,
ËœSi(T ) âˆˆ{â€œsubâ€ spanning trees obtained after cutting no more than b{V1, V2} edges from Ti âˆˆS(Vi)}.
Together with Lemma S.3, we hence conclude that
|{ ËœSi(T ) : T âˆˆS(V)}|
â©½
H(Vi) Ã—
b({V1,V2})
X
j=0
ï£«
ï£¬
ï£­
|Vi| âˆ’1
j
ï£¶
ï£·
ï£¸
â©½
H(Vi) Ã— exp{b({V1, V2}) log(|Vi|)}.
On the other hand, note that if T can induce a âˆˆA, T can be converted back by adding back those
removed edges from a. Since the number of removed edges is no more than b(V1, V2), we conclude
that
max
aâˆˆA |{T âˆˆS(V) : T can induce a}| â©½2b({V1,V2}) = exp{log(2)b({V1, V2})}.
Putting the result together, the lemma is proved.
The following lemma extends Lemma S.15 to a more general case.
S15

Lemma S.16. Let {Vi}m
i=1 be m connected vertex sets, with Vm âˆ©Vmâ€² = âˆ…if m Ì¸= mâ€². Write V =
âˆªm
i=1Vi, we have
m
Y
i=1
H(Vi) â©½H(V) â©½
( m
Y
i=1
H(Vi)
)
Ã— exp
"
b({Vi}m
i=1)
(
2 log
 m
X
i=1
|Vi|
!
+ log(2)
)#
.
(S.19)
Proof. We will prove the lemma by induction. It is easy to see that inequality holds for m = 1.
Besides, from Lemma S.15, we can see that the inequality holds for m = 2. Next, we prove that if
(S.19) holds for m = m0 â©¾2, it also holds for m = m0 + 1. Let {Vi}m0+1
i=1
be m0 + 1 connected vertex
sets with Vm âˆ©Vmâ€² = âˆ…if m Ì¸= mâ€². We discuss the following two cases of Vm0+1:
If min1â©½iâ©½m0{dG(Vi, Vm0+1)} > 1:
From the definition of H(Â·), we can see
H(V) = H(Vm0+1)H(âˆªm0
i=1Vi).
We can thus apply Equation (S.19) to H(âˆªm0
i=1Vi), obtaining
H(V) = H(Vm0+1)H(âˆªm0
i=1Vi) â©¾
m0+1
Y
i=1
H(Vi),
and
H(V)
=
H(Vm0+1)H(âˆªm0
i=1Vi)
â©½
(m0+1
Y
i=1
H(Vi)
)
Ã— exp
"
b({Vi}m0
i=1)
(
log(2) + 2 log
 m0
X
i=1
|Vi|
!)#
â©½
(m0+1
Y
i=1
H(Vi)
)
Ã— exp
"
b({Vi}m0+1
i=1
)
(
log(2) + 2 log
 m0+1
X
i=1
|Vi|
!)#
.
The inequality is thus proved.
If min1â©½iâ©½m0{dG(Vi, Vm0+1)} = 1:
Without loss of generality, we assume dG(Vm0, Vm0+1) = 1. Write ËœV = Vm0 âˆªVm0+1, we can see
that ËœV is connected. So we can view V as a union of m0 connected vertex sets, i.e., V = {âˆªm0âˆ’1
i=1
Vi}âˆªËœV.
By applying (S.19) to H(V) = H({âˆªm0âˆ’1
i=1
Vi} âˆªËœV), we have
H(V) â©¾
(m0âˆ’1
Y
i=1
H(Vi)
)
H(ËœV)
(i)
â©¾
(m0+1
Y
i=1
H(Vi)
)
where in (i) we apply Equation (S.19) to H(ËœV) = H(Vm0 âˆªVm0+1). On the other hand, Equation
(S.19) also entails that
H(V)
â©½
(m0âˆ’1
Y
i=1
H(Vi)
)
H(ËœV) Ã— exp
"
b({V1, . . . , Vm0âˆ’1, ËœV})
(
log(2) + 2 log
 m0âˆ’1
X
i=1
|Vi| + |ËœV|
!)#
(ii)
â©½
(m0+1
Y
i=1
H(Vi)
)
Ã— exp[b({Vm0, Vm0+1}){log(2) + 2 log(|ËœV|)}]
Ã— exp
"
b({V1, . . . , Vm0âˆ’1, ËœV})
(
log(2) + 2 log
 m0âˆ’1
X
i=1
|Vi| + |ËœV|
!)#
S16

â©½
(m0+1
Y
i=1
H(Vi)
)
Ã— exp
"
b({Vi}m0+1
i=1
)
(
log(2) + 2 log
 m0+1
X
i=1
|Vi|
!)#
,
where in (ii) we apply Equation (S.19) to H(ËœV) = H(Vm0 âˆªVm0+1). Thus, by induction, the lemma
is proved.
We furthermore extend Lemma S.16 to the following general result, where we do not require each
Vi to be connected.
Lemma S.17. Let {Vi}m
i=1 be m vertex sets, with Vm âˆ©Vmâ€² = âˆ…if m Ì¸= mâ€². Write V = âˆªm
i=1Vi, we
have
m
Y
i=1
H(Vi) â©½H(V) â©½
( m
Y
i=1
H(Vi)
)
Ã— exp
"
b({Vi}m
i=1)
(
2 log
 m
X
i=1
|Vi|
!
+ log(2)
)#
.
(S.20)
Proof. We first perform decomposition on Vi such that Vi = âˆªni
j=1Vij, where dG(Vij, Vijâ€²) > 1 if j Ì¸= jâ€²
holds for all i = 1, . . . , m, each Vij is connected, and ni is the number of connected vertex sets for Vi.
By the definition of H(Â·), we have
m
Y
i=1
H(Vi)
=
m
Y
i=1
ni
Y
j=1
H(Vij).
On the other hand, note that V = âˆªm
i=1âˆªni
j=1Vij is a union of connected vertex sets {Vij}, after applying
Lemma S.16, we have
m
Y
i=1
ni
Y
j=1
H(Vij) â©½
H(V),
and
H(V)
â©½
m
Y
i=1
ni
Y
j=1
H(Vij) Ã— exp
"
b({Vij}m,ni
i=1,j=1)
(
log(2) + 2 log
 m
X
i=1
|Vi|
!)#
(i)
=
m
Y
i=1
H(Vi) Ã— exp
"
b({Vi}m
i=1)
(
log(2) + 2 log
 m
X
i=1
|Vi|
!)#
,
where the equality (i) uses the fact that dG(Vij, Vijâ€²) > 1 if j Ì¸= jâ€². Combining the result above, the
lemma is proved.
S.7.2
Proof of Proposition 4
Recall that in our model, the domain partition Ï€âˆ—is induced from the contiguous partition of blocks
V = {Bm}K2
m=1, say Ï€(V) = {V1, . . . , Vk}. In Section S.4, we have shown that Ï€âˆ—
0 in Proposition 1 is
induced from Ï€0(V) = {V1,0, . . . , Vk0,0}. We give the proof of Proposition 4 as follows.
Proof. Based on results in Section S.7.1, we have
|{T : T can induce Ï€âˆ—}|
S17

(i)
â©½
ï£±
ï£²
ï£³
k
Y
j=1
H(Vj)
ï£¼
ï£½
ï£¾Ã—
ï£«
ï£¬
ï£­
2(K âˆ’1)2
k âˆ’1
ï£¶
ï£·
ï£¸=
ï£®
ï£°
k
Y
j=1
H {âˆªk0
l=1(Vj âˆ©Vl,0)}
ï£¹
ï£ºï£»Ã—
ï£«
ï£¬
ï£­
2(K âˆ’1)2
k âˆ’1
ï£¶
ï£·
ï£¸
(Lemma S.17)
â©½
ï£®
ï£°
k
Y
j=1
ï£±
ï£²
ï£³
ï£®
ï£°
k0
Y
l=1
H{(Vj âˆ©Vl,0)}
ï£¹
ï£»Ã— exp(b[{(Vj âˆ©Vl,0)}k0
l=1] Ã— {4 log(K) + log(2)})
ï£¼
ï£½
ï£¾
ï£¹
ï£»Ã—
ï£«
ï£¬
ï£­
2(K âˆ’1)2
k âˆ’1
ï£¶
ï£·
ï£¸
(ii)
â©½
ï£®
ï£°
k
Y
j=1
ï£±
ï£²
ï£³
ï£®
ï£°
k0
Y
l=1
H{(Vj âˆ©Vl,0)}
ï£¹
ï£»Ã— exp{cK log(K)}
ï£¼
ï£½
ï£¾
ï£¹
ï£»Ã—
ï£«
ï£¬
ï£­
2(K âˆ’1)2
k âˆ’1
ï£¶
ï£·
ï£¸
=
ï£«
ï£­
k0
Y
l=1
ï£®
ï£°
k
Y
j=1
H{(Vj âˆ©Vl,0)}
ï£¹
ï£»
ï£¶
ï£¸Ã— exp{cK log(K)} Ã—
ï£«
ï£¬
ï£­
2(K âˆ’1)2
k âˆ’1
ï£¶
ï£·
ï£¸
(Lemma S.17)
â©½
ï£«
ï£­
k0
Y
l=1
[H{âˆªk
j=1(Vj âˆ©Vl,0)}]
ï£¶
ï£¸Ã— exp{cK log(K)} Ã—
ï£«
ï£¬
ï£­
2(K âˆ’1)2
k âˆ’1
ï£¶
ï£·
ï£¸
(iii)
â©½
|{T : T can induce Ï€âˆ—
0}| Ã— exp{cK log(K)} Ã—
ï£«
ï£¬
ï£­
2(K âˆ’1)2
k âˆ’1
ï£¶
ï£·
ï£¸.
For inequality (i), we use the fact that each Vj is connected, thus H(Vj) is the number of spanning
trees of Vj. 2(K âˆ’1)2 is the total number of graph edges, and
ï£«
ï£¬
ï£­
2(K âˆ’1)2
k âˆ’1
ï£¶
ï£·
ï£¸is the number of
possible ways of cutting edges of T âˆˆ{T : T can induce Ï€âˆ—} to obtain Ï€âˆ—. For inequality (ii), we use
the fact that b[{Vj âˆ©Vl,0, Vj âˆ©Vlâ€²,0}] â©½cK according to (S.6). For inequality (iii), we use the fact that
âˆªk
j=1(Vj âˆ©Vl,0) is connected since Ï€âˆ—
0 is contiguous.
Thus, for any partition Ï€âˆ—âˆˆÎâˆ—, we obtain
|{T : T can induce Ï€âˆ—}|
|{T : T can induce Ï€âˆ—
0}| â©½exp{cK log(K)} Ã—
ï£«
ï£¬
ï£­
2(K âˆ’1)2
k âˆ’1
ï£¶
ï£·
ï£¸â©½exp{CK log(K)}.
(S.21)
S18

References
[1] Aldous, D. J. (1990). The random walk construction of uniform spanning trees and uniform labelled
trees. SIAM Journal on Discrete Mathematics, 3(4):450â€“465.
[2] Ascolani, F., Lijoi, A., Rebaudo, G., and Zanella, G. (2023). Clustering consistency with dirichlet
process mixtures. Biometrika, 110(2):551â€“558.
[3] Burton, R. and Pemantle, R. (1993). Local characteristics, entropy and limit theorems for spanning
trees and domino tilings via transfer-impedances. Annals of Probability, 21(3):1329â€“1371.
[4] Cadez, I. V., Gaffney, S., and Smyth, P. (2000). A general probabilistic framework for clustering
individuals and objects. In Proceedings of the Sixth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 140â€“149.
[5] Dahl, D. B., Johnson, D. J., and MÂ¨uller, P. (2022).
Search algorithms and loss functions for
bayesian clustering. Journal of Computational and Graphical Statistics, 31(4):1189â€“1201.
[6] Dasgupta, A. and Raftery, A. E. (1998). Detecting features in spatial point processes with clutter
via model-based clustering. Journal of the American Statistical Association, 93(441):294â€“302.
[7] Denison, D. G., Mallick, B. K., and Smith, A. F. (1998). A bayesian cart algorithm. Biometrika,
85(2):363â€“377.
[8] Dobrin, R. and Duxbury, P. (2001).
Minimum spanning trees on random networks.
Physical
Review Letters, 86(22):5076.
[9] Feng, W., Lim, C. Y., Maiti, T., and Zhang, Z. (2016). Spatial regression and estimation of disease
risks: A clustering-based approach. Statistical Analysis and Data Mining: The ASA Data Science
Journal, 9(6):417â€“434.
[10] Fraley, C. and Raftery, A. E. (2002). Model-based clustering, discriminant analysis, and density
estimation. Journal of the American Statistical Association, 97(458):611â€“631.
[11] Frick, K., Munk, A., and Sieling, H. (2014). Multiscale change point inference. Journal of the
Royal Statistical Society Series B: Statistical Methodology, 76(3):495â€“580.
[12] Frieze, A. M. (1985). On the value of a random minimum spanning tree problem. Discrete Applied
Mathematics, 10(1):47â€“56.
[13] Gnedin, A. and Pitman, J. (2006). Exchangeable gibbs partitions and stirling triangles. Journal
of Mathematical Sciences, 138:5674â€“5685.
S19

[14] Gneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation.
Journal of the American Statistical Association, 102(477):359â€“378.
[15] Gramacy, R. B. and Lee, H. K. H. (2008).
Bayesian treed gaussian process models with an
application to computer modeling. Journal of the American Statistical Association, 103(483):1119â€“
1130.
[16] Guha, A., Ho, N., and Nguyen, X. (2021).
On posterior contraction of parameters and
interpretability in bayesian mixture modeling. Bernoulli, 27(4):2159â€“2188.
[17] Hu, G., Geng, J., Xue, Y., and Sang, H. (2023). Bayesian spatial homogeneity pursuit of functional
data: an application to the us income distribution. Bayesian Analysis, 18(2):579â€“605.
[18] Kim, H.-M., Mallick, B. K., and Holmes, C. C. (2005). Analyzing nonstationary spatial data using
piecewise gaussian processes. Journal of the American Statistical Association, 100(470):653â€“668.
[19] Knorr-Held, L. and RaÃŸer, G. (2000). Bayesian detection of clusters and discontinuities in disease
maps. Biometrics, 56(1):13â€“21.
[20] Laurent, B. and Massart, P. (2000). Adaptive estimation of a quadratic functional by model
selection. Annals of Statistics, pages 1302â€“1338.
[21] Lee, C., Luo, Z. T., and Sang, H. (2021). T-loho: A bayesian regularization model for structured
sparsity and smoothness on graphs. Advances in Neural Information Processing Systems, 34:598â€“
609.
[22] Lee, J., Gangnon, R. E., and Zhu, J. (2017). Cluster detection of spatial regression coefficients.
Statistics in Medicine, 36(7):1118â€“1133.
[23] Li, F. and Sang, H. (2019). Spatial homogeneity pursuit of regression coefficients for large datasets.
Journal of the American Statistical Association, 114(527):1050â€“1062.
[24] Lian, H. (2010). Posterior convergence and model estimation in Bayesian change-point problems.
Electronic Journal of Statistics, 4(none):239â€“253.
[25] Luo, Z. T., Sang, H., and Mallick, B. (2021a). Bast: Bayesian additive regression spanning trees
for complex constrained domain. Advances in Neural Information Processing Systems, 34:90â€“102.
[26] Luo, Z. T., Sang, H., and Mallick, B. (2021b). A Bayesian contiguous partitioning method for
learning clustered latent variables. Journal of Machine Learning Research, 22(37):1â€“52.
[27] Luo, Z. T., Sang, H., and Mallick, B. (2023). A nonstationary soft partitioned gaussian process
model via random spanning trees. Journal of the American Statistical Association, 119:2105â€“2116.
S20

[28] Miller, J. W. and Harrison, M. T. (2018).
Mixture models with a prior on the number of
components. Journal of the American Statistical Association, 113(521):340â€“356.
[29] Mu, J., Wang, G., and Wang, L. (2020). Spatial autoregressive partially linear varying coefficient
models. Journal of Nonparametric Statistics, 32(2):428â€“451.
[30] Nguyen, X. (2013). Convergence of latent mixing measures in finite and infinite mixture models.
Annals of Statistics, 41(1):370â€“400.
[31] Paci, L. and Finazzi, F. (2018).
Dynamic model-based clustering for spatio-temporal data.
Statistics and Computing, 28:359â€“374.
[32] Page, G. L. and Quintana, F. A. (2016). Spatial product partition models. Bayesian Analysis,
11(1):265â€“298.
[33] Pan, T., Hu, G., and Shen, W. (2023). Identifying latent groups in spatial panel data using a
markov random field constrained product partition model. Statistica Sinica, 30:2281â€“2304.
[34] Prim, R. C. (1957). Shortest connection networks and some generalizations. The Bell System
Technical Journal, 36(6):1389â€“1401.
[35] Quintana, F. A., MÂ¨uller, P., Jara, A., and MacEachern, S. N. (2022). The dependent dirichlet
process and related models. Statistical Science, 37(1):24â€“41.
[36] Schramm, O. (2000). Scaling limits of loop-erased random walks and uniform spanning trees.
Israel Journal of Mathematics, 118(1):221â€“288.
[37] Sugasawa, S. and Murakami, D. (2021).
Spatially clustered regression.
Spatial Statistics,
44:100525.
[38] Talley, L. (2011). Descriptive physical oceanography: an introduction. Academic Press.
[39] Teixeira, L. V., Assuncao, R. M., and Loschi, R. H. (2015). A generative spatial clustering model
for random data through spanning trees. In 2015 IEEE International Conference on Data Mining,
pages 997â€“1002. IEEE.
[40] Teixeira, L. V., AssunÂ¸cËœao, R. M., and Loschi, R. H. (2019). Bayesian space-time partitioning by
sampling and pruning spanning trees. Journal of Machine Learning Research, 20:1â€“35.
[41] Van Dongen, S. (2000). Performance criteria for graph clustering and markov cluster experiments.
Report-Information Systems, (12):1â€“36.
[42] Watanabe, S. and Opper, M. (2010). Asymptotic equivalence of bayes cross validation and widely
applicable information criterion in singular learning theory. Journal of Machine Learning Research,
11:3571â€“3594.
S21

[43] Willett, R., Nowak, R., and Castro, R. (2005). Faster rates in regression via active learning.
Advances in Neural Information Processing Systems, 18.
[44] Yu, S., Wang, G., and Wang, L. (2024).
Distributed heterogeneity learning for generalized
partially linear models with spatially varying coefficients.
Journal of the American Statistical
Association, 0(0):1â€“15.
[45] Yu, S., Wang, G., Wang, L., Liu, C., and Yang, L. (2020). Estimation and inference for generalized
geoadditive models. Journal of the American Statistical Association, 115:761â€“774.
[46] Zeng, C., Miller, J. W., and Duan, L. L. (2023). Consistent model-based clustering using the
quasi-bernoulli stick-breaking process. Journal of Machine Learning Research, 24(153):1â€“32.
[47] Zheng, Y., Duan, L. L., and Roy, A. (2024). Consistency of graphical model-based clustering:
robust clustering using bayesian spanning forest. arXiv preprint arXiv:2409.19129.
[48] Zhong, Y., Sang, H., Cook, S. J., and Kellstedt, P. M. (2023). Sparse spatially clustered coefficient
model via adaptive regularization. Computational Statistics & Data Analysis, 177:107581.
S22
