Rational Kriging
V. Roshan Joseph
H. Milton Stewart School of Industrial and Systems Engineering
Georgia Institute of Technology, Atlanta, GA 30332, USA
roshan@gatech.edu
Abstract
This article proposes a new kriging that has a rational form. It is shown that the
generalized least squares estimate of the mean from rational kriging is much more
well behaved than that from ordinary kriging. Parameter estimation and uncertainty
quantification for rational kriging are proposed using a Gaussian process framework.
Its potential applications in emulation and calibration of computer models are also
discussed.
Keywords: Calibration; Computer experiments; Gaussian process; Radial basis functions;
Uncertainty quantification.
1
Introduction
Kriging is a technique for multivariate interpolation of arbitrarily scattered data. It is origi-
nated from some mining-related applications, which is developed into the field of geostatistics
by the pioneering work of Matheron (1963). It has now become a prominent technique
for function approximation and uncertainty quantification in spatial statistics (Cressie,
2015), computer experiments (Santner et al., 2003), and machine learning (Rasmussen and
Williams, 2006).
Kriging can be briefly explained as follows.
Suppose we have observed the data
{(xi, yi)}n
iâˆ’1, where x âˆˆRp is the p-dimensional inputs and y the output. The aim is
to predict y for a future x. To do this, construct a linear predictor Ë†y(x) = a(x)â€²y =
Pn
i=1 ai(x)yi. Kriging gives the best linear unbiased predictor (BLUP) under some as-
sumptions of the data generating process. Specifically, if the data are generated from a
second-order stationary stochastic process with mean Âµ, variance Ï„ 2, and correlation function
1
arXiv:2312.05372v1  [stat.ME]  8 Dec 2023

cor{Y (u), Y (v)} = R(u âˆ’v), then the kriging predictor can be obtained by minimizing the
mean squared prediction error (Santner et al., 2003)
E {Y (x) âˆ’Ë†y(x)}2
with respect to a(x) subject to the condition that E{Ë†y(x)} = Âµ for all x. The optimal
solution is given by
a(x)â€² = {1 âˆ’r(x)â€²Râˆ’11} 1â€²Râˆ’1
1â€²Râˆ’11 + r(x)â€²Râˆ’1,
where R = {R(xi âˆ’xj)}nÃ—n is the correlation matrix, r(x) = (R(x âˆ’x1), . . . , R(x âˆ’xn))â€²,
and 1 is a vector of n 1â€™s. Substituting the solution in the linear predictor and simplifying,
we obtain the (ordinary) kriging predictor as
Ë†yOK(x) = Ë†ÂµOK + r(x)â€²Râˆ’1(y âˆ’Ë†ÂµOK1),
(1)
where
Ë†ÂµOK = 1â€²Râˆ’1y
1â€²Râˆ’11.
(2)
The expression in (2) shows that Ë†ÂµOK is the well-known Generalized Least Squares (GLS)
estimate of Âµ.
Joseph (2006) noticed that the ordinary kriging predictor has sometimes a â€œmean
reversionâ€ issue and proposed a modified predictor
Ë†y(x) = r(x)â€²Râˆ’1y
r(x)â€²Râˆ’11,
(3)
whose predictions tend towards the nearest neighbor value when the correlations go to zero
and thus, avoids the mean reversion issue. This predictor can be viewed as a limiting case
of a simple kriging predictor with a recursive estimation of Âµ and hence it is called limit
kriging. Different from ordinary kriging, limit kriging has a rational form. The purpose of
2

this article is to examine optimal rational predictors of the form
Ë†y(x) = a(x)â€²y
a(x)â€²1.
(4)
Although rational polynomials have a long history in interpolation, its extension to
radial basis functions (RBFs) is very recent. Jakobsson et al. (2009) proposed to use rational
RBFs for modeling resonance phenomena. Sarra and Bai (2018) also found that rational
RBFs perform exceptionally well for modeling functions with discontinuities and steep
gradients. In a more recent work, Buhmann et al. (2020) showed that rational RBFs have
comparable approximation accuracy to the classical RBFs, but has more robust prediction
performance. However, RBFs cannot provide any uncertainty quantification. In contrast,
owing to its probabilistic formulation, kriging can automatically provide prediction intervals
and can easily be integrated into Bayesian methods and non-normal data settings.
Different from the RBF literature, we will motivate the benefit of rational predictors
using parameter estimation accuracy. As an example, consider the deflection of a simply
supported beam with uniform load shown as an inset in the left panel of Figure 1. The
deflection at a distance x from the left end of the beam is given by
y = âˆ’
P
24EI x(x3 âˆ’2Lx2 + L3),
where P is the uniform load density, E is the elastic modulus, I is the area moment of
inertia, L is the length of the beam, and x âˆˆ[0, L]. Let P/(24EI) = 1 and L = 1. The
function is plotted in the left panel of Figure 1 along with 11 equi-spaced xiâ€™s from [0, 1]. An
ordinary kriging was fitted to this data with a Gaussian correlation function R(h) = eâˆ’(h/Î¸)2,
where the unknown correlation parameter Î¸ is estimated from the data using maximum
likelihood. We used the R package DiceKriging (Roustant et al., 2012) for estimation,
where a small nugget of 10âˆ’6 is applied for numerical stability. The predictions in [0, 1]
are plotted in the left panel of Figure 1, which are almost indistinguishable with the true
3

function values showing excellent prediction performance. The GLS estimate of Âµ from (2)
is obtained as Ë†ÂµOK = 0.224. Interestingly, this value is outside the range of the observed
function values, which are from [âˆ’0.3125, 0].
0.0
0.2
0.4
0.6
0.8
1.0
âˆ’0.3
âˆ’0.2
âˆ’0.1
0.0
0.1
0.2
0.3
Beam deflection curve
x
y
Î¼^ = 0.224
Gauss
Exponential
Matern 3/2
âˆ’0.1
0.0
0.1
0.2
0.3
0.4
Mean
correlation function
Î¼
Gauss
Exponential
Matern 3/2
0.00
0.02
0.04
0.06
Root Meanâˆ’Squared Error
correlation function
RMSE
Figure 1: (left) Plot of the beam deflection curve (solid-green), data (blue-pluses), and ordinary
kriging predictor (dashed-red). A simply supported beam with uniform load is shown as an inset of
this plot. (middle) Boxplot of Ë†Âµâ€™s from 50 simulations using three correlation functions where the
xiâ€™s are randomly sampled in [0, 1]. (right) Root mean squared errors from the 50 simulations.
We repeated this exercise 50 times by uniformly sampling xiâ€™s from [0, 1] and using
two more correlation functions: exponential and MatÂ´ern 3/2 (Rasmussen and Williams,
2006, p.84). We can see from the middle panel of Figure 1 that the estimates of Âµ using
exponential correlation function are around âˆ’0.1, which are in the range of the observed
values, but the estimates of Âµ from the MatÂ´ern 3/2 correlation function are generally higher
than those obtained from the Gaussian correlation function. For each simulation, the root
mean squared error (RMSE) is calculated over a grid of 1,001 values and is shown in the
right panel of Figure 1. We can see that the Gaussian correlation function gives the best
prediction in this example. The prediction from the exponential correlation function is the
worst in spite of having the mean in the observed range of yiâ€™s.
Although better prediction is obtained when Ë†Âµ is outside the observed range of function
values, the interpretation of those estimates becomes questionable. One could argue that Âµ is
the mean of a stochastic process in which the beam deflection curve is just a realization and
4

thus a value around 0.2 is an admissible estimate. However, if Âµ has a physical interpretation,
then this estimate is meaningless. For example, a positive value of mean would imply that
the beam will deflect in the opposite direction of the force, which is against the law of
nature! This is a common dilemma in model calibration problems (Kennedy and Oâ€™Hagan,
2001). We will show that the use of rational kriging can surprisingly avoid this issue without
sacrificing the prediction performance.
A quick fix to the foregoing issue is to estimate the mean using ordinary least squares
(Pronzato and Zhigljavsky, 2023). However, it leads to inconsistencies in the modeling
frameworkâ€“ an uncorrelated process for estimation and a correlated process for prediction.
Plumlee and Joseph (2018) argued that the estimation problems are caused by identifiability
issues between the stochastic process and the mean function (a constant function in the
case of ordinary kriging). They proposed to overcome the identifiability issue by making
the stochastic process orthogonal to the mean function. Although their approach is very
general, it leads to a nonstationary correlation function that involves high-dimensional
integrals making the estimation computationally challenging and numerically unstable. In
contrast, rational kriging requires only a rescaling of the original predictor, which is very
easy to implement in practice.
The article is organized as follows. Section 2 develops the rational kriging and a Gaussian
process framework for parameter estimation and uncertainty quantification. Simulations
with several test functions are provided in Section 3. Potential applications of rational
kriging in emulation and calibration of computer models are illustrated with some examples
in Section 4. Some concluding remarks are given in Section 5.
2
Methodology
We will first derive the optimal rational predictor and then investigate its estimation
properties by assuming Gaussianity for the stochastic process.
5

2.1
Rational Kriging
Let X be the input region for data collection. Most of the time, it can be scaled in [0, 1]p.
Notice that this region does not come into the formulation or derivation of the ordinary
kriging predictor because we assume the stationary stochastic process has a constant mean
Âµ and variance Ï„ 2 for all x âˆˆRp. This could be the reason why the estimate of Âµ went
outside the observed range of y values in the example that we saw earlier. So, we can
possibly overcome the issue by assuming a nonstationary variance Ï„ 2(x), where it should
increase as x goes outside of X.
Now consider a rational predictor of the form
Ë†y(x) = a(x)â€²y
b(x) ,
(5)
where a(x) = (a1(x), . . . , an(x))â€² and b(x) are functions of the input variables x =
(x1, . . . , xp)â€².
Assume that the data y is a realization from a second-order stationary
stochastic process with mean Âµ, variance Ï„ 2(x), and correlation function R(Â·). Then, the
predictor in (5) will be unbiased if
E{Ë†y(x)} = a(x)â€²E{y}
b(x)
= Âµa(x)â€²1
b(x)
= Âµ
for all x, which implies b(x) = a(x)â€²1. Now we can find the best rational unbiased predictor
by minimizing
MSPE = E

Y (x) âˆ’a(x)â€²y
a(x)â€²1
2
with respect to a(x). It is easy to show that cov(Y (x), y) = Ï„(x)diag(Ï„)r(x) and var{y} =
diag(Ï„)Rdiag(Ï„), where Ï„ = (Ï„(x1), . . . , Ï„(xn))â€² and diag(Ï„) is a diagonal matrix with
diagonal elements Ï„. Thus,
MSPE = Ï„ 2(x) âˆ’2 a(x)â€²
a(x)â€²1Ï„(x)diag(Ï„)r(x) + a(x)â€²
a(x)â€²1diag(Ï„)Rdiag(Ï„) a(x)
a(x)â€²1.
6

Differentiating with respect to a(x) and equating to zero, we obtain
diag(Ï„)

âˆ’2Ï„(x)r(x) + 2Rdiag(Ï„) a(x)
a(x)â€²1

âˆ‚
âˆ‚a(x)
 a(x)
a(x)â€²1

= 0.
Thus,
a(x)
a(x)â€²1 = Ï„(x)diag(Ï„ âˆ’1)Râˆ’1r(x)
is a solution, provided Ï„(x)r(x)â€²Râˆ’1diag(Ï„ âˆ’1)1 = 1. Therefore, we can let
Ï„(x) =
1
r(x)â€²Râˆ’1(1/Ï„),
(6)
where 1/Ï„ = (1/Ï„1, . . . , 1/Ï„n)â€² and Ï„i = Ï„(xi) for i = 1, . . . , n.
Since r(xi)â€²Râˆ’1 =
(0, . . . , 1, . . . , 0)â€² with 1 at the ith position, (6) holds for i = 1, . . . , n. However, (6) is
meaningful only if r(x)â€²Râˆ’1(1/Ï„) > 0 for all x. This can be ensured by putting a constraint
on Ï„ â‰¥0 such that Râˆ’1(1/Ï„) â‰¥0 and choosing a correlation function that does not vanish.
Interestingly, Ï„(x) â†’âˆas ||x âˆ’xi|| â†’âˆfor all i, which agrees with our intuition.
Thus, we obtain the optimal rational kriging predictor as
Ë†y(x) = r(x)â€²Râˆ’1(y/Ï„)
r(x)â€²Râˆ’1(1/Ï„).
(7)
We can see that the limit kriging predictor in (3) is a special case of this predictor with
Ï„ = 1. However, limit kriging is not an admissible predictor in the new formulation because
Râˆ’11 is not guaranteed to be nonnegative. The new predictor has n additional unknown
parameters Ï„ = (Ï„1, . . . , Ï„n)â€², which can be chosen to ensure that Râˆ’1(1/Ï„) â‰¥0 and Ï„ â‰¥0.
Let Râˆ’1(1/Ï„) = c/Î½, where c â‰¥0 and Î½ is a positive constant. Since R is a positive
matrix, c â‰¥0 implies 1/Ï„ = Rc/Î½ â‰¥0. Thus, the rational kriging predictor can be written
as
Ë†y(x) = r(x)â€²Râˆ’1diag(Rc)y
r(x)â€²c
,
(8)
where c â‰¥0. This is the same predictor obtained by Kang and Joseph (2016) as the limiting
7

case of an iterated kernel regression. The choice of c will be discussed in the next section.
The derivation of rational kriging predictor does not give any estimate of Âµ. However,
since y is a random vector with mean Âµ1 and variance diag(Ï„)Rdiag(Ï„), we can use the
GLS estimate for Âµ:
Ë†Âµ
=
1â€²diag(Ï„ âˆ’1)Râˆ’1diag(Ï„ âˆ’1)y
1â€²diag(Ï„ âˆ’1)Râˆ’1diag(Ï„ âˆ’1)1
=
câ€²diag(Rc)y
câ€²Rc
.
(9)
Since c â‰¥0, we have the following result, which is in stark contrast to the GLS estimate of
Âµ in ordinary kriging, where it can go outside the range of the data as we have observed in
an example in the introduction.
Theorem 1. In rational kriging, the GLS estimate of Âµ is a convex combination of {yi}n
i=1
and therefore, it will always be in the range [mini yi, maxi yi] for any positive definite
correlation function.
In order to understand if the GLS estimate from rational kriging is good or not, we need
to define the notion of a â€œtrue valueâ€ for Âµ. Define the true value as the L2-projection of
the underlying function as in Tuo and Wu (2015):
Âµâˆ—= argmin
Âµ
Z
X
{y(x) âˆ’Âµ}2dF(x) =
Z
X
y(x)dF(x),
where F(Â·) is the distribution function of x with support X from which the input points
are generated. Since Âµâˆ—is a convex combination of the y(x) for all x âˆˆX, we can expect
the rational kriging estimate Ë†Âµ to be closer to Âµâˆ—than Ë†ÂµOK to Âµâˆ—. We will investigate this
more in Section 3 using simulations.
The mean squared prediction error for the optimal rational kriging predictor is given by
MSPE =
Î½2
{r(x)â€²c}2{1 âˆ’r(x)â€²Râˆ’1r(x)},
8

which can be used for uncertainty quantification. It can be computed only after specifying
the parameter Î½ and the coefficients c. Moreover, there are unknown parameters in the
correlation function that need to be specified. We will develop their estimation procedure
after introducing Gaussian Process (GP) in the next section.
2.2
Rational Gaussian Process
It is well known that the ordinary kriging predictor can be obtained as the posterior mean
if we assume a GP prior for the true function that generated the data (Currin et al., 1991;
Rasmussen and Williams, 2006). A similar framework can be developed for rational kriging.
Following Kang and Joseph (2016), assume
y(x) = Âµ +
Î½
r(x)â€²cZ(x), Z(x) âˆ¼GP(0, R(Â·)).
(10)
It is easy to show that
y(x)|y âˆ¼N

Ë†y(x),
Î½2
{r(x)â€²c}2{1 âˆ’r(x)â€²Râˆ’1r(x)}

,
(11)
where Ë†y(x) is the rational kriging predictor given in (8). As alluded to in the introduction,
(11) can be used for constructing the prediction intervals, which is a major advantage of
GPs over RBFs.
There are several unknown parameters in (11): Âµ, Î½, and c. In addition, the correlation
functions have unknown parameters; denote them by Î¸. Among all these parameters, we
will give a fully Bayesian treatment only for Âµ. All the other parameters will be estimated
or specified as follows.
The likelihood is given by
y|Âµ, Î½, c, Î¸ âˆ¼N(Âµ1, Î½2diag(1/Rc) R diag(1/Rc)).
9

Assuming a non-informative prior for Âµ: p(Âµ) âˆ1, we obtain
Âµ|y, Î½, c, Î¸ âˆ¼N

Ë†Âµ,
Î½2
câ€²Rc

,
where Ë†Âµ is the GLS estimate of Âµ given in (9). Looking at the posterior variance of Âµ, it is
tempting to choose c to maximize câ€²Rc. In fact, an elegant solution to this optimization
problem exists. Under the constraint âˆ¥câˆ¥2 = 1, the quadratic form câ€²Rc is maximized by
the eigenvector corresponding to the largest eigenvalue of R. Since R is a positive matrix,
this eigenvector is positive by Perronâ€™s theorem (Perron, 1907). Thus, we have the following
result.
Proposition 1.
The posterior variance of Âµ can be minimized by taking c to be the
eigenvector of R corresponding to its largest eigenvalue.
Buhmann et al. (2020) also suggests to use this estimate for c. Their suggestion is based
on minimizing the native space norm of functions with kernel K(u, v) = R(uâˆ’v). With this
choice of c, r(x)â€²c can be viewed as the NystrÂ¨om approximation of the first eigenfunction of
R(Â·) (Rasmussen and Williams, 2006, Sec. 4.3.2). In our trials, we found this estimate of c
to work well when the functions are smooth, but poorly when the functions are non-smooth.
This is because r(x)â€²c can become very small for some value of x, which can make the
predictions erratic.
Another possibility is to let c = Râˆ’11 as in limit kriging (Joseph, 2006), but this does
not ensure nonnegativity of c. We can overcome the nonnegativity issue as follows. Let Ë†Î³ be
the smallest Î³ âˆˆ[0, 1] such that [(1 âˆ’Î³)R + Î³I]âˆ’11 â‰¥âˆ†1 component-wise, where âˆ†âˆˆ[0, 1].
Such a Ë†Î³ always exists because Î³ = 1 trivially satisfies the inequality. Therefore, let
Ë†c = [(1 âˆ’Ë†Î³)R + Ë†Î³I]âˆ’11.
(12)
Empirically, we found that âˆ†= Î»1/n works well, where Î»1 is the largest eigenvalue of R.
When correlations are high, R â‰ˆÎ»1E1Eâ€²
1, where E1 the eigenvector corresponding to Î»1.
10

Then,
RË†c â‰ˆ
Î»1
(1 âˆ’Ë†Î³)Î»1 + Ë†Î³ E1Eâ€²
11 âˆÎ»1E1 = RE1.
That is, the solution given in (12) behaves exactly like the eigenvector solution of Buhmann
et al. (2020) when correlations are high (smooth functions). On the other hand, when
correlations are small (nonsmooth functions), Ë†c â‰ˆ[(1 âˆ’Ë†Î³)I + Ë†Î³I]âˆ’11 âˆ1, whereas E1 will
be approximately the unit vector (1, 0, . . . , 0)â€². When this happens, the eigenvector solution
will make r(x)â€²c â‰ˆ0 for a large portion of X, whereas r(x)â€²Ë†c â‰ˆ1 for x in the neighborhood
of the observed data points. Thus, the solution given in (12) will be better behaved in all
correlations regimes and therefore, will be adopted in this article. We also note that this
solution is quite different from that of Kang and Joseph (2016), where they estimated c by
maximizing the unnormalized posterior, which is computationally prohibitive.
Thus,
p(Î½, Î¸|y, Ë†c)
âˆ
Z
p(y|Âµ, Î½, Ë†c, Î¸) dÂµ
âˆ
|diag(RË†c)|
Î½nâˆ’1|R|1/2(Ë†cRË†c)1/2 exp

âˆ’1
2Î½2(y âˆ’Ë†Âµ1)â€²diag(RË†c)Râˆ’1diag(RË†c)(y âˆ’Ë†Âµ1)

,
where Ë†c is given in (12). Maximizing this with respect to Î½ and Î¸, we obtain
Ë†Î½2
=
1
n âˆ’1(y âˆ’Ë†Âµ1)â€²diag(RË†c)Râˆ’1diag(RË†c)(y âˆ’Ë†Âµ1),
(13)
Ë†Î¸
=
argmin
Î¸
(
(n âˆ’1) log Ë†Î½2 + log |R| âˆ’2
n
X
i=1
log(RiË†c) + log(Ë†câ€²RË†c)
)
,
(14)
where Ri is the ith row of R.
2.3
Correlation Functions
Rational kriging or rational GP can be used with any positive definite correlation function.
One of the most commonly used correlation function in computer experiments is the
11

Gaussian correlation function given by R(h) = exp{âˆ’Pp
i=1(hi/Î¸i)2}. Let Î¸2
i = Î¸2/wi, where
Pp
i=1 wi = 1 and wi â‰¥0 for i = 1, . . . , n. Then the Gaussian correlation function can be
written as
R(h) = exp

âˆ’âˆ¥hâˆ¥2
w/Î¸2	
,
(15)
where âˆ¥hâˆ¥2
w = Pp
i=1 wih2
i . It is interesting to study the properties of the rational kriging
predictor when the length-scale parameter (Î¸) becomes small. Using a result in Kang and
Joseph (2016), it is easy to show that the rational kriging tends to the nearest neighbor
predictor defined by the norm âˆ¥Â·âˆ¥w as Î¸ â†’0. This property helps rational kriging to
overcome the â€œmean reversionâ€ problem commonly observed with ordinary kriging.
There is another correlation function that makes the foregoing limiting case even more
interesting. Consider the rational quadratic function (Rasmussen and Williams, 2006) (also
known as Cauchy function) given by
R(h) =
 1 + âˆ¥hâˆ¥2
w/Î¸2âˆ’1 .
(16)
When the length-scale parameter Î¸ â†’0, we have R â†’I and therefore Ë†c â†’1. Moreover,
R(x âˆ’xi)/r(x)â€²Ë†c â†’âˆ¥x âˆ’xiâˆ¥âˆ’2
w / Pn
j=1âˆ¥x âˆ’xjâˆ¥âˆ’2
w . The predictor
Ë†yIDW(x) =
Pn
i=1âˆ¥x âˆ’xiâˆ¥âˆ’2
w yi
Pn
j=1âˆ¥x âˆ’xjâˆ¥âˆ’2
w
is the well-known inverse distance weighting (IDW) predictor (Shepard, 1968; Joseph and
Kang, 2011). Thus, we have the following result.
Theorem 2. Under rational quadratic correlation function in (16), the rational kriging
predictor converges to the inverse distance weighting predictor as the length-scale parameter
goes to 0.
12

3
Simulations
3.1
One-dimensional function
Consider again the beam deflection function used in the introduction: y = âˆ’x(1 âˆ’2x2 + x3)
for x âˆˆ[0, 1]. Let xi
iid
âˆ¼U(0, 1) for i = 1, . . . , 11. These points are re-scaled such that
x1 = 0 and x11 = 1. Ordinary kriging is fitted to the data using the Gaussian correlation
function R(h) = exp{âˆ’(h/Î¸)2} and Rational Quadratic function R(h) = {1 + (h/Î¸)2}âˆ’1.
Rational Kriging (RK) is also fitted to the same data using both the correlation functions
following the procedure in Section 2.2. This simulation is repeated for 50 times. The left
panel of Figure 2 shows the Root Mean-Squared Errors (RMSEs) computed over a grid of
1,001 points in [0, 1]. It shows that, on the average, rational kriging is more accurate than
ordinary kriging. Interval Score (Gneiting and Raftery, 2007)
IS = 1
N
N
X
i=1

(u âˆ’l) + 2
Î±{(l âˆ’ti)+ + (ti âˆ’u)+}

is computed for assessing the accuracy of (1 âˆ’Î±) confidence intervals [l, u], where (x)+ = x
if x > 0 and 0 otherwise, and {ti}N
i=1 are the N = 1001 testing locations. This is shown
in the middle panel of Figure 2 for 95% confidence intervals. A small IS value indicates
better confidence intervals (small width at prescribed coverage). In this example, IS shows
comparable performance for rational kriging and ordinary kriging. The most striking result
is the plot on the right panel of Figure 2. While ordinary kriging produces Ë†Âµâ€™s much larger
than the maximum value of yiâ€™s, the estimate from rational kriging is around the true value
Âµâˆ—=
R 1
0 sin(2x) dx = 0.708 (shown as a red line in the same figure).
For ordinary kriging, a priori, 95% of the function is believed to lie in [Âµ âˆ’2Ï„, Âµ + 2Ï„],
whereas for rational kriging the 95% prior confidence interval is [Âµâˆ’2Î½/r(x)â€²Ë†c, Âµ+2Î½/r(x)â€²Ë†c].
They are plotted in Figure 3 by setting Âµ = 0 and Ï„ = Î½ = 1 for one of the simulations.
We can see that they pretty much agree within the input region [0, 1]. Outside [0, 1], the
13

0e+00
1eâˆ’04
2eâˆ’04
3eâˆ’04
4eâˆ’04
5eâˆ’04
6eâˆ’04
Root Meanâˆ’Squared Error
RMSE
OK
RK
0.001
0.002
0.003
0.004
0.005
0.006
Interval Score
IS
OK
RK
âˆ’0.2
âˆ’0.1
0.0
0.1
0.2
0.3
Mean
Î¼
OK
RK
Gauss
RQ
Figure 2: Boxplots of RMSE (left), IS (middle), and Ë†Âµ (right) from the simulation using the
beam deflection function. The simulation is done by randomly sampling {xi}11
i=1 from [0, 1]. The
true value Âµâˆ—is plotted as a red line in the right plot.
âˆ’1.0
âˆ’0.5
0.0
0.5
1.0
1.5
2.0
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
Scaled Prior Confidence Intervals
x
y
OK
RK
input region
Figure 3: Scaled 95% prior confidence regions for the function with mean centered at 0 are shown
as shaded regions for OK and RK. For one of the simulations, Â±2/r(xi)â€²Ë†c, i = 1, . . . , 11 are plotted
as points.
confidence intervals for OK remain constant, but they increase for RK. In other words,
OK assigns equal â€œweightâ€ to the whole of R, whereas RK assigns more â€œweightâ€ in the
input region and less â€œweightâ€ outside the input region. This could be the reason why the
estimates from RK are well behaved.
Additional simulations with three other one-dimensional test functions under a similar
14

setup are reported in the Appendix A1. It can be seen from Figure 7 that in terms of
prediction performance, OK and RK are comparable on the first function, OK is better
on the second function, and RK is better on the third function. On the other hand, the
mean estimates from RK are much closer to the true value compared to OK for all the three
functions. Some outliers are observed for RK when Gaussian correlation function is used,
whereas its performance with rational quadratic is found to be much more stable.
3.2
Universal kriging
The universal kriging model is given by
y(x) = Î²â€²f(x) + Ï„Z(x),
where f(x) = (f0(x), . . . , fm(x))â€² is a set of known functions, Î² a set of unknown parameters,
and Z(x) is a second-order stationary stochastic process with mean zero, variance 1, and
correlation function R(Â·). Ordinary kriging is a special case of universal kriging with m = 0
and f0(x) = 1.
The rational version of the universal kriging can be defined as
y(x) = Î²â€²f(x) +
Î½
r(x)â€²cZ(x),
where c is chosen as in (12). As before, assume Z(x) âˆ¼GP(0, R(Â·)) and a noninformative
prior for Î²: p(Î²) âˆ1. Then, the posterior distribution of the function can be obtained as
(Santner et al., 2003)
y(x)|y âˆ¼N

Ë†Î²
â€²f(x), s2(x)

,
(17)
where
Ë†Î²
=
{Fâ€²Î£âˆ’1F}âˆ’1Fâ€²Î£âˆ’1y,
15

s2(x)
=
Î½2
1 âˆ’r(x)â€²Râˆ’1r(x)
{r(x)â€²Ë†c}2
+ h(x)â€²{Fâ€²Î£âˆ’1F}âˆ’1h(x)

,
where F is the n Ã— (m + 1) regression model matrix, Î£ = diag(1/RË†c)Rdiag(1/RË†c), and
h(x) = f(x) âˆ’Fâ€²diag(RË†c)Râˆ’1r(x)/r(x)â€²Ë†c. The unknown parameters can be estimated
using empirical Bayes:
Ë†Î½2
=
1
n âˆ’m âˆ’1(y âˆ’FË†Î²)â€²Î£âˆ’1(y âˆ’FË†Î²),
Ë†Î¸
=
argmin
Î¸
(
(n âˆ’m âˆ’1) log Ë†Î½2 + log |R| âˆ’2
n
X
i=1
log(RiË†c) + log |Fâ€²Î£âˆ’1F|
)
.
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Î²0
Î²0
OK
RK
0.5
0.6
0.7
0.8
0.9
1.0
1.1
Î²1
Î²1
OK
RK
Gauss
RQ
Figure 4: Results of simulation using y = sin(2x) with a universal kriging model. Boxplots of Ë†Î²0
and Ë†Î²1 from the original universal kriging and the universal version of rational kriging are shown.
The true least squares estimates of the two parameters are shown as red lines.
Consider a simple function y = sin(2x) for x âˆˆ[0, 1]. The simulation in the previous
section is repeated with n = 30 using a universal kriging model having mean E{y(x)} =
Î²0 + Î²1(x âˆ’.5). The GLS estimates of Î²0 and Î²1 are plotted in Figure 4. The results of
RMSE and IS are omitted for brevity. The â€œtrueâ€ values of the two parameters can be
16

obtained as
(Î²âˆ—
0, Î²âˆ—
1) = argmin
(Î²0,Î²1)
Z 1
0
{sin(2x) âˆ’Î²0 âˆ’Î²1(x âˆ’.5)}2dx.
We obtain Î²âˆ—
0 = 0.708 and Î²âˆ—
1 = 0.976. They are plotted as red lines in Figure 4. We can
see that RK gives excellent estimates of Î²0 compared to OK. There is high variability for
the estimates of Î²1 for RK, but on the average they are still better than those from OK.
Although Theorem 1 guarantees better estimation for only a constant mean function, this
example shows that improving the estimation of the overall mean can indirectly improve
the estimation of all the parameters in the mean model.
4
Applications
In this section, we use rational kriging in two important applications of computer experiments:
emulation and calibration.
4.1
Emulation
Borehole function (Morris et al., 1993) is widely used as a test function for emulation in
computer experiments. It is given by
y =
2Ï€Tu(Hu âˆ’Hl)
ln(r/rw)
h
1 +
2LTu
ln(r/rw)r2wKw + Tu
Tl
i,
where the ranges of interest for the eight variables are: rw âˆˆ[0.05, 0.15], r âˆˆ[100, 50000],
Tu âˆˆ[63070, 115600], Hu âˆˆ[990, 1110], Tl âˆˆ[63.1, 116], Hl âˆˆ[700, 820], L âˆˆ[1120, 1680],
and Kw âˆˆ[9855, 12045]. We scaled the variables to [0, 1]8 and generated n = 10 Ã— 8 = 80
points using MaxPro design (Joseph et al., 2015). Both ordinary kriging (using the R
package mlegp) and rational kriging are fitted to this data using Gaussian correlation
function. The root-mean squared leave-one-out cross validation error for ordinary and
rational kriging are 0.654 and 0.403, respectively, showing that rational kriging is better
17

for emulating the borehole function compared to ordinary kriging. Since the borehole
function is a simple analytical function, we can compute the actual errors on a large testing
set. Using 1,001 uniform samples from [0, 1]8, we obtain the root-mean squared errors as
RMSEOK = 0.413 and RMSERK = 0.267, which agrees with the results of cross validation.
Similar improvements were observed for rational kriging over ordinary kriging with rational
quadratic correlation function as well.
We repeated the foregoing exercise 50 times by randomly sampling 80 uniform points
from [0, 1]8 each time and the results are summarized in Figure 5 along with interval
score and the estimated mean. We can see that rational kriging outperforms ordinary
kriging on both of the prediction and uncertainty quantification metrics. Using a very large
uniform sample from [0, 1]8, the overall mean of the borehole function is estimated to be
approximately 77.74. This is plotted as a red line in the last panel of Figure 5. We can see
that the estimates of mean from rational kriging are much closer to the true mean of the
function than those from the ordinary kriging.
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Root Meanâˆ’Squared Error
RMSE
OK
RK
Gauss
RQ
1
2
3
4
Interval Score
IS
OK
RK
100
150
200
Mean
Î¼
OK
RK
Figure 5: Boxplots of RMSE (left), IS (middle), and Ë†Âµ (right) from the simulation using the
borehole function. The simulation is done by randomly sampling 80 points from [0, 1]8. The true
value Âµâˆ—is plotted as a red line in the right plot.
Simulations using three more widely used test functions for emulation in computer
experiments are reported in the Appendix. RKâ€™s prediction performance was superior to
18

OK in all of the three cases along with better and more consistent estimates for the mean.
4.2
Calibration
Consider a physics-based model y = f(x; Î·), where Î· = (Î·1, . . . , Î·q)â€² are the unknown
calibration parameters that need to be estimated from the real data {(xi, yi)}n
i=1. Since
the physics-based model could be biased, Kennedy and Oâ€™Hagan (2001) proposed to use a
Gaussian process model to capture the discrepancy between the physics-based model and
the data. Their model can be written as
y = f(x; Î·) + Ï„Î´(x) + Ïµ, Î´(x) âˆ¼GP(0, R(Â·)) and Ïµ
iid
âˆ¼N(0, Ïƒ2).
(18)
Tuo and Wu (2015) have shown that this model could produce poor estimates of Î· because
of the non-identifiability between Î· and Î´(Â·). Since then several proposals have appeared in
the literature aimed at tackling the identifiability issue (Plumlee, 2017; Gu and Wang, 2018;
Tuo, 2019).
Encouraged by the results of previous sections, we could consider using rational GP in
the Kennedy-Oâ€™Hagan model:
y = f(x; Î·) +
Î½
r(x)â€²cÎ´(x) + Ïµ, Î´(x) âˆ¼GP(0, R(Â·)) and Ïµ
iid
âˆ¼N(0, Ïƒ2).
(19)
We make no claims about overcoming the identifiability issue with this new model. Our
hope is that this model would produce better estimates of Î· than with the original Kennedy-
Oâ€™Hagan model.
Consider a simple example from Plumlee (2017). Suppose f(x; Î·) = Î·x, but the data is
generated from y = 4x + x sin(5x) + Ïµ with Ïµ
iid
âˆ¼N(0, 0.022). Input values are generated by
taking 17 equally spaced points in [0, 0.8]. Since f(Â·) is linear in Î·, we can use the results of
Section 3.2 with Î£ = diag(1/RË†c)Rdiag(1/RË†c) + Ïƒ2/Î½2I, where I is the identity matrix.
Figure 6 shows the plot of Ë†Î· for various values of Î¸ using Gaussian and rational quadratic
19

correlation functions. The least squares estimate of Î· is around 4.0 and is plotted in the
same figure as a red dotted line. We can see that the estimates of Î· from the rational version
of the Kennedy-Oâ€™Hagan (RK-KOH) model are much closer to the least squares estimate
than those from the original Kennedy-Oâ€™Hagan (KOH) model for both the correlation
functions. Clearly there is bias from the RK-KOH, but at least the use of rational kriging
seems to stabilize the parameter estimates making Ë†Î· more robust to the misspecification of
the correlation parameters. Now the ideas from Plumlee (2017), Gu and Wang (2018), or
Tuo (2019) could be used in conjunction with rational kriging to overcome the identifiability
issue and further improve the estimates. We leave this as a topic for future research.
0
2
4
6
8
âˆ’4
âˆ’2
0
2
4
Gaussian
Î¸
Î·^
KOH
RKâˆ’KOH
0
2
4
6
8
âˆ’4
âˆ’2
0
2
4
Rational Quadratic
Î¸
Î·^
KOH
RKâˆ’KOH
Figure 6: Plot of Ë†Î· over various values of the lengthscale parameter Î¸ using original Kennedy-
Oâ€™Hagan (KOH) model and rational version of the Kennedy-Oâ€™Hagan (RK-KOH) model. The least
squares estimate of Î· is shown as a red dotted line.
5
Conclusions
Although ordinary kriging has been widely used in statistics, the generalized least squares
estimate of the mean parameter can sometimes be nonsensical. This issue has been largely
ignored in the literature because prediction and uncertainty quantification can still be good
20

if the correlation parameters are carefully tuned. Therefore, many practitioners replace
the generalized least squares estimate of the mean with ordinary least squares estimate.
However, this leads to inconsistencies in the modeling framework, especially when Bayesian
modeling is applied. Furthermore, there are situations such as in model calibration problems,
where the parameters in the mean function have physical interpretation and thus meaningful
estimates of them are desired. The rational kriging proposed in this article seems to
overcome these issues. It gives comparable prediction and uncertainty quantification to
those of ordinary kriging, but with substantially improved estimates for the mean parameters.
This is achieved by simply scaling the stochastic part of the kriging/Gaussian process by a
scaling function. Therefore, the proposed method can be implemented easily in complex
statistical models. Moreover, the scaling function turned out to be closely related to the
first eigenfunction of the kernel used in kriging, which can be easily estimated.
The rational kriging provides a new perspective for kriging with a nonstationary variance
function. From the inception of the kriging technique, constant variance has been widely
used for the stochastic component of the statistical model. This is under the assumption of
stationarity that the true function is expected to lie within a constant band throughout the
region of interest. This approach works well when the true function is indeed stationary.
However, in practice, we never know if it is stationary or not. Thus, it makes sense to
place a prior that has smaller confidence intervals in the region of data collection and that
becomes bigger as the prediction point deviates from the input region of data (see Figure 3).
This introduces a fundamental shift in the way we deal with kriging and Gaussian process
models.
Acknowledgments
This research is supported by a U.S. National Science Foundation grant DMS-2310637.
21

Appendix: Additional Simulations
A1. One-dimensional functions
The simulations in Section 3.1 with n = 30 are repeated for three one-dimensional
functions:
y = sin{30(x âˆ’.9)4} cos{2(x âˆ’.9)} + (x âˆ’.9)/2, x âˆˆ[0, 1],
y = sin 10Ï€x
2x
+ (x âˆ’1)4, x âˆˆ[0.5, 2.5],
y =
x8
tan(1 + x2) + .5, x âˆˆ[âˆ’1, 1],
which are taken from Xiong et al. (2007), Gramacy and Lee (2012), and Buhmann et al.
(2020), respectively. The results are summarized in Figure 7.
A2. Multidimensional functions
Three test functions that are widely used for emulation in computer experiments
are chosen: 8-dimensional Dette-Pepelyshev function (Dette and Pepelyshev, 2010), 7-
dimensional piston simulation function (Kenett and Zacks, 2021), and 6-dimensional OTL
circuit function (Ben-Ari and Steinberg, 2007). The details of these functions are available
at the Virtual Library of Simulation Experiments maintained by Surjanovic and Bingham
https://www.sfu.ca/ ssurjano/index.html. We repeated the simulations in Section 4.1 with
n = 10p on these three test functions. The results are summarized in Figure 8.
22

0.00
0.05
0.10
0.15
0.20
0.25
Root Meanâˆ’Squared Error
RMSE
OK
RK
Gauss
RQ
0
1
2
3
Interval Score
IS
OK
RK
âˆ’0.6
âˆ’0.5
âˆ’0.4
âˆ’0.3
âˆ’0.2
âˆ’0.1
Mean
Î¼
OK
RK
0.0
0.5
1.0
1.5
2.0
2.5
Root Meanâˆ’Squared Error
RMSE
OK
RK
Gauss
RQ
0
2
4
6
8
10
12
Interval Score
IS
OK
RK
0.6
0.8
1.0
1.2
1.4
1.6
Mean
Î¼
OK
RK
0.00
0.02
0.04
0.06
0.08
Root Meanâˆ’Squared Error
RMSE
OK
RK
Gauss
RQ
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Interval Score
IS
OK
RK
âˆ’3.0
âˆ’2.5
âˆ’2.0
âˆ’1.5
âˆ’1.0
âˆ’0.5
0.0
Mean
Î¼
OK
RK
Figure 7: Boxplots of RMSE (left column), IS (middle column), and Ë†Âµ (right column) from the
simulation using Xiong et al function (top row), Gramacy and Lee function (middle row), and
Buhmann et al function (bottom row). The simulation is done by randomly sampling {xi}n
i=1 from
[0, 1]. The true value Âµâˆ—is plotted as a red line in the right panels.
23

0.5
1.0
1.5
Root Meanâˆ’Squared Error
RMSE
OK
RK
Gauss
RQ
2
4
6
8
10
12
14
Interval Score
IS
OK
RK
100
200
300
400
Mean
Î¼
OK
RK
0.008
0.010
0.012
0.014
0.016
Root Meanâˆ’Squared Error
RMSE
OK
RK
Gauss
RQ
0.04
0.06
0.08
0.10
0.12
Interval Score
IS
OK
RK
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Mean
Î¼
OK
RK
0.010
0.015
0.020
0.025
Root Meanâˆ’Squared Error
RMSE
OK
RK
Gauss
RQ
0.05
0.10
0.15
0.20
Interval Score
IS
OK
RK
5
6
7
8
9
10
Mean
Î¼
OK
RK
Figure 8: Boxplots of RMSE (left column), IS (middle column), and Ë†Âµ (right column) from the
simulation using Dette-Pepelyshev function (top row), piston simulation function (middle row),
and OTL circuit function (bottom row). The simulation is done by randomly sampling n = 10p
points from [0, 1]p. The true value Âµâˆ—is plotted as a red line in the right panels.
24

References
Ben-Ari, E. N. and Steinberg, D. M. (2007). Modeling data from computer experiments:
an empirical comparison of kriging with mars and projection pursuit regression. Quality
Engineering, 19(4):327â€“338.
Buhmann, M. D., De Marchi, S., and Perracchione, E. (2020). Analysis of a new class of
rational RBF expansions. IMA Journal of Numerical Analysis, 40(3):1972â€“1993.
Cressie, N. (2015). Statistics for spatial data. John Wiley & Sons.
Currin, C., Mitchell, T., Morris, M., and Ylvisaker, D. (1991).
Bayesian prediction
of deterministic functions, with applications to the design and analysis of computer
experiments. Journal of the American Statistical Association, 86(416):953â€“963.
Dette, H. and Pepelyshev, A. (2010). Generalized latin hypercube design for computer
experiments. Technometrics, 52(4):421â€“429.
Gneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and
estimation. Journal of the American statistical Association, 102(477):359â€“378.
Gramacy, R. B. and Lee, H. K. (2012).
Cases for the nugget in modeling computer
experiments. Statistics and Computing, 22:713â€“722.
Gu, M. and Wang, L. (2018). Scaled gaussian stochastic process for computer model
calibration and prediction. SIAM/ASA Journal on Uncertainty Quantification, 6(4):1555â€“
1583.
Jakobsson, S., Andersson, B., and Edelvik, F. (2009).
Rational radial basis function
interpolation with applications to antenna design. Journal of computational and applied
mathematics, 233(4):889â€“904.
Joseph, V. R. (2006). Limit kriging. Technometrics, 48(4):458â€“466.
Joseph, V. R., Gul, E., and Ba, S. (2015). Maximum projection designs for computer
experiments. Biometrika, 102(2):371â€“380.
Joseph, V. R. and Kang, L. (2011). Regression-based inverse distance weighting with
applications to computer experiments. Technometrics, 53(3):254â€“265.
25

Kang, L. and Joseph, V. R. (2016). Kernel approximation: From regression to interpolation.
SIAM/ASA Journal on Uncertainty Quantification, 4(1):112â€“129.
Kenett, R. S. and Zacks, S. (2021). Modern industrial statistics: With applications in R,
MINITAB, and JMP. John Wiley & Sons.
Kennedy, M. C. and Oâ€™Hagan, A. (2001). Bayesian calibration of computer models. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 63(3):425â€“464.
Matheron, G. (1963). Principles of geostatistics. Economic geology, 58(8):1246â€“1266.
Morris, M. D., Mitchell, T. J., and Ylvisaker, D. (1993). Bayesian design and analysis of
computer experiments: use of derivatives in surface prediction. Technometrics, 35(3):243â€“
255.
Perron, O. (1907). Zur theorie der matrices. Mathematische Annalen, 64(2):248â€“263.
Plumlee, M. (2017). Bayesian calibration of inexact computer models. Journal of the
American Statistical Association, 112(519):1274â€“1285.
Plumlee, M. and Joseph, V. R. (2018). Orthogonal gaussian process models. Statistica
Sinica, pages 601â€“619.
Pronzato, L. and Zhigljavsky, A. (2023). BLUE against OLSE in the location model: energy
minimization and asymptotic considerations. Statistical Papers, 64:1187â€“1208.
Rasmussen, C. E. and Williams, C. K. (2006). Gaussian processes for machine learning.
The MIT Press, Cambridge, MA.
Roustant, O., Ginsbourger, D., and Deville, Y. (2012). Dicekriging, diceoptim: Two r
packages for the analysis of computer experiments by kriging-based metamodeling and
optimization. Journal of statistical software, 51:1â€“55.
Santner, T. J., Williams, B. J., and Notz, W. I. (2003). The Design and Analysis of
Computer Experiments. Springer, New York.
Sarra, S. A. and Bai, Y. (2018). A rational radial basis function method for accurately
resolving discontinuities and steep gradients. Applied Numerical Mathematics, 130:131â€“
142.
Shepard, D. (1968). A two-dimensional interpolation function for irregularly-spaced data.
26

In Proceedings of the 1968 23rd ACM national conference, pages 517â€“524.
Tuo, R. (2019).
Adjustments to computer models via projected kernel calibration.
SIAM/ASA Journal on Uncertainty Quantification, 7(2):553â€“578.
Tuo, R. and Wu, C. F. J. (2015). Efficient calibration for imperfect computer models. The
Annals of Statistics, 43(6):2331â€“2352.
Xiong, Y., Chen, W., Apley, D., and Ding, X. (2007). A non-stationary covariance-based
kriging method for metamodelling in engineering design.
International Journal for
Numerical Methods in Engineering, 71(6):733â€“756.
27
