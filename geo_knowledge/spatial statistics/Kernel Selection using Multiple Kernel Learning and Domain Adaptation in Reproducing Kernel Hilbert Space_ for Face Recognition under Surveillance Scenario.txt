1
Kernel Selection using Multiple Kernel Learning and Domain
Adaptation in Reproducing Kernel Hilbert Space, for Face
Recognition under Surveillance Scenario
Samik Banerjee1 and Sukhendu Das1
1Department of Computer Science and Engineering, Indian Institute of Technology, Madras, Chennai 600036, INDIA
Face Recognition (FR) has been the interest to several researchers over the past few decades due to its passive nature of biometric
authentication. Despite high accuracy achieved by face recognition algorithms under controlled conditions, achieving the same
performance for face images obtained in surveillance scenarios, is a major hurdle. Some attempts have been made to super-resolve
the low-resolution face images and improve the contrast, without considerable degree of success. The proposed technique in this
paper tries to cope with the very low resolution and low contrast face images obtained from surveillance cameras, for FR under
surveillance conditions. For Support Vector Machine classiﬁcation, the selection of appropriate kernel has been a widely discussed
issue in the research community. In this paper, we propose a novel kernel selection technique termed as MFKL (Multi-Feature
Kernel Learning) to obtain the best feature-kernel pairing. Our proposed technique employs a effective kernel selection by Multiple
Kernel Learning (MKL) method, to choose the optimal kernel to be used along with unsupervised domain adaptation method in the
Reproducing Kernel Hilbert Space (RKHS), for a solution to the problem. Rigorous experimentation has been performed on three
real-world surveillance face datasets : FR SURV [33], SCface [20] and ChokePoint [44]. Results have been shown using Rank-1
Recognition Accuracy, ROC and CMC measures. Our proposed method outperforms all other recent state-of-the-art techniques by
a considerable margin.
Index Terms—Kernel Selection, Surveillance, Multiple Kernel Learning, Domain Adaptation, RKHS, Hallucination
I. INTRODUCTION
Face Recognition (FR) is a classical problem which is far
from being solved. Face Recognition has a clear advantage
of being natural and passive over other biometric techniques
requiring co-operative subjects. Most face recognition algo-
rithms perform well under a controlled environment. A face
recognition system trained at a certain resolution, illumination
and pose, recognizes faces under similar conditions with very
high accuracy. In contrary, if the face of the same subject is
presented with considerable change in environmental condi-
tions, then such a face recognition system fails to achieve a
desired level of accuracy. So, we aim to ﬁnd a solution to the
face recognition under unconstrained environment.
Face images obtained by an outdoor panoramic surveillance
camera, are often confronted with severe degradations (e.g.,
low-resolution, blur, low-contrast, interlacing and noise). This
signiﬁcantly limits the performance of face recognition sys-
tems used for binding “security with surveillance” applica-
tions. Here, images used for training are usually available be-
forehand which are taken under a well controlled environment
in an indoor setup (laboratory, control room), whereas the
images used for testing are captured when a subject comes
under a surveillance scene. With ever increasing demands
to combine “security with surveillance” in an integrated and
automated framework, it is necessary to analyze samples of
face images of subjects acquired by a surveillance camera
from a long distance. Hence the subject must be accurately
recognized from a low resolution, blurred and degraded (low
contrast, aliasing, noise) face image, as obtained from the
surveillance camera. These face images are difﬁcult to match
because they are often captured under non-ideal conditions.
Thus, face recognition in surveillance scenario is an impor-
tant and emerging research area which motivates the work
presented in this paper.
Performance of most classiﬁers degrade when both the
resolution and contrast of face templates used for recognition
are low. There have been many advancement in this area
during the past decade, where attempts have been made to
deal with this problem under an unconstrained environment.
For surveillance applications, a face recognition system must
recognize a face in an unconstrained environment without the
notice of the subject. Degradation of faces is quite evident in
the surveillance scenario due to low-resolution and camera-
blur. Variations in the illuminating conditions of the faces
not only reduces the recognition accuracy but occasionally
degrades the performance of face detection which is the ﬁrst
step of face recognition. The work presented in this paper deals
with such issues involved in FR under surveillance conditions.
In the work presented in this paper, the face samples from
both gallery and probe are initially passed through a robust
face detector, the Chehra face tracker, to ﬁnd a tightly cropped
face image. A domain adaptation (DA) based algorithm,
formulated using eigen-domain transformation is designed to
bridge the gap between the features obtained from the gallery
and the probe samples. A novel Multiple kernel Learning
(MKL) based learning method, termed MFKL (Multi-Feature
Kernel Learning), is then used to obtain an optimal combi-
nation (pairing) of the feature and the kernel for FR. The
novelty of the work presented in this paper is the optimal
pairing of feature and kernel to provide best performance with
DA based learning for FR. Results of performance analysis on
arXiv:1610.00660v1  [cs.CV]  3 Oct 2016

2
three real-world surveillance datasets (SCFace [20], FR SURV
[33], ChokePoint [44]) exhibit the superiority of our proposed
method of kernel selection by MFKL, with DA in Reproducing
Kernel Hilbert Space (RKHS) [13].
II. DISCUSSION ON RELATED WORK
The problem of automatic face recognition consists of three
key steps/subtasks: (1) detection and rough normalization of
faces, (2) feature extraction and accurate normalization of
faces, (3) identiﬁcation and/or veriﬁcation. These different
subtasks are not totally isolated. For example, the discriminat-
ing facial features (eyes, nose, mouth) used for face recogni-
tion are often used in face detection. Face detection and feature
extraction can be achieved simultaneously. Recent advances of
face detection, MKL and DA are discussed henceforth.
The most widely used algorithm for FD (face detection)
was proposed by Viola et al. in [41]. The algorithm proposed
in [41] is based on a simple and efﬁcient classiﬁer which
is built using the AdaBoost learning algorithm to select a
small number of critical visual features from a very large
set of potential features. The authors proposed a method for
combining classiﬁers in a cascade which allows background
regions of the image to be quickly discarded while spending
more computation on promising face-like regions. A very
recent state-of-the-art technique proposed by Zhu et al. [48]
presents a uniﬁed model for face detection, pose estimation,
and landmark estimation in real-world, cluttered images. Their
proposed model is based on a mixture of trees with a shared
pool of parts which model every facial landmark as a part
and use global mixtures to capture topological changes due to
viewpoint. The tree-structured models are surprisingly effec-
tive at capturing global elastic deformation, while being easy
to optimize unlike dense graph structures. Yow et al. [47] also
proposed a feature-based algorithm for detecting faces that is
sufﬁciently generic and is also easily extensible to cope with
more demanding variations of the imaging conditions. The
algorithm detects feature points from the image using spatial
ﬁlters and groups them into face candidates using geometric
and gray level constraints. A probabilistic framework is then
used to reinforce probabilities and to evaluate the likelihood
of the candidate as a face. Our proposed method uses a set
of 49 ﬁducial landmark points detected by the Chehra face
detector [4]. The incremental training of discriminative models
used by Chehra is not only important for building person-
speciﬁc models but also to update a generic model in case
a new annotated data arrives, since the training procedure is
very expensive and time consuming. In particular, incremental
training of discriminative models use a cascade of linear
regressors to learn the mapping from facial texture to the
shape.
While classical kernel-based classiﬁers are based on a single
kernel, in practice it is often desirable to form classiﬁers based
on combinations of multiple kernels. Bach et al. [6] proposed
the sequential minimal optimization (SMO) techniques that are
essential in large-scale implementations of the SVM for large
number of kernels. Shrivastava et al. [36] proposed a multiple
kernel learning (MKL) algorithm that is based on the sparse
representation-based classiﬁcation (SRC) method efﬁciently
representing the nonlinearities in the high-dimensional feature
space, based on the kernel alignment criteria. This method uses
a two step training method to learn the kernel weights and
sparse codes. At each iteration, the sparse codes are updated
ﬁrst while ﬁxing the kernel mixing coefﬁcients, and then the
kernel mixing coefﬁcients are updated while ﬁxing the sparse
codes. These two steps are repeated until a stopping criteria
is met. Lanckriet et al. [25] considered conic combinations of
kernel matrices for classiﬁcation, leading to a convex quadrat-
ically constrained quadratic program. Sonnenburg et al. [37]
show that it can be rewritten as a semi-inﬁnite linear program
that can be efﬁciently solved by recycling the standard SVM
implementations. Moreover, Sonnenburg et al. [37] generalize
the formulation and method to a larger class of problems,
including regression and one-class classiﬁcation. The method
proposed in our paper uses Structured MKL [45], which
is a modiﬁed version of simpleMKL [2], through a primal
formulation involving a weighted L2-norm regularization.
Domain adaptation has gained enormous importance in the
recent past. One of the popular solutions of this problem is
to weigh each instance in the source domain appropriately
such that, when the weighted instances of the source domain
are used for training the expected loss of classiﬁers tested
for target domain is minimized. Some of the works where
instance weights of source domain have been calculated are
[38], [11], [21], [30]. Sugiyama et al. [38] proposed a method
to calculate weights for instances in source domain, by using
a convex optimization framework which minimizes the KL-
divergence between the source and the target domain. There
have been some work for building robust classiﬁers for domain
adaptation [5], [46], [12]. Yang et. al. [46] has proposed
a method to calculate weights for each instances in source
domain, which are used to effectively retrain a pre-learned
SVM for target domain. Domain Adaptive Machine (DAM),
proposed by Duan et al. [12], learns a robust decision function
for labeling the instances in target domain, by leveraging a set
of base classiﬁes learned on multiple source domains. Pan et
al. [29] proposed transfer component analysis (TCA), which
minimizes the disparity of distribution by considering the
difference of means between two domains and it also preserves
local geometry of underlying manifold. The study of subspaces
spanned by the input data have found important applications in
computer vision tasks [40]. These subspaces identify important
properties of the data which can be used to model the data.
Subspace based modeling have also been widely used for the
task of visual domain adaptation [15], [7]. Fernando et. al.
[15] has calculated a subspace using eigen-vectors of two
domains such that the basis vectors of transformed source
and target domains are aligned. Manifold alignment has also
been used for domain adaptation earlier. Wang et al. [43] has
considered the manifold of each domain and estimated a latent
space, where the manifolds of both the domains are similar to
each other. However, the structures or the distributions of the
domains have not been considered in this case. In this paper we
transform the source domain data into the target domain based
on the eigen vectors of both the domains. Samanta et al. in
[35] describe a new method of unsupervised domain adaptation

3
(DA) using the properties of the sub-spaces spanning the
source and target domains, when projected along a path in
the Grassmann manifold. A new technique of unsupervised
domain adaptation based on eigenanalysis in kernel space, for
the purpose of categorisation tasks, had also been proposed
in [34]. Here, the authors propose a transformation of data
in source domain, such that the eigenvectors and eigenvalues
of the transformed source domain become similar to that of
the target domain. They extend this idea to the RKHS [13],
which enables to deal with non-linear transformation of source
domain. To handle non-linearity of the data, we also rely on
the DA projected in RKHS.
Recently, face recognition research in real-life surveillance
has become very popular. For high data transmission speed
and easy data storage, surveillance cameras generally produce
images in low resolution, and face images captured directly by
surveillance cameras are usually very small. Besides, images
taken by surveillance cameras are generally with noises and
corruptions, due to the uncontrolled circumstances and dis-
tances. Zou et al. [49] proposed a super-resolution approach to
increase the recognition performance for very low-resolution
face images. They employ a minimum mean square error esti-
mator to learn the relationship between low and high resolution
training pairs. A further discriminative constraint is added
to the learning approach using the class label information.
Biswas et al. [10] proposed a matching algorithm through
using Multidimensional Scaling (MDS). In their approach both
the low and high resolution training pairs are projected into a
kernel space. Transformation relationship is then learned in the
kernel space through iterative majorization algorithm, which
is used to match the low-resolution test faces to the high-
resolution gallery faces. Similarly, Ren et al. [32] proposed the
Coupled Kernel Embedding approach, where they map the low
and high resolution face images onto different kernel spaces
and then transform them to a learned subspace for recognition.
Rudrani et al. in [33] proposed an approach with the
combination of partial restoration (using super-resolution) of
probe samples and degradation of gallery samples. The authors
also proposed a outdoor surveillance dataset, FR SURV [33]
for evaluating their approach. In our previous work proposed
in [8], we aim to bridge the gap of resolution and contrast
using super-resolution and contrast stretching on the probe
samples and degrading the gallery samples by downsampling
the gallery samples and introducing a Gaussian blur to the
downsampled images. In addition to these measures, we had
also proposed a DA technique based on an eigen-domain trans-
formation to make the distributions of the features obtained
from the gallery and probe samples identical.
In the following sections, we ﬁrst brieﬂy present a few tech-
nical background details, followed by our proposed framework
and then our experimental results of the proposed technique.
III. BRIEF TECHNICAL BACKGROUND
A. Multiple kernel learning problem
A seemingly unrelated problem in machine learning, the
problem of multiple kernel learning is in fact intimately
connected with sparsity-inducing norms by duality. It actually
corresponds to the most natural extension of sparsity to
Reproducing Kernel Hilbert Spaces [13]. It can be shown that
for a large class of norms and, among them, many sparsity-
inducing norms, there exists for each of them a corresponding
multiple kernel learning scheme, and, vice-versa, each multiple
kernel learning scheme deﬁnes a new norm.
In the multiple kernel learning problem, n data points
(xi, yi) are given, where xi ∈X for some input space
X ∈Rl, and where yi ∈{−1, 1}. Also, assume that m
kernels are given Kj ∈Rn×n, which are assumed to be
symmetric positive semideﬁnite, obtained from evaluating a
kernel function on the data xi. Consider the problem of
learning the best linear combination Pm
j=1 ηjKj of kernels
Kj with non-negative coefﬁcients ηj > 0 and with a trace
constraint tr Pm
j=1 ηjKj = Pm
j=1 ηjtrKj = c, where c > 0
is ﬁxed. Lanckriet et al. [24] show that this setup yields the
following optimization problem:
min ζ −2eT α
w.r.t. ζ ∈R, α ∈Rn
s.t. 0 ≤α ≤C, αT y = 0, and
αT D(y)KjD(y)α ≤trKj
c
ζ, j ∈{1, ..., m}
(1)
where D(y) is a diagonal matrix with all diagonal elements
as y, e ∈Rn the vector of all ones, and C a positive constant.
The coefﬁcients ηj are recovered as Lagrange multipliers for
the constraints αT D(y)KjD(y)α ≤trKj
c ζ.
B. Support Kernel Learning Machine
Consider a decomposition of Rl into p blocks (subspaces):
Rl = Rl1 × ... × Rlp, such that, xf
i ∈Rlf , (xf
i is a vector).
The classiﬁcation algorithm, ”support kernel machine”, as
proposed by Bach et al. [6], is exactly the dual of the problem
deﬁned in equation 1.
The aim is to obtain a linear classiﬁer of the form y =
sign(wT x + b), where w has identical block-level decompo-
sition as w = (w1, ..., wp) ∈Rl1+...+lp. To induce sparsity
in the vector w at the level of blocks, such that most of its
components wi go to zero, the l1-norm of the square of a
weighted block of w, (Pp
f=1 df∥wf∥2)2, is minimized, where
the l2-norm is used within every block. The primal problem
is thus:
min 1
2(
p
X
f=1
df∥wf∥2)2 + C
n
X
i=1
ξi
w.r.t. w = (w1, ..., wp) ∈Rl1+...+lp, b ∈R, ξ ∈Rn
+
s.t. yi(
X
f
wT
f xf
i + b) ≥1 −ξi, ∀i ∈{1, ..., n}.
(2)
The problem posed in equation 2 is treated as a second-
order cone program (SOCP) [27], which yields the following
dual:

4
min 1
2γ2 −αT e
w.r.t. γ ∈R, α ∈Rn
s.t. 0 ≤α ≤C, αT y = 0, and
∥
X
i
αiyixf
i ∥2 ≤dfγ, ∀f ∈{1, ..., p}.
(3)
To induce sparsity in the formulation, the following KKT
conditions play an instrumental role to give the complementary
slackness equations:
1) αi(yi(P
f wT
f xf
i + b) −1 + ζi) = 0, ∀i
2) (C −αi)ζi = 0, ∀i
3)
 wf
∥wf ∥2
T  −P
i αiyixf
i
df γ

= 0, ∀f
4) γ(P dftf −γ) = 0
In RKHS, the data points xi are embedded in an Euclidean
space via a mapping φ : X →Rc and φ(x) has m distinct
block components φ(x) = (φ1(x), ..., φm(x)). Following the
usual recipe for kernel methods, this embedding is performed
implicitly, by specifying the inner product in Rc using a kernel
function, which in this case is the sum of individual kernel
functions on each block component:
k(xi, xj) = φ(xi)T φ(xj) =
m
X
s=1
φs(xi)T φs(xj)
=
m
X
s=1
ks(xi, xj)
(4)
The minimization task described in equation 2 is kernelized
[13] using this kernel function (equation 4). In particular, con-
sidering the dual of the problem in equation 2 and substituting
the kernel function for the inner products in equation 3, one
obtains:
min 1
2γ2 −αT e
w.r.t. γ ∈R, α ∈Rn
s.t. 0 ≤α ≤C, αT y = 0
(αT D(y)Kj(y)α)
1
2 ≤djγ, ∀j.
(5)
where Kj is the j-th Gram matrix of the points {xi}
corresponding to the j-th kernel.
Equation 5 is rearranged to yield an equivalent formulation
in which the quadratic constraints are incorporated into the
objective function:
min maxj
1
2d2
j
αT D(y)KjD(y)α −αT e
w.r.t. α ∈Rn
s.t. 0 ≤α ≤C, αT y = 0
(6)
Let, Jj(α) denote
1
2d2
j αT D(y)KjD(y)α−αT e and J(α) =
maxjJj(α). Minimization of J(α) now reduces to an convex
optimization problem as described by equation 6, as J(α) is a
non-differentiable convex function subject to linear constraints.
C. Domain Adaptation (DA) based on Eigen Domain trans-
formation [8]
Given a data, its distribution can be estimated using the
covariance matrix or Eigen-vectors. Hence, if the Eigen-
vectors of two datasets are same, then we can say that the
distributions of the two datasets are approximately similar
to each other. Hence, in this paper we aim to transform
the source domain in such a way that the Eigen-vectors of
the transformed source domain is same as that of the target
domain. We extend this idea of transformation of source
domain in Reproducing Kernel Hilbert Space (RKHS) [13]
to handle non-linear transformation of data, when necessary.
In the following sub-sections, we give the mathematical details
necessary for the unsupervised method of domain adaptation
(as in our earlier work in [8]).
1) Learning the Transformation
Let S and T be the source and target domains having nS and
nT number of instances respectively and ˜S be the transformed
source domain. Let the ith and jth instance of S and T
be represented as Si and Tj respectively. Let, the principal
components of S and T be denoted by the matrices US and
UT respectively, where the ith column represents the Eigen-
vector corresponding to the ith largest Eigen-value. Then the
principal components of T & ˜S = SUSU T
T are the same,
which is shown in Lemma 1. Hence, a simple transformation
of the source domain can be obtained by multiplying it with
the Eigen-vectors of the source and target domains.
Lemma 1 If UP and UQ are the principal components of two
datasets P and Q respectively, then the principal component
of QUQU T
P is UP .
Let ΛP and ΛQ be two diagonal matrices whose diagonal
elements are the eigen-values of P and Q respectively. The
covariance matrices of P and Q can be written as UP ΛP U T
P
and UQΛQU T
Q respectively. Now, the covariance matrix of
ˆQ = (QUQU T
P ) is given as:
ˆQT ˆQ
=
UP U T
QQT QUQU T
P
=
UP U T
QUQΛQU T
QUQU T
P = UP ΛQU T
P
This shows that the principle components of QUQU T
P is UP .
This can be followed by shifting of ˜S such that its mean is
same as that of the target domain data.
2) Extension to the RKHS
The above formulation of DA performs linear transforma-
tion of the source domain data. In order to handle non-linear
transformation of data, we extend the formulation to RKHS.
If Φ(.) is a universal kernel function, then in kernel space the
source and target domains are Φ(S) and Φ(T) respectively.
Let KSS and KT T be the Gram matrices of Φ(S) and Φ(T)
respectively (KSS = Φ(S)Φ(S)T , KST = Φ(S)Φ(T)T and
KT T
= Φ(T)Φ(T)T ). Let, U Φ
S and U Φ
T
be the principle
components of Φ(S) and Φ(T) respectively. Also, let V Φ
S and
V Φ
T be the eigen-vectors of KSS and KT T respectively. Then
we can write,
U Φ
S
=
Φ(S)T V Φ
S
(7)
U Φ
T
=
Φ(T)T V Φ
T
(8)

5
Then the transformed source domain in RKHS is given by:
Φ( ˜S) = Φ(S)U Φ
S U Φ
T
T = KSSV Φ
S V ΦT
T
Φ(T)
(9)
The corresponding Gram matrices are given by:
K ˜S ˜S = Φ( ˜S)Φ( ˜S)T = KSSV Φ
S V ΦT
T
KT T V Φ
T V ΦT
S
KSS (10)
K ˜ST = Φ( ˜S)Φ(T)T = KSSV Φ
S V ΦT
T
KT T
(11)
Once we obtain the Gram matrices, we need to modify them
appropriately such that the mean of the transformed source
domain is same as that of the target domain. If oS ∈RnS
and oT ∈RnT be two vectors with all elements as 1/nS
and 1/nT respectively, then the mean of transformed source
domain and target domain in RKHS is given by oT
SΦ( ˜S) and
oT
T Φ(T) respectively. Let the ith row of a gram matrix K is
denoted by K(i,·) and let the jth column of K is denoted
by K(·, j). Let K(i, j) denote the value corresponding to the
ith row and jth column of K.
If ˆK ˜S ˜S represents the mean shifted Gram matrix, then each
element of this matrix can be calculated as:
ˆK ˜S ˜S(i, j)
=
(Φ( ˜Si) −oT
SΦ( ˜S) + oT
T Φ(T)) ×
(Φ( ˜Si) −oT
SΦ( ˜S) + oT
T Φ(T))T
=
K ˜S ˜S(i, j) −K ˜S ˜S(i,·)oS + K ˜ST (i,·)oT
−oT
SK ˜S ˜S(·, j) + oT
SK ˜S ˜SoS −oT
SK ˜ST oT
+oT
T KT ˜S(·, j) −oT
T KT ˜SoS + oT
T KT T oT (12)
Similarly, each element of the mean-shifted Gram matrix
ˆK ˜ST can be calculated as:
ˆK ˜ST (i, j)= (Φ( ˜Si) −oT
SΦ( ˜S) + oT
T Φ(T)) × Φ(Tj)T
=K ˜ST (i, j) −oT
SK ˜ST (·, j) + oT
T KT T (·, j) (13)
3) Classiﬁcation
Once we obtain the Gram matrices ˆK ˜S ˜S and ˆK ˜ST , we can
calculate the overall Gram matrix
ˆK =
"
ˆK ˜S ˜S
ˆK ˜ST
ˆKT
˜ST
KT T
#
(14)
We can now calculate the Euclidean distance between any two
instances (i and j) in RKHS, which is given by:
dist(i, j) = ˆK(i, i) + ˆK(j, j) −2 × ˆK(i, j)
(15)
Hence, we can now use this distance matrix for classifying
test samples using KNN-classiﬁer. The unsupervised method
of DA considers the Training set to be the source and the Test
set (in a FR dataset) to be the target domains. A subset of
a few samples from the target domain are used to estimate
the distribution of the target domain (see table I in section
IV). Transformation from source to target is estimated using
eigen-analysis of the BOW-based features. The unsupervised
method of DA, enhances the performance of FR algorithm on
surveillance conditions.
IV. PROPOSED FRAMEWORK
The proposed framework as shown in ﬁgure 1, has been
designed for dealing with the problem of FR under low
resolution and low contrast, using multiple kernel learning [6]
and DA. The stages are discussed in the following:
Fig. 1: The Proposed Framework of FR, designed using DA
and MKL.
A. The pre-processing stage
Since the faces appear with background and noise, pre-
processing of the images is always necessary before they are
passed into the feature extraction stage. This treatment of
images is called the pre-processing stage as shown in ﬁgure 2
and described below step-wise:
Fig. 2: The modules of the pre-processing stage in the pro-
posed framework (see ﬁgure 1).
1) Robust Face Detection
The gallery and the probe images are passed through the
Chehra face tracker [4] which uses a cascade linear regression
for discriminative face alignment. The incremental update
of the cascade of linear regression is a very challenging
task, since the results from one level have to propagated
to the next. Due to this sequential nature of the training
procedure, we refer to this method as Sequential Cascade
of Linear Regression which is learned by a Monte-Carlo
procedure [4]. This method deals with the problem of updating
a discriminative facial deformable model, a problem that has
not been thoroughly studied in the literature. In particular, the
strategies to update a discriminative model that is trained by a
cascade of regressors is handled by this method. Very efﬁcient
strategies to update the model is adopted and it is possible
to automatically construct robust discriminative person and
imaging condition speciﬁc models.
2) Face Hallucination
The probe samples are upsampled using a state-of-the-art
Face hallucination method proposed by Felix et al. in [22].
This method of single image face hallucination is based on
solo dictionary learning. The core idea of the proposed method
[22] is to recast the superresolution task as a missing pixel
problem, where the low resolution image is considered as
its high-resolution counterpart with many pixels missing in
a structured manner. A single dictionary is therefore sufﬁcient
for recovering the super-resolved image by ﬁlling the missing
pixels.

6
3) Degradation of Gallery samples
The gallery samples are downsampled by two using simple
bicubic interpolation method [18]. The gallery samples are
then blurred using an estimated Gaussian blur, σ. The σ is
estimated using KL-Divergence [17], between the distributions
of the downsampled gallery and the upsampled probe images.
The distribution of the target domain is estimated using three
samples per class but with no class labels (hence this is
unsupervised). The graphs in ﬁgure 3 (a), (b) and (c) show the
plots of KL-divergence with increasing values of Gaussian blur
kernel, σ, for the three different datasets, FR SURV, SCface
and ChokePoint, respectively. The optimal σopt is obtained at
1.75, 1.7 and 1.2 for the three datasets and are recorded in
table I. The gallery samples are degraded using this uniform
blur σopt to obtain a degraded gallery.
Fig. 3: Plot of KL-Divergence between degraded gallery and
probe images with different values of σ used for degradation,
in case of (a) FR SURV [33], (b) SCface [20] and (c)
ChokePoint [44] datasets. The optimal values of σ are obtained
as: (a) 1.75, (b) 1.7 and (c) 1.2, respectively for the three
datasets.
TABLE I: Number of probe samples (per subject) used for
estimation of σopt and in DA, for three datasets. Class (subject
ID) information was made unavailable in both cases (as the
method is unsupervised). Only for the estimation of σopt, the
entire gallery is used.
Dataset
No. of samples used, for
Values of σopt
Estimation of σopt
DA
SCface
5
3
1.7
[20]
for 30 subjects
FR SURV
5
3
1.75
[33]
for 20 subjects
ChokePoint
5
6
1.2
[44]
for 20 subjects
4) Power Law Transformation
To cope with the contrast degradation, we perform Power
Law transformation [14] for contrast stretching the probe
images. The transformation function used is:
P(i, j) = k.C(i, j)γ
(16)
where, P(i, j) and C(i, j) denotes the gray-level pixel values
of the input and output image. We use γ = 1.25, k = 1. Visual
results as shown in ﬁgure 4 depicts contrast enhancement of
the image for varying values of γ. We set γ = 1.25 based on
visual observation (empirical).
Fig. 4: (a) Probe images of three subjects; (b) Outputs of
CHEHRA [4] process; Gamma-corrected (equation 16) images
with (c) γ = 1.25; (d) γ = 1.5; (e) γ = 1.75; and (f) γ = 1.9,
from (I) FR SURV [33], (II) SCFace [20] and (III) ChokePoint
[44] datasets.
For probe samples, we use:
Probeupsampled = Probecrop ↑v
(17)
Probetransformed(i, j) = Probeupsampled(i, j)γ
(18)
where, Probecrop denotes the cropped probe sample based on
Chehra [4], ↑denotes upsampling of the image by a factor
v mentioned to the right of it, Probecrop denotes upsampled
image and Probetransformed denotes the image that will be
used for feature-extraction. The γ is the parameter for gamma-
correction of the image based on Power Law transformation
which is given by equation 16.
B. Feature Extraction
The feature extraction process in the proposed method is
based on the set of features extracted from the degraded gallery
and the enhanced probe face images. The set of features used
in the methods proposed includes Local Binary Pattern (LBP),
Eigen Faces, Fisher Faces, Gabor faces, Weber Faces, Bag
of Words (BOW), Vector of Linearly Agregated Descriptors
encoding based on Scale Invariant Feature Transform (VLAD-
SIFT) [3], Fisher Vector encoding based on SIFT (FV-SIFT)
[31].
1) Eigen Faces
This approach [40] of the detection and identiﬁcation of
human faces and describe a working, near-real-time face
recognition system which tracks the face of a subject and
then recognizes the person by comparing characteristics of
the face to those of known individuals. The approach treats
face recognition as a two-dimensional recognition problem,
taking advantage of the fact that faces are normally upright
and thus may be described by a small set of 2-D characteristic
views. Face images are projected onto a feature space (face
space) that best encodes the variation among known face
images. The face space is deﬁned by the eigenfaces, which are
the eigenvectors of the set of faces; they do not necessarily
correspond to isolated features such as eyes, ears, and noses.
2) Fisher Faces
This face recognition algorithm [9] is insensitive to large
variation in lighting direction and facial expression. Taking a

7
pattern classiﬁcation approach, each pixel is considered in an
image as a coordinate in a high-dimensional space. The images
of a particular face, under varying illumination but ﬁxed pose,
lie in a 3D linear subspace of the high dimensional image
space if the face is a Lambertian surface without shadowing.
However, since faces are not truly Lambertian surfaces and do
indeed produce self-shadowing, images will deviate from this
linear subspace. Rather than explicitly modeling this deviation,
the image is projected into a subspace in a manner which
discounts those regions of the face with large deviation. The
projection method is based on Fishers Linear Discriminant
and produces well separated classes in a low-dimensional
subspace, even under severe variation in lighting and facial
expressions.
3) Gabor Faces
The Gabor Feature Classiﬁer (GFC) method [26] employs
an enhanced Fisher discrimination model on an augmented
Gabor feature vector; which is derived from the Gabor wavelet
transformation ofﬁce images. For the three datasets used for
experimentation, table II gives the values of the parameters
used, where v represents the different scales used, nµ is the
number of orientations and µ is the orientation. Parameter σ,
which determines the ratio of the Gaussian window width to
wavelength, is set to 2π, kmax is the wave-vector set to π
and f the spatial frequency set to
√
2. The Gabor wavelets,
whose kernels are similar to the 20 receptive ﬁeld proﬁles
of the mammalian cortical simple cells, exhibit desirable
characteristics of spatial locality and orientation selectivity. As
a result, the Gabor transformed face images produce salient
local and discriminating features that are suitable for face
recognition.
TABLE II: Parameters for experimenation in Gabor faces for
three datasets.
Datasets
v
nµ
µ
FR SURV [33]
{0, ..., 7}
8
{0, ..., 7}
SCFace [20]
{0, ..., 5}
16
{0, ..., 15}
ChokePoint [44]
{0, ..., 4}
4
{0, ..., 3}
4) Weber Faces
Webers law suggests that for a stimulus, the ratio be-
tween the smallest perceptual change and the background
is a constant, which implies stimuli are perceived not in
absolute terms but in relative terms. Inspired from this, a
novel illumination insensitive representation of face images
is exploited and analyzed under varying illuminations via a
ratio image, called Weber-face, [42] where a ratio between
local intensity variation and the background is computed.
5) Local Binary Pattern
Local binary patterns (LBP) [1] is a type of feature used for
classiﬁcation in computer vision. LBP is the particular case of
the Texture Spectrum model proposed in 1990. The face image
is divided into several regions from which the LBP feature
distributions are extracted and concatenated into an enhanced
feature vector to be used as a face descriptor. The procedure
consists of using the texture descriptor to build several local
descriptions of the face and combining them into a global
description. The operator assigns a label to every pixel of an
image by thresholding the 3X3-neighborhood of each pixel
with the center pixel value and considering the result as a
binary number. Then, the histogram of the labels can be used
as a texture descriptor.
6) Bag-of-Words (BOW)
The feature extraction method proposed here is based on
the BOW [16] based on Dense-SIFT features. The dense-SIFT
features are calculated with a single-pixel shift of the window
over the face. The words used in processing are local image
features. They may be constructed around interest points such
as scale-space extrema (e.g. SIFT keypoints [28]), or simply
on windows extracted from the image at regular positions and
various scales. The features can be image patches, histograms
of gradient orientations or color histograms. As these features
are sensitive to noise and are represented in high dimension
spaces, they are not directly used as words, but are categorized
using a vector quantization technique such as k-means. The
output of this discretization is the dictionary.
7) Fisher Vector Encoding on dense-SIFT features (FV-
SIFT)
This encoding [31] serves a similar purposes: summarizing
in a vectorial statistic a number of local feature descriptors
(e.g. SIFT [28]). Similarly to bag of visual words, they assign
local descriptor to elements in a visual dictionary, obtained
with a Gaussian Mixture Models for Fisher Vectors. However,
rather than storing visual word occurrences only, the represen-
tation stores a statistics of the difference between dictionary
elements and pooled local features. The Fisher encoding uses
GMM to construct a visual word dictionary.
8) VLAD encoding on dense-SIFT features (VLAD-SIFT)
The Vector of Linearly Agregated Descriptors [3] is similar
to Fisher vectors, but (i) it does not store second-order
information about the features and (ii) it typically use KMeans
instead of GMMs to generate the feature vocabulary (although
the latter is also an option). VLAD is constructed as follows:
regions are extracted from an image using an afﬁne invariant
detector, and described using the 128 dimensional SIFT de-
scriptor. Each descriptor is then assigned to the closest cluster
of a vocabulary of size k (where, k is typically 64 or 256, so
that clusters are quite coarse). For each of the k clusters, the
residuals (vector differences between descriptors and cluster
centers) are accumulated, and the k - 128 dimensional sums
of residuals are concatenated into a single k×128 dimensional
descriptor.
C. Kernel Selection by Multiple Kernel Learning
In support vector machine (SVM), selecting the kernel func-
tion and its parameters are important issues during training.
Generally, to select the best performing kernel among the set
of kernel function (like Linear, RBF, etc.), a cross validation
procedure is used. In recent years, several MKL techniques
have been proposed, where instead of selecting one speciﬁc
kernel function and its corresponding parameters, multiple
kernels are learned. MKL has two main advantages: (a)
Different kernels correspond to different notions of similarity

8
Fig. 5: The Training Phase after pre-processing (see ﬁgure 2),
to generate transformed features in RKHS for classiﬁcation.
and instead of ﬁnding which works best, a MKL learning
method helps to pick the best kernel or a combination of
kernels; and (b) Different kernels may use inputs coming from
different representations, possibly from different sources. In
such cases, combining kernels is one possible way to combine
sources of multiple information. The training phase (see ﬁgure
5) is described in section IV-D.
In this paper, we introduce a novel technique based on
MKL, termed MFKL (Multi-feature Kernel Learning), for
selecting the optimal feature-kernel combination for classiﬁca-
tion. The MFKL method determines the optimal weights for
the different kernels used for each feature category. All the
features are extracted individually from each of the gallery
face images and passed into the MFKL module for optimal
kernel selection for each feature. The ordered pair of the
feature and kernel < F i
H, Ki > is selected and stored for
further processing. We assume that we have p features and
their corresponding feature space be represented by X f, where
f ∈{1, .., p}. Data points (xf
i , yi) are given, where xf
i ∈X f
represents a feature vector in a particular feature space X f,
and yi ∈{−1, 1}, ∀i ∈1, ..., n, are the class-labels. For
each feature space X f, the choice is one out of m kernels
Kf
j ∈Rn×n.
We consider X = Sp
f=1 X f and X ∈Rl, where l =
l1+...+lp, such that xf
i ∈Rlf , where lf is the dimensionality
of the feature vector f. This problem follows a similar for-
mulation as described in the classiﬁcation algorithm, ”support
kernel machine”, as proposed by Bach et al. [6], and also
discussed in section III-B.
The primal problem is given by equation 2 and its dual by
equation 3, with the same KKT conditions as mentioned in
section III-B. In RKHS, we assume the embeddings of the data
points xi in each feature space via a mapping φ : X f →Rc.
We also assume that φ(x) has m distinct block components
φ(x) = (φ1(x), ..., φm(x)). Following the usual recipe for
kernel methods, we assume that this embedding is performed
implicitly, by specifying the inner product in Rc using a kernel
function, which in this case is the sum of individual kernel
functions on each block (subspace) component:
kf(xf
i , xf
j ) = φ(xf
i )T φ(xf
j ) =
m
X
s=1
φs(xf
i )T φs(xf
j )
=
m
X
s=1
ks(xf
i , xf
j )
(19)
Now, in the feature space, X, we have
k(xi, xj) =
p
X
f=1
βfkf(xf
i , xf
j )
(20)
We now kernelize the problem described in equation 2 using
this kernel function. In particular, we consider the dual of the
problem in equation 2 and substitute the kernel function for
the inner products in the equation 3 with the constraint in
a particular feature space, rather than over the whole space,
as (αT D(y)Kf
j D(y)α)
1
2 ≤djγ, ∀j, f, where Kf
j is the j-
th Gram matrix of the points {xf
i } corresponding to the j-
th kernel, for the f-th feature formed using k(xi, xj). These
constraints are derived from equations 3 and 5, which lead
to the simultaneous selection of feature and its corresponding
non-zero kernels, based on the objective function (similar to
equation 6), formulated as:
min maxj
1
2d2
j
αT D(y)Kf
j D(y)α −αT e
w.r.t. α ∈Rn
s.t. 0 ≤α ≤C, αT y = 0
(21)
Since the sparsity of the weights of the kernels is en-
sured by KKT conditions, the non-zero kernels are used
for classiﬁcation in the testing phase. Let Jf
j (α) denote
1
2d2
j αT D(y)Kf
j D(y)α −αT e (see equation 21) and Jf(α) =
maxjJf
j (α). Minimization of Jf(α) now reduces to an convex
optimization problem, as Jf(α) is also a non-differentiable
convex function subject to linear constraints. Our global ob-
jective function is J(α) = Sp
f=1 Jf(α). Union of convex
functions is not necessarily convex. Hence, a subgradient
method [23] is used to solve each of these convex optimization
sub-problems, Jf(α), and ﬁnally the union of these are used
to obtain a global solution. Sparse solutions ensure that most
of the kernel weights are negligible (go near to zero) and a
very few non-zero kernel weights remain for each feature.
In the proposed work, we take into account a set of kernel
functions for the MKL method. The set of kernels consists
of Linear, Polynomial, Gaussian, RBF, Chi-square and RBF
+ Chi-square. The equations of each of these kernels are
tabulated in Table III.
TABLE III: Different types of kernel used in the MFKL, with
their formulae.
Type of Kernel
Formula
Linear
k(x, y) = xT y + c
Polynomial
k(x, y) = (αxT y + c)d
Gaussian
k(x, y) = exp

−∥x−y∥2
2σ2

RBF
k(x, y) = exp

−∥x−y∥
2σ2

Chi-square
k(x, y) = 1 −Σn
i=1
(xi−yi)2
1
2 (xi+yi)
RBF + Chi-square
k(x, y) = 1 −Σn
i=1
(xi−yi)2
1
2 (xi+yi)
+exp

−∥x−y∥
2σ2


9
D. The Training Phase
In this phase of our proposed framework as shown in ﬁgure
5, we have a set of feature F and a set of kernels K pairings.
F ={LBP, EigenFaces, FisherFaces, Gaborfaces,
WeberFaces, V LAD −SIFT, FV −SIFT, BOW}
(22)
where each Fi ∈F is the feature extracted from a face image;
and
K ={Linear, Polynomial, Gaussian, RBF,
Chi −square, RBF + Chi −square}
(23)
where each Ki ∈K is a kernel function for the projection in
the RKHS, Hi.
The combination of F and K is passed into the MKL
module to obtain the set of optimized pair of {Fj, Kj}
using the kernel selection method described in section IV-C.
Based on the best feature-kernel pair obtained, the feature
vector is projected into a higher dimensional space of RKHS.
The training in DA is performed to obtain the ﬁnal model
parameters along with the feature-kernel pairs. The number of
probe samples used as targets for DA is mentioned in the table
I.
E. Classiﬁcation based on K-Nearest Neighbor Classiﬁer
In the testing phase as shown in ﬁgure 6, a query low-
resolution face image is ﬁrst pre-processed based on the pre-
processing techniques described in section IV-A. Features are
extracted from the pre-processed probe and passed into the DA
module (RKHS), as shown in ﬁgure 6. The process of kernel
selection corresponding to a feature is based on the MFKL
technique proposed in the kernel selection stage. The overall
gram matrix is created for each of these features. A majority
voting is used based on the Nearest neighbor classiﬁcation for
the probe samples. The winning class ID is selected as the
best match.
Fig. 6: The Testing phase in the proposed method (see ﬁgure
1).
V. DETAILS OF SURVEILLANCE FACE DATABASES USED
For the experimentation purpose we have used three real-
world surveillance face datasets, which are discussed below.
In all three real-world datasets the gallery samples are taken
in laboratory conditions, while for probes two out of the three
datasets are shot indoor, while one is shot outdoor.
A. FR SURV [33]
FR SURV is a challenging database for FR, because the
gallery and the probe images are taken at different resolutions
with two different cameras. The gallery samples, taken indoor
with high resolution camera, have a resolution of 250 × 250
pixels, while the probe samples, taken by surveillance camera,
have a very low resolution of 45×45 pixels. The probe samples
are taken at a distance of 50-100 meters outdoor. Using Chehra
[4] on both the gallery and probe samples, we produce cropped
face regions at an average of 150X150 pixels and 33 × 33
pixels respectively. The database consists 51 subjects with 20
samples per class. Figure 7 shows two samples of the gallery
images and the their respective probe image (cropped using
Chehra [4]).
Fig. 7: Samples of two subjects from FR SURV Database
[33]: (I) (a), (c) Gallery images; and (I) (b), (d) corresponding
Probe images; (II) (a)-(d) Cropped faces from (I) using Chehra
[4].
B. SCFace [20]
SCface is also a challenging database for FR as the images
were taken at different surveillance conditions. The database
has a huge collection of static images of 130 different people.
Images were captured by ﬁve different video surveillance
cameras (cam1, cam2, cam3, cam4, cam5). Two cameras were
also used in the night vision mode (cam6 and cam7). All
these images were collected indoor with varying quality and
resolution levels at three different distances. The training set
consists of nine images: one frontal and four each in left
and right rotations. The dataset has an image taken from
an infra-red camera(cam8). Figure 8 shows the images for a
single person in the dataset. The gallery has images of size
2048×3072 pixels which are cropped by VJFD to an average
of 800 × 600 pixels. The probe images at Distance 1,2 and 3
has a resolution of 75 × 100, 108 × 144 and 168 × 224 pixels
respectively which are cropped by Chehra [4] to an average
of 40 × 40, 60 × 60 and 100 × 100 pixels respectively. We do
not use the cam6 and cam7 as they are IR images.
C. ChokePoint [44]
Wong et al. [44] collected a new video dataset, termed
ChokePoint, designed for experiments in person identiﬁca-

10
Fig. 8: SCface Database [20]: (I) Gallery images; Probe
Images at (II) Distance 1; (III) Distance 2; and (IV) Distance
3; Right column shows the Chehra [4] output for any one of
the samples in each row.
tion/veriﬁcation under real-world surveillance conditions using
existing technologies. An array of three cameras was placed
above several portals (natural choke points in terms of pedes-
trian trafﬁc) to capture subjects walking through each portal
in a natural way (see ﬁgures 9 and 10).
Fig. 9: An example of the recording setup used for the
ChokePoint dataset [44]. A camera rig contains 3 cameras
placed just above a door, used for simultaneously recording
the entry of a person from 3 viewpoints. The variations
between viewpoints allow for variations in walking directions,
facilitating the capture of a near-frontal face by one of the
cameras.
Fig. 10: Example shots from the ChokePoint dataset [44],
showing portals with various backgrounds.
The dataset consists of 25 subjects (19 male and 6 female)
in portal 1 and 29 subjects (23 male and 6 female) in portal
2. In total, it consists of 48 video sequences and 64,204 face
images. Each sequence was named according to the recording
conditions (eg. P2E S1 C3) where P, S, and C stand for portal,
sequence and camera, respectively. E and L indicate subjects
either entering or leaving the portal. The numbers indicate the
respective portal, sequence and camera label. For example,
P2L S1 C3 indicates that the recording was done in Portal 2,
with people leaving the portal, and captured by camera 3 in the
ﬁrst recorded sequence. The ChokePoint dataset does not have
variation in resolution. But the difference lies in the different
cameras used for capturing the image due to different camera
parameters and the illumination variations. This feature makes
it suitable to be tackled with DA.
For experimentation, we consider the images obtained from
camera, C1 as the Gallery set, since it contains maximum
frontal images with better lighting conditions. The other
cameras are considered as probe images. We do not consider
the sequence S5, as it contains crowded scenario. Since the
images obtained from C1 are crisp and have better illumi-
nation conditions than C2 and C3, the gallery and probe is
passed through all the pre-processing stages, except the face
hallucination stage, as the resolution of the images obtained
from all these cameras are similar.
D. Intermediate results of face processing
The face images go through the several stages of pre-
processing, as described in section IV-A. An example to
illustrate the pre-processing stages is shown in ﬁgure 11, for
the SCFace [20] dataset. The top row illustrates the effect of
pre-processing on the gallery images, while the bottom row
illustrates the effect of pre-processing on the probe images.
Figure 11(a) shows the original images available in the dataset,
while ﬁgure 11(b) shows the landmark points detected by the
Chehra [4] on the face images. Based on these landmark
points, we obtain a tightly cropped face image as shown in
ﬁgure 11(c). Figure 11(d) shows the result of downsampling
of the gallery images and upsampling of the probe images
by Face Hallucination technique, as discussed in section
IV-A2. The downsampled gallery images are blurred using a
Gaussian kernel to obtain the degraded gallery images while
the upsampled probe images are passed through Power law
transformation to obtain moderately enhanced probe images
as shown in ﬁgure 11(e). Face hallucination is not applied
on the probe samples in the ChokePoint [44] dataset, as the
resolution of the gallery and probe samples are similar in the
dataset.
Figure 12 shows the examples of the degraded gallery (in
the top row) and the enhanced probe (in the bottom row)
images of a single subject from the three surveillance datasets,
used in our experimentation. These degraded gallery and the
enhanced probe images are used for feature extraction. The
original gallery and probes are also shown with similar spatial
resolution to illustrate the efﬁciency of the pre-processing
stage (ignore resolution which is different).
VI. EXPERIMENTAL RESULTS
Rigorous experimentation is carried on three real-world
datasets; SCface [20], FR SURV [33], and ChokePoint [44]

11
Fig. 11: Pre-processing stages on SCFace [20] for a subject:
(a) Original image, (b) Landmarks detected by Chehra [4], (c)
the cropped faces obtained using Chehra, (d) Downsampled
image of gallery and upsampled image of probe, (e) Final
degraded gallery and enhanced probe images.
Fig. 12: Example shots from the three datasets (one sample
each), showing the degraded gallery and the enhanced probe
images on the cropped faces produced by Chehra [4]
The proposed methods are compared with several other recent
state-of-the-art methods and the results are reported in table
IV using Rank-1 Recognition Rate.
TABLE IV: Rank-1 Recognition Rate for different methods.
Results in bold, exhibit the best performance.
Sl.
Algorithm
SCface
FR SURV
ChokePoint
[20]
[33]
[44]
1
EDA1 [8]
47.65
7.82
54.21
2
COMP DEG
[33]
4.32
43.14
62.59
3
MDS [10]
42.26
12.06
52.13
4
KDA1 [8]
35.04
38.24
56.25
5
Gopalan [19]
2.06
2.06
58.62
6
Kliep [39]
37.51
28.79
63.28
7
Naive
75.27
45.78
65.76
8
Proposed
Method
78.31
55.23
84.62
In case of Naive combination, the source and the target
domain samples are used without transformation for training
and domain samples are used as probes. In EDA1 method,
proposed by Banerjee et al. [8], DA processing based on
an eigenvector based transformation, whose extension in the
RKHS is termed as KDA1. Rudrani et al. [33] (COMP DEG)
tries to bridge the gap between the gallery and the probe
samples by projecting them both into a lower dimensional
subspace determined by the principal components of the
feature vectors obtained from each face. This paper also acts as
the source for the dataset, FR SURV [33]. Multi-dimensional
scaling (MDS) proposed by Biswas et al. in [10] projects both
the gallery and the probe samples into a common subspace
for classiﬁcation. The methods proposed by Gopalan et al.
[19] and Kliep [39] are two DA based techniques used for
object classiﬁcation accross domains. We can observe that the
method proposed in this paper have outperformed (our results
are given in bold, in Table IV) all the other competing methods
by a considerable margin. The complexity of the datasets is
also observed by the results of FR, which are all moderately
low in many cases.
Results are also reported using ROC (for identiﬁcation) and
CMC (for veriﬁcation) measures, as shown in ﬁgures 13 - 15,
for the three datasets respectively. The plot drawn in red show
the performance of our proposed method. We can observe that
the red curves in all the plots outperform all other competing
methods. On an average, the second best performance is given
by the naive approach since the MFKL is also incorporated
into it, while the method proposed by Gopalan et al. [19]
performs generally the worst.
Fig. 13: (a) ROC and (b) CMC plots for performance analysis
of different methods, using SCFace [20] dataset.
Fig. 14: (a) ROC and (b) CMC plots for performance analysis
of different methods, using FR SURV [33] dataset.
As we look closer into the the table IV row-wise, we
can easily observe that the FR SURV datasets has the least
accuracy. This is an indication that there is still further scope
of improvement in this ﬁeld. Also, it shows that the database is
quite tough to handle. As we can see that the gallery samples in
FR SURV are all taken in Indoor laboratory conditions and the
probe samples are taken in Outdoor conditions which results
in the large complexity of the database. Since FR SURV is
an outdoor dataset, we can see the accuracy of FR is less than
that of the ChokePoint dataset, which is the easiest to handle
among the three. The SCface and the ChokePoint datasets are

12
Fig. 15: (a) ROC and (b) CMC plots for performance analysis
of different methods, using ChokePoint [44] dataset.
two indoor surveillance datasets. Experiments are done in both
identiﬁcation and veriﬁcation mode. There is still scope of
improvement to ﬁnd a more effective effective transformation
such that the distribution of the features of the gallery and
the probe become similar. The other two datasets have both
the gallery and the probe taken in Indoor scenario. The effec-
tiveness of the DA used in this paper is clearly visible in the
results of EDA1 and KDA1. The non-linear transformations in
KDA1 proves to be more effective which motivates this paper
to concentrate mostly on the DA in RKHS. The effectiveness
of the MKL based method is evident when we try to focus
on the Naive combination results. It is very competitive in
all the three datasets. The Naive combination is the complete
Framework without the DA module, incorporating also the
MKL process. When these two powerful tools are combined,
our proposed method outperforms all other methods by a
considerable margin.
VII. CONCLUSION
An efﬁcient method to tackle the problem of low-contrast
and low-resolution in face recognition under surveillance sce-
nario is proposed in this paper. The method proposes a novel
kernel selection method using MFKL to obtain an optimal
pairing of feature and kernel for eigen-domain transformation
based unsupervised DA in the RKHS. The three metrics used
to compare the performance of our proposed method with
the recent state-of-the-art techniques, show a great deal of
superiority of our method than the other techniques, using
three real-world surveillance face datasets.
REFERENCES
[1] T. Ahonen, A. Hadid, and M. Pietikainen. Face description with local
binary patterns: Application to face recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 28(12):2037–2041, 2006.
[2] R. Alain, R. Francis, C. Phane, and S. Yves. Simple mkl. Journal of
Machine Learning Research, 9:2491–2521, 2008.
[3] R. Arandjelovic and A. Zisserman. All about vlad. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pages
1578–1585, 2013.
[4] A. Asthana, S. Zafeiriou, S. Cheng, and M. Pantic. Incremental face
alignment in the wild.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2014.
[5] Y. Aytar and A. Zisserman.
Tabula rasa: Model transfer for object
category detection.
In IEEE International Conference on Computer
Vision, pages 2252–2259. IEEE, 2011.
[6] F. R. Bach, G. R. Lanckriet, and M. I. Jordan. Multiple kernel learning,
conic duality, and the smo algorithm. In Proceedings of the twenty-ﬁrst
international conference on Machine learning, page 6. ACM, 2004.
[7] M. Baktashmotlagh, M. T. Harandi, B. C. Lovell, and M. Salzmann.
Unsupervised domain adaptation by domain invariant projection.
In
International Conference on Computer Vision, pages 769–776. IEEE,
2013.
[8] S. Banerjee, S. Samanta, and S. Das. Face recognition in surveillance
conditions with bag-of-words, using unsupervised domain adaptation.
In Proceedings of Indian Conference on Computer Vision Graphics and
Image Processing, page 50. ACM, 2014.
[9] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman. Eigenfaces vs.
ﬁsherfaces: Recognition using class speciﬁc linear projection.
IEEE
Transactions on Pattern Analysis and Machine Intelligence, 19(7):711–
720, 1997.
[10] S. Biswas, K. W. Bowyer, and P. J. Flynn. Multidimensional scaling
for matching low-resolution face images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 34(10):2019–2030, 2012.
[11] W. Dai, G.-R. Xue, Q. Yang, and Y. Yu.
Transferring naive bayes
classiﬁers for text classiﬁcation. In Proceedings of the national con-
ference on artiﬁcial intelligence, volume 22, page 540. Menlo Park,
CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2007.
[12] L. Duan, D. Xu, and I. W. Tsang. Domain adaptation from multiple
sources: A domain-dependent regularization approach. IEEE Transac-
tions on Neural Networks and Learning Systems, 23(3):504–518, 2012.
[13] H. Dym.
J contractive matrix functions, reproducing kernel Hilbert
spaces and interpolation, volume 71.
American Mathematical Soc.,
1989.
[14] H. Farid. Blind inverse gamma correction. IEEE Transactions on Image
Processing, 10(10):1428–1433, 2001.
[15] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Unsupervised
visual domain adaptation using subspace alignment. In International
Conference on Computer Vision, pages 2960–2967. IEEE, 2013.
[16] D. Filliat.
A visual bag of words method for interactive qualitative
localization and mapping. In IEEE International Conference on Robotics
and Automation, pages 3921–3926. IEEE, 2007.
[17] J. Goldberger, S. Gordon, and H. Greenspan.
An efﬁcient image
similarity measure based on approximations of kl-divergence between
two gaussian mixtures. In Internation Conference on Computer Vision,
pages 487–493. IEEE, 2003.
[18] R. C. Gonzalez and R. E. Woods. Digital Image Processing. Addison-
Wesley, Reading, MA, 1992.
[19] R. Gopalan, R. Li, and R. Chellappa.
Domain adaptation for object
recognition: An unsupervised approach. In Internation Conference on
Computer Vision, pages 999–1006, 2011.
[20] M. Grgic, K. Delac, and S. Grgic. Scface–surveillance cameras face
database. Multimedia tools and applications, 51(3):863–879, 2011.
[21] J. Jiang and C. Zhai. Instance weighting for domain adaptation in nlp. In
Association for Computer Linguistics, volume 7, pages 264–271, 2007.
[22] F. Juefei-Xu and M. Savvides. Single face image super-resolution via
solo dictionary learning. In IEEE International Conference on Image
Processing (ICIP), volume 2, 2015.
[23] S. Kim and H. Ahn. Convergence of a generalized subgradient method
for nondifferentiable convex optimization. Mathematical Programming,
50(1-3):75–80, 1991.
[24] G. R. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I.
Jordan. Learning the kernel matrix with semideﬁnite programming. The
Journal of Machine Learning Research, 5:27–72, 2004.
[25] G. R. Lanckriet, T. De Bie, N. Cristianini, M. I. Jordan, and W. S.
Noble. A statistical framework for genomic data fusion. Bioinformatics,
20(16):2626–2635, 2004.
[26] C. Liu and H. Wechsler. Gabor feature based classiﬁcation using the
enhanced ﬁsher linear discriminant model for face recognition. IEEE
Transactions on Image processing, 11(4):467–476, 2002.
[27] M. S. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret. Applications of
second-order cone programming. Linear algebra and its applications,
284(1):193–228, 1998.
[28] D. G. Lowe. Distinctive image features from scale-invariant keypoints.
International Journal of Computer Vision, 60(2):91–110, 2004.
[29] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via
transfer component analysis. IEEE Transactions on Neural Networks,
22(2):199–210, 2011.
[30] M. A. Pathak and E. H. Nyberg.
Learning algorithms for domain
adaptation. In Advances in Machine Learning, pages 293–307. Springer,
2009.
[31] F. Perronnin, J. S´anchez, and T. Mensink. Improving the ﬁsher kernel for
large-scale image classiﬁcation. In European Conference on Computer
Vision, pages 143–156. Springer, 2010.

13
[32] C.-X. Ren, D.-Q. Dai, and H. Yan.
Coupled kernel embedding for
low-resolution face image recognition.
IEEE Transactions on Image
Processing, 21(8):3770–3783, 2012.
[33] S. Rudrani and S. Das. Face recognition on low quality surveillance
images, by compensating degradation. In ICIAR, pages 212–221. LNCS,
Springer, 2011.
[34] S. Samanta and S. Das. Unsupervised domain adaptation using eigen-
analysis in kernel space for categorisation tasks. Image Processing, IET,
9(11):925–930, 2015.
[35] S. Samanta, T. Selvan, and S. Das. Modeling sequential domain shift
through estimation of optimal sub-spaces for categorization. In British
Machine Vision Conference, 2014.
[36] A. Shrivastava, V. M. Patel, and R. Chellappa. Multiple kernel learning
for sparse representation-based classiﬁcation.
IEEE Transactions on
Image Processing, 23(7):3013–3024, 2014.
[37] S. Sonnenburg, G. R¨atsch, C. Sch¨afer, and B. Sch¨olkopf. Large scale
multiple kernel learning. The Journal of Machine Learning Research,
7:1531–1565, 2006.
[38] M. Sugiyama, S. Nakajima, H. Kashima, P. V. Buenau, and M. Kawan-
abe. Direct importance estimation with model selection and its appli-
cation to covariate shift adaptation. In Advances in neural information
processing systems, pages 1433–1440, 2008.
[39] M. Sugiyama, S. Nakajima, H. Kashima, P. von B¨unau, and M. Kawan-
abe. Direct importance estimation with model selection and its appli-
cation to covariate shift adaptation. In Neural Information Processing
Systems, pages 1962–1965, 2007.
[40] M. Turk and A. Pentland.
Eigenfaces for recognition.
Journal of
cognitive neuroscience, 3(1):71–86, 1991.
[41] P. Viola and M. J. Jones. Robust real-time face detection. International
Journal of Computer Vision, 57(2):137–154, 2004.
[42] B. Wang, W. Li, W. Yang, and Q. Liao. Illumination normalization based
on weber’s law with application to face recognition. Signal Processing
Letters, IEEE, 18(8):462–465, 2011.
[43] C. Wang and S. Mahadevan. Heterogeneous domain adaptation using
manifold alignment. In IJCAI Proceedings-International Joint Confer-
ence on Artiﬁcial Intelligence, volume 22, page 1541, 2011.
[44] Y. Wong, S. Chen, S. Mau, C. Sanderson, and B. C. Lovell. Patch-based
probabilistic image quality assessment for face selection and improved
video-based face recognition. In IEEE Biometrics Workshop, Computer
Vision and Pattern Recognition (CVPR) Workshops, pages 81–88. IEEE,
June 2011.
[45] Z. Xu, R. Jin, H. Yang, I. King, and M. R. Lyu. Simple and efﬁcient
multiple kernel learning by group lasso.
In Proceedings of the 27th
international conference on machine learning (ICML-10), pages 1175–
1182, 2010.
[46] J. Yang, R. Yan, and A. G. Hauptmann. Cross-domain video concept
detection using adaptive svms. In Proceedings of the 15th international
conference on Multimedia, pages 188–197. ACM, 2007.
[47] K. C. Yow and R. Cipolla. Feature-based human face detection. Image
and vision computing, 15(9):713–735, 1997.
[48] X. Zhu and D. Ramanan. Face detection, pose estimation, and landmark
localization in the wild.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 2879–2886. IEEE,
2012.
[49] W. W. Zou and P. C. Yuen. Very low resolution face recognition problem.
IEEE Transactions on Image Processing, 21(1):327–340, 2012.
