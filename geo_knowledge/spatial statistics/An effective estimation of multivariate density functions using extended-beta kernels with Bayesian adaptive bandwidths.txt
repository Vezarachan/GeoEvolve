An effective estimation of multivariate
density functions using extended-beta
kernels with Bayesian adaptive
bandwidths
Sobom M. Som´e*, Célestin C. Kokonendji†
and Francial G.B. Libengu´e Dob´el´e-Kpoka‡
February 11, 2025
Abstract
Multivariate kernel density estimations have received much spate
of interest. In addition to conventional methods of (non-)classical
associated-kernels for (un)bounded densities and bandwidth selec-
tions, the multiple extended-beta kernel (MEBK) estimators with
Bayesian adaptive bandwidths are invested to gain a deeper and bet-
ter insight into the estimation of multivariate density functions. Being
unimodal, the univariate extended-beta smoother has an adaptable
compact support which is suitable for each dataset, always limited.
The support of the density MBEK estimator can be known or es-
timated by extreme values.
Thus, asymptotical properties for the
(non-)normalized estimators are established.
Explicit and general
choices of bandwidths using the flexible Bayesian adaptive method
are provided. Behavioural analyses, specifically undertaken on the
sensitive edges of the estimator support, are studied and compared
to Gaussian and gamma kernel estimators. Finally, simulation stud-
ies and three applications on original and usual real-data sets of the
proposed method yielded very interesting advantages with respect
to its flexibility as well as its universality.
Keywords: Associated-kernel, bounded data, convergence, modified
beta-PERT distribution, prior distribution, support estimation.
*(Corresponding Author) Université Thomas SANKARA, Laboratoire Sciences et
Techniques, 12 BP 417 Ouagadougou 12, Burkina Faso. sobom.some@uts.bf; & Université
Joseph KI-ZERBO, Laboratoire d’Analyse Numérique d’Informatique et de BIOmathé-
matique, 03 BP 7021 Ouagadougou 03, Burkina Faso. sobom.some@univ-ouaga.bf
†Université
Marie
&
Louis
Pasteur,
Laboratoire
de
mathématiques
de
Be-
sançon UMR 6623 CNRS-UMLP, 16 route de Gray, 25030 Besançon Cedex, France.
celestin.kokonendji@univ-fcomte.fr; & Université de Bangui, Laboratoire de mathéma-
tiques et connexes de Bangui, B.P. 908 Bangui, Centrafrique. kokonendji@gmail.com
‡Université de Bangui, Laboratoire de mathématiques et connexes de Bangui, BP 908
Bangui, Centrafrique. libengue@gmail.com
1
arXiv:2502.05366v1  [math.ST]  7 Feb 2025

1
Introduction and motivation
Since the pioneers Rosenblatt (1956) and Parzen (1962), the symmetric ker-
nel density estimations have undergone several evolutions. For instance,
the multivariate case was first considered in a single bandwidth by Cacoul-
los (1966) and with a vector of bandwidth by Epanechnikov (1969) who
provided an optimal symmetric kernel. It has equally been addressed by
Silverman (1986), Scott and Wand (1991), Terrell and Scott (1992), Wand
and Jones (1995), Marshall and Hazelton (2010) and Kang et al. (2018). Ow-
ing to equivalence of symmetric continuous kernels, much ink has been
spilled upon the bandwidth matrix selection; see, e.g., Duin (1976), Duong
and Hazelton (2005) and Zougab et al. (2014). Another path of evolution
concerns the asymmetric kernels for solving the so-called edge effects in
the estimation of densities with bounded support at least on one side; see,
e.g., Chen (1999, 2000b) for univariate beta and gamma kernels, Bouez-
marni and Roumbouts (2010) and Somé et al. (2024) in the corresponding
multivariate contexts. For other asymmetric ones, one can also refer to Jin
and Kawczak (2003), Marchant et al. (2013), Hirukawa and Sakudo (2015),
Funke and Kawka (2015) and Libengué Dobélé-Kpoka and Kokonendji
(2017).
Basically, it is now desirable to unify all these kernels, continuous
(a)symmetric or not with the discrete cases, under the so-called associated-
kernels for estimating (continuous) density or (discrete) probability mass
function f on its d-dimensional support Td ⊆Rd.
In this regard, an
associated-kernel can be defined as follows. Let x = (x1, . . . , xd)⊤∈Td be
the point of estimation of f, and H a (d×d)-symmetric and positive definite
matrices (referred to as bandwidth matrices). A multivariate associated-
kernel function Kx,H(·) parametrized by x and H with support Sx,H ⊆Rd
satisfies the following conditions:
(i) x ∈Sx,H; (ii) E(Zx,H) = x + A(x, H); (iii) Var(Zx,H) = B(x, H).
(1)
The d-dimensional random vector Zx,H has a probability density mass func-
tion (pdmf) Kx,H(·) such that vector A(x, H) →0 and positive definite ma-
trix B(x, H) →0d as H →0d, where 0 and 0d denote the d-dimensional null
vector and the d × d null matrix, respectively. Thus, the usual associated-
kernel estimator bf(x) of f(x) is defined as bf(x) = (1/n) Pn
i=1 Kx,H(Xi) for an
iid-sample (X1, . . . , Xn) from the unknown f. One can refer to Kokonendji
and Somé (2018) for some properties, where a construction and illustra-
tions of the so-called full, Scott and diagonal bandwidth matrices for gen-
eral, multiple and classical associated kernels, are respectively provided.
See also Aboubacar and Kokonendji (2024), Esstafa et al. (2023a,b), Senga
Kiessé and Durrieu (2024) and Somé et al. (2023) for additional properties
and references therein.
Within this framework, there are two common techniques for generat-
2

ing the well-known multivariate associated-kernels satisfying (1):
(i) Kx,H(·) = (det H)−1/2K

H−1/2(x −·)

and (ii) Kx,H(·) =
d
Y
k=1
K[k]
xk,hkk(·), (2)
where K(·) refers to a d-variate (classical) symmetric kernel, K[k]
xk,hkk(·) corre-
spondstoaunivariate(non-classical)associated-kernelwithH = (hjk)j,k=1,...,d,
and det H stands for the determinant of H. The first multivariate kernel
displayed in Part (i) of (2) can be called classical multivariate associated-kernel,
as for a given symmetric kernel K on Sd ⊆Rd, one has Sx,H = x −H1/2Sd,
A(x, H) = 0 and B(x, H) = H1/2ΣH1/2, where Σ is a covariance matrix not
depending on x and H. The second kernel exhibited in Part (ii) of (2) is
commonly labelled as multiple associated-kernels with d-univariate kernels
K[k]
xk,hkk(·) which can be either classical or non-classical. One of the merits of
the multiple associated-kernels with diagonal bandwidth matrices resides
in mixing two or more different sources of the univariate associated-kernels
following the support T[k]
1 ⊆R of the kth component of x. From this per-
spective, exploring univariate setup provides a better understanding and
deeper vizualisations of the problem.
Figure 1: Histogram with its corresponding smoothings of the cholesterol
data on [1, 2] from Table 6 using univariate Gaussian and extended-beta
kernels with both cross-validation and Bayes selectors of bandwidths.
In the current paper and for simplicity in the univariate situation, we
consider the kernel estimating problem of an unknown density function f1
3

on its known or unknown continuous support T1 ⊆R. Among many con-
tinuous associated-kernels depending on the support T1, which of them
is more interesting and versatile for (un)bounded T1 := [a, b] with a < b?
How to estimate the support T1 when it is unknown too? How to select
easily the smoothing parameter? Obviously, we exclude all classical or
symmetrical (associated-)kernels which have been only designed for fully
unbounded T1 = R, but never for compact T1 = [a, b]. For instance, one
can take a look at Figure 1 which may serve as an eye opening indicat-
ing the crux of this problem, mainly at the edges. This problem shall be
subsequently analyzed then solved in Section 6. In this respect, Gaussian
kernel is not suitable for estimating density with compact support, while
extended-beta kernel can work for any support. See also Figure 6. Note
that no datum value is found at infinity. Therefore, the observed extreme
values are always finite which lead to estimating the support T1 through
percentiles. As far as we know, the only solution to this task lies in con-
sidering an extension of the standard beta kernel on the compact support
S1 = [a, b], namely extended-beta kernel of Libengué Dobélé-Kpoka and
Kokonendji (2017). Indeed, we shall demonstrate that it is adaptable to
estimate all density functions on a given support (un)bounded T1 ⊆R
using appropriate handlings of the corresponding extended-beta kernel
estimator at the edges.
The central objective of this work is to provide an effective kernel es-
timation of any multivariate density functions.
We first introduce the
multiple extended-beta kernels in the sense of Part (ii) of (2) highlight-
ing some of its (asymptotical) properties. Next, we set forward explicit
bandwidths through the Bayesian adaptive method before undertaking
numerical illustrations and discussions.
Notably, the rest of the paper
is structured as follows. Section 2 is devoted to the multiple extended-
beta kernel estimators for density functions on compact support. Section
3 foregrounds main asymptotical properties of the corresponding (non-
)normalized estimators. Section 4 presents explicit bandwidths by using
the Bayesian adaptive approach. In Section 5, the practical performances
of our proposed method are investigated by simulation studies. Section
6 is dedicated to three original and classic real-data applications with dis-
cussions. Section 7 concludes the paper with final remarks. Proofs of all
results are summarized and incorporated into Appendix of Section 8.
2
Multiple extended-beta kernel estimators
Let X1, . . . , Xn be iid d-variate random variables with an unknown density f
on its compact support Td := ×d
j=1[aj, bj] for d ≥1 and aj < bj. The multiple
extended-beta kernel (MEBK) estimator bfn of f is defined as
bfn(x) = 1
n
n
X
i=1
d
Y
j=1
EBxj,hj,aj,bj(Xij),
(3)
4

where x = (x1, . . . , xd)⊤∈Td = ×d
j=1[aj, bj], Xi = (Xi1, . . . , Xid)⊤, i = 1, . . . , n,
h = (h1, . . . , hd)⊤is the vector of the bandwidth parameters with hj = hj(n) >
0 and hj →0 as n →∞for j = 1, . . . , d. The function EBx,h,a,b(·) is the
(standard) univariate extended-beta kernel defined on its compact support
Sx,h,a,b = [a, b] = T1 with −∞< a < b < ∞, x ∈T1 and h > 0 such that
EBx,h,a,b(u) =
(u −a)(x−a)/{(b−a)h}(b −u)(b−x)/{(b−a)h}
(b −a)1+1/hB{1 + (x −a)/(b −a)h, 1 + (b −x)/(b −a)h}1[a,b](u),
(4)
where B(r, s) =
R 1
0 tr−1(1 −t)s−1dt is the usual beta function with r > 0, s > 0,
and 1A denotes the indicator function in any given set A.
The MEBK Qd
j=1 EBxj,hj,aj,bj(·) from (3) easily verifies all conditions (1); in
particular, one here has the d-vector
A(x, H) =
 (aj + bj −2xj)hj
1 + 2xj
!⊤
j=1,...,d
(5)
and the diagonal d × d matrix
B(x, H) = Diagd
 {(xj −aj + (bj −aj)hj}{(bj −xj + (bj −aj)hj}hj
(1 + 2hj)2(1 + 3hj)
!
j=1,...,d
(6)
from the univariate case of Libengué Dobélé-Kpoka and Kokonendji (2017).
This extended-beta kernel (4) is the pdf of the extended-beta distribu-
tion on [a, b], which can be denoted by B (c, d) with c := 1+(x−a)/[(b−a)h] >
1 and d := 1+(b−x)/[(b−a)h] > 1 as shape parameters under conditions of
unimodaliy. In this case, its mode and dispersion parameter are indicated
by [(c −1)b + (d −1)a/(c + d −2)] = x and 1/(c + d −2) = h, respectively. This
kernel derives from a reparameterization in target x and tuning parameter
h, called the Mode-Dispersion method, of the unimodal extended-beta dis-
tribution B(c, d) on [a, b] with c > 1 and d > 1; see Libengué Dobélé-Kpoka
and Kokonendji (2017) for further details. Note that it is appropriate for
any compact support of observations. For a = 0 and b = 1, it corresponds
to the standard beta kernel of Chen (1999); see also Bertin and Klutchnikoff
(2014).
Notice that this unimodal extended-beta distribution or kernel EBx,h,a,b
in (4) is also known as the (four-parameter) modified beta-PERT1 distri-
bution from the minimum (a), maximum (b), mode (m := x) and a fourth
parameter (γ := 1/h); see, e.g., Clark (1962), Grubbs (1962) and also Figure
2. The parameter γ := 1/h is the only shape in this context of reparametriza-
tion such that h := 1/γ plays its crucial role in the smoothing parameter or
bandwidth; see Part (b) of Figure 2. Basically, up to the shape parameter
γ, this distribution continuously displays a frequency of values around
the mode that drops softly in both directions, which decreases sharply
from mode to extreme values. Estimations of the support [a, b] (or possible
1Project Evaluation and Review Technique
5

(a)
(b)
Figure 2: Shapes of univariate extended-beta kernels with different targets
x and same smoothing parameter h = 0.2 (a); with same target x = 5 and
different smoothing parameters (b).
ends) through extreme values can be easily performed through the use of
percentiles. This refers to the fact that the parameters a and b are intuitive
since they represent the possible interval of values occurence. One can
refer in this regard to some softwares at the origin of this (modified) beta-
PERT distribution in Operation Research for modeling activities through
probabilistic approach; e.g., VOSE Software (2008).
3
Main properties of MEBK estimators
We need the following technical lemma for some below results and proofs.
Lemma 3.1. The MEBK Qd
j=1 EBxj,hj,aj,bj(·) from (3) to (6) is such that


d
Y
j=1
EBxj,hj,aj,bj


2
2
=
d
Y
j=1
(bj −aj)−1B
 
1 +
2(xj −aj)
(bj −aj)hj
, 1 +
2(bj −xj)
(bj −aj)hj
!
(
B
 
1 +
(xj −aj)
(bj −aj)hj
, 1 +
(bj −xj)
(bj −aj)hj
!)2
.
Furthermore, for some αj > 0 (j = 1, . . . , d) and any x = (x1, . . . , xd)⊤∈Td =
×d
j=1[aj, bj], one has:
||A(x, H)||2 = O


d
Y
j=1
h
αj
j

, trace[B(x, H)] = O


d
Y
j=1
h
2αj
j

,
6

max
y∈Td EBxj,hj,aj,bj(y) = O


d
Y
j=1
h
−αj
j

and


d
Y
j=1
EBxj,hj,aj,bj


2
2
= O


d
Y
j=1
h
−αj
j

.
The next two assumptions are required afterwards for asymptotic prop-
erties of the MEBK estimator in (3).
(a1) The unknown pdf f belongs to the class C 2(Td) of twice continuously
differentiable functions.
(a2) When n →∞, then hj := hj(n) →0, j = 1, . . . , d and n−1
d
Y
j=1
h−1/2
j
→0.
Proposition 3.2. Under assumption (a1) on f, the MEBK estimator bfn in (3) of
f satisfies
Bias
n bfn(x)
o
=
d
X
j=1
(aj + bj −2xj)hj
1 + 2hj
∂f(x)
∂xj
+ 1
2
d
X
j=1

"(aj + bj −2xj)hj
1 + 2hj
#2
+
{xj −aj + (bj −aj)hj}{aj −xj + (bj −aj)hj}
(1 + 2hj)2(1 + 3hj)
) ∂2 f(x)
∂x2
j
+ o


d
X
j=1
h2
j


for any x = (x1, . . . , xd)⊤∈Td. Moreover, if (a2) holds, then the variance is such
that
Var
n bfn(x)
o
= 1
n f(x)


d
Y
j=1
EBxj,hj,aj,bj


2
2
+ o


1
n
d
Y
j=1
h−1/2
j

.
According to some general limit results in Esstafa et al. (2023b), Koko-
nendji and Libengué Dobélé-Kpoka (2018) as well as Kokonendji and Somé
(2018) for continuous associated-kernel estimators, MEBK estimator in (3)
is pointwisely consistent and we also have its asymptotic normality.
Proposition 3.3. If the sequences

hj(n)

n≥1 are selected such that nh
αj
j (n) −→
n→∞∞
with αj ≥1, for j = 1, . . . , d; then, for any x ∈Td,
bfn(x)
L2, a.s.
−−−−→
n→∞
f(x),
where “
L2, a.s.
−→” stands for both “mean square and almost surely convergences”.
Furthermore, if

hj(n)

n≥1 are also chosen such that √nh
(3/2)αj
j
(n) −→
n→∞0, for
j = 1, . . . , d; then, for any x ∈Td,
√
n


d
Y
j=1
h
(1/2)αj
j
(n)


 bfn(x) −f(x)

L
−−−→
n→∞N(0, f(x)λx,α),
7

where “
L
−→” and N(0, τ) indicate “convergence in law” and centered normal
distribution with variance τ > 0, respectively, and
λx,α = lim
n→∞


d
Y
j=1
h
αj
j (n)




d
Y
j=1
EBxj,hj(n),aj,bj


2
2
.
(7)
In what follows, according to the practical use and for given band-
widths, we always consider the standard normalized MEBK estimator efn
of f defined from the unnormalized one bfn of (3) as follows:
efn(x) :=
bfn(x)
Cn
with
Cn :=
Z
Td
bfn(x) dx > 0.
(8)
One can see in Libengué Dobélé-Kpoka and Kokonendji (2017) in addition
to Kokonendji and Somé (2018) certain numerical results suggesting that
the normalizing random variable Cn fails to be equal to 1 and is always
around 1.
Proposition 3.4. Under assumptions of Proposition 3.3, then Cn
L2
−−−→
n→∞1.
Now, we deduce that efn(x) is weakly consistent and also asymptotically
normal.
Proposition 3.5. Under the same assumptions of Proposition 3.3, one has
efn(x)
P
−−−→
n→∞f(x),
where “
P
−→” stands for “convergence in probability”. Moreover, one also has
√
n


d
Y
j=1
h
(1/2)αj
j
(n)


 efn(x) −f(x)

L
−−−→
n→∞N(0, f(x)λx,α)
with the same λx,α provided in (7).
Finally, since Td is a compact set, the next proposition holds that efn
globally outperforms bfn, in the sense of L2 criterion.
Proposition 3.6. Under assumptions of Proposition 3.3, for any ε > 0, there
exists N ∈N such that for all n ≥N,
E
"Z
Td
 efn(x) −f(x)

2
dx
#
< E
"Z
Td
 bfn(x) −f(x)

2
dx
#
+ ε.
8

4
Bayesian adaptive bandwidth selector
In this section, we first provide the Bayesian adaptive approach in order to
select the variable smoothing parameter, suitable for the MEBK estimator
(3) of an unknown pdf f. In this vein, we set forward two remarks including
one on the nonparametric support estimation of f.
For a d-variate random sample X1, . . . , Xn drawn from f, we assign a
random variable bandwidth vector hi =

hij
⊤
j=1,...,d with a prior distribution
π(·) to each observation Xi:
bfn(x) = 1
n
n
X
i=1
d
Y
j=1
EBxj,hij,aj,bj(Xij).
(9)
Accordingly, we start with the formula for the leave-one-out kernel estima-
tor of f(Xi) deduced from (9) and built up considering the sample without
the i-th observation,
bf−i(Xi | hi) =
1
n −1
n
X
j=1,j,i
d
Y
ℓ=1
EBXiℓ,hiℓ,aℓ,bℓ(Xjℓ).
(10)
From the Bayesian rule, it follows that the posterior distribution for each
variable bandwidth vector hi given Xi is expressed in terms of
π(hi | Xi) =
bf−i(Xi | hi)π(hi)
R
Td
bf−i(Xi | hi)π(hi)dhi
.
(11)
The Bayesian estimatorehi of hi is obtained through the usual quadratic loss
function as
ehi = E (hi | Xi) = (E(hi1 | Xi), . . . , E(hid | Xi))⊤.
(12)
In what follows, we assume that each component hiℓ= hiℓ(n), ℓ=
1, . . . , d, of hi has the univariate inverse-gamma prior IG(α, βℓ) distribution
with the same shape parameters α > 0 and, eventually, different scale
parameters βℓ> 0 such that β = (β1, . . . , βd)⊤. We recall that the pdf of
IG(α, βℓ) with α, βℓ> 0 is determined by
IGα,βℓ(u) =
βα
ℓ
Γ(α)u−α−1 exp(−βℓ/u)1(0,∞)(u), ℓ= 1, . . . , d,
(13)
where Γ(·) denotes the usual gamma function. The mean and the variance
of the prior distribution (13) for each component hiℓof the vector hi are
identified by βℓ/(α−1) for α > 1, and β2
ℓ/(α−1)2(α−2) for α > 2, respectively.
Note that for a fixed βℓ> 0, ℓ= 1, . . . , d, and if α →∞, then the distribution
of the bandwidth vector hi is concentrated around the null vector 0; easy
to see for d = 1.
The consideration of the inverse-gamma prior distribution is legitimate
since, for instance, we explicitely get the corresponding posterior distribu-
tion in the following theorem.
9

Theorem 4.1. For a fixed i ∈{1, 2, . . . , n}, consider each observation Xi =
(Xi1, . . . , Xid)⊤with its corresponding vector hi = (hi1, . . . , hid)⊤of univari-
ate bandwidths and defining the subsets Iiak = {k ∈{1, . . . , d}; Xik = ak}, Iibs =
{s ∈{1, . . . , d} ; Xis = bs}andtheircomplementarysetIc
i = {ℓ∈{1, . . . , d} ; Xiℓ∈(aℓ, bℓ)}.
Using the inverse-gamma prior IGα,βℓof (13) for each component hiℓof hi in the
MEBK estimator (9) with α > 3/2 and β = (β1, . . . , βd)⊤∈(0, ∞)d as n →∞,
then:
(i) the posterior density (11) has the behavior of a weighted sum of inverse-
gamma
π(hi | Xi)
=
1
Di(α, β)
n
X
j=1,j,i


Y
k∈Iiak
[Fjk(α, βk) IGα,Ejk(βk)(hik) + Hjk(α, βk) IGα+1,Ejk(βk)(hik)]


×


Y
ℓ∈Ic
i
[Aijℓ(α, βℓ) IGα+1/2,Bijℓ(α,βℓ)(hiℓ) + Cijℓ(α, βℓ) IGα−1/2,Bijℓ(α,βℓ)(hiℓ)]


×


Y
k∈Iibs
[Jjs(α, βs) IGα,Gjs(βs)(his) + Kjs(α, βs) IGα+1,Gjs(βk)(his)]


with
Aijℓ(α, βℓ)
=
{2π(Xiℓ−aℓ)(bℓ−Xiℓ)}−1/2βα
ℓ[Bijℓ(α, βℓ)]−α−1/2Γ(α + 1/2),
Bijℓ(α, βℓ)
=
βℓ−(bℓ−aℓ)−1{(Xiℓ−aℓ) log[(Xjℓ−aℓ)/(Xiℓ−aℓ)]
+(bℓ−Xiℓ) log[(bℓ−Xjℓ)/(bℓ−Xiℓ)]},
Cijℓ(α, βℓ)
=
{2π(Xiℓ−aℓ)(bℓ−Xiℓ)}−1/2βα
ℓ[Bijℓ(α, βℓ)]−α+1/2Γ(α −1/2),
Ejk(βk)
=
βk −log[(bk −Xjk)/(bk −ak)],
Fjk(α, βk)
=
βα
k(bk −ak)−1Γ(α)[Ejk(βk)]−α,
Gjs(βs)
=
βs −log[(Xjs −as)/(bs −as)],
Hjk(α, βk)
=
βα
k(bk −ak)−1Γ(α + 1)E−α−1
jk
(βk),
Jjs(α, βs)
=
βα
s (bs −as)−1Γ(α)G−α
js (βs),
Kjs(α, βs)
=
βα
s (bs −as)−1Γ(α + 1)[Gjs(βs)]−α−1
and
Di(α, β)
=
n
X
j=1,j,i


Y
k∈Iiak
n
Fjk(α, βk) + Hjk(α, βk)
o




Y
ℓ∈Ic
i
n
Aijℓ(α, βℓ) + Cijℓ(α, βℓ)
o


×


Y
k∈Iibs
n
Jjs(α, βs) + Kjs(α, βs)
o

;
(ii)underthequadraticlossfunction, theBayesianestimatorehi =
ehi1, . . . ,ehid
⊤
10

of hi, introduced in (12), is
ehim =
1
Di(α, β)
n
X
j=1,j,i


Y
k∈Iiak
[Fjk(α, βk) + Hjk(α, βk)]


×


Y
ℓ∈Ic
i
[Aijℓ(α, βℓ) + Cijℓ(α, βℓ)]




Y
k∈Iibs
[Jjs(α, βs) + Kjs(α, βs)]


×
( Fjm(α, βm)
α −1
+
Hjm(α, βm)
α
!
Ejm(βm)
Fjm(α, βm) + Hjm(α, βm)1{am}(Xim)
+
 Aijm(α, βm)
α −1/2
+
Cijm(α, βm)
α −3/2
!
Bijm(α, βm)
Aijm(α, βm) + Cijm(α, βm)1(am,bm)(Xim)
+
 Jjm(α, βm)
α −1
+
Kjm(α, βm)
α
!
Gijm(βm)
Jjm(α, βm) + Kjm(α, βm)1{bm}(Xim)
)
, (14)
for
m = 1, 2, . . . , d, with the previous notations of Aijℓ(α, βℓ), Bijm(α, βm),
Cijℓ(α, βℓ), Ejk(βk), Fjm(α, βm), Gijm(βm), Hjk(α, βk), Jjm(α, βm) and Kjs(α, βs).
Remark 4.2. The implementation of the Bayesian adaptive approach with
inverse-gammapriorsrequiresthechoiceofparametersα andβ = (β1, . . . , βd)⊤.
We recall that the optimal bandwidth that minimizes the mean integrated
squared error is of O

n−2/5
order for the univariate extended-beta kernel;
go back to Libengué Dobélé-Kpoka and Kokonendji (2017) for further de-
tails. Thus, departing from the first and second moments of the prior (13),
we need to consider α = αn = n2/5 > 2 and βℓ= 1 for all ℓ= 1, 2, . . . , d
to ensure the convergence of the variable bandwidths to zero and also in
practice; see also Tables 1 and 2 of sensitivity analysis. .
Although these previous choices in Remark 4.2 do not generate nec-
essarily the best smoothing quality; it would be possible to modify them
slightly in practice in order to enhance the smoothing. See below Figure
3 depicting a sensitivity analysis and certain numerical illustrations; see
equally Somé and Kokonendji (2022) for gamma kernels.
When the compact support Td of the unknown d-variate pdf f will be
estimated from its random sample X1, . . . , Xn, we can consider the simple
and intuitive nonparametric support estimator bTd which has been intro-
duced by Devroye and Wise (1980) as unions of the closed Euclidian balls
centered at Xi = (Xi1, . . . , Xid)⊤, i = 1, . . . , n, with common radius bn which
is a global smoothing parameter. For a review along with some properties,
we refer the reader, for instance, to Baíllo et al. (2000) and Biau et al. (2009).
Remark 4.3. Within the framework of the adaptive vector bandwidths
hi(n) = (hi1(n), . . . , hid(n))⊤, we suggest to use in practice the following bTd
defined as
bTd :=
n
[
i=1

×d
j=1[Xij ± hij(n)]

= ×d
j=1[X(1)j −h(1)j(n), X(n)j + h(n)j(n)],
11

where X(1)j := min{Xij; i = 1, . . . , n}, X(n)j := max{Xij; i = 1, . . . , n} and h(k)j(n)
represents the corresponding adaptive bandwidth of X(k)j for k = 1, n and
j = 1, . . . , d.
5
Simulation results
This section is dedicated to the numerical findings of simulation studies
designed to assess the performance of the suggested approach so as to cap-
ture the exact shape of unknown densities. Notably, we focus on Bayesian
adaptive bandwidth selection for MEBK estimators. The computations
were carried out on a PC equipped with a 2.30 GHz processor using the R
software (R Core Team, 2023).
These simulation studies aim to achieve three primary objectives re-
lated to unbiased cross-validation (UCV) method. Firstly, we explore the
capability of our elaborated method, as described in (8) and (14) with The-
orem 4.1, to generate accurate estimates, as denoted in Equation (9), of
unknown true densities on the given domain Td = ×d
j=1[aj, bj] or on an
estimated one bTd, for d ≥1 and aj < bj < ∞; see also Remark 4.3. Secondly,
we assess the sensitivity of the proposed method, in univariate case, con-
cerning varying sample sizes n and prior parameters α and β. Lastly, we
compare the computational time needed for both UCV and our Bayesian
adaptive procedure.
In fact, we obtain the optimal bandwidth vector hucv of h in Equation
(3) through UCV as follows:
hucv = arg min
h∈(0,∞)d UCV(h),
where
UCV(h) =
Z
×d
j=1[aj,bj]
n bfn(x)
o2
dx −2
n
n
X
i=1
bfn,h,−i(Xi),
with
bfn,h,−i(Xi) =
1
n −1
n
X
ℓ=1,ℓ,i
d
Y
j=1
EBXij,hj(Xℓj),
being computed as bfn(Xi) of Equation (3) by excluding the observation Xi.
The efficiency of the smoothers shall be examined through the empirical
estimate d
ISE of the integrated squared errors (ISE):
ISE :=
Z
×d
j=1[aj,bj]
n efn(x) −f(x)
o2 dx,
where efn stands for the normalized kernel estimators (8) and (3) with UCV
method or the variable multivariate one efn of Equations (8) and (9) for
12

our adaptive Bayesian method.
All d
ISE values are computed with the
number of replications N = 100. The results of these different simulations
are reported in Tables 1-7 and Figures 3-4 for uni-/multivariate cases (d =
1, 2, 3 and 5).
5.1
Univariate case and sensitivity analysis
At this stage, we consider four scenarios denoted A, B, C and D to simulate
bounded datasets with respect to the support of univariate extended-beta
kernel (i.e. Sx,h = [a, b] = T with a < b). These scenarios have also been
selected to compare the performances of smoothers (8) with (3) and (9)
with regard to convex, unimodal, multimodal or U-shape distributions.
• Scenario A is generated by an asymmetric bell-shaped beta distribu-
tion
fA(x) = (x −1)4 (5 −x)0
1024B(5, 1)
1[1,5](x);
• Scenario B is a PERT (or extended-beta) distribution
fB(x) = (x −1) (5 −x)3
1024B(2, 4) 1[1,5](x);
• Scenario C is a mixture of two PERT distributions
fC(x) =
 
3
5
(x −1)6/11 (5 −x)38/11
B(17/11, 49/11) (11)4 + 2
5
(x −1)36/11 (5 −x)8/11
B(47/11, 19/11) (11)4
!
1[−2,9](x);
• Scenario D derives from logit-normal distribution with a U-shape
fD(x) =
1
3
√
2π
1
x(1 −x) exp
 
−

log

x
1 −x

−0.25
2,
18
!
1(0,1)(x).
First, asensitivityanalysis oftheperformancesoftheunivariateextended-
beta smoother (3) is conducted using the prior hyperparameters α = αn =
n2/5 and βj > 0, ∈1, . . . , d. Proceeding in the same way as the previously
stated procedure, these choices are expected to ensure the convergence of
the adaptive Bayesian tuning parameters to zero. Figure 3 sheds light on
the behavior of β 7→ISE(β) on Scenario B for six values β ∈{0.5, 1, 2, 5, 10, 20}
with sample size n = 500. We observe that α = 5002/5 = 12.0112 with β < 1
lead nearly to the minimum values of d
ISE. Additionally, increasing values
of β imply lower convergences to zero of the estimated ISEs. Furthermore,
Table 1 reveals that the ISE values become smaller when the sample sizes in-
crease for a given α ∈{2, 5, 10, 15, 20, 30}, β ∈{0.1, 0.25, 0.5, 1, 2, 5} and sam-
ple sizes n = 100, 300 and 500. As plotted in Figure 3, the lower values of
β < 1 display a very satisfactory smoothing quality with α = n2/5 = 6.3095,
8.3255 and 12.0112 for n = 100, 300 and 500, respectively. These choices
13

Figure 3: Plots of the ISE using extended-beta smoother (3) in Scenario B
with n = 500 and for different values of α and βℓ= β of prior distribution
(13).
are not necessarily the optimum ones to obtain the best smoothing qual-
ity. Finally, Table 2 reveals all these behaviours for all scenarios A, B, C,
D, α ∈{2, 5, 10, 15, 20, 30}, β ∈{0.25, 0.5, 1, 2, 5} and only for a larger sam-
ple size n = 500. Moreover, smoothing quality consistently increases for
uni-/bi-modal scenarios compared to convex distributions, and even excels
further when contrasted with U-shape ones.
Next, Table 3 illustrates the time required for computing both band-
width selection methods across single replications of sample sizes n =
10, 25, 50, 100, 200, 500 and 1000 for the target function A. The findings dis-
tinctly highlight the superiority of the Bayesian adaptive approach over
the UCV method. Furthermore, the contrast in Central Processing Unit
(CPU) times grows more pronounced with rising sample sizes.
Figure 4 depicts the true density as well as the smoothing densities
for both UCV and Bayesian adaptive bandwidth selectors with extended-
beta kernel estimators related to the considered models, and for only one
replication. Basically, the performances are quite similar exhibiting the
same difficulties in capturing the excat shape for convex distributions (e.g.,
Part A1–A2 of Figure 4).
Table 4 displays some expected values of d
ISE with respect to the four
Scenarios A, B, C, and D, and according to the sample sizes n = 20, 50,
100, 200, 500. Thus, we infer the following behaviors for both UCV and
Bayesian tuning methods. As expected, the smoothings get better as the
14

Table 1: Expected values (×103) and their standard deviations in parenthe-
ses of d
ISE with N = 100 replications for different values of α and βℓ= β in
Scenario B.
βℓ= β
α
100
200
500
0.1
2
18.7287 (6.3474)
18.3059 (3.8405)
17.2210 (2.7223)
5
9.1042 (4.8952)
4.2784 (2.5375)
3.7654 (1.9544)
10
14.4691 (8.5864)
4.8034 (2.5289)
2.8789 (1.4144)
15
18.6310 (8.3888)
4.6785 (3.1343)
4.0605 (2.0708)
20
17.2893 (7.8134)
6.1571 (2.7281)
4.0990 (2.0571)
30
26.8312 (12.1096)
8.4850 (2.8171)
5.4464 (2.3582)
0.25
2
43.1767 (4.3814)
43.1625 (2.7282)
43.4670 (2.6271)
5
12.1927 (6.2350)
10.5988 (3.4377)
9.2283 (2.4320)
10
9.1671 (5.2336)
5.1064 (3.9126)
3.8825 (1.9345)
15
11.8874 (7.6368)
4.1319 (2.2354)
3.4151 (1.9849)
20
13.1828 (7.7800)
4.3724 (2.4961)
2.9817 (1.8935)
30
15.3643 (7.1915)
5.5363 (2.7498)
3.1472 (1.7428)
0.5
2
74.97054 (4.4021)
74.7534 (1.9998)
74.3326 (1.8412)
5
24.3335 (6.5485)
22.2138 (3.7373)
22.0404 (2.4209)
10
11.6635 (6.8541)
9.3185 (3.0857)
8.4889 (2.2937)
15
9.2364 (5.6802)
6.1882 (3.2737)
4.4179 (1.6962)
20
9.7162 (7.1223)
5.7172 (3.5299)
3.6523 (1.8512)
30
10.1732 (5.8679)
4.2829 (2.3619)
4.1718 (2.0531)
1
2
105.1241 (3.5493)
105.4775 (2.0350)
105.2314 (1.5433)
5
43.0005 (4.9704)
41.8417 (2.6120 )
41.9609 (2.6125)
10
21.8739 (5.9657)
20.3022 (3.2174)
19.7279 (2.6202)
15
15.5373 (7.9574)
12.5755 (3.7075)
11.7732 (2.7833)
20
11.4649 (5.4483)
9.1578 (3.6119)
9.1415 (2.9671)
30
7.8978 (5.4276)
5.9386 (2.5145)
5.1303 (2.1371)
2
2
126.1075 (2.2575)
126.3647 (1.2414)
126.1581 (0.9926)
5
69.0168 (4.5467)
69.3718 (2.5288)
68.6149 (2.0435)
10
39.7711 (4.5098)
38.8178 (2.7311)
38.0123 (1.9684)
15
28.4291 (5.9826)
25.1541 (2.9731)
26.5472 (2.6282)
20
22.1367 (7.4000)
19.4300 (3.5791)
18.6882 (2.7722)
30
14.8064 (6.0704)
12.4760 (4.0745)
11.7934 (2.7008)
5
2
139.1179 (0.9074)
139.1025 (0.5417)
139.1103 (0.3637)
5
104.9637 (3.5356)
104.9425 (1.5118)
104.7899 (1.3307)
10
74.2584 (5.0845)
73.8912 (2.1682)
73.2654 (1.6401)
15
55.7195 (4.8940)
55.9576 (2.1805)
55.7695 (2.0331)
20
44.9449 (4.8001)
44.7349 (2.9943)
44.8518 (2.2091)
30
32.0037 (5.5659)
31.6434 (2.4360)
31.8495 (2.6716)
15

Table 2: Expected values (×103) and their standard deviations in parenthe-
ses of d
ISE with n = 500, N = 100 replications for different values of α and
βℓ= β in Scenarios A, B, C and D.
βℓ= β
α
d
ISEA
d
ISEB
d
ISEC
d
ISED
0.25
2
63.8438 (7.2056)
43.4670 (2.6271)
16.3635 (0.3782)
383.5863 (22.2413)
5
9.0693 (4.2419)
9.2283 (2.4320)
5.8430 (0.6635)
162.8053 (19.0522)
10
3.7929 (2.6865)
3.8825 (1.9345)
2.5855 (0.9735)
119.3822 (15.2558)
15
3.7638 (2.9230)
3.4151 (1.9849)
1.7976 (0.7535)
130.4549 (14.7381)
20
4.1106 (3.2985)
2.9817 (1.8935)
1.3575 (0.5927)
125.3927 (13.6744)
30
5.1270 (3.1515)
3.1472 (1.7428)
1.3943 (0.5857)
158.7471 (28.6446)
0.5
2
152.7892 (6.8013)
74.3326 (1.8412)
20.3976 (0.2804)
514.8053 (11.2818)
5
22.7191 (5.3033)
22.0404 (2.4209)
10.6514 (0.6107)
259.3906 (25.4461)
10
9.5054 (4.7427)
8.4889 (2.2937)
5.1302 (0.8876)
164.6618 (17.3598)
15
4.7907 (3.5728)
4.4179 (1.6962)
3.3448 (0.7940)
133.3951 (18.4147)
20
4.4400 (3.7643)
3.6523 (1.8512)
2.3837 (0.7789)
121.0451 (13.6062)
30
4.0428 (2.4452)
4.1718 (2.0531)
1.5003 (0.6352)
117.2256 (12.0876)
1
2
261.2879 (5.3160)
105.2314 (1.5433)
22.3939 (0.2219)
584.8077 (3.6770)
5
62.9280 (6.5670)
41.9609 (2.6125)
15.9701 (0.4509)
356.9220 (21.3331)
10
21.7644 (4.6890)
19.7279 (2.6202)
9.8481 (0.5760)
240.7989 (21.0377)
15
10.6315 (4.2699)
11.7732 (2.7833)
6.8407 (0.6573)
186.2314 (23.3280)
20
7.1220 (3.9561)
9.1415 (2.9671)
5.1353 (0.9249)
160.6313 (19.1270)
30
5.1178 (3.0613)
5.1303 (2.1371)
3.3004 (0.7536)
135.4001 (18.6294)
2
2
346.9498 (2.5905)
126.1581 (0.9926)
23.6747 (0.1220)
610.7817 (1.6115)
5
132.8912 (6.1537)
68.6149 (2.0435)
19.6494 (0.2765)
463.8578 (13.3935)
10
56.3239 (7.7844)
38.0123 (1.9684)
15.0934 (0.3721)
332.8535 (21.5032)
15
29.4171 (6.9545)
26.5472 (2.6282)
11.9898 (0.5531)
271.1553 (23.7918)
20
18.8908 (5.1843)
18.6882 (2.7722)
9.6315 (0.5236)
228.5061 (22.3149)
30
10.4811 (3.6480)
11.7934 (2.7008)
6.5363 (0.7434)
183.3435 (18.9414)
5
2
406.5433 (1.1392)
139.1103 (0.3637)
24.5520 (10.0483)
621.7521 (0.6549)
5
258.0829 (4.8576)
104.7899 (1.3307)
22.2954 (0.1869)
567.3611 (17.2918)
10
145.1572 (8.1281)
73.2654 (1.6401)
19.9091 (0.2749)
516.9667 (12.2717)
15
92.4672 (5.5989)
55.7695 (2.0331)
18.0590 (0.3550)
448.6337 (21.6033)
20
65.6836 (7.4113)
44.8518 (2.2091)
16.2298 (0.3685)
392.0456 (18.8470)
30
36.1508 (6.3353)
31.8495 (2.6716)
13.4353 (0.5259)
333.2493 (24.2946)
Table 3: Typical CPU times (in seconds) of both UCV and Bayesian adaptive
bandwidth selections (14) using one replication of Scenario B.
n
tUCV
tBayes
10
11.97
0.00
25
30.06
0.03
50
59.71
0.03
100
122.22
0.14
200
261.85
0.38
500
710.67
3.49
1000
1619.75
10.93
16

(A1)
(A2)
(B1)
(B2)
(C1)
(C2)
(D1)
(D2)
Figure 4: True pdf and its corresponding smoothings using extended-beta
kernels with UCV and Bayesian adaptive bandwidths for Scenarios A, B,
C and D with n = 50 (left); n = 200 (right).
17

sample sizes increase. The Bayesian approach is clearly better than the
UCV one for all samples size and all considered models. The difference of
smoothing quality is smaller for larger sample sizes (i.e. n = 500 and 1000).
Both methods seem to have difficulties in terms of smoothing enough
convex distributions and even more U-shape ones; e.g., Parts A and D of
Table 4.
Table 4: Expected values (×103) and their standard deviations in paren-
theses of d
ISE with N = 100 replications using global UCV and Bayesian
adaptive bandwidths (with prior parameters α = n2/5 > 2 and β1 = 1/4) for
all univariate Scenarios A, B, C and D.
n
d
ISEUCV
d
ISEBayes
A
10
115.8009 (119.8057)
91.5578 (65.4830)
20
82.0050 (86.2389)
78.2823 (86.2389)
50
28.8678 (26.6640)
16.6236 (13.8403)
100
14.4373 (14.2570)
11.3620 (10.7994)
200
10.7450 (9.9227)
8.5159 (9.1898)
500
4.1750 (2.5537)
4.3024 (3.6539)
1000
3.0077 (2.2489)
2.5830 (2.0727)
B
10
131.5619 (142.1452)
51.1909 (51.2114)
20
57.9147 (47.2916)
35.3433 (31.5520)
50
29.9359 (24.0753)
15.2518 (13.3836)
100
17.0892 (12.3043)
14.1387 (12.5297)
200
9.1671 (7.0321)
6.7505 (6.1550)
500
4.5435(3.8951)
4.3132(4.2758)
1000
1.9028(1.1876)
1.9655(1.2394)
C
10
24.24109 (15.9713)
16.93086 (5.3865)
20
18.0597 (11.4405)
12.2576 (4.3113)
50
9.3314 (5.5185)
7.8745 (3.1088)
100
6.4595 (2.6487)
5.4703 (2.2149)
200
2.9873 (1.2543)
3.6521 (1.3319)
500
1.3891 (0.6393)
2.0440 (0.7081)
1000
0.8519 (0.3368)
1.2272 (0.4151)
D
10
883.8449 (915.7151)
419.7151 (166.3445)
20
524.7636 (391.8950)
296.8862 (125.4970)
50
355.0219 (174.9401)
213.1530 (67.6651)
100
264.1031 (117.9244)
182.3334 (55.0430)
200
188.1498 (55.2339)
146.2614 (36.7259)
500
155.5031 (26.2496)
118.4205 (14.6518)
1000
141.0610 (14.7649)
113.1771 (6.8750)
5.2
Multivariate cases
In order to investigate simulations in dimensions beyond or equal to d ≥2,
we analyze five test pdfs corresponding to Scenarios E, F, G, H, and I. These
18

pdfs are examined for d = 2, 3, and 5, incorporating both independent and
correlated margins, respectively.
• Scenario E is generated using the bivariate PERT distribution
fE(x1, x2) = (x1 −1) (5 −x1)3
1024B(2, 4)
× (x2 −1) (5 −x2)3
1024B(2, 4)
1[1,5]2(x1, x2);
• Scenario F is a mixture of three bivariate normal pdfs with a negative
correlation structure (from Duong and Hazelton , 2005)
fF (x1, x2) = 4
11N2
 µ2, Σ2
 + 3
11N2
 µ3, Σ3
 + 4
11N2
 µ4, Σ4
 ,
withµ2 = (5, 9)⊤, µ3 = (7, 7)⊤, µ4 = (9, 5)⊤, Σ2 =
 
1
0
0
1
!
, Σ3 =
 
0.80
−0.72
−0.72
0.80
!
and Σ4 =
 
1
0
0
1
!
;
• Scenario G is the truncated bivariate normal distribution denoted
T N2
 µ5, Σ5
 on truncated domain [−1, 5] × [−1, 5]
fG (x1, x2) = T N2
 µ5, Σ5
 ,
with mean vector µ5 = (0, 0)T and covariance matrix Σ5 =
 
1
0.8
0.8
1
!
;
• Scenario H is a trivariate mixture of two PERT distributions
fH(x1, x2, x3)
=
3
Y
ℓ=1
 
3
5
(xℓ−1)6/11 (5 −xℓ)38/11
B(17/11, 49/11) (11)4 + 2
5
(xℓ−1)36/11 (5 −xℓ)8/11
B(47/11, 19/11) (11)4
!
× 1[−2,9]3(x1, x2, x3);
• Scenario I is a 5-variate extended-beta distribution
fI(x1, x2, x3, x4, x5) =
5
Y
ℓ=1
 (xℓ−1) (5 −xℓ)3
1024B(2, 4)
!
1[1,5]5(x1, x2, x3, x4, x5).
Table 5 portrays the smoothing investigation within the multivariate
framework (for dimensions d = 2, 3, and 5), focusing on densities E, F, G,
H, and I. We explore sample sizes of n = 20, 50, 100, 200, 500, and 1000 for
each model. Additionally, for dimension d = 5, we introduce sample sizes
n ∈2000, 5000, as n = 1000 can be considered in this context moderate or
small. Across all sample sizes, ISE values display a very acceptable degree
of smoothing.
19

Table 5: Expected values (×103) and their standard deviations in paren-
theses of d
ISE with N = 100 replications using global UCV and Bayesian
adaptive bandwidths (with prior parameters α = n2/5 > 2 and β1 = 1/4) for
all multivariate Scenarios E, F, G, H and I.
n
d
ISEBayes
E
20
26.2165 (10.9673)
50
16.8885 (7.9130)
100
11.2012 (4.2758)
200
7.4936 (3.3838)
500
4.1682 (1.2423)
F
20
17.9456 (2.4143)
50
14.1387 (1.2529)
100
12.0115 (1.0830)
200
9.0897(1.3408)
500
7.4302(0.8877)
G
20
23.7323 (9.2013)
50
14.6309 (4.6107)
100
9.6835 (3.7420)
200
7.3814 (2.7526)
500
3.9949 (1.2094)
H
20
2.9588 (0.4699)
50
2.8549 (0.3186)
100
2.6787 (0.2648)
200
2.5187 (0.2028)
500
2.4163 (0.1262)
1000
2.1639 (0.1210)
I
20
6.4316 (1.3573)
50
5.7605 (1.3839)
100
5.0563 (1.0231)
200
4.5133 (0.7663)
500
3.7025 (0.3025)
1000
2.0125 (0.1256)
2000
1.2398 (0.0502)
5000
1.0873 (0.0372)
6
Illustrative applications
The performance of the nonparametric method introduced in this study
is assessed through three distinct examples featuring (new for) univariate,
(usual for) bivariate, and (original for) trivariate real datasets. Specifically,
we invest multivariate extended-beta kernel estimators (3) with Bayesian
adaptive bandwidths (14) or UCV to smooth the joint distributions. The
goodness-of-fit of these estimators is sometimes contrasted with those us-
ing a gamma kernel with Bayesian adaptive bandwidth, as reported by
Somé et al. (2024), as well as a Gaussian smoother with plug-in tuning
20

parameters, as clarified in Duong (2007) and Duong et al. (2023). Unless
explicitly stated otherwise, the prior model parameters for bandwidths are
consistently set to α = n2/5 and βℓ= 0.1 for all ℓ= 1, . . . , d for the extended-
beta kernel, and α = n2/5 and βℓ= 1 for all ℓ= 1, . . . , d for the gamma
kernel estimation. We recall that the previous opted for parameters are not
necessarily the optimal ones; see, e.g., resuts of the sentivity analysis sum-
marized in Figure 3 and Table 2 for the extended-beta kernel and, Figure 1
and Table 1 of Somé and Kokonendji (2022) for the gamma kernel.
In addition to graphical comparisons of smoothings, a numerical pro-
cedure, following the approach of Filippone and Sanguinetti (2011), is
applied for our three illustrative examples. Initially, subsets of size mn
are sampled for each sample size n: mn ∈15, 30, 50, 70 for the first uni-
variate dataset with n = 82, mn ∈20, 30, 40, 50 for the second univariate
dataset with n = 86, mn ∈100, 150, 200, 250 for the bivariate dataset with
n = 272, and mn ∈100, 250, 400, 500 for the trivariate dataset with n = 590.
Subsequently, the remaining data are used to compute the average log-
likelihood. This process is repeated 100 times for each mn; see also Somé
et al. (2024).
The three univariate illustrative examples first concern total choles-
terol levels for n = 82 age groups of 1986 individuals sampled during
a period of 12 months in the year 2020 at the “Laboratoire National de
Biologie Clinique et Santé Publique (LBCSP)” in Bangui (Central African
Republic). This is to obtain the overall trend, regardless of age. Tables 6
and 7 provide a descriptive summary of data, with skewness (CS), among
other indicators. These nonnegative data are left-skewed within the range
[1, 2] and suggest the use of asymmetric kernels and in particular the
extended-beta smoother. Figure 5 depicts the histogram and the smoothed
distributions on data support T1 = [1, 2] and an estimated extension of T1
(i.e. bT1 = [0.95, 2.05]), using extended-beta kernels, namely the solid line
for Bayesian adaptive selector of tuning parameter and dashed line for
cross-validation one; consult also Remark 4.3. An estimated support from
the range of data proves to be extremely useful as cholesterol data may
have a slightly bigger range in other samples or regions. We record nearly
identical performances with regard to both methods (UCV and Bayesian
adaptive), with a significant impact observed on the smoothing at the
edges owing to extension bT1 = [0.95, 2.05]. Thus, the estimated support
can be crucial in terms of capturing the exact shape of the density. These
previous statements are put into evidence by the numerical results of the
cross-validated average log-likelihood method displayed in Table 8.
The other univariate examples refer to students’ marks in [0, 20] for
’Introduction to Practical Work’ (X1), ’Written and oral expression tech-
niques’ (X2) and ’Scientific English’ (X3) for the first year (n = 590) of the
Mathematics-Physics-Chemistry-Computer Science program at Université
Thomas Sankara; refer back to Tables 9 and 13 for some descriptive sum-
mary. Both variables X1 and X3 are left-skewed while X3 is right-skewed,
21

Table 6: Average data of cholesterol level (Xi) per age (a) in ml/dl (or g/l)
for n = 82 age groups of 1986 individuals sampled during the year 2020 in
Bangui (Central African Republic).
a
Xi
a
Xi
a
Xi
a
Xi
2
1.40
30
1.65
51
1.57
72
1.47
3
2.00
31
1.80
52
1.73
73
1.69
4
1.80
32
1.76
53
1.75
74
1.57
8
1.70
33
1.32
54
1.70
75
1.48
10
1.50
34
1.61
55
1.71
76
1.42
12
1.58
35
1.68
56
1,62
77
1.70
13
1.90
36
1.59
57
1.55
78
1.,41
14
1.03
37
1.74
58
1.65
79
2.00
15
1.00
38
1.79
59
1.78
80
1.79
16
1.90
39
1.66
60
1.64
81
1.60
17
1.14
40
1.73
61
1.72
82
2.00
18
1.43
41
1.59
62
1.56
83
1.77
21
1.16
42
1,64
63
1.61
84
1.72
22
1.50
43
1.64
63
1.61
84
1.72
23
1.40
44
1.57
65
1.70
86
1.83
24
1.62
45
1.62
66
1.57
87
1.50
25
1.53
46
1.69
67
1.57
89
1.70
26
1.51
47
1.65
68
1.67
90
1.73
27
1.58
48
1.63
69
1.54
96
1.80
28
1.75
49
1.55
70
1.61
29
1.56
50
1.67
71
1.62
Table 7: Summary of the analyzed average cholesterol (Xi) data of Table 6
Data set
n
max.
min.
median
mean
SD
CS
Xi
82
2.000
1.000
1.635
1.621
0.181
−0.976
Table 8: Mean average log-likelihood and its standard errors (in paren-
theses) for cholesterol data of Table 6 and based on 100 replications us-
ing univariate extended-beta smoother with UCV and Bayesian adaptive
bandwidths for α = n2/5 and β1 = 0.5.
mn
UCV
Bayesian adaptive
15
0.3538 (0.0567)
0.3012 (0.0331)
30
0.2861 (0.1382)
0.2832 (0.0616)
50
0.1761 (0.2607)
0.2449 (0.0863)
70
0.0967 (0.3475)
0.1951 (0.1068)
22

(a)
(b)
Figure 5: Histogram with its corresponding smoothings of the cholesterol
data of Table 6 using univariate extended-beta kernels with both UCV and
Bayes selectors of bandwidths for α = n2/5 and β = 0.5 on: (a) [a1, b1] = [1, 2]
and, (b) [a1, b1] = [0.95, 2.05].
suggesting the use of extended-beta kernels that can have the same behav-
ior; see Figure 2. Additionally, two extended-beta smoothers are consid-
ered to estimate both the marginal densities; also, we use the Gaussian one
with the best tuning parameter (plug-in) available for this dataset with the
ks package of Duong (2007). As previously asserted, both extended-beta
kernel estimators are comparable and better than the Gaussian smoother;
see Figure 6 and also Table 9 of average log-likelihood.
Table 9: Summary of the analyzed marks data of Table 13
Data set
n
max.
min.
median
mean
CS
X1
570
20.00
0.00
11.35
7.61
−0.0603
X2
570
20.00
0.00
5.00
5.495
0.2972
X3
570
20.00
0.00
8.90
7.147
−0.0310
The bivariate illustration is derived from the Old Faithful geyser data
already discussed in literature, see, e.g., Filippone and Sanguinetti (2011)
and recently Somé et al. (2024), and available in the Datasets Library of
the R software R Core Team (2023).
Data concern n = 272 measure-
ments of the eruption for the Old Faithful geyser in Yellowstone National
Park, Wyoming, USA. Both covariates represent, in minutes, the waiting
time between eruptions and the duration of the eruption of these non-
negative real data with mean vector and covariance matrix estimated as
bµ = (3.48, 70.89)⊤and bΣ =
 
1.30
13.97
13.97
184.82
!
, respectively.
Figure 7 reports contour and surface plots of the smoothed distribu-
tion using gamma and extended-beta kernel with Bayesian bandwidth
23

X1
X2
Figure 6: Histogram with its corresponding smoothings of the two first
marks data from Table 13 (X1 and X2) using univariate Gaussian kernel
with plug-in selector of bandwidth and univariate extended-beta kernels
with both UCV and Bayes selectors of bandwidths for α = n2/5 and β = 0.1
on [a1, b1] = [0, 20].
Table 10: Mean average log-likelihood and its standard errors (in paren-
theses) for marks data exhibited in Table 13 and based on 100 replications
using univariate extended-beta kernels with both UCV and Bayes selectors
of bandwidths for α = n2/5 and β = 0.1 and Gaussian kernel estimator with
plug-in selector of bandwidth matrix.
mn
Extended-betaUCV
Extended-betaBayes
GaussianPlug−in
X1
100
−0.9140 (0.0380)
−0.8701 (0.0387)
−1.9992 (0.0227)
250
−0.9259 (0.0527)
−0.9122 (0.0531)
−2.0438 (0.0290)
400
−0.9285 (0.1061)
−0.9779 (0.1050)
−2.1243 (0.0627)
500
−1.0780 (0.1919)
−1.2242 (0.1803)
−2.3075 (0.0805)
X2
100
−1.3906 (0.0404)
−1.3588 (0.0416)
−2.3014 (0.0262)
250
−1.3841 (0.0696)
−1.3781 (0.0704)
−2.3388 (0.0429)
400
−1.3820 (0.1235)
−1.4340 (0.1210)
−2.4128 (0.0644)
500
−1.4338 (0.2137)
−1.5977 (0.1967)
−2.5624 (0.1042)
24

selections for the Old Faithful geyser dataset. The points designate the
scatter plots and the solid lines indicate the contour plot estimates. The
smoothing with gamma and extended-beta kernels are quite similar and
point out the bimodality of the Old Faithful geyser data. In contrast, the
extended-beta smoother seems to be the best with respect to Table 11 of
average log-likelihood.
Table 11: Mean average log-likelihood and its standard errors (in parenthe-
ses) for Old Faithful Geyser data based on 100 replications using multiple
gamma and MEBK with Bayesian adaptive bandwidths (α = n2/5 and
β1 = β2 = 1) and (α = n2/5 and β1 = β2 = 1/5), respectively.
mn
Gamma
Extended-beta
100
−792.41 (4.95)
−744.42 (6.72)
150
−570.01 (5.42)
−532.49 (6.45)
200
−329.50 (5.69)
−317.17 (4.42)
250
−111.28 (2.47)
−100.61 (2.63)
Table 12: Mean average log-likelihood and its standard errors (in parenthe-
ses) for marks data illustrated in Table 13 and grounded on 100 replications
using MEBK estimator with Bayes selectors of bandwidths for α = n2/5 and
β = 0.1, and multivariate Gaussian kernel estimator with plug-in selector
of bandwidth matrix.
mn
Extended-betaBayes
GaussianPlug−in
ALL
100
−4.3397 (0.0810)
−4.0658 (0.1119)
250
−4.1458 (0.1551)
−4.2512 (0.1674)
400
−3.8879 (0.2614)
−4.4601 (0.2870)
500
−3.6765 (0.4490)
−4.6988 (0.4773)
The trivariate example relates to students’ marks in [0, 20], already fore-
grounded above with mean vector and covariance matrix amounting to
bµ = (7.609, 5.495, 7.147)⊤and bΣ =


49.271
30.341
33.148
30.341
28.295
26.600
33.148
26.600
36.803

, respectively;
see also Tables 9 and 13. The Gaussian kernel, utilizing a full bandwidth
matrix via the plug-in method, proves to be less effective compared to
the extended-beta smoother employing a diagonal bandwidth matrix ob-
tained by means of Bayesian methods. However, it remains comparable
in goodness-of-fit; see Table 12. Notice that using a full bandwidth matrix
achieves a certain level of smoothing that might not be achievable with di-
agonal matrices; see, e.g., Duong (2007) and Kokonendji and Somé (2018)
for comparisons with multivariate Gaussian and multiple beta kernels,
respectively.
25

(Contour gamma)
(Surface gamma)
(Contour extended-beta)
(Surface extended-beta)
Figure 7: Contour (left) and surface (right) plots of smoothed distribution
from the Old Faithful geyser dataset according to Bayes selectors of band-
widths vector h with multiple gamma kernels (α = n2/5 and β1 = β2 = 1)
and MEBK (α = n2/5 and β1 = β2 = 1/5).
26

Table 13: Number of students per mark out of 20 for ’Introduction to Prac-
tical Work’ (X1), ’Written and oral expression techniques’ (X2) and ’Scien-
tific English’ (X3) for the first year (n = 590) of the Mathematics-Physics-
Chemistry-Computer Science program at Université Thomas Sankara.
Mark1 (X1)
n1
Mark2 (X2)
n2
Mark3 (X3)
n3
0
262
0
239
0
220
4
1
1
4
3
1
5
2
2
7
4
2
6
2
3
15
5
7
7
2
3,1
1
5,3
1
8
4
4
14
5,4
1
10
1
5
23
5,5
3
10,4
1
6
20
5,6
1
10,5
1
7
25
6
6
11
18
8
26
6,3
1
11,2
1
9
34
6,5
4
11,5
1
9,1
1
6,6
1
12
48
10
50
6,7
1
13
75
11
31
7
15
13,5
2
12
35
7,5
2
14
58
13
18
7,8
1
14,5
1
14
25
8
22
14,6
1
15
12
8,5
5
15
53
16
7
8,8
1
15,5
2
16,1
1
9
23
15,6
1
17
2
10
42
16
24
10,4
1
17
12
10,5
5
18
8
10,7
1
18,2
1
11
48
19
4
11,5
2
19,2
1
12
35
20
3
12,5
2
13
33
14
39
15
26
15,7
1
16
19
16,5
1
17
8
18
6
19,5
1
20
2
27

7
Concluding remarks
In the current research paper, we have elaborated a nonparametric smooth-
ing method using extended-beta kernels and both UCV and Bayesian adap-
tive selectors for the bandwidth vector. These (non-)normalized MEBK es-
timators have equally exhibited interesting asymptotic properties. We have
explored the performance of this extended-beta smoother with Bayesian
adaptive bandwidths for any density smoothing on either a given or esti-
mated compact support. This can be regarded as a unified estimation of
densities on bounded and unbounded domains. Efficient posterior and
Bayes estimators for the bandwidth vector under a quadratic loss function
have been explicitly derived.
Simulation studies and analysis of three real datasets have yielded
the powerful performance of the introduced approaches for nonparamet-
ric bandwidth estimation in terms of ISE and log-likelihood criteria. As
expected, MEBK estimators with UCV or Bayesian adaptive bandwidths
serve as workable alternatives to Gaussian and gamma methods, granting
flexibility in selecting either global or variable bandwidth vectors. For an
efficient as well as practical use of the MEBK estimator, it is recommended
to assess the range of the multivariate data to make the optimal choice of
compact support.
At this stage, it is noteworthy that the proposed approach is promis-
ing and can be extended in several ways. This involves working on an R
package dedicated to smoothing with MEBK, incorporating various tun-
ing parameters, including Bayesian adaptive methods; see also the Ake
package by Wansouwé et al. (2016). Finally, and in order to reduce biases
via modified versions of (standard) univariate extended-beta kernel esti-
mators (e.g., Chen (1999)), another way for improving this work would be
to investigate some effects of the so-called combined MEBK estimations in
the same sense of multiple combined gamma kernel estimations in Somé
et al. (2024).
8
Appendix: Proofs
Proof of Lemma 3.1. Departing from (4) and relying upon the following ver-
sion of the beta function
B(r, s) =
Z b
a
(u −a)r−1(b −u)s−1
(b −a)r+s−1
du,
28

for all a < b, r > 0 and s > 0, we have


d
Y
j=1
EBxj,hj,aj,bj


2
2
:=
d
Y
j=1
Z
[aj,bj]
EB2
xj,hj,aj,bj(u)du
=
d
Y
j=1
Z bj
aj
(u −aj)2(xj−aj)/{(bj−aj)hj}(bj −u)2(bj−xj)/{(bj−aj)hj}(bj −aj)−2−2/hj
B2{1 + (xj −aj)/(bj −aj)hj, 1 + (bj −xj)/(bj −aj)hj}
du
=
d
Y
j=1
B

1 + 2(xj −aj)/(bj −aj)hj, 1 + 2(bj −xj)/(bj −aj)hj

/(bj −aj)
h
B

1 + (xj −aj)/(bj −aj)hj, 1 + (bj −xj)/(bj −aj)hj
i2
;
which is the desired result of the first part of the lemma. The last parts
of the lemma are trivial, which are also inferred from (5) and (6), with
||A||2 := A⊤A.
□
Proof of Proposition 3.2. One can apply Proposition 2.9 of Kokonendji and
Somé (2018) with both characteristics (5) and (6) of the MEBK estimator.
Thus, the pointwise bias is easily verified. Furthermore, the pointwise
variance is deduced from Lemma 3.1 and Eq. (20) of Libengué Dobélé-
Kpoka and Kokonendji (2017) with r2 = 1/2.
□
Proof of Proposition 3.3. Since the bandwidth matrix H is here diagonal and
resting on both characteristics (5) and (6) in addition to Lemma 3.1 of
the MEBK estimator, one can deduce both results of mean square and
almost surely convergences from Theorem 2.2 of Kokonendji and Libengué
Dobélé-Kpoka (2018) with r2 = 1/2.
As for the convergence in law, one can consider Theorem 3.3 of Esstafa
et al. (2023b) in its multiple versions as follows. Writing
√
n


d
Y
j=1
h
(1/2)αj
j
(n)


 bfn(x) −f(x)

=
bfn(x) −E bfn(x)
q
Var bfn(x)
v
u
u
t
n


d
Y
j=1
h
αj
j (n)

Var bfn(x)
+
v
u
u
t
n


d
Y
j=1
h
αj
j (n)


h
E bfn(x) −f(x)
i
with
q
n
Qd
j=1 h
αj
j (n)
 h
E bfn(x) −f(x)
i
= O
 √n Qd
j=1 h
(3/2)αj
j
(n)

and
n


d
Y
j=1
h
αj
j (n)

Var bfn(x) = f(x)


d
Y
j=1
h
αj
j (n)




d
Y
j=1
EBxj,hj(n),aj,bj


2
2
+ O


d
Y
j=1
h
αj
j (n)

,
one deduces [ bfn(x) −E bfn(x)]/
q
Var bfn(x)
L
−−−→
n→∞N(0, 1) resting on triangular
array technique with the Lyapunov condition.
□
29

Proof of Proposition 3.4. Following the proof of Proposition 3.4 of Esstafa et
al. (2023b), we first rewrite
E(Cn −1)2 = VarCn + (ECn −1)2.
Therefore, one demonstrates that VarCn −→
n→∞0 since one can successively
get
VarCn
≤
1
n


d
Y
j=1
EBxj,hj,aj,bj


2
2
Z
y∈Td
f(y)dy
+ c
n
Z
y∈Td
f(y)dy + c
n
Z
z∈Td
Z
y∈Td
f(y)f(z)dydz
≤
c
n Qd
j=1 h
αj
j (n)
+ c
n,
where c denotes our generic constant that can change from line to line.
Finally, one gets (ECn −1)2 −→
n→∞0 since E bfn(x) −f(x) = O
Qd
j=1 h
αj
j (n)

and
|E(Cn −1)| ≤
Z
x∈Td
|E
 bfn(x) −f(x)

|dx −→
n→∞0.
This completes the proof.
□
Proof of Proposition 3.5. Since bfn(x)convergesinmeansquareto f(x)through
the first part of Proposition 3.3, the result of the convergence in probability
is easily inferred from
efn(x) −f(x) = 1
Cn
n bfn(x) −f(x)

+ (1 −Cn) f(x)
o
and Proposition 3.4 with the Slutsky theorem.
As for the second part of the asymptotic normality, we use the multiple
version of Theorem 3.6 of Esstafa et al. (2023b) through decomposing
√
n


d
Y
j=1
h
(1/2)αj
j
(n)


 efn(x) −f(x)

=
1
Cn
bfn(x) −E bfn(x)
q
Var bfn(x)
v
u
u
t
n


d
Y
j=1
h
αj
j (n)

Var bfn(x)
+ 1
Cn
v
u
u
t
n


d
Y
j=1
h
αj
j (n)


h
E bfn(x) −f(x)
i
+ f(x)
Cn
v
u
u
t
n


d
Y
j=1
h
αj
j (n)

(1 −Cn)
=
L
−−−→
n→∞N(0, f(x)λx,α) +
P
−−−→
n→∞0 +
P
−−−→
n→∞0.
30

The last convergence in probability is obtained by
v
u
u
t
n


d
Y
j=1
h
αj
j (n)

E (1 −Cn) = O


√
n
d
Y
j=1
h
(3/2)αj
j
(n)


and the Hoeffding inequality.
□
Proof of Proposition 3.6. Following the proof of Proposition 3.8 in Esstafa et
al. (2023b), we can characterize this multiple case by easily admitting that
Cn tends towards 1 for L4 criterion of the MEBK estimator via Lemma 3.1.
Thus, we successively get
E
"Z
Td
 efn(x) −f(x)

2
dx
#
=
E


Z
Td

bfn(x)
Cn
−f(x)
Cn
+ f(x)
Cn
−f(x)

2
dx


=
E
"Z
Td
 bfn(x) −f(x)

2
dx
#
+ E
"
1 −Cn
Cn

2# Z
Td
f(x)dx
+E
"1 −C2
n
C2
n
Z
Td
 bfn(x) −f(x)

2
dx
#
+2 E
"
C−2
n
Z
Td
{ bfn(x) −f(x)}(1 −Cn)f(x)dx
#
.
This gives rise to these inqualities after a few consecutive increases
E
"Z
Td
 efn(x) −f(x)

2
dx
#
≤
E
"Z
Td
 bfn(x) −f(x)

2
dx
#
+ c E
"
1 −Cn
Cn

2#
+c E


1 −C2
n

C2
n

+ c E
"|1 −Cn| (Cn + 1)
C2
n
#
≤
E
"Z
Td
 bfn(x) −f(x)

2
dx
#
+ c
q
E[(Cn −1)4]E[C−4
n ]
+c
q
E[(Cn −1)4] E[C−2
n ] + E[C−3
n ] + E[C−4
n ]	,
where c corresponds to our generic constant that can change from line to
line. Therefore, one has to deduce the desired result.
□
Proof of Theorem 4.1. (i) Relying upon (10) and (13), the numerator of (11)
is first equal to
N(hi | Xi)
=
bf−i(Xi | hi)π(hi)
=


1
n −1
n
X
j=1,j,i
d
Y
ℓ=1
BEXiℓ,hiℓ,aℓ,bℓ(Xjℓ)




d
Y
ℓ=1
βα
ℓ
Γ(α)h−α−1
iℓ
exp(−βℓ/hiℓ)


=
[Γ(α)]−d
n −1
n
X
j=1,j,i
d
Y
ℓ=1
BEXiℓ,hiℓ,aℓ,bℓ(Xjℓ)
β−α
ℓhα+1
iℓ
exp(βℓ/hiℓ).
(15)
31

Grounded on (4) and applying the usual properties B(s, t) = Γ(s)Γ(t)/Γ(s+t)
and Γ(z + 1) = zΓ(z) for z > 0, part of expression (15) becomes
BEXiℓ,hiℓ,aℓ,bℓ(Xjℓ)
β−α
ℓhα+1
iℓ
exp(βℓ/hiℓ)
=
(Xjℓ−aℓ)(Xiℓ−aℓ)/[(bℓ−aℓ)hiℓ](bℓ−Xjℓ)(bℓ−Xiℓ)/[(bℓ−aℓ)hiℓ]
B(1 + (Xiℓ−aℓ)/(bℓ−aℓ)hiℓ, 1 + (bℓ−Xiℓ)/(bℓ−aℓ)hiℓ)
× (bℓ−aℓ)(−1−1/hiℓ)βα
ℓh−α−1
iℓ
exp(−βℓ/hiℓ)
=
(1 + 1/hiℓ)Γ(1 + 1/hiℓ)
Γ(1 + (Xiℓ−aℓ)/(bℓ−aℓ)hiℓ)Γ(1 + (bℓ−Xiℓ)/(bℓ−aℓ)hiℓ)
×
(Xjℓ−aℓ)(Xiℓ−aℓ)/[(bℓ−aℓ)hiℓ](bℓ−Xjℓ)(bℓ−Xiℓ)/[(bℓ−aℓ)hiℓ]
(bℓ−aℓ)(1+1/hiℓ)β−α
ℓhα+1
iℓ
exp(βℓ/hiℓ)
. (16)
Consider the largest part Ic
i = {ℓ∈{1, . . . , d} ; Xiℓ∈(aℓ, bℓ)}. Following
Chen (1999, 2000a), we assume that for all Xiℓ∈(aℓ, bℓ), one has (Xiℓ−
aℓ)/hℓ→∞and (bℓ−Xiℓ)/hℓ→∞as n →∞for all ℓ∈1, 2, . . . , d. As a matter
of fact, it follows from the Sterling formula Γ(z + 1) ≃
√
2πzz+1/2 exp(−z) as
z →∞that as n →∞, the previous term (16) can be successively calculated
as
BEXiℓ,hiℓ,aℓ,bℓ(Xjℓ)
β−α
ℓhα+1
iℓ
exp(βℓ/hiℓ)
=
{(Xiℓ−aℓ)(bℓ−Xjℓ)}−1/2
√
2πβ−α
ℓexp(βℓ/hiℓ)

h−α−3/2
iℓ
+ h−α−1/2
iℓ

exp
" 
1
hiℓ(bℓ−aℓ)
!
×
 
(Xiℓ−aℓ) log
 Xjℓ−aℓ
Xiℓ−aℓ
!
+ (bℓ−Xiℓ) log
 bℓ−Xjℓ
bℓ−Xiℓ
!!#
=
{(Xiℓ−aℓ)(bℓ−Xjℓ)}−1/2
√
2πβ−α
ℓ
×

Γ(α + 1/2)
[Bijℓ(α, βℓ)]α+1/2 ×
[Bijℓ(α, βℓ)]α+1/2 exp[−Bijℓ(α, βℓ)/hiℓ]
hα+3/2
iℓ
Γ(α + 1/2)
+
Γ(α −1/2)
[Bijℓ(α, βℓ)]α−1/2 ×
[Bijℓ(α, βℓ)]α−1/2 exp[−Bijℓ(α, βℓ)/hiℓ]
hα+1/2
iℓ
Γ(α −1/2)

= Aijℓ(α, βℓ)IGα+1/2,Bijℓ(α,βℓ)(hiℓ) + Cijℓ(α, βℓ)IGα−1/2,Bijℓ(α,βℓ)(hiℓ), (17)
where Aijℓ(α, βℓ), Bijℓ(α, βℓ) and Cijℓ(α, βℓ) are as stated in the theorem, and
both IGα+1/2,Bijℓ(α,βℓ)(hiℓ) and IGα−1/2,Bijℓ(α,βℓ)(hiℓ) are easily deduced from (13).
Moreover, consideringtheleftsmallestpartIiak = {k ∈{1, . . . , d} ; Xik = ak},
32

for each Xik = ak, the term of sum (15) in (16) can be expressed as follows:
BEak,hik,ak,bk(Xjk)
β−α
k hα+1
ik
exp(βk/hik)
=
(1 + 1/hik)(bk −Xjk)(1/hik)
(bk −ak)(1+1/hik)β−α
k hα+1
ik
exp(βk/hik)
=
βα
k
(bk −ak)

h−α−1
ik
+ h−α−2
ik

exp
(
−
"
βk −log
 bk −Xjk
bk −ak
!#
/hik
)
=
 
Γ(α)
[Ejk(βk)]α
[Ejk(βk)]α
Γ(α)
h−α−1 +
Γ(α + 1)
[Ejk(βk)]α+1
[Ejk(βk)]α+1
Γ(α + 1) h−α−2
!
×
βα
k
(bk −ak) exp[−Ejk(βk)/hik]
=
Fjk(α, βk) IGα,Ejk(βk)(hik) + Hjk(α, βk) IGα+1,Ejk(βk)(hik)
(18)
where Ejk(βk), Fjk(α, βk) and Hjk(α, βk) are as presented in the theorem and,
both IGα,Ejk(βk)(hik) and IGα+1,Ejk(βk)(hik) derive from (13).
Similarly, byfocusingontherightsmallestpartIibs = {s ∈{1, . . . , d} ; Xis = bs},
for each Xik = bk, the term of sum (15) in (16) can be indicated as follows:
BEbs,his,as,bs(Xjs)
β−α
s hα+1
is
exp(βs/his)
=
(1 + 1/his)(Xjs −as)(1/his)
(bs −as)(1+1/his)β−α
s hα+1
is
exp(βs/his)
=
βα
s
(bs −as)

h−α−1
is
+ h−α−2
is

exp
(
−
"
βs −log
 Xjs −as
bs −as
!#
h−1
is
)
=
 
Γ(α)
[Gjs(βs)]α
[Gjs(βs)]α
Γ(α)
h−α−1 +
Γ(α + 1)
[Gjs(βs)]α+1
[Gjs(βs)]α+1
Γ(α + 1) h−α−2
!
×
βα
s
(bs −as) exp[−Gjs(βs)/his]
=
Jjs(α, βs) IGα,Gjs(βs)(his) + Kjs(α, βs) IGα+1,Gjs(βk)(his)
(19)
where Gjs(βs), Jjs(α, βs) and Kjs(α, βs) are as defined in the thoerem and, both
IGα,Gjs(βs)(his) and IGα+1,Gjs(βk)(his) are deduced from (13).
Combining (17), (18) and (19), the expression of N(hi | Xi) in (15) be-
comes
N(hi | Xi)
=
[Γ(α)]−d
n −1
n
X
j=1,j,i


Y
k∈Iiak
[Fjk(α, βk) IGα,Ejk(βk)(hik) + Hjk(α, βk) IGα+1,Ejk(βk)(hik)]


×


Y
ℓ∈Ic
i
[Aijℓ(α, βℓ) IGα+1/2,Bijℓ(α,βℓ)(hiℓ) + Cijℓ(α, βℓ) IGα−1/2,Bijℓ(α,βℓ)(hiℓ)]


×


Y
k∈Iibs
[Jjs(α, βs) IGα,Gjs(βs)(his) + Kjs(α, βs) IGα+1,Gjs(βk)(his)]

.
(20)
Referring to (20), the denominator of (11) is successively determined as
33

follows
Z
×d
ℓ=1[aℓ,bℓ]
N(hi | Xi) dhi
=
[Γ(α)]−d
(n −1)
n
X
j=1,j,i


Y
k∈Iiak
"
Fjk(α, βk)
Z bk
ak
IGα,Ejk(βk)(hik)dhik
+Hjk(α, βk)
Z bk
ak
IGα+1,Ejk(βk)(hik)dhik
#!
×


Y
ℓ∈Ic
i
"
Aijℓ(α, βℓ)
Z bℓ
aℓ
IGα+1/2,Bijℓ(α,βℓ)(hiℓ)dhiℓ
+Cijℓ(α, βℓ)
Z bℓ
aℓ
IGα−1/2,Bijℓ(α,βℓ)(hiℓ)dhiℓ
#!
×


Y
k∈Iibs
"
Jjs(α, βs)
Z bs
as
IGα,Gjs(βs)(his)dhis
+Hjs(α, βs)
Z bs
as
IGα+1,Gjs(βs)(his)dhis
#!
=
[Γ(α)]−d
(n −1)
n
X
j=1,j,i


Y
k∈Iiak
[Fjk(α, βk) + Kjk(α, βk)]


×


Y
ℓ∈Ic
i
[Aijℓ(α, βℓ) + Cijℓ(α, βℓ)]


×


Y
k∈Iibs
[Jjs(α, βs) + Kjs(α, βs)]


=
[Γ(α)]−d
(n −1) Di(α, β),
(21)
with Di(α, β) being as given in the theorem. Thus, the ratios of (20) and
(21) allow us to conclude Part (i) of the theorem.
(ii) We recall that the mean of the inverse-gamma distribution IG(α, βℓ)
is βℓ/(α −1) and E(hiℓ| Xi) =
R bm
am hiℓπ(him | Xi) dhim with π(him | Xi) referring
to the marginal distribution him obtained by integration of π(hi | Xi) for
all components of hi except him. Then, π(him | Xi) =
R
Td(−m) π(hi | Xi) dhi(−m),
where Td(−m) = ×d
ℓ=1,ℓ,m[aℓ, bℓ] and dhi(−m) is the vector dhi without the m-th
component.
34

If m ∈Ic
i and α > 3/2, one has
π(him | Xi)
=
1
Di(α, β)
n
X
j=1,j,i


Y
k∈Iiak
h
Fjk(α, βk) + Hjk(α, βk)
i


×


Y
ℓ∈Ic
i ,ℓ,m
h
Aijℓ(α, βℓ) + Cijℓ(α, βℓ)
i




Y
k∈Iibs
h
Jjs(α, βs) + Kjs(α, βs)
i


×

Aijm(α, βm)IGα+1/2,Bijm(α,βm)(him) + Cijm(α, βm)IGα−1/2,Bijm(α,βm)(him)

.
As a matter of fact,
ehim
=
1
Di(α, β)
n
X
j=1,j,i


Y
k∈Iiak
h
Fjk(α, βk) + Hjk(α, βk)
i


×


Y
ℓ∈Ic
i
h
Aijℓ(α, βℓ) + Cijℓ(α, βℓ)
i




Y
k∈Iibs
h
Jjs(α, βs) + Kjs(α, βs)
i


×
 Aijm(α, βm)
α −1/2
+
Cijm(α, βm)
α −3/2
!
Bijm(α, βm)
Aijm(α, βm) + Cijm(α, βm).
(22)
If m ∈Iiak and α > 1, one gets
π(him | Xi)
=
1
Di(α, β)
n
X
j=1,j,i


Y
k∈Iiak,k,m
h
Fjk(α, βk) + Hjk(α, βk)
i


×


Y
ℓ∈Ic
i
h
Aijℓ(α, βℓ) + Cijℓ(α, βℓ)
i




Y
k∈Iibs
h
Jjs(α, βs) + Kjs(α, βs)
i


×

Fjm(α, βm)IGα,Ejm(βm)(him) + Hjm(α, βm)IGα+1,Ejm(βm)(him)

and, consequently
ehim
=
1
Di(α, β)
n
X
j=1,j,i


Y
k∈Iiak
h
Fjk(α, βk) + Hjk(α, βk)
i


×


Y
ℓ∈Ic
i
h
Aijℓ(α, βℓ) + Cijℓ(α, βℓ)
i




Y
k∈Iibs
h
Jjs(α, βs) + Kjs(α, βs)
i


×
 Fjm(α, βm)
α −1
+
Hjm(α, βm)
α
!
Ejm(βm)
Fjm(α, βm) + Hjm(α, βm).
(23)
35

If m ∈Iibs and α > 0, one obtains
π(him | Xi)
=
1
Di(α, β)
n
X
j=1,j,i


Y
k∈Iiak
h
Fjk(α, βk) + Hjk(α, βk)
i


×


Y
ℓ∈Ic
i
h
Aijℓ(α, βℓ) + Cijℓ(α, βℓ)
i




Y
s∈Iibs,s,m
h
Jjs(α, βs) + Kjs(α, βs)
i


×

Jjm(α, βm)IGα,Gjm(βm)(him) + Kjm(α, βm)IGα+1,Gjm(βm)(him)

.
It follows therefore that
ehim
=
1
Di(α, β)
n
X
j=1,j,i


Y
k∈Iiak
h
Fjk(α, βk) + Hjk(α, βk)
i


×


Y
ℓ∈Ic
i
h
Aijℓ(α, βℓ) + Cijℓ(α, βℓ)
i




Y
s∈Iibs,s,m
h
Jjs(α, βs) + Kjs(α, βs)
i


×
 Jjm(α, βm)
α −1
+
Kjm(α, βm)
α
!
Gijm(βm)
Jjm(α, βm) + Kjm(α, βm)
(24)
Finally, combining (22), (23) and (24) yields the closed expression of Part
(ii), which completes the proof.
□
Acknowledgments
The authors dedicate this paper to Professor Faustin Archange Touadera
for his 66th birthday, with our gratitute for his availability and support.
Part of this work was undertaken while the first author was at Laboratoire
de mathématiques de Besançon as a visiting scientist, with financial support
from Université Thomas SANKARA. This work has been conducted within
the frame of EIPHI graduate school (contrat "ANR-17-EURE-0002") of the
second author.
References
Aboubacar, A. and Kokonendji, C.C. (2024). Asymptotic results for recur-
sive multivariate associated-kernel estimators of the probability density
mass function of a data stream, Commun. Statist. Theory Meth. 54, 2109–
2019.
Baíllo, A., Cuevas, A. and Justel, A. (2000). Set estimation and nonpara-
metric detection, Canad. J. Statist. 28, 765–782.
36

Bertin, K. and Klutchnikoff, N. (2014). Adaptive estimation of a density
function using beta kernels, ESAIM Probab. Statsit. 18, 400–417.
Biau, G., Cadre, B., Mason, D.M. and Pelletier, B. (2009). Asymptotic nor-
mality in density support estimation, Electr. J. Probab. 14, 2617–2635.
Bouezmarni, T. and Roumbouts, J.V.K. (2010). Nonparametric density esti-
mation for multivariate bounded data, J. Statist. Plann. Infer. 140, 139–152.
Cacoullos, T. (1966). Estimation of a multivariate density, Ann. Instit. Statist.
Math. 18, 179-189.
Chen, S.X. (1999). A beta kernel estimation for density functions, Comput.
Statist. Data Anal. 31, 131–145.
Chen, S.X. (2000a). Beta kernel smoothers for regression curves, Statistica
Sinica 10, 73–91.
Chen, S.X. (2000b). Gamma kernel estimators for density functions, Ann.
Instit. Statist. Math. 52, 471–480.
Clark, C.E. (1962). The PERT model for the distribution of an activity,
Operations Research 10, 405–406.
Devroye, L. and Wise, G. (1980). Detection of abnormal behavior via non-
parametric estimation of the support, SIAM J. Appl. Math. 38, 480–488.
Duin, R.P.W. (1976). On the choice of smoothing parameters of Parzen
estimators of probability density functions, IEEE Trans. Comput. C-25,
1175–1179.
Duong, T. (2007). ks: Kernel density estimation and kernel discriminant
analysis for multivariate data in R, J. Statist. Soft. 21, 1–16.
Duong, T. and Hazelton, M.L. (2005). Convergence rates for unconstrained
bandwidth matrix selectors in multivariate kernel density estimation. J.
Multiv. Anal. 93, 417–433.
Duong, T., Wand, M., Chacon, J. and Gramacki, A. (2023). ks: Kernel
Smoothing, R package version 1.14.1.
Esstafa, Y., Kokonendji, C.C. and Somé, S.M. (2023a). Asymptotic proper-
ties of the normalised discrete associated-kernel estimator for probability
mass function, J. Nonparam. Statist. 35, 355–372.
Esstafa, Y., Kokonendji, C.C. and Ngô, T-B-T. (2023b). New developments
on (non-)normalized continuous associated-kernel estimators, Preprint
https://hal.science/hal-04112846v1.
Epanechnikov, V. (1969). Nonparametric estimates of a multivariate prob-
ability density, Theor. Probab. Appl. 14, 153–158.
37

Filippone, M. and Sanguinetti, G. (2011). Approximate inference of the
bandwidth in multivariate kernel density estimation, Comput. Statist.
Data Anal. 55, 3104–3122.
Funke, B. and Kawka, R. (2015). Nonparametric density estimation for
multivariate bounded data using two non-negative multiplicative bias
correction methods, Comput. Statist. Data Anal. 92, 148–162.
Grubbs, F.E. (1962). Attempts to validate certain PERT statistics or picking
on PERT, Operations Research 10, 912–915.
Hirukawa, M. and Sakudo, M. (2015). Family of the generalised gamma
kernels: a generator of asymmetric kernels for nonnegative data J. Non-
param. Statist. 27, 41–63.
Jin, X. and Kawczak, J. (2003). Birnbaum-Saunders and lognormal kernel
estimators for modelling durations in high frequency financial data, Ann.
Econ. Fin. 4, 1003–1024.
Kang, Y.J., Noh, Y. and Lim, O.K. (2018). Kernel density estimation with
bounded data, Struct. Multidisc. Optim. 57, 95–113.
Kokonendji, C.C. and Libengué Dobélé-Kpoka, F.G.B. (2018). Asymptotic
results for continuous associated kernel estimators of density functions,
Afr. Diaspora J. Math. 21, 87–97.
Kokonendji, C.C. and Somé, S.M. (2018). On multivariate associated ker-
nels for smoothing general density function, J. Korean Statist. Soc. 47,
112–126.
Libengué Dobélé-Kpoka, F.G.B. and Kokonendji, C.C. (2017). The mode-
dispersion approach for constructing continuous associated kernels,
Afrika Statistika 12, 1417–1446.
Marchant, C., Bertin, K., Leiva, V. and Saulo, H. (2013). Generalized
Birnbaum-Saunders kernel density estimators and an analysis of finan-
cial data, Comput. Statist. Data Anal. 63, 1–15.
Marshall, J.C. and Hazelton M.L. (2010). Boundary kernels for adaptive
density estimators on regions with irregular boundaries, J. Multiv. Anal.
101, 949–963.
Parzen, E. (1962). On estimation of a probability density function and
Mode, Ann. Math. Statist. 33, 1065–1076.
R Core Team, (2023). R: A Language and Environment for Statistical Com-
puting, R Foundation for Statistical Computing: Vienna, Austria, 2023.
Available online: http://cran.r-project.org/
Rosenblatt, M. (1956). Remarks on some nonparametric estimates of a
density function, Ann. Math. Statist. 27, 832–837.
38

Santafe, G., Calvo B., Perez A. and Lozano, J.A. (2022). bde: Bounded
Density Estimation, R package version 1.0.1.1, 2022.
Senga Kiessé, T. and Durrieu, G. (2024). On a discrete symmetric optimal
associated kernel for estimating count data distributions, Statist. Probab.
Lett. 208, Paper No. 110078.
Scott, D.W. and Wand, M.P. (1991). Feasibility of multivariate density esti-
mates, Biometrika 78, 197–206.
Somé, S.M. and Kokonendji, C.C. (2022). Bayesian selector of adaptive
bandwidth for multivariate gamma kernel estimator on [0, ∞)d. J. Appl.
Statist. 49, 1692–1713.
Somé, S.M., Kokonendji, C.C., Belaid, N., Adjabi, S. and Abid, R. (2023).
Bayesian local bandwidths in a flexible semiparametric kernel estimation
for multivariate count data with diagnostics, Statist. Meth. Appl. 32, 843–
865.
Somé, S.M., Kokonendji, C.C., Adjabi, S., Mamode Khan, N.A. and Beddek,
S. (2024). Multiple combined gamma kernel estimations for nonnegative
data with Bayesian adaptive bandwidths, Comput. Statist. 39, 905–937.
Silverman, B.W. (1986). Density Estimation for Statistics and Data Analysis,
Chapman and Hall, London.
Terrell, G.R. and Scott, D.W. (1992). Variable kernel density estimation,
Ann. Statist. 20, 1236–1265.
VOSE Software (2008). Modified PERT distribution: Quantitative risk anal-
ysis software, - Available online: https://www.vosesoftware.com/
Wand, M.P. and Jones, M.C. (1995). Kernel Smoothing, Chapman and Hall,
New York.
Wansouwé, W.E., Somé S.M. and Kokonendji, C.C. (2016). Ake: an R pack-
age for discrete and continuous associated kernel estimations, The R
Journal 8, 258–276.
Zougab, N., Adjabi, S. and Kokonendji, C.C. (2014). Bayesian estimation of
adaptive bandwidth matrices in multivariate kernel density estimation,
Comput. Statist. Data Anal. 75, 28–38.
39
