Localized Conformal Prediction:
A Generalized Inference Framework to Conformal Prediction
Leying Guan∗
Abstract
We propose a new inference framework called localized conformal prediction.
It generalizes the
framework of conformal prediction by oﬀering a single-test-sample adaptive construction that emphasizes
a local region around this test sample, and can be combined with diﬀerent conformal score constructions.
The proposed framework enjoys an assumption-free ﬁnite sample marginal coverage guarantee, and it
also oﬀers additional local coverage guarantees under suitable assumptions.
We demonstrate how to
change from conformal prediction to localized conformal prediction using several conformal scores, and
we illustrate a potential gain via numerical examples.
1
Introduction
Conformal prediction (CP) is an increasingly popular framework for measuring prediction uncertainty. Let
Zi := (Xi, Yi) for i = 1, . . . , n be i.i.d. regression data from some joint distribution PXY , where Xi ∈Rp is
the feature and Yi ∈R is the response. Given a new feature Xn+1 with its response Yn+1 unobserved, the
goal of CP is to construct a prediction interval (PI) C(Xn+1) that covers Yn+1 with probability at least α:
P(Yn+1 ∈C(Xn+1)) ≥α,
(1.1)
for some desired coverage level α ∈(0, 1) (usually close to 1). Setting Zn+1 = (Xn+1, Yn+1) as the (n + 1)th
observation, CP achieves (1.1) under the assumption that Zn+1 is also independently generated from PXY ,
without additional distributional assumptions on PXY itself (Vovk et al. 2005, Shafer & Vovk 2008, Vovk
et al. 2009, Lei & Wasserman 2014, Lei et al. 2018).
Let Z = {Z1, . . . , Zn+1} be the unordered set of feature-response pairs, including Zn+1 = (Xn+1, Yn+1).
CP is based on a conformal score function V (.) for observations z = (x, y), whose form may depend also on
the unordered data Z, i.e. V (z) = V (z; Z). We will consider score functions V (.) where large values of V (z)
indicate that z is less likely to be a sample from PXY . For instance, we may choose V (z) = |ˆµ(x) −y| for a
prediction function ˆµ(.) that is learned from the data Z or from a separate independent data set.
It is guaranteed that Vi := V (Zi) are exchangeable when Z1, . . . , Zn+1 are i.i.d. Thus, letting Q(α; V1:n+1)
denote the level-α quantile of the empirical distribution of V1, . . . , Vn+1, we have
P {Vn+1 ≤Q(α; V1:n+1)} ≥α.
(1.2)
CP constructs the level-α PI C(Xn+1) for Yn+1 by inverting the above relationship for Vn+1:
C(Xn+1) = {y : V (Xn+1, y) ≤Q(α; V1:n ∪V (Xn+1, y))}.
(1.3)
Note that if the form of V (z) = V (z; Z) depends also on Z, then in the above, each score Vi = V (Zi; Z)
depends also on y and is understood to be evaluated at Zn+1 = (Xn+1, y). By the guarantee (1.2), C(Xn+1)
constructed in this way satisﬁes (1.1) for any distribution PXY .
It is common in data applications for the conditional distribution of Y given X = x to be heterogeneous
across diﬀerent values of x ∈Rp. In such settings, it is desirable for the constructed PI C(Xn+1) to adapt to
this heterogeneity. However, by deﬁnition, the CP interval C(Xn+1) is based on the global exchangeability
∗Dept. of Biostatistics, Yale University, leying.guan@yale.edu
1
arXiv:2106.08460v2  [math.ST]  28 Feb 2022

of the conformal scores V1, . . . , Vn+1, and depends equally on scores where Xi is far from Xn+1 as on scores
where Xi is close to Xn+1. To adapt to the heterogeneity of Y given X = x, one active area of research has
been to design the score function V (.) to directly capture this heterogeneity, in a way so that the quantiles
of V (.) are more homogeneous across diﬀerent x ∈Rp (Lei & Wasserman 2014, Izbicki et al. 2019, Lei et al.
2018, Romano et al. 2019, Gupta et al. 2021). For example, in Romano et al. (2019), the authors consider
the quantile regression score V (z) = max{ˆqlo(x) −y, y −ˆqhi(x)} where ˆqlo(x) and ˆqhi(x) are estimated
quantiles for the conditional distribution of Y given X = x. However, this approach may yield deteriorated
performance when these quantile functions are diﬃcult to estimate for some regions of the feature space.
In this paper, we take a diﬀerent approach, and generalize the inference framework itself by weighting the
conformal scores V1, . . . , Vn diﬀerently based on the observed feature value Xn+1. Our method places more
weight on scores Vi for which Xi belongs to a local region around Xn+1. Performing conformal inference
while emphasizing the unique role of Xn+1 is an interesting and open problem, and we provide the ﬁrst such
generalization with theoretical guarantees. We call this generalized framework localized conformal prediction
(LCP), which can be ﬂexibly combined with recently developed conformal score functions.
The main idea of LCP is to introduce a localizer around Xn+1, and up-weight samples close to Xn+1
according to this localizer. For example, we may take the localizer H(Xn+1, Xi) = e−5|Xi−Xn+1|, consider
the weighted empirical distribution where Vi has weight proportional to H(Xn+1, Xi), and include the value
y in C(Xn+1) if and only if V (Xn+1, y) is smaller than the ˜α quantile of this weighted distribution. As
this weighted distribution is no longer exchangeable, we will need to choose ˜α strategically to guarantee
ﬁnite-sample coverage as described in (1.1).
We demonstrate the diﬀerence between LCP and CP with a simple example: Features X ∼Unif(−5, 5)
follow a uniform distribution on [−5, 5], and the response Y given X follows a mean-zero normal distribution
with heterogeneous variance across X:
Y |X ∼
 cos( π
10Xi) × N(0, 1),
if |X| ≤4.5,
2 × N(0, 1),
if |X| > 4.5.
We ﬁx the desired coverage level α = 0.95, take n = 1000 samples, and perform both CP and LCP using two
score functions: (1) the regression score V (z) = |µ(x) −y| = |y| where here µ(x) = 0 (Lei et al. 2018), and
(2) the quantile regression score V (z) = max{ˆqlo(x) −y, y −ˆqhi(x)} (Romano et al. 2019) where ˆqlo and ˆqhi
are 0.025 and 0.975 quantile curves estimated from 2000 independent samples using a neural network model
as described in Section 4. The localizer for LCP is H(Xn+1, Xi) = e−5|Xi−Xn+1| as above. We refer to the
two corresponding CP procedures as CR and CQR, and the two LCP procedures as LCR and LCQR.
The left panel of Figure 1 shows the conformal conﬁdence bands for Vn+1 using CR/LCR (upper left,
blue/red, dashed curve) and CQR/LCQR (lower left, blue/red, solid curve). The right panel shows the
inverted PI for Yn+1 using the four procedures. The green curves on the right panel represent the true
level-α conﬁdence bands for Y given X. This example demonstrates that, by deﬁnition, the CR and CQR
intervals are homogeneous for V . In this example, the CR intervals are furthermore homogeneous for Y .
CQR provides a heterogeneous PI for Y by inverting the interval for V . However, the true quantile functions
are hard to estimate at the two ends here, and thus some heterogeneity of Vn+1 still remains even for the
quantile regression score. In comparison, LCP introduces more ﬂexibility by directly constructing intervals
that are heterogeneous for V . It yields an improvement even when applied to the quantile regression score,
where it better captures the remaining heterogeneity of this score.
We summarize our contributions as follows:
• We generalize the probabilistic framework of CP to LCP, where we assign a unique role to the test
point by introducing a localizer around it. The generalized framework still enjoys a distribution-free
and ﬁnite-sample marginal coverage guarantee. CP is a special case of LCP where the localizer takes
a constant value.
• We focus on sample-splitting LCP and develop an eﬃcient implementation. We also demonstrate how
to combine LCP with some recently developed conformal scores, with numerical examples.
• We investigate the local behavior of sample-splitting LCP and show that it enjoys additional local
coverage guarantees under proper assumptions.
We postpone all proofs to Appendix B in the online Supplement.
2

−4
−2
0
2
4
0
2
4
6
regression score
x
v
−4
−2
0
2
4
−4
−2
0
2
quantile regression score
x
v
−4
−2
0
2
4
−6
−4
−2
0
2
4
6
x
y
CR:3.86
CQR:3.66
LCR:3.44
LCQR:3.46
truth:3.32
Figure 1: Conformal bands (blue) and localized conformal bands (red) using regression score (dashed) and quantile
regression score (solid). Bands are shown for the conformal score Vn+1 (left) and response value Yn+1 (right). True
prediction bands for the distribution of Y given X are shown on the right in green, and dots show the realized test
observation values. The legend indicates average PI length, which is shorter for the localized procedures.
2
LCP: A generalization of conformal prediction
2.1
Notations
For any distribution F on R, we deﬁne its level α quantile as
Q(α; F) = inf{t : PT ∼F{T ≤t} ≥α}.
Let X = {X1, . . . , Xn+1} be the unordered set of feature values from all (n + 1) samples. LCP weights
samples diﬀerently based on a bi-variate localizer function H(x, x′) : Rp × Rp 7→[0, 1], whose function form
may depend on the data through only X. We require H(x, x) = 1 for all x and use H(x, x′) to capture the
dissimilarity between two given feature values. In the Introduction, we considered H(x, x′) = exp(−5|x−x′|)
as an example where H(Xn+1, Xi) is the localizer evaluated at Xn+1 and Xi. The localizer function H(x, x′)
is used on construct diﬀerent weighted distributions for performing LCP. Deﬁne Hi(.) := H(Xi, .) as the
localizer centered at Xi, and Hi,j := Hi(Xj) = H(Xi, Xj) as a measure of dissimilarity between samples Xi
and Xj.
Let δv be a point mass at v ∈R. Deﬁne weighted distributions
ˆFi :=


n+1
X
j=1
pH
i,jδVj

, for all i = 1, . . . , n + 1,
where the empirical weights pH
i,j :=
Hij
Pn+1
k=1 Hik for j = 1, . . . , n + 1 are constructed using the localizer centered
at Xi. We also deﬁne
ˆF :=


n
X
j=1
pH
n+1,jδVj + pH
n+1,n+1δ∞

,
3

as the distribution when replacing Vn+1 by ∞in ˆFn+1. Note that both Vn+1 and Vi for i = 1, . . . , n may
depend on Yn+1 when V (.) depends on the set Z. Consequently, there could be a dependence on Yn+1 from
both ˆF and ˆFi for i = 1, . . . , n + 1. We have masked such a dependence for convenience.
Throughout this paper, we call {Z1, . . . , Zn} the calibration set and assume Zi
i.i.d
∼PXY for i = 1, . . . , n+
1, and α ∈(0, 1) is a constant and user-speciﬁed targeted coverage.
2.2
LCP and marginal coverage guarantee
We now establish the probabilistic guarantees of LCP regarding its marginal coverage. Instead of using
the level α quantile of the empirical distribution as in CP, LCP considers a level ˜α quantile of a weighted
empirical distribution, with weight proportional to Hn+1,i. Recall that Hn+1,i measures the distance between
a training sample Xi and the test sample Xn+1. This weighted distribution allows more emphasis on training
samples closer to Xn+1.
Theorem 2.1 states how we can choose ˜α to achieve ﬁnite sample coverage. In Theorem 2.6, we show that
a randomized decision rule can lead to a PI with exact coverage. Let Γ = {P
k∈Ii pH
ik : i = 1, . . . , n + 1, Ii ⊆
{1, . . . , n + 1}} to represent all possible empirical CDF function values from weighted distributions ˆFi for
i = 1, . . . , n + 1, under all possible ordering of V1, . . . , Vn+1.
Theorem 2.1. Let ˜α be the smallest value in Γ such that
n+1
X
i=1
1
n + 11Vi≤Q(˜α; ˆ
Fi) ≥α
(2.1)
Then P
n
Vn+1 ≤Q(˜α; ˆFn+1)
o
≥α. Equivalently, P
n
Vn+1 ≤Q(˜α; ˆF)
o
≥α.
Remark 2.2. If Hi,j = 1 for all i, j = 1, . . . , n + 1, then we have ˆF =
1
n+1 (Pn
i=1 δVi + δ∞) and ˆFi =
1
n+1
Pn+1
i=1 δVi

for all i = 1, . . . , n + 1.
Then (2.1) holds if and only if ˜α ≥α by deﬁnition.
Also,
Γ = {
k
n+1 : k = 1, . . . , n + 1}. Thus, we recover usual conformal prediction (Vovk et al. 2005), and
P
(
Vn+1 ≤Q
 
k
n + 1;
1
n + 1
n+1
X
i=1
δVi
!)
≥α, for k ≥⌈(n + 1)α⌉.
Here, we provide some intuition for why such ˜α can guarantee level α coverage. Conformal prediction
relies on the exchangeability of data. Conditional on the set Z, the set of observed values V = {v1, . . . , vn+1}
for V1:(n+1) is ﬁxed and Vn+1 has equal probability of taking each of value in V. Hence Q(α,
1
n+1
Pn+1
i=1 δvi)
leads to a coverage guarantee conditional on the observed values, and a marginal coverage guarantee after
marginalizing over all value sets. When our PI is constructed as Q(˜α, ˆFn+1), since ˆFn+1 changes as we
permute the value assignments, we need to account for this change while calculating the conditional coverage.
The left-hand-side of (2.1) turns out to be this coverage conditional on {v1, . . . , vn+1} for any given ˜α. As
in CP, we can invert the relationship (2.1) to construct the PI for Yn+1.
Corollary 2.3. In the setting of Theorem 2.1, deﬁne ˜α(y) ∈Γ as the smallest value in Γ such that (2.1) holds
at Zn+1 = (Xn+1, y). Let C(Xn+1) := {y : Vn+1 ≤Q(˜α(y); ˆF)}. Then, we have P {Yn+1 ∈C(Xn+1)} ≥α.
What will happen if we simply let ˜α = α without tuning it based on (2.1)? The answer depends on the
localizer H. Setting ˜α = α can lead to over-coverage in the simple example described by Proposition 2.4
below, where we tend to assign too little weight to the calibration samples. A more interesting example is
given in Proposition 2.5 below, showing that we may end up achieving arbitrarily bad under-coverage by
naively setting ˜α = α.
Proposition 2.4. Consider the localizer H(x1, x2) = exp(−|x1−x2|
h
) with some small h > 0, such that
P(Pn+1
i=1 H(Xn+1, Xi) <
1
1−α) ≥ε ∈(α, 1). Then,
P(Q(α; ˆF) = ∞) ≥ε,
P(Vn+1 ∈C(Xn+1)) ≥ε.
4

Proposition 2.5. Let {ej : j = 1, . . . , p} be the standard basis in Rp.
Set q1 =
(1−α)
2p(1−α)+α and q0 =
α
2p(1−α)+α. Suppose that the feature X ∈Rp and response Y are distributed as
Y |X ∼
 Unif(−1, 1),
when X ̸= 0.
0,
otherwise.
, X =



ej,
w.p. q1, for all j = 1, . . . , p,
−ej,
w.p. q1, for all j = 1, . . . , p,
0,
w.p. q0.
Let V (Zi) = |Yi| be the regression score. Then for any constant p ≥1, we have limn→∞P(Vn+1 ≤Q(α; ˆF)) =
q0 < α.
Proposition 2.5 shows an example in which we no longer enjoy the distribution-free marginal coverage
guarantee, and the under-coverage can be arbitrarily poor for large p. Hence, strategically choosing ˜α is
crucial to obtain such a guarantee. We note that this distribution-free marginal coverage guarantee is usually
motivation for using conformal prediction as opposed to other model-based prediction intervals.
As in the case of CP, we may not have exact level α-coverage due to rounding issues using a non-random
construction rule. However, we can have exact α-coverage if we allow for some additional randomness, as
stated in Theorem 2.6.
Theorem 2.6. Consider the setting of Theorem 2.1. Let ˜α1/˜α2 be the smallest/largest value in Γ∪{0} such
that
α1 :=
n+1
X
i=1
1
n + 11Vi≤Q(˜α1; ˆ
Fi) ≥α,
α2 :=
n+1
X
i=1
1
n + 11Vi≤Q(˜α2; ˆ
Fi) < α.
Set ˜α =
 ˜α1
w.p.
α−α2
α1−α2
˜α2
w.p.
α1−α
α1−α2
. Then, P
n
Vn+1 ≤Q(˜α; ˆF)
o
= α.
In this section, we presented LCP with general and potentially data-dependent V (.) = V (.; Z), and
showed that CP is its special case with Hij = 1. The discussion of this general construction is for theoretical
completeness, as the general recipe described in Theorem 2.1 or Corollary 2.3 is too computationally expen-
sive: for every Yn+1 = y, we need to retrain our prediction model to get V (.; Z). This problem exists in CP
with data-dependent scores, and sample splitting is often used to reduce the computation cost (Papadopoulos
et al. 2002, Lei et al. 2015).
For the remainder of this paper, we shift our focus to sample-splitting LCP, where we divide the observed
data into a training set and calibration set. The score function V (.) is estimated with the training set and
considered ﬁxed afterwards, and the PI is constructed using the ﬁxed score function and the calibration set.
3
Sample-splitting LCP
3.1
Sample-splitting LCP and marginal coverage guarantee
This section considers sample-splitting LCP and develops an eﬃcient algorithm. In sample-splitting LCP, we
divide the observed data into the training set D0 of size n0, and calibration set D of size n. We ﬁrst construct
the score function V (.) based on D0. For example, we may let V (Z) = |Y −ˆµ(X)| where ˆµ(.) is a prediction
function for Y learned using D0. Since V (Z) does not depend on the calibration set and the test sample,
we refer to it as a data-independent score. We let {Z1, . . . , Zn} denote samples of the calibration set, and
Zn+1 the test sample. In this setting, because V (.) is ﬁxed, the empirical distributions ˆFi for i = 1, . . . , n+1
depend on the value y of a test sample (Xn+1, y) only via v = V (Xn+1, y). Thus ˜α(y) as deﬁned in Corollary
2.3 also depends on y only via v. With a small abuse of notation, we will henceforth write ˜α(v) in place
of ˜α(y), where v = V (Xn+1, y). To make explicit the dependence of the empirical distribution ˆFi on v, we
introduce
ˆFi(v) := ˆFi when Vn+1 = v.
(3.1)
We express Theorem 2.1 and Corollary 2.3 with sample-splitting using Lemma 3.1 below, where we can
easily check that the PI for Vn+1 is an interval.
5

Lemma 3.1. Let V (.) be a ﬁxed score function. At Vn+1 = v, deﬁne ˜α(v) to be the smallest value of ˜α ∈Γ
such that,
n+1
X
i=1
1
n + 11Vi≤Q(˜α; ˆ
Fi(v)) ≥α.
(3.2)
Set CV (Xn+1) = {v : v ≤Q(˜α(v); ˆF)}, C(Xn+1) = {y : V (Xn+1, y) ∈CV (Xn+1)}. Then CV (Xn+1) is an
interval, and
P {Vn+1 ∈CV (Xn+1)} ≥α,
P {Yn+1 ∈C(Xn+1)} ≥α.
Lemma 3.1 is intuitively simple. However, even though the score function V (.) is pre-speciﬁed, it is still
unrealistic to compute for every possible value of vn+1 = V (Xn+1, y) its own value of ˜α(vn+1). In Section
3.2, we provide an eﬃcient implementation of LCP to tackle this problem.
3.2
An eﬃcient implementation of LCP
We provide an O(n log n) implementation of LCP, given pre-calculated localizer function values for each pair
of calibration samples and the associated unnormalized cumulative probabilities.
Without loss of generality, we assume that the calibration samples are ordered by their score values and
V1 ≤V2 ≤. . . ≤Vn. Let V i be the augmented observation with V i = Vi for i = 1, . . . , n, V n+1 = ∞and
V 0 = −∞. For all i = 1, . . . , n + 1, we deﬁne
• ℓ(i) = max{i′ ∈{1, . . . , n} : Vi′ < V i} as the largest index of values Vi′ that are smaller than V i. In
the case where all Vi values are distinct, we have ℓ(i) = i −1. We set the maximum of an empty set
as 0, so in particular, ℓ(1) = 0 always.
• θi := Pℓ(i)
j=1 pH
i,j as the cumulative probability at the value V l(i) in the distribution ˆFi(∞).
• ˜θi := Pℓ(i)
j=1 pH
n+1,j as the cumulative probability at V l(i) in the distribution ˆF.
• θi = ˜θi = 0 if ℓ(i) = 0. In particular, θ1 = ˜θ1 = 0 always.
Lemma 3.2 below is the foundation of our implementation. The ﬁrst part of Lemma 3.2 describes a formula-
tion to construct the closure of the PI CV (Xn+1) from Lemma 3.1 that does not explicitly require calculation
of ˜α(vn+1) for diﬀerent values of vn+1 = V (Xn+1, y). This formulation depends on a quantity S(k) deﬁned
in (3.3). The second part of Lemma 3.2 gives another equivalent characterization of S(k) that enables its
computation for all k = 1, . . . , n + 1 in O(n log n) time.
Lemma 3.2 (Practical implementation of LCP).
1. Let k∗be the largest index k ∈{1, . . . , n + 1} such that
S(k) :=
n
X
i=1
1
n + 11Vi≤Q(˜θk; ˆ
Fi(V ℓ(k))) < α.
(3.3)
Then, ¯CV (Xn+1) = {v : v ≤V k∗} is the closure of CV (Xn+1) from Lemma 3.1.
2. We may partition the n calibration samples into three sets: A1 := {i : pH
i,n+1 + θi < ˜θi}, A2 := {i : θi ≥
˜θi}, and A3 := {i : pH
i,n+1 + θi ≥˜θi, θi < ˜θi}. For k = 1, . . . , n + 1, we have
S(k) =
X
i∈A1
1
n + 11θi+pH
i,n+1<˜θk +
X
i∈A2
1
n + 11θi<˜θk +
X
i∈A3
1
n + 11l(i)<ℓ(k).
(3.4)
Here, we provide some intuition for why (3.3) and (3.4) are equivalent. Observe that ˜θk and V ℓ(k) are
both non-decreasing in k. Then the quantile Q(˜θk, ˆFi(V ℓ(k))) is also non-decreasing in k, where we recall
the deﬁnition (3.1) for ˆFi(v). As a result, deﬁning the event Jik = {Vi ≤Q(˜θk; ˆFi( ¯Vℓ(k)))} in the indicator
of (3.3), once Jik holds for some k, it holds also for all larger k. Thus, for each i = 1, . . . , n, we need only
determine the smallest k for which Jik ﬁrst holds. There are two cases:
6

• If Jik ﬁrst holds at a value k with Vi > ¯Vℓ(k), by deﬁnition of ˆFi(V ℓ(k)), we need
˜θk >
X
j≤n:Vj<Vi
pH
i,j + pH
i,n+1 = θi + pH
i,n+1.
• If Jik ﬁrst holds at a value k with Vi ≤¯Vℓ(k), then we need instead ˜θk > P
j≤n:Vj<Vi pH
i,j = θi. To
guarantee that Vi ≤¯Vℓ(k), we also require ℓ(k) > ℓ(i).
Let ki be the smallest index k for which Jik ﬁrst holds. We can show that
• A1 contains all i such that Vi > ¯Vℓ(ki).
• A2 contains all i such that Vi ≤¯Vℓ(ki) and {˜θki > θi, ℓ(ki) > ℓ(i)} = {˜θki > θi}.
• A3 contains all i such that Vi ≤¯Vℓ(ki) and {˜θki > θi, ℓ(ki) > ℓ(i)} = {ℓ(ki) > ℓ(i)}.
This will establish the equivalence between (3.3) and (3.4).
The desirable aspect of dividing calibration samples into A1, A2, A3 is that we can now order the cali-
bration samples in each set based on the values of θi + pH
i,n+1, θi, and l(i) for A1, A2, A3 respectively, and
then compute all values S(k) from (3.4) using a single scan through the values k = 1, . . . , n + 1. Algorithm
1 implements this idea:
• Line 3 calculates ˜θi, θi, and θi + pH
i,n+1 for each i = 1, . . . , n + 1; Line 4 creates A1, A2, A3 according to
Lemma 3.2.
• Line 5 orders i ∈A1 by θi + pH
i,n+1, i ∈A2 by θi, and i ∈A3 by l(i). As we increase k, samples i in
each set A1, A2, A3 will satisfy Vi ≤Q(˜θk; ˆFi(V ℓ(k))) sequentially.
• Lines 7-8, 9-10 and 11-12 perform these sequential checks within each set A1, A2, A3.
• Finally, line 14 produces the largest k∗such that (3.3) holds for any given target level α.
1
Input: (1) Ordered conformal scores V1 ≤. . . ≤Vn, (2) associated unnormalized cumulative
probability matrix Qik = Pk
j=1 Hij for i, k = 1, . . . , n, (3) Hn+1,i and Hi,n+1 for i = 1, . . . , n, and
(4) the targeted level α.
2
Output: A constructed PI CV for Vn+1.
3 θi + pH
i,n+1 ←
Qi,ℓ(i)+Hi,n+1
Qi,n+Hi,n+1 , θi ←
Qi,ℓ(i)
Qi,n+Hi,n+1 , ˜θi ←
Qn+1,l(i)
Pn+1
j=1 Hn+1,j for i = 1, . . . , n + 1.
4 A1 ←{i : θi + pH
i,n+1 < ˜θi}, A2 ←{i : θi ≥˜θi}, A3 ←{i : θi + pH
i,n+1 ≥˜θi, θi < ˜θi}.
5 Set ˇθA1, ˇθA2, ˇθA3 as the ordered values of {θi + pH
i,n+1 : i ∈A1}, {θi : i ∈A2}, and {ℓ(i) : i ∈A3}
respectively. Set cm = 0, Lm = |Am|, for m = 1, 2, 3.
6 for k = 1, 2,. . . , n, n+1 do
7
while c1 < L1 and ˇθA1
c1+1 < ˜θk do
8
c1 ←c1 + 1;
9
while c2 < L2 and ˇθA2
c2+1 < ˜θk do
10
c2 ←c2 + 1;
11
while c3 < L3 and ˇθA3
c3+1 < ℓ(k) do
12
c3 ←c3 + 1;
13
Set S(k) = c1+c2+c3
n+1
.
14 Set k∗= arg max{k : S(k) < α}, and return CV = {v : v ≤V k∗}.
Algorithm 1: Algorithm for LCP
7

3.3
Choice of H
The choice of H will inﬂuence the localization. Given d(x1, x2) as a measure of dissimilarity between two
samples x1, x2, there are numerous ways of deﬁning the functional form for the localizer. In our experiments,
we consider the localizer Hh(x1, x2) = exp(−d(x1,x2)
h
).
A smaller h results in more localization. We want to choose h to have relatively narrow PIs for most
samples. More speciﬁcally, we consider the following constrained objective:
J(h) = Average of PIfinite length + λ × Average of conditional PIfinite length’s variability,
s.t. Average percent of inﬁnite PIs is at most ε.
The parameter λ reﬂects our aversion for the variability of constructed PI length at each ﬁxed point Xn+1 = x
of the sample space. We set λ = 1 by default.
These averages are unknown and need to be estimated from the data. Recall that the score function
V (.) is constructed using an independent training set D0, whose model complexity is often tuned with cross-
validation. We suggest using D0 and its cross-validated scores to empirically estimate the three terms in the
above objective. The mathematical deﬁnitions of J(h) and details of the empirical estimates are given in
Appendix D.
In low dimensions, we can have asymptotic conditional coverage as n →∞using typical distance dis-
similarities, e.g., Euclidean distance, and by choosing h →0 under suitable assumptions (see Section 5).
This is an ideal setting. In practice, a good user-speciﬁed dissimilarity function d(., .) will lead to improved
performance in terms of constructed PI length and adaptation to the underlying heterogeneity.
Such a
dissimilarity function should capture directions of feature space in which the PI (of V) is more likely to
vary. A comprehensive and in-depth discussion of d(., .), especially in high dimensions, is beyond the scope
of this paper. In our numerical experiments, we will deﬁne d(., .) as a weighted sum of three components:
(1) d1(x1, x2) = ∥ˆρ(x1) −ˆρ(x2)∥2, where ˆρ(X) is the estimated spread of V (X, Y ) conditional on X (Lei
et al. 2018); (2) d2(x1, x2) = ∥P∥(x1 −x2)∥2, where P∥is the projection onto the space spanned by the top
singular vectors of the Jacobian matrix of ˆρ(X) for X ∈D0; and (3) d3(x1, x2) = ∥P⊥(x1 −x2)∥2, where P⊥
is the projection onto the space orthogonal to P∥.
We include the ﬁrst component since ˆρ(X) is trying to capture the heterogeneity of V (X, Y ). We include
the second and the third components because ˆρ(X) may not fully capture this heterogeneity, so that the
dissimilarity still depends on other directions of feature space. Intuitively, we can think of the projection
P∥as capturing the directions of feature space in which ρ(X) is more variable across the training set, and
P⊥as capturing the remaining less important directions. We provide more details on constructing d(., .) in
Appendix D.
4
Empirical studies: Comparison of CP and LCP
We compare LCP and CP in this section, using diﬀerent numerical examples to demonstrate their diﬀerences
and potential gains using LCP. We consider the usual regression problem:
Y = µ(X) + ε, ε ⊥⊥X,
and four types of conformal score construction:
Regression score V R(X, Y ) = |Y −ˆµ(X)| where ˆµ(X) is an estimate of µ(X) learned from the training
set. We denote the two diﬀerent procedures based on the regression score as conformalized regression (CR)
and localized&conformalized regression (LCR).
Locally weighted regression score V R−local(X, Y ) = V R(X,Y )
ˆρ(X)
, where ˆρ(X) is the estimated spread of
V R(X, Y ) (Lei et al. 2018). The locally weighted regression score also leads to two procedures: conformalized
locally weighted regression (CLR) and localized&conformalized locally weighted regression (LCLR).
Quantile regression score V QR(X, Y ) = max{ˆqlo(Xi) −Y, Y −ˆqhi(X)}, where ˆqlo(.) and ˆqhi(.) are the
estimated lower and upper α
2 quantiles from the training set Romano et al. (2019). The two procedures
based on the quantile regression score are conformalized quantile regression (CQR), and local&conformalized
quantile regression (LCQR).
8

Table 1: Example 4.1. Coverage and length comparison for CR and LCR (auto-tuned) across four simulation setups.
Column names representing LCP procedure are in bold. We also highlight the smallest ave.PI and the method that
is within .05 away from it in diﬀerent settings.
setting A
setting B
setting C
setting D
CR
LCR
CR
LCR
CR
LCR
CR
LCR
coverage
0.95
0.94
0.95
0.95
0.95
0.95
0.94
0.94
inﬁnite PI%
–
0.00
–
0.00
–
0.01
–
0.00
ave.PI
2.77
2.27
3.14
3.01
4.26
3.15
3.81
3.86
Table 2: Example 4.1.
Comparisons of coverage, percent of inﬁnite PI, ave.PI and ave.PI0 for diﬀerent tuning
parameters h, where ave.PI is the average length for ﬁnite PIs at the given h, and ave.PI0 is the average length for
PIs that are ﬁnite using all h considered. We highlight all ave.PI0 for h no greater than the auto-tuned ˆh in each
setting.
setting A
0.05
0.07
0.09
0.13
0.17
0.22
0.29
0.39
0.52
0.69
0.91
1.21
1.61
2.14
2.84
3.78
5.01
6.66
8.84
11.74
coverage
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.94
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
inﬁnite PI%
0.23
0.15
0.07
0.04
0.02
0.01
0.01
0.01
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
ave.ﬁnitePI
2.22
2.27
2.27
2.28
2.29
2.32
2.32
2.32
2.32
2.29
2.27
2.36
2.43
2.51
2.57
2.61
2.67
2.70
2.71
2.74
ave.PI0
2.22
2.21
2.20
2.18
2.19
2.23
2.24
2.26
2.25
2.22
2.22
2.32
2.40
2.49
2.55
2.61
2.66
2.69
2.70
2.73
setting B
0.08
0.1
0.13
0.18
0.24
0.32
0.43
0.57
0.76
1.02
1.36
1.81
2.42
3.24
4.32
5.77
7.7
10.28
13.73
18.33
coverage
0.94
0.95
0.95
0.94
0.94
0.95
0.94
0.94
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
inﬁnite PI%
0.17
0.09
0.07
0.05
0.04
0.03
0.01
0.01
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
ave.ﬁnitePI
2.81
2.68
2.67
2.63
2.65
2.65
2.66
2.73
2.82
2.87
2.98
3.01
3.03
3.07
3.08
3.11
3.12
3.13
3.13
3.13
ave.PI0
2.81
2.81
2.84
2.81
2.83
2.83
2.84
2.89
2.96
2.98
3.06
3.07
3.07
3.10
3.11
3.13
3.13
3.14
3.13
3.14
setting C
0.04
0.06
0.09
0.12
0.17
0.24
0.33
0.46
0.65
0.91
1.27
1.78
2.49
3.48
4.87
6.83
9.56
13.38
18.74
26.23
coverage
0.94
0.94
0.94
0.94
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
inﬁnite PI%
0.29
0.22
0.14
0.09
0.07
0.05
0.03
0.02
0.01
0.01
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
ave.ﬁnitePI
1.88
2.12
2.42
2.68
2.81
2.87
3.02
3.08
3.15
3.24
3.34
3.48
3.62
3.80
3.94
4.01
4.10
4.16
4.19
4.21
ave.PI0
1.88
1.88
1.87
1.91
1.91
1.93
1.99
2.06
2.20
2.44
2.65
2.97
3.25
3.57
3.80
3.90
4.03
4.12
4.16
4.19
setting D
0.08
0.11
0.15
0.2
0.28
0.38
0.53
0.73
1
1.38
1.9
2.61
3.6
4.95
6.82
9.4
12.94
17.82
24.54
33.8
coverage
0.94
0.95
0.94
0.94
0.95
0.94
0.94
0.94
0.94
0.94
0.94
0.94
0.94
0.94
0.94
0.94
0.94
0.94
0.94
0.94
inﬁnite PI%
0.14
0.10
0.05
0.02
0.01
0.01
0.01
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
ave.ﬁnitePI
3.79
3.85
3.92
3.92
3.92
3.90
3.89
3.92
3.89
3.89
3.89
3.87
3.86
3.86
3.88
3.87
3.87
3.86
3.85
3.85
ave.PI0
3.79
3.80
3.80
3.78
3.82
3.82
3.82
3.86
3.84
3.85
3.86
3.85
3.84
3.86
3.87
3.87
3.86
3.86
3.85
3.84
Locally weighted quantile regression score V QR−local = V QR(X,Y )
ˆρ(X)
, which combines quantile regression
with the locally weighted step.
The two related procedures are conformalized locally weighted quantile
regression (CLQR), and local&conformalized locally weighted quantile regression (LCLQR).
In Example 4.1, we visually demonstrate CR and LCR to highlight the procedural diﬀerences, and
compare LCP results with diﬀerent values for h. In Example 4.2, we use synthetic data and compare the
performance of the eight procedures. Example 4.3 compares the results using four publicly available data
sets from UCI. In all empirical examples, we learn the conformal scores using a neural network with three
fully connected layers and 32 hidden nodes.
Example 4.1 (An illustrating example on CR and LCR). Let Y = ε, X ∼N(0, 1), ε ∼ρ(X), with four
diﬀerent cases for ρ(X): (A) ρ(X) = sin(X); (B) ρ(X) = cos(X); (C) ρ(X) =
p
|X|; (D) ρ(X) = 1.
We compare CR and auto-tuned (Section 3.3) LCR, as well as results from LCR using ﬁxed h values.
(The preﬁxed grids for h can be diﬀerent for diﬀerent settings because they are chosen by looking at the
dissimilarity measures on the training set.) The sizes for the training and calibration sets are both 1000. Table
1 compares CR with auto-tuned LCR, and shows the achieved coverage, percents of samples with inﬁnite PI,
and the average length of ﬁnite PI (ave.PI). Table 2 compares LCR from using diﬀerent h. Figure 2 provides
visual demonstrations for CR and LCR using the smallest h with less than 5% of inﬁnite PI for LCR (h1),
the largest h considered (h2) and the auto-tuned h (h3). Choice of h1 results in a highly localized LCR with
PIs better capturing the underlying heterogeneity but potentially less stable and containing inﬁnite PIs with
higher probability, while the choice of h2 results in PIs with almost no localization and almost identical to
CR.
We do not observe an increased average PI length on samples that are well represented by the calibration
9

Figure 2: Example 4.1. Conﬁdence bands constructed using CR, LCR with diﬀerent tuning parameter values for h
at the targeted level α = .95. In each of the sub-plot, we show the test data points with black dots, and the true
conﬁdence bands across diﬀerent Xn+1 as the blue curves, and the estimated PIs using diﬀerent methods as red
curves. We sometimes encounter inﬁnite PIs using LCP. We represent those inﬁnite PIs by widths larger than the
gray horizontal lines.
−3
−2
−1
0
1
2
3
−4
−2
0
2
4
A:CR
Truth
Estimated
−3
−2
−1
0
1
2
3
−4
−2
0
2
4
A:LCR−h1
−3
−2
−1
0
1
2
3
−4
−2
0
2
4
A:LCR−h2
−3
−2
−1
0
1
2
3
−4
−2
0
2
4
A:LCR−h3
−3
−2
−1
0
1
2
3
−4
−2
0
2
4
B:CR
Truth
Estimated
−3
−2
−1
0
1
2
3
−4
−2
0
2
4
B:LCR−h1
−3
−2
−1
0
1
2
3
−4
−2
0
2
4
B:LCR−h2
−3
−2
−1
0
1
2
3
−4
−2
0
2
4
B:LCR−h3
−3
−2
−1
0
1
2
3
−5
0
5
C:CR
Truth
Estimated
−3
−2
−1
0
1
2
3
−5
0
5
C:LCR−h1
−3
−2
−1
0
1
2
3
−5
0
5
C:LCR−h2
−3
−2
−1
0
1
2
3
−5
0
5
C:LCR−h3
−3
−2
−1
0
1
2
3
−6
−4
−2
0
2
4
D:CR
Truth
Estimated
−3
−2
−1
0
1
2
3
−6
−4
−2
0
2
4
D:LCR−h1
−3
−2
−1
0
1
2
3
−6
−4
−2
0
2
4
D:LCR−h2
−3
−2
−1
0
1
2
3
−6
−4
−2
0
2
4
D:LCR−h3
10

Table 3: Example 4.2. Empirical Coverage for diﬀerent procedures across diﬀerent simulation settings, with a targeted
level at α = 0.95. Column names representing LCP procedure are in bold.
CR
LCR
CLR
LCLR
CQR
LCQR
CLQR
LCLQR
setting A
0.952
0.955
0.953
0.954
0.953
0.955
0.953
0.954
setting B
0.951
0.953
0.950
0.954
0.953
0.953
0.953
0.954
setting C
0.948
0.950
0.949
0.950
0.950
0.951
0.951
0.952
setting D
0.948
0.948
0.948
0.949
0.948
0.948
0.947
0.948
Table 4: Example 4.2.
Average lengths of PIs for diﬀerent procedures across four diﬀerent simulation settings.
Column names representing LCP procedure are in bold. The smallest average PI length and those within 0.05 from
it are also highlighted in each setting.
CR
LCR
CLR
LCLR
CQR
LCQR
CLQR
LCLQR
setting A
3.27
2.84
3.05
2.81
2.87
2.87
2.88
2.87
setting B
2.86
2.19
2.54
2.20
2.26
2.27
2.27
2.27
setting C
4.95
3.91
4.42
3.90
3.94
3.95
4.03
4.02
setting D
3.88
3.90
3.89
3.90
3.92
3.93
3.92
3.93
set as we decrease h in a wide range. A smaller h makes the procedure more alert by producing PIs with
inﬁnite length for underrepresented new observations. Is this a bad thing? We believe that the answer to
this question is subjective and depends on the speciﬁc task at hand.
Example 4.2 (Comparisons of diﬀerent procedures, synthetic data). We consider eight procedures by
applying CP and LCP (auto-tuned) to four diﬀerent conformal scores regarding their coverage and PI
lengths at a targeted level α = 0.95. We consider the same simulation setup as in Example 4.1 except with
X ∼U[−2, 2]. In this example, the test observations are reasonably well represented by the calibration
samples (with high probability). We do not observe samples with inﬁnite PIs using LCP and auto-tuned h.
Table 3 and 4 show the results of average coverage and average length of PI in the four simulation settings.
Example 4.3 (Performance comparison on four UCI datasets). We investigate the performances of eight
procedures (with auto-tuned LCP) on four UCI datasets (Rana 2013): CASP (Yeh 1998), Concrete (Yeh
1998), Facebook variant 1 (facebook1) and variant 2 (facebook2)(Singh et al. 2015, Singh 2015). The sizes
of samples and features are (45730,9), (1030,8), (40949, 53), (81312, 53) for the four datasets respectively.
We subsample 5000 training/calibration samples without replacement from CASP, Facebook variant 1,
and Facebook variant 2, and 400 training/calibration samples from the Concrete dataset. We construct PIs
using the remaining samples for each data set and repeat it 20 times. Tables 5 - 6 show the results of average
coverage and average length of ﬁnite PI for the four data sets. The percent of inﬁnite PIs ranges from 0%
to 3% for diﬀerent LCP constructions. The samples with inﬁnite PI using LCP methods on the Facebook
datasets tend to have wider PIs, hence, we also show the average PI length using only samples with ﬁnite
PI from all procedures for a fair comparison.
The CQR procedure has been shown as a top performer
in Romano et al. (2019) and Sesia & Cand`es (2020). Our numerical experiments also conﬁrm that it has
an overall better performance than the CLR. Not only is LCP framework conceptually novel; it uses the
estimated spread ˆρ(X) in a more robust way. When combined with V R and V R−local, the average PI lengths
are smaller for three out of the four real data sets compared with the CLR procedure. In particular, LCR
and LCLR are even noticeably better than CQR in the two Facebook examples.
Table 5: Example 4.3. Empirical Coverage for diﬀerent procedures, with a targeted level at α = 0.95. Column names
representing LCP procedure are highlighted.
CR
LCR
CLR
LCLR
CQR
LCQR
CLQR
LCLQR
CASP
0.949
0.950
0.950
0.950
0.950
0.950
0.950
0.950
Concrete
0.947
0.949
0.943
0.947
0.951
0.953
0.952
0.954
facebook1
0.949
0.950
0.949
0.949
0.950
0.951
0.950
0.951
facebook2
0.951
0.951
0.951
0.951
0.953
0.952
0.953
0.952
11

Table 6: Example 4.3. The upper half shows the average length using samples with ﬁnite PIs for the given procedure.
The lower half shows the average PI length on the common set of samples with ﬁnite PI for all procedures. Column
names representing LCP procedure are in bold. The smallest average PI length and those within 0.05 from it are
also highlighted in each setting.
Procedure-speciﬁc samples
CR
LCR
CLR
LCLR
CQR
LCQR
CQLR
LCQLR
CASP
3.03
2.82
2.88
2.81
2.65
2.64
2.65
2.64
Concrete
1.52
1.48
1.45
1.46
1.58
1.59
1.58
1.57
facebook1
1.12
0.67
> 100
0.69
0.92
0.92
0.92
0.92
facebook2
1.05
0.65
> 100
0.65
0.92
0.92
0.93
0.92
Common samples
CR
LCR
CLR
LCLR
CQR
LCQR
CQLR
LCQLR
CASP
3.03
2.82
2.88
2.81
2.65
2.64
2.65
2.64
Concrete
1.52
1.47
1.43
1.45
1.56
1.56
1.56
1.55
facebook1
1.12
0.66
0.95
0.66
0.74
0.74
0.74
0.74
facebook2
1.05
0.62
0.87
0.63
0.74
0.74
0.74
0.74
5
Local behavior of LCP
In this section, we consider asymptotic and approximate conditional coverage properties for LCP, as well as
for a simpliﬁed version of LCP that uses the choice ˜α = α. We have shown in Proposition 2.5 that choosing
˜α = α does not yield a distribution-free coverage guarantee. Our results here indicate that this choice may
lead to asymptotic or approximate conditional coverage, under certain assumptions.
For simplicity, in this section we restrict attention to a localizer H(x1, x2) = exp(−d(x1,x2)
hn
) where hn is
an n-dependent bandwidth parameter, and d(x1, x2) ≥0 is a measure of dissimilarity satisfying d(x1, x1) = 0.
Asymptotic conditional coverage. Non-trivial ﬁnite sample and distribution-free conditional coverage
is impossible for continuous distributions (Lei & Wasserman 2014, Vovk 2012). Thus, it is common to consider
asymptotic conditional coverage under proper assumptions on PXY . Diﬀerent conformal score constructions
with such asymptotic conditional coverage are studied in the literature. For instance, in Izbicki et al. (2019),
the authors consider using the estimated conditional density as the conformal score, and in Romano et al.
(2019), the authors use the conformal score based on estimated quantile functions. Here, we consider the
asymptotic behavior of LCP.
Assumption 5.1. X has continuous distribution on [0, 1]p, and V (Z) has continuous distribution conditional
on X = x. Furthermore, there exist constants L > 0 and β ≥0 such that the density of X satisﬁes pX(x) ≥1/
L for all x ∈[0, 1]p, and
(i) The conditional distribution of V given X satisﬁes for all x, x′ ∈[0, 1]p
max
v∈R
PV |x(v) −PV |x′(v)
 ≤Ld(x, x′),
where PV |x(v) is the probability that V (Z) ≤v conditional on X = x.
(ii) P(X ∈{x : d(x0, x) ≤ε}) ≥εβ/L for all ε ≤hn and all x0 ∈[0, 1]p.
(iii) hn is chosen such that hn →0 and (nhβ
n/ ln n) →∞as n →∞.
Under this assumption, statement (5.1) of the following theorem guarantees that LCP with ˜α(v) chosen
as in Lemma 3.1 achieves asymptotic conditional coverage at the target level α. Furthermore, statements
(5.2) and (5.3) show that ˜α(v) converges to α in probability asymptotically, and asymptotic conditional
coverage holds also if LCP is applied with the simpler choice ˜α = α.
Theorem 5.2. Deﬁne ˜α(v) and CV (Xn+1) as in Lemma 3.1. Under Assumption 5.1, for any x0 ∈[0, 1]p,
we have
lim
n→∞P(Vn+1 ∈CV (Xn+1)|Xn+1 = x0) = α,
(5.1)
12

lim
n→∞P(Vn+1 ≤Q(α; ˆF)|Xn+1 = x0) = α,
(5.2)
lim
n→∞P(max
v
|˜α(v) −α| < ε|Xn+1 = x0) = 1, for all ε > 0.
(5.3)
In Assumption 5.1, the measure d(x, x′) can be deﬁned to capture the directions where the conditional
distribution of V given X is more likely to change as we vary X. Assumption 5.1 (i) allows more variability in
some directions and less in others based on how the data is generated, and scales better with the dimension
compared to a symmetric distance such as the Euclidean distance. Assumption 5.1 (ii) assumes that d(x0, x)
has enough concentration around 0, and it holds for a typical dissimilarity measure in low dimensions. In
high dimensions, this assumption holds if d(·, ·) emphasizes a few directions instead of treating all directions
equally. For example, if d(x0, x) = |xj −x0,j| depends only on feature j, then P({X : d(x0, X) ≤ε}) ≥1
Lε
for some large constant L. Assumption 5.1 (iii) requires hn to decay to 0 at a suﬃciently slow rate. This is so
that, combined with Assumption 5.1 (ii), we may ensure that Pn
i=1 H(Xj, Xi) →∞for all j = 1, . . . , n + 1,
with high probability. In particular, a setting such as described in Proposition 2.4 cannot occur.
Approximate conditional coverage.
In Vovk (2012) and Lei & Wasserman (2014), the authors
partition the feature space into K ﬁnite subsets and apply conformal inference to each of the subsets:
This guarantees P{Yn+1 ∈bC(Xn+1)|Xn+1 ∈Xk} ≥α for all k = 1, 2, . . . , K and some ﬁxed partition
∪K
k=1Xk = Rp. In Barber et al. (2019b), the authors consider a potentially stronger version where diﬀerent
regions Xk may overlap.
Barber et al. (2019a) introduce a diﬀerent notion of approximate conditional
coverage, where instead of ﬁnding C(x0) that achieves conditional coverage of Yn+1 given Xn+1 = x0, the
authors consider C(x0) that covers ˜Y whose feature value ˜X is distributed according to some locally weighted
distribution around x0. (See Eqs. (18–19) of Barber et al. (2019a).) When this weighted distribution becomes
increasingly concentrated around x0, the distribution of ˜Y intuitively approaches the conditional distribution
of Yn+1, so this serves as an approximation to conditional coverage. Here, we show that for a local weighting
given by H(x0, x) = exp(−d(x0,x)
hn
), an LCP procedure using this same H(·, ·) as its localizer and ˜α = α can
achieve this guarantee for every ﬁxed x0.
Theorem 5.3. Fix any x0. Deﬁne the weighted distribution
d ˜
Px0
X (x)
dx
∝dPX(x)
dx
H(x0, x). Conditional on
Xn+1 = x0, let ˜Z = ( ˜X, ˜Y ) ∼˜Px0
X × PY |X. Deﬁne
ε(Xn+1) =
max
x:d(Xn+1,x)<∞max
y
|V (Xn+1, y) −V (x, y)|,
˜C(Xn+1) =
n
y : V (Xn+1, y) ≤Q(α; ˆF) + ε(Xn+1)
o
,
where d(., .) is the dissimilarity measure that deﬁnes H(., .). Then
P{V ( ˜Z) ≤Q(α; ˆF)|Xn+1 = x0} ≥α,
P{ ˜Y ∈˜C(Xn+1)|Xn+1 = x0} ≥α.
(5.4)
The interval ˜C(Xn+1) above remains a PI at Xn+1, and does depend on ˜X in its construction. The term
ε(Xn+1) is introduced to bound the discrepancy in the score function as we vary x in a deﬁned neighborhood
around Xn+1 = x0 with d(x0, x) < ∞. The value of ε(Xn+1) depends only on the score function V (.) and
our deﬁnition of d(., .), not the data distribution PXY . For example, we can choose d(., .) to exclude samples
that are far from each other by setting d(x0, x) = 0 when ∥x0 −x∥2 ≤h and d(x0, x) = ∞otherwise. In this
case, when V (Z) = |µ(X)−Y | is the regression score, we have ε(Xn+1) ≤max∥Xn+1−x∥2≤h |µ(Xn+1)−µ(x)|
by triangle inequality.
6
Discussion
We propose LCP as an extension to the conventional conformal prediction framework, which uses a weighted
empirical distribution around the test sample. In our numerical experiments, LCP improves over CP when
there is heterogeneity in the distribution of the score function, and the localizer in LCP is deﬁned by a
dissimilarity measure d(., .) that captures the relevant directions of such heterogeneity. Otherwise, auto-
tuned LCP ends up with PIs very similar to those from CP, given the same conformal score function. Thus,
ignoring the computational cost, there is little loss in replacing CP with LCP.
13

One downside of LCP is its computation compared to CP. The bulk of the additional computation lies
in calculating and sorting the weights for the empirical distributions. One future direction is to reduce the
computational cost of LCP for a huge calibration set. For example, we may combine LCP with proper clus-
tering methods or estimate an approximated cumulative probability matrix using machine learning methods
to reduce the computational cost.
CP has been used in classiﬁcation problems for outlier detection (Hechtlinger et al. 2018, Guan & Tib-
shirani 2019). LCP may also be a useful framework for making predictions in the presence of outliers. When
choosing a suitably small h, LCP becomes sensitive to outliers while not increasing much the length of PI
for test samples well-represented by calibration data.
In this paper, we considered the one-dimensional regression response.
CP has also been applied to
other data types, including survival data and data with multi-dimensional responses (Cand`es et al. 2021,
Izbicki et al. 2019, Feldman et al. 2021). For multi-dimensional responses, a rectangular region formed by
outer products of PIs of the individual responses does not capture potential relationships between diﬀerent
responses. Various authors have worked on constructions of PIs for multi-dimensional responses to address
this issue (Paindaveine & ˇSiman 2011, Kong & Mizera 2012) and Feldman et al. (2021) has incorporated
such constructions into CP recently. Another direction for future work is to apply the idea of LCP in similar
contexts.
References
Barber, R. F., Candes, E. J., Ramdas, A. & Tibshirani, R. J. (2019a), ‘Conformal prediction under covariate
shift’, arXiv preprint arXiv:1904.06019 .
Barber, R. F., Candes, E. J., Ramdas, A. & Tibshirani, R. J. (2019b), ‘The limits of distribution-free
conditional predictive inference’, arXiv preprint arXiv:1903.04684 .
Cand`es, E. J., Lei, L. & Ren, Z. (2021), ‘Conformalized survival analysis’, arXiv preprint arXiv:2103.09763
.
Feldman, S., Bates, S. & Romano, Y. (2021), ‘Calibrated multiple-output quantile regression with represen-
tation learning’, arXiv preprint arXiv:2110.00816 .
Guan, L. & Tibshirani, R. (2019), ‘Prediction and outlier detection in classiﬁcation problems’, arXiv preprint
arXiv:1905.04396 .
Gupta, C., Kuchibhotla, A. K. & Ramdas, A. (2021), ‘Nested conformal prediction and quantile out-of-bag
ensemble methods’, Pattern Recognition p. 108496.
Hechtlinger,
Y.,
P´oczos,
B. & Wasserman,
L. (2018),
‘Cautious deep learning’,
arXiv preprint
arXiv:1805.09460 .
Izbicki, R., Shimizu, G. T. & Stern, R. B. (2019), ‘Flexible distribution-free conditional predictive bands
using density estimators’, arXiv preprint arXiv:1910.05575 .
Kong, L. & Mizera, I. (2012), ‘Quantile tomography: using quantiles with multivariate data’, Statistica
Sinica pp. 1589–1610.
Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J. & Wasserman, L. (2018), ‘Distribution-free predictive
inference for regression’, Journal of the American Statistical Association 113(523), 1094–1111.
Lei, J., Rinaldo, A. & Wasserman, L. (2015), ‘A conformal prediction approach to explore functional data’,
Annals of Mathematics and Artiﬁcial Intelligence 74(1-2), 29–43.
Lei, J. & Wasserman, L. (2014), ‘Distribution-free prediction bands for non-parametric regression’, Journal
of the Royal Statistical Society: Series B (Statistical Methodology) 76(1), 71–96.
Paindaveine, D. & ˇSiman, M. (2011), ‘On directional multiple-output quantile regression’, Journal of Mul-
tivariate Analysis 102(2), 193–212.
14

Papadopoulos, H., Proedrou, K., Vovk, V. & Gammerman, A. (2002), Inductive conﬁdence machines for
regression, in ‘European Conference on Machine Learning’, Springer, pp. 345–356.
Rana, P. (2013), ‘Physicochemical properties of protein tertiary structure data set’, UCI Machine Learning
Repository .
Romano, Y., Patterson, E. & Candes, E. (2019), Conformalized quantile regression, in ‘Advances in Neural
Information Processing Systems’, pp. 3538–3548.
Sesia, M. & Cand`es, E. J. (2020), ‘A comparison of some conformal quantile regression methods’, Stat
9(1), e261.
Shafer, G. & Vovk, V. (2008), ‘A tutorial on conformal prediction’, Journal of Machine Learning Research
9(Mar), 371–421.
Singh, K. (2015), ‘Facebook comment volume prediction’, International Journal of Simulation: Systems,
Science and Technologies 16(5), 16–1.
Singh, K., Sandhu, R. K. & Kumar, D. (2015), Comment volume prediction using neural networks and deci-
sion trees, in ‘IEEE UKSim-AMSS 17th International Conference on Computer Modelling and Simulation,
UKSim2015 (UKSim2015)’.
Vovk, V. (2012), Conditional validity of inductive conformal predictors, in ‘Asian conference on machine
learning’, pp. 475–490.
Vovk, V., Gammerman, A. & Shafer, G. (2005), Algorithmic learning in a random world, Springer Science
& Business Media.
Vovk, V., Nouretdinov, I., Gammerman, A. et al. (2009), ‘On-line predictive linear regression’, The Annals
of Statistics 37(3), 1566–1590.
Yeh, I.-C. (1998), ‘Modeling of strength of high-performance concrete using artiﬁcial neural networks’, Ce-
ment and Concrete research 28(12), 1797–1808.
15

In this Supplement, we describe a few supplemental Lemmas used in our proofs to results in the main
paper in Appendix A. We then give proofs to results in the main paper in Appendix B and proofs to the
supplemental Lemmas in Appendix C. We provide details of the construction of the dissimilarity measure
d(., .) used in this paper and the automatic choice of h in Appendix D.
Without loss of generality, we always assume that V1 ≤V2 ≤. . . ≤Vn in this supplement.
A
A collection of supplemental Lemmas
Lemma A.1 describes the elementary relationship used in the proof from previous work on weighted conformal
prediction (Barber et al. 2019a), and we state it here for the reader’s convenience. Lemma A.2 states the
monotone dependence of Q(˜α; ˆFi(v)) on ˜α or v. Lemma A.3 is a core Lemma on the marginal coverage
guarantee for LCP with strategically chosen ˜α. Lemma A.4 collects basic bounds used in the proofs of
Theorem 5.2.
Lemma A.1 (A.1). For any α and sequence {V1, . . . , Vn+1}, we have
Vn+1 ≤Q(α;
n
X
i=1
wiδVi + wn+1δVn+1) ⇔Vn+1 ≤Q(α;
n
X
i=1
wiδVi + wn+1δ∞),
where Pn
i=1 wiδVi + wn+1δVn+1 and Pn
i=1 wiδVi + wn+1δ∞are some weighted empirical distributions with
weights wi ≥0 and Pn+1
i=1 wi = 1.
Lemma A.2 (A.2). Suppose {Vi, i = 1, . . . , n}, the target level α, and empirical weights pH
ij are given. Then,
(i) Given Vn+1, Q

˜α; ˆFi(Vn+1)

for i = 1, . . . , n + 1 and Q

˜α; ˆF

are non-decreasing, right-continuous
and piece-wise constant on ˜α, and with value changing only at the cumulative probabilities at diﬀerent Vi.
(ii) Given ˜α, Q

˜α; ˆFi(v)

is non-decreasing on v for i = 1, . . . , n + 1.
(iii) If Vn+1 = v is accepted in the CV (Xn+1) in Lemma 3.1, then v′ is accepted for any v′ ≤v.
Lemma A.3 (A.3). Let Vi = V (Zi; Z) be the score for sample i, and Zi is i.i.d generated for i = 1, . . . , n+1.
For any event
T := {{Zi, i = 1, . . . , n + 1} = {zi := (xi, yi), i = 1, . . . , n + 1}} ,
we have
P{Vn+1 ≤Q(˜α;
n+1
X
i=1
pH
n+1,iδVi)|T } = E
(
1
n + 1
n+1
X
i=1
1vi≤v∗
i |T
)
,
where vi = V (zi; (z1, . . . , zn, zn+1)), v∗
i = Q(˜α; Pn+1
j=1 pH
i,jδvj) for i = 1, 2, . . . , n+1, and ˜α can be random but
is independent of the data conditional on T . The expectation on the right side is taken over the randomness
of ˜α conditional on T .
Lemma A.4 (A.4). Suppose that Assumption 5.1 holds and V (.) is a ﬁxed function. For any x0, deﬁne
B(x0) = Pn+1
j=1 H(x0, Xi), ∆(x0, X) = H(x0, X) maxv |PV |X(v) −PV |x0(v)| and ∆(x0) = Pn
i=1 ∆(x0, Xi). Then,
(i) There exists a constant C > 0 such that, for all x0 ∈[0, 1]p, we have
P(B(x0) ≤nhβ
n
2eL ) ≤exp(−nhβ
n
8L ),
∆(x0)
B(x0) ∨(nhβ
n)
≤Chn ln(h−1
n ).
(ii) Set Bi = B(Xi), Ri =
P
j̸=i

1Vj <Vi−PV |Xj (Vi)

Bi
. Then, for all Vi and i = 1, . . . , n + 1, we have
P(|Ri| ≥
r
ln n
Bi
|X, Vi) ≤2
n2 .
16

B
Proofs Propositions, Lemmas and Theorems
In this section, we provide proofs omitted from the main paper. We ﬁrst give arguments to Proposition 2.4
and Proposition 2.5 for the counterexamples. We then present proofs to Theorem 2.1, Theorem 2.6, Lemma
3.1, Lemma 3.2 that characterize the marginal behavior of LCP and our implementation. After that, we
prove Theorem 5.2 - 5.3 on the asymptotic and local behaviors of LCP-type procedures.
Proofs of the counter examples
B.1
Proof of Proposition 2.4
Proof. When Pn+1
i=1 H(Xn+1, Xi) <
1
1−α, by deﬁnition, we have
n
X
i=1
pH
n+1,i =
Pn
i=1 Hn+1,i
Pn+1
i=1 Hn+1,i
<
1
1−α −1
1
1−α
= α.
We thus have Q(α; ˆF) = ∞, and consequently,
P(Q(α; ˆF) = ∞) = P(
n
X
i=1
pH
n+1,i < α) = P(
n+1
X
i=1
H(Xn+1, Xi) <
1
1 −α) ≥ε.
P(Yn+1 ∈C(Xn+1)) ≥P(Q(α; ˆF) = ∞) ≥ε.
B.2
Proof of Proposition 2.5
Proof. For Xn+1 ∈{±ej, j = 1, . . . , p}, let n0 is the number of samples with Xi = 0 and n1 is the number
of samples with Xi = Xn+1. The achieved conditional coverage at ˜α = α given X = X1:(n+1) can be upper
bounded as below:
P(Vn+1 ≤Q(α; ˆF)|X) = P

Vn+1 ≤Q(α;
1
n1 + n0 + 1
X
i:Xi∈{0,Xn+1}
δVi +
1
n1 + n0 + 1δ∞)
X


(a)
≤
1
n1 + 1 + n0 + n1 + 1
n1 + 1
[α −
n0
n0 + n1 + 1]+,
(B.1)
Step (a) holds because:
• When α ≤
n0
n1+n0+1, the α quantile of the weighted empirical distribution is 0, and we will have 0
coverage for Xn+1 ̸= 0 and (B.1) is true.
• When α >
n0
n1+n0+1, the α quantile of the weighted empirical distribution in (B.1) is the ⌈(n1 +
n0 + 1)α⌉−n0 largest value in {Vi : Xi = Xn+1} ∪V∞, which is the
⌈(n1+n0+1)α⌉−n0
n1+1
quantile of
the unweighted empirical distribution formed by {Vi : Xi = Xn+1} ∪V∞. By Lemma A.1, {Vn+1 ≤
Q(t; {Vi : Xi = Xn+1} ∪V∞)} ⇔{Vn+1 ≤Q(t; {Vi : Xi = Xn+1} ∪Vn+1)}. Hence, we have
P(Vn+1 ≤Q(α; ˆF)|X) = P

Vn+1 ≤Q(⌈(n1 + n0 + 1)α⌉−n0
n1 + 1
; {Vi : Xi = Xn+1} ∪Vn+1)}
X

(b)
= ⌈(n1 + n0 + 1)α⌉−n0
n1 + 1
≤
1
n1 + 1 + n0 + n1 + 1
n1 + 1
(α −
n0
n0 + n1 + 1),
where step (b) uses the fact that Vi ∼Unif[−1, 1] for all i with Xi = Xn+1. Hence, (B.1) holds.
17

Next, we marginalize over X1:n but conditional on m = n0 + n1 (the total number of samples with Xi ∈
{0, Xn+1}). From (B.1):
P(Vn+1 ≤Q(α; ˆF)|m, Xn+1) ≤E[
1
n1 + 1|m] + E

[α m + 1
n1 + 1 −
n0
n1 + 1]+|m

,
= E[
1
n1 + 1|m] + E

[αm + 1 −n0
n1 + 1
−(1 −α)
n0
n1 + 1]+|m

,
= E[
1
n1 + 1|m] + (1 −α)E

[
α
1 −α −
n0
n1 + 1]+|m

.
(B.2)
Notice that conditional on m, Xi falls at 0 or Xn+1 following an independent Bernoulli law:
Xi =
 0
w.p.
q0
q0+q1 = α,
Xn+1
w.p.
q1
q0+q1 = 1 −α.
From direct calculations, we obtain that
E[
1
n1 + 1|m] =
m
X
n1=0
1
n1 + 1
m!
n1!(m −n1)!(1 −α)n1αm−n1
=
1
(m + 1)(1 −α)
m+1
X
n1=1
(m + 1)!
n1!(m + 1 −n1)!(1 −α)n1αm+1−n1 ≤
1
m(1 −α).
(B.3)
Also, we have
E

[
α
1 −α −
n0
n1 + 1]+|m

(c)
=
α
1 −αP(n0 ≤α(m + 1)|m) −
n0≤α(m+1)
X
n0=1
n0
m −n0 + 1
m!
n0!(m −n0)!αn0(1 −α)m−n0
=
α
1 −α

P(n0 ≤α(m + 1)|m) −
n0≤α(m+1)−1
X
n0=0
m!
n0!(m −n0)!αn0(1 −α)m−n0


=
α
1 −α (P(n0 ≤α(m + 1)|m) −P(n0 ≤α(m + 1) −1|m))
=
α
1 −αP(n0 = ⌊α(m + 1)⌋
|
{z
}
n∗
|m) =
α
1 −α
m
n∗

αn∗(1 −α)m−n∗.
(B.4)
We now use the Stirling’s approximation:
√
2πk(k
e )ke
1
12k+1 ≤k! ≤
√
2πk(k
e )ke
1
12k , for all k ≥1.
Plug the Stirling’s approximation into (B.4), there exist a constant C > 0 such that when m ≥C, we have:
E

[
α
1 −α −
n0
n1 + 1]+|m

≤
α
1 −α exp(
1
12m)
r
m
2π(n∗)(m −n∗)(α m
n∗)n∗
(1 −α)
m
m −n∗
m−n∗
(c)
≤
α
1 −α exp(
1
12m)
r
m
2π(mα −1)(m(1 −α) −1)(
mα
mα −1)n∗
(1 −α)m
m(1 −α) −1
m−n∗
≤C
r
1
m(1 + 2
m)m ≤Ce2
√m,
(B.5)
where we have used the fact that mα + 1 ≤n∗≥αm −1 at step (c). Notice that m itself follows a binomial
distribution with n trials and successful rate (q1 + q0). Apply the Chernoﬀbound, we have
P(m ≤(q1 + q0)n
2
) ≤exp(−n(q1 + q0)
8
).
(B.6)
18

For any constant p ≥1, n(q1 + q0) →∞. Combine it with (B.2), (B.3), (B.5) and (B.6), there exist a
constant C > 0, such that for all Xn+1 ∈{±ej, j = 1, . . . , p}, we have
P(Vn+1 ≤Q(α; ˆF)|Xn+1)
≤P({Vn+1 ≤Q(α; ˆF)|Xn+1} ∩{m ≥(q1 + q0)n
2
}) + P(m ≥(q1 + q0)n
2
|Xn+1) ≤C
s
1
(q1 + q0)n.
Marginalize over Xn+1, we reach the desired result: there exists a suﬃciently large constant C, such that
P(Vn+1 ≤Q(α; ˆF)) ≤P(Xn+1)
C
p
(q0 + q1)n
+ P(Xn+1 = 0) ≤
C
p
(q0 + q1)n
+ q0 →q0.
B.3
Proof of Theorem 2.1
Proof. Deﬁne
T := {{Zi, i = 1, . . . , n + 1} = {zi := (xi, yi), i = 1, . . . , n + 1}} .
Let σ be a permutation of numbers 1, 2, . . . , n + 1 that speciﬁes how the values are assigned, e.g., Zi takes
value zσi. Since V (.; Z) and H(., .; X) are ﬁxed conditional on T , we can set v∗
σi = Q(˜α; Pn
j=1 pH
σijδvj) as
the realized empirical quantile at ˜α for ˆFi given a particular permutation ordering σ. Hence, for any given
˜α ∈Γ, conditional T and the permutation ordering σ, we have
n+1
X
i=1
1Vi≤Q(˜α; ˆ
Fi)|T , σ =
n+1
X
i=1
1vσi≤v∗σi =
n+1
X
i=1
1vi≤v∗
i .
(B.7)
In other words, the achieved value for the left side of (B.7) or Theorem 2.1 (2.1) remains the same for all σ.
Since Γ is ﬁxed conditional on T , the smallest value in Γ satisfying (2.1) is also ﬁxed conditional on T , by
Lemma A.3, we obtain that
P{Vn+1 ≤Q(˜α; ˆFn+1)|T } = E
(
1
n + 1
n+1
X
i=1
1vi≤v∗
i |T
)
=
1
n + 1
n+1
X
i=1
1vi≤v∗
i ≥α
Marginalize over T , we have
P{Vn+1 ≤Q(˜α; ˆFn+1)} ≥α.
(B.8)
By Lemma A.1, equivalently, we also have
P{Vn+1 ≤Q(˜α; ˆF)} ≥α.
(B.9)
B.4
Proof of Theorem 2.6
Deﬁne
T := {{Zi, i = 1, . . . , n + 1} = {zi := (xi, yi), i = 1, . . . , n + 1}} .
By (B.7) and the fact that Γ is ﬁxed conditional on T , we know that ˜α1, ˜α2 and α1, α2 are ﬁxed conditional
on T . As a result, when ˜α =
 ˜α1
w.p.
α−α2
α1−α2
˜α2
w.p.
α1−α
α1−α2
, and it is independent of the data conditional on T . Apply
Lemma A.3, we have
P{Vn+1 ≤Q(˜α; ˆFn+1)|T } = E
(
1
n + 1
n+1
X
i=1
1vi≤v∗
i |T
)
19

= α1
α −α2
α1 −α2
+ α2
α1 −α
α1 −α2
= α.
Marginalizing over T , we have
P{Vn+1 ≤Q(˜α; ˆFn+1)} = α.
By Lemma A.1, equivalently, we have
P{Vn+1 ≤Q(˜α; ˆF)} = α.
B.5
Proof of Lemma 3.1
Proof. As a direct application of Theorem 2.1 and Crorllary 2.3, we obtain that
P {Vn+1 ∈CV (Xn+1)} ≥α, P {Yn+1 ∈C(Xn+1)} ≥α.
The fact that CV (Xn+1) is an interval comes directly from Lemma A.2 (iii).
B.6
Proof of Lemma 3.2
Proof.
• Proof of part 1: By deﬁnition, Vn+1 = v ∈CV (Xn+1) iﬀ(if and only if) the smallest value ˜α ∈Γ that
makes (3.2) hold is greater than P
Vi<v pH
n+1,i ∈Γ. That is, v ∈CV (Xn+1) iﬀ
1
n + 1
n
X
i=1
1Vi≤Q(P
Vi<v pH
n+1,i; ˆ
Fi(v)) < α.
(B.10)
(a) When v = V k for some 1 ≤k ≤n + 1, P
Vi<V k pH
n+1,i = ˜θk by deﬁnition. Hence v ∈CV (Xn+1)
iﬀ
1
n + 1
n
X
i=1
1Vi≤Q(˜θk; ˆ
Fi(V k)) < α.
(B.11)
(b) When v ∈(V ℓ(k), V k) for some 1 ≤k ≤n + 1, P
Vi<v pH
n+1,i = P
Vi<V k pH
n+1,i = ˜θk. Hence
v ∈CV (Xn+1) iﬀ
1
n + 1
n
X
i=1
1Vi≤Q(˜θk; ˆ
Fi(v)) < α.
(B.12)
A key observation is that the status of event {Vi ≤Q(˜θk; ˆFi(v))} does not change as we vary
v ∈[V ℓ(k), V k). That is, for all 1 ≤i ≤n, we have
Jik(v) := {Vi ≤Q(˜θk; ˆFi(v))} = {Vi ≤Q(˜θk; ˆFi(V ℓ(k))} := Jik.
This can be easily veriﬁed:
⋄If Vi < V k, we have Vi ≤V ℓ(k) < v, and
{Vi ≤Q(˜θk; ˆFi(v))} = {˜θk > θi} = {Vi ≤Q(˜θk; ˆFi(V ℓ(k)))}
⋄If Vi ≥V k, then Vi > v > V ℓ(k), and we have
{Vi ≤Q(˜θk; ˆFi(v))} = {˜θk > θi + pH
i,n+1} = {Vi ≤Q(˜θk; ˆFi(V ℓ(k)))},
Hence, we obtain
1
n + 1
n
X
i=1
1Vi≤Q(˜θk; ˆ
Fi(V l(k))) < α.
(B.13)
20

Combine part (a) and part (b), and the fact that V ℓ(k) ≤V k and Q(˜θk; ˆFi(v)) is non-decreasing in v
(Lemma A.2), we immediately reach the desired result that
¯CV (Xn+1) = {v : v ≤Q(˜θk∗; ˆF)},
where k∗is the largest value of k such that (B.13) holds.
• Proof of part 2: As we increase k, both V l(k) and ˜θk are non-decreasing, hence, Q(˜θk; ˆFi(V ℓ(k))) is
non-decreasing in k. Thus, Jik = {Vi ≤Q(˜θk; ˆFi( ¯Vℓ(k)))} is a monotone event in k: for all k′ ≥k, we
have Jik ⊆Jik′. Consequently, suppose k∗
i is when Jik ﬁrst holds, then 1Jik = 1 iﬀk ≥k∗
i . We can
divide Jik into two subsets:
Jik =

{Vi > ¯Vℓ(k)} ∩{Vi ≤Q(˜θk; ˆFi( ¯Vℓ(k)))}

∪

{Vi ≤¯Vℓ(k)} ∩{Vi ≤Q(˜θk; ˆFi( ¯Vℓ(k)))}

(a)
=

{ℓ(i) ≥ℓ(k)} ∩{θi + pH
i,n+1 < ˜θk}

|
{z
}
J1
ik
∪

{ℓ(i) < ℓ(k)} ∩{θi < ˜θk}

|
{z
}
J2
ik
.
(B.14)
At step (a), we have used the fact that
Vi > V ℓ(k) ⇔ℓ(i) ≥ℓ(k),
and that
– when Vi > V ℓ(k), we have P
1≤j≤n+1:Vj<Vi pH
ij = θi + pH
i,n+1 when Vn+1 = ¯Vℓ(k) Hence,
Vi ≤Q(˜θk; ˆFi( ¯Vl(k))) ⇔θi + pH
i,n+1 < ˜θk.
– when Vi ≤V ℓ(k), we have P
1≤j≤n+1:Vj<Vi pH
ij = θi when Vn+1 = ¯Vℓ(k). Hence,
Vi ≤Q(˜θk; ˆFi( ¯Vl(k))) ⇔θi < ˜θk.
We now consider when Jik turns true for samples from categories A1, A2 and A3.
– For i ∈A1, by the deﬁnition of A1 and (B.14), we know that Jik is true at k = i and k∗
i ≤i, and
Jik = J1
ik = {θi + pH
i,n+1 < ˜θk}.
– For i ∈A2 ∪A3, since i /∈A1 and k∗
i > i, J1
ik fails to hold for all k. Hence, Jik holds when J2
ik
holds.
⋄When i ∈A2: since θi ≥˜θi, in order for θi < ˜θk to hold, by deﬁnition, we must have
P
j≤l(k) pH
n+1,j = ˜θk > ˜θi = P
j≤l(i) pH
n+1,j, which automatically guarantees that l(k) > l(i).
As a result, for i ∈A2, we have Jik = J2
ik = {θi < ˜θk}.
⋄When i ∈A3: in order to have l(k) > l(i), we automatically ˜θk ≥˜θi > θi for samples in A3.
Thus, for i ∈A3, we have Jik = J2
ik = {l(i) < l(k)}.
Combine them together, we have
S(k) =
n
X
i=1
1
n + 11Vi≤Q(˜θk; ˆ
Fi(V ℓ(k)))
=
1
n + 1
 X
i∈A1
1J1
ik +
X
i∈A2
1J2
ik +
X
i∈A3
1J2
ik
!
=
1
n + 1
 X
i∈A1
1{θi+pH
i,n+1<˜θk} +
X
i∈A2
1θi<˜θk +
X
i∈A3
1l(i)<l(k)
!
.
We have proved the second part of Lemma 3.2.
21

Local coverage properties of LCP
B.7
Proof of Theorem 5.2
Proof. We ﬁrst prove the convergence from ˜α(v) to α in (5.3) and then show that the achieved coverage
levels converge to the nominal level for both ˜α = α and ˜α = ˜α(v) as described in Lemma 3.1.
Deﬁne
Ii =
Pn
j=1,j̸=i PV |Xj (Vi)Hij
Bi
and Ri = Pn
j=1,j̸=i
Hij(1Vj <Vi−PV |Xj (Vi))
Bi
for all i = 1, . . . , n + 1.
1. Proof of (5.3): For i = 1, . . . , n, deﬁne Bi = Pn+1
j=1 Hij and, for any ˜α ∈[0, 1] and v ∈R, deﬁne
Ji(v, ˜α) := {Vi ≤Q(˜α; ˆF(v))} = {˜α >
P
j≤n:Vj<Vi Hij + 1v<Vi
Bi
}.
Ji(v, ˜α) is the event for wether sample i contributes to the left side of Lemma 3.1 (3.2). We can deﬁne
a subset event Ji(˜α) ⊆Ji(v, ˜α) for all v values for all v. Decompose the condition of Ji(v, ˜α) as below:
P
j≤n:Vj<Vi Hij + 1v<Vi
Bi
≤
P
j≤n:Vj<Vi Hij
Bi
+ 1
Bi
= Ii + Ri + 1
Bi
.
(B.15)
Set G =
n
i ∈{1, . . . , n} : Bi ≥
1
2eLnhβ
n, |Ri| ≤
q
2eL ln n
nhβ
n
o
. By Lemma A.4 (i), there exists a constant
C > 0, such that for all i ∈G:
Bi −1 −Hi,n+1
Bi
∈[1 −4eL
nhβ
n
, 1],
|Ii −Bi −1 −Hi,n+1
Bi
PV |Xi(Vi)| ≤Chn ln(h−1
n ).
(B.16)
Combine (B.15) with (B.16), there exist a constant C > 0 such that for all i ∈G, we have
Ji(˜α) :=
(
˜α > PV |Xi(Vi) + C
 
hn ln(h−1
n ) +
s
ln n
nhβ
n
!)
⊆Ji(v, ˜α), for all v.
(B.17)
We can also deﬁne a superset event ¯Ji(˜α) ⊇Ji(v, ˜α) for all v values:
P
j≤n:Vj<Vi Hij + 1v<Vi
Bi
≥
P
j≤n:Vj<Vi Hij
Bi
= Ii + Ri.
(B.18)
Combine (B.18) with (B.16), there exists a constant C > 0 such that for all i ∈G, we have
Ji(v, ˜α) ⊆¯Ji(˜α) :=
(
˜α > PV |Xi(Vi) −C
 
hn ln(h−1
n ) +
s
ln n
nhβ
n
!)
, for all v.
(B.19)
Hence, we can then upper and lower bound the left side of (3.2) using ¯Ji(˜α) and Ji(˜α):
1
n + 1
n+1
X
i=1
Ji(v, ˜α) ≤
1
n + 1 +
1
n + 1
X
i∈G
¯Ji(˜α) + |Gc|
n + 1,
(B.20)
1
n + 1
n+1
X
i=1
Ji(v, ˜α) ≥
1
n + 1
X
i∈G
Ji(˜α).
(B.21)
Set Wi = PV |Xi(Vi), which is i.i.d generated from Unif[0, 1] when V |Xi is a continuous variable. By
Lemma A.4, we know that
P{|Gc| = 0} ≤P{
n
min
i=1 Bi ≤nhβ
n
2eL } + P{∃i ∈{1, . . . , n} : Bi > nhβ
n
2eL , |Ri| ≤
s
2eL ln n
nhβ
n
}
≤P{
n
min
i=1 Bi ≤nhβ
n
2eL } + P{ max
1≤i≤n |Ri| ≤
r
ln n
Bi
}
≤n exp(−nhβ
n
8L ) + n × 1
n2 →0.
(B.22)
When {|Gc| = 0} holds:
22

• When ˜α makes (3.2) hold, by (B.20), we must have
1
n + 1
 
1 +
n
X
i=1
¯Ji(˜α)
!
≥α ⇒˜α ≥Q(n + 1
n
α −1
n; 1
n
n
X
i=1
δWi) −C
 
hn ln(h−1
n ) +
s
ln n
nhβ
n
!
.
(B.23)
• By (B.21), ˜α makes (3.2) hold as long as
1
n + 1
n
X
i=1
Ji(˜α) ≥α ⇒˜α ≥Q(n + 1
n
α; 1
n
n
X
i=1
δWi) + C
 
hn ln(h−1
n ) +
s
ln n
nhβ
n
!
.
Further, since Γ includes all possible empirical CDF values from weighted distribution ˆFi for
i = 1, . . . , n + 1 under all possible ordering of V1, . . . , Vn+1.
Let Bmax = maxn+1
i=1 Bi.
The
diﬀerences between two adjacent values in Γ is upper bounded by
1
Bmax ≤
2eL
nhβ
n . Hence, there
exists a constant C > 0 such that the smallest value in Γ that makes (3.2) is upper bounded by
˜α ≤Q(n + 1
n
α; 1
n
n
X
i=1
δWi) + C
 
hn ln(h−1
n ) +
s
ln n
nhβ
n
!
.
(B.24)
The bounds (B.23) and (B.24) hold for all Vn+1 = v. By Dvoretzky–Kiefer–Wolfowitz inequality and
the fact that Wi ∼Unif[0, 1], there exists a constant C > 0 such that
P(max
t
|Q(t;
n
X
i=1
δWi) −t| ≤
r
ln n
n ) ≤C
n2 .
(B.25)
Combine B.23, (B.25) and (B.22), there exist a constant C > 0, such that
P
(
| min
vn+1 ˜α(vn+1) −α| < C
 
hn ln(h−1
n ) +
s
ln n
nhβ
n
!)
≥C
n2 + P(|Gc| > 0) →0.
(B.26)
Since C

hn ln(h−1
n ) +
q
ln n
nhβ
n

→0, this concludes our proof.
2. Proofs of (5.1) and (5.2): By deﬁnition, for any given ˜α, Vn+1 ≤Q(˜α; ˆF) if and only if
n
X
i=1
H(Xn+1, Xi)
Bn+1
1Vi<Vn+1 = In+1 + Rn+1 < ˜α.
(B.27)
Deﬁne G = {Bn+1 ≥nhβ
n
2eL , |Rn+1| ≤
q
2eL ln n
nhβ
n
}. When G holds, following the same routine as bounding
J(˜α, v) with ¯J(˜α) and J(˜α), we can lower and upper bound the left side of (B.27) using Lemma A.4
(i): there exists a constant C > 0, such that
In+1 + Rn+1 ≤PV |Xn+1(Vn+1) + C
 
hn ln(h−1
n ) +
s
ln n
nhβ
n
!
.
(B.28)
In+1 + Rn+1 ≥PV |Xn+1(Vn+1) −C
 
hn ln(h−1
n ) +
s
ln n
nhβ
n
!
.
(B.29)
Wn+1 = PV |Xn+1(Vn+1) ∼Unif[0, 1] since V |Xn+1 is a continuous variable. By Lemma A.4 (i) and
(ii), P(Gc) →0. Hence, for any given ˜α, there exists a constant C > 0, such that
P(In+1 + Rn+1 < ˜α) ≤˜α + C
 
hn ln(h−1
n ) +
1
(nhβ
n)1/3
!
+ P {Gc} →˜α,
(B.30)
23

P(In+1 + Rn+1 < ˜α) ≥˜α −C
 
hn ln(h−1
n ) +
1
(nhβ
n)1/3
!
→˜α.
(B.31)
Consequently, when ˜α = α or ˜α = ˜α(v) →α for all v in probability as described in (5.3), we achieve
an asymptotic conditional coverage at level α.
B.8
Proof of Theorem 5.3
Proof. We use the result from Barber et al. (2019a) which extends CP to the setting with covariate shift:
Proposition B.1 (B.1 (Barber et al. (2019a), Corollary 1)). For any ﬁxed x0. Set wx0(.) =
d ˜
Px0
X
dPX and
px0
i (x) =
wx0(Xi)
Pn
j=1 wx0(Xi)+wx0(x) for i = 1, . . . , n, and px0
n+1(x) =
wx0(x)
Pn
j=1 wx0(Xi)+wx0(x). Then,
P(V (Xn+1, Yn+1) ≤Q(α;
n+1
X
i=1
px0
i (Xn+1)δV i}) ≥α.
In our setting, wx0(x) ∝H(x0, x). As a direct application of Proposition B.1, when ( ˜X, ˜Y ) is distributed
from ˜PXn+1
XY
, we have
P{V ( ˜X, ˜Y ) ≤Q(α;
n+1
X
i=1
px0
i ( ˜X)δV i)|Xn+1 = x0} ≥α.
Since the H(x0, x0) ≥H(x0, ˜X) by deﬁnition, the distribution ˆF dominates the distribution Pn+1
i=1 px0
i ( ˜X)δV i:
given Xn+1 = x0, for any α, we have
Q(α; ˆF) = Q(α;
n+1
X
i=1
H(x0, Xi)
Pn+1
j=1 H(x0, Xj)
δ ¯Vi)
≥Q(α;
n
X
i=1
H(x0, Xi)
Pn
j=1 H(x0, Xj) + H(x0, ˜X)
δ ¯Vi +
H(x0, ˜X)
Pn
j=1 H(x0, Xj) + H(x0, ˜X)
δ ¯Vi) = Q(α;
n+1
X
i=1
px0
i ( ˜X)δV i).
Hence, we have
P{V ( ˜X, ˜Y ) ≤Q(α; ˆF)|Xn+1 = x0} ≥α, for all x0.
Next, we turn to the achieved coverage using ˜C(Xn+1). By construction, we have
{ ˜Y ∈˜C(Xn+1)} = {V (Xn+1, ˜Y ) ≤Q(α; ˆF) + ε(Xn+1)}
⊇{V ( ˜X, ˜Y ) ≤Q(α; ˆF)}.
Consequently, we obtain
P( ˜Y ∈˜C(Xn+1)|Xn+1 = x0) ≥P(V ( ˜X) ≤Q(α; ˆF))|Xn+1 = x0) ≥α.
C
Proof of Lemmas in the Appendix
C.1
Proof of Lemma A.1
Proof. By deﬁnition, we know
Vn+1 ≤Q(α;
n
X
i=1
wiδVi + wn+1δVn+1) ⇒Vn+1 ≤Q(α;
n
X
i=1
wiδVi + wn+1δ∞).
24

To show that Lemma A.1 holds, we only need to show that,
Vn+1 > Q(α;
n
X
i=1
wiδVi + wn+1δVn+1) ⇒Vn+1 > Q(α;
n
X
i=1
wiδVi + wn+1δ∞).
Let Q(α; Pn
i=1 wiδVi + wn+1Vn+1) = Vi∗for some index 1 ≤i∗≤n + 1. When Vn+1 > Vi∗, we must have
Vi∗< ∞. By deﬁnition:
α ≥
n+1
X
i=1
wi1Vi≤Vi∗=
n
X
i=1
wi1Vi≤Vi∗=
n
X
i=1
wi1Vi≤Vi∗+ wn+11∞≤Vi∗
⇒Q(α;
n
X
i=1
wi1Vi + wn+1δ∞) ≤Vi∗< Vn+1.
C.2
Proof of Lemma A.2
Proof. We can prove Lemma A.2 with elementary calculus arguments.
(i) Given V1, . . . , Vn+1, Q(˜α; ˆFi) = inf{t : P(v ≤t) ≥˜α, v ∼ˆFi}. The empirical distribution ˆF is discrete
with mass pH
ij on Vi, we can have an explicit expression for Q(˜α; ˆFi):
Q(˜α; ˆFi) =



V 0,
˜α = 0,
V i,
Pi−1
j=1 pH
ij < ˜α ≤Pi
j=1 pH
ij, i = 1, . . . , n,
V n+1,
Pn
j=1 pH
ij < ˜α.
Hence, Q(˜α; ˆFi) is non-decreasing and right-continuous piece-wise constant on ˜α, and v∗
i can only
change its value at Pk
j=1 pH
ij for k = 1, . . . , n. The same is true for Q(˜α; ˆF).
(ii) Given ˜α, when increasing Vn+1 from Vn+1 = v′ to Vn+1 = v for v > v′, the empirical distribution ˆFi(v)
dominates the empirical distribution ˆFi(v′) by construction: ∀˜α, we have
P(t ≤˜α|t ∼ˆFi(v′)) ≥P(t ≤˜α|t ∼ˆFi(v)).
As a result, Q(˜α; ˆFi(v)) is non-decreasing on v for any given ˜α, for i = 1, . . . , n + 1.
(iii) Suppose that v ∈CV (Xn+1). Let ˜α ∈Γ be the smallest value such that
n+1
X
i=1
1
n + 11Vi≤Q(˜α; ˆ
Fi(v)) ≥α,
by deﬁnition, we have v ≤Q(˜α; ˆF). Now, we consider Vn+1 = v′ for v′ ≤v. By the monotonicity
of Q(˜α; ˆFi(v)) on ˜α and v from Lemma A.2 (i) and (ii), we must have ˜α′ ≥˜α∗where ˜α′ ∈Γ is the
smallest value satisfying
n+1
X
i=1
1
n + 11Vi≤Q(˜α′; ˆ
Fi(v′)) ≥α,
Hence, we have v′ ≤v ≤Q(˜α; ˆF) ≤Q(˜α′; ˆF) and v′ is included in the PI. This concludes our proof.
25

C.3
Proof of Lemma A.3
Proof. Let σ be a permutation of numbers 1, 2, . . . , n + 1. We know that
P(σn+1 = i|T ) =
#{σ : σn+1 = i}
Pn+1
j=1 #{σ : σn+1 = j}
=
1
n + 1.
Set X = {X1, . . . , Xn+1} be the unordered set of the features. Since the function V (., Z) and the localizer
H(., ., X) are ﬁxed functions conditional on T , and ˜α (can be random) is independent of the data conditional
T , we obtain
P(Vn+1 ≤Q(˜α;
n+1
X
j=1
pH
n+1,jδVj)|T , ˜α)
=
n+1
X
i=1
P(σn+1 = i|T )1{Vn+1≤v∗
n+1(σ)|T ,σn+1=i}
=
n+1
X
i=1
1
n + 11{vi≤v∗
n+1(σ),σn+1=i}
(C.1)
where
v∗
i (σ) := Q(˜α;
n+1
X
j=1
pH
σi,σjδvσj ) = Q(˜α;
n+1
X
j=1
H(xσi, xσj)
Pn+1
j′=1 H(xσi, xσj′)
δvσj )
is the realization of v∗
i := Q(˜α; ˆFi) under permutation σ, conditional on T and ˜α. We immediately observe
that,
v∗
i (σ) = v∗
σi
(C.2)
Combine (C.1) and (C.2), we obtain that P{Vn+1 ≤v∗
n+1|T , ˜α} = Pn+1
i=1
1
n+11{vi≤Q(˜α; ˆ
Fi)}. Marginalize over
˜α|T , we have
P{Vn+1 ≤Q(˜α; ˆFn+1)|T } = E{
n+1
X
i=1
1
n + 11{vi≤Q(˜α; ˆ
Fi)}|T }
C.4
Proof of Lemma A.4
Proof.
Part (i): We divide the space into non-overlapping subregions Ak = {x : (k −1)hn ≤d(x0, x) <
khn}. Then,
B(x0) =
n+1
X
i=1
exp(−d(x0, Xi)
hn
) ≥exp(−1)
n
X
i=1
1Xi∈A1,
and 1Xi∈A1 follows a Bernoulli distribution with success probability qi ≥
1
Lnhβ
n according to Assumption
5.1 (ii). We can apply ChernoﬀBounds to lower bound B(x0):
P(
n
X
i=1
1Xi∈A1 ≤nhβ
n
2L ) ≤exp(−nhβ
n
8L ),
P(B(x0) ≤nhβ
n
2eL ) ≤exp(−nhβ
n
8L ).
Using the partitions {Aj} and Assumption 5.1 (i):
∆(x0) ≤L
n
X
i=1
d(x0, Xi) exp(−d(x0, Xi)
hn
),
≤Lhn exp(1)
∞
X
k=1
k
X
i:Xi∈Ak
exp(−k)
26

≤min
k0


L exp(1)k0hn
X
k≤k0
X
i:Xi∈Ak
H(x0, Xi) + Lhn exp(1)
X
k>k0
X
i:Xi∈Ak
k exp(−k)



≤min
k0 {eLk0hnB(x0) + eLhnk0 exp(−k0)n}
≤eLβ⌈ln h−1
n ⌉hn
 B(x0) + nhβ
n

,
(C.3)
where we have taken k0 = β⌈ln h−1
n ⌉at the last step. Hence, there exists a constant C > 0 such that
∆(x0)
B(x0) ∨(nhβ
n)
≤2eLβ⌈ln h−1
n ⌉hn ≤C ln(h−1
n )hn, for all x0 ∈[0, 1]p.
Part (ii): Set Zij = Hij
Bi
 1Vj<Vi −PV |Xj(Vi)

, and Ri = P
j̸=i Zij. By Hoeﬀding’s lemma, the centered
variable Zij is sub-Gaussian with parameter νij = Hij
2Bi for all i, j and Vi, e.g., for all j ̸= i:
E[exp(λZij)|Vi] ≤exp(ν2
ijλ
2 ), for all λ ∈R.
Hence, the weighted sum Ri is sub-Gaussian with parameter νi =
rP
j̸=i,j≤n
H2
ij
4B2
i ≤
q
1
4Bi (recall that
Hij ≤1 and Bi = Pn+1
j=1 Hij). Combining it with the sub-Gaussian concentration results, we obtain that
P(|Ri| ≥t|X, Vi) ≤2 exp(−t2
2ν2
i
) ≤2 exp(−2t2Bi), for all Vi, i = 1, . . . , n + 1.
Take t =
q
ln n
Bi , we obtain the desired bound.
D
Choice of H
D.1
Estimation of the default distance
Let V be the CV fold partitioning when learning V . We will estimate the spread by learning |Vi| for Vi from
the cross-validation step and i = 1, . . . , n0:
Vi ←ˆV −i(X0
i , Y 0
i ),
where ˆV −i is the score function learned using samples excluding i.
The spread learning step is using the same CV partitioning V. To learn the spread ρ(X), we consider
minimizing the MSE with the response log(|Vi| + |Vi|), with |Vi| be the mean absolute value for Vi across
samples in D0 = {Z0
i = (X0
i , Y 0
i ), i = 1, . . . , n0}. This additional term |Vi| is added to reduce the inﬂuence
of samples with very small empirical |Vi|.
We do not claim that learning ρ(X) in such a way is always a good choice. This is a reasonable choice
the for regression score. However, for quantile regression score, |Vi| is large around regions with both severe
under-coverage and over-coverage. Despite this, we observe that the LCP ends up similarly as CP with a
poorly chosen ˆρ(x) for the quantile regression score in our empirical studies.
Our estimated ˆρ is deﬁned as ˆρ = exp( ˆf(x)) where ˆf(x) is the estimated function from the learning step.
We let ρi = ˆρ−i(X0
i ) be the estimated spread from the cross-validation step. Let J ∈Rn0×p be the Jacobian
matrix with Ji, = ∂ˆ
f −i(X0
i )
∂X0
i
. Let u∥∈Rp×p0 and u⊥∈Rp×(p−p0) be the top p0 and the remaining right
singular vectors, with p0 be a small constant. By default, p0 = 1. We form the projection matrix P∥and P⊥
with u∥and u⊥:
P∥= u∥u⊤
∥, P⊥= u⊥u⊤
⊥.
The ﬁnal dissimilarity measure d(x1, x2) is a weighted sum of the three components, and
d(x1, x2) = d1(x1, x2)
σ2
+ (ωd2(x1, x2) + (1 −ω)d3(x1, x2))
σ1
,
27

where d2(x1, x2), d3(x1, x2) are projected distances onto P∥and P⊥, and d1(x1, x2) are distance in the space
of the learned spreading function ˆρ(x1), ˆρ(x2) as described in Section 3.3:
• d1(x1, x2) = ∥ˆρ(x1) −ˆρ(x2)∥2.
• d2(x1, x2) = ∥P∥(x1 −x2)∥2.
• d3(x1, x2) = ∥P⊥(x1 −x2)∥2.
We set ω and σ1, σ2 as following:
• Let µ∥/µ⊥be the mean of d2(X0
i , X0
j ) or d3(X0
i , X0
j ) for i ̸= j, then we let w =
µ⊥
µ⊥+µ∥.
• We let σ1 be the mean of
 ωd2(X0
i , X0
j ) + (1 −ω)d3(X0
i , X0
j )

and σ2 be that mean of d1(X0
i , X0
j ),
using all pairs i ̸= j from D0.
D.2
Empirical estimate of the objective
We want to minimize a penalized average length of ﬁnite PIs:
J(h) = Average PIfinite length + λ × Average conditional PIfinite length variability
s.t. P(Inﬁnite PI) ≤δ.
Let EXf(X) denote the expectation of some function f(.) with over X. In this tuning section, we consider
two speciﬁc types of V (.): the scaled regression score and the scaled quantile score, and
V (X, Y ) =
1
σ(X)|Y −f(X)|,
or
V (X, Y ) =
1
σ(X) max{qlo(X) −Y, Y −qhi(X)}.
These two score classes will include the four scores considered in our numerical experiments. Let k∗be the
selected index from Lemma 3.2, for this two classes of scores, the PI of Yn+1 is constructed as
C(Xn+1) = [f(Xn+1) −σ(Xn+1)V k∗, f(Xn+1) + σ(Xn+1)V k∗],
or
C(Xn+1) = [qlo(Xn+1) −σ(Xn+1)V k∗, qhi(Xn+1) + σ(Xn+1)V k∗].
In both cases, the length over the constructed PI of Yn+1 is additive on σ(Xn+1)V k∗, and hence, minimizing
PI of Yn+1 is equivalent to minimizing σ(Xn+1)V k∗, and the conditional variability of the PI is the same as
the variability of σ(Xn+1)V k∗conditional on Xn+1. Hence, after omitting components that do not depend
on h, we can express the terms in the above objective as
• Average PIfinite length: EZ1:n,Xn+1

σ(Xn+1)V k∗|k∗≤n

. It depends on Z1:n, Xn+1 as well as the
tuning parameter h. (Recall that when k∗= n + 1, V n+1 = ∞. )
• Average conditional PIfinite length variability:
r
EZ1:n,Xn+1
h
σ(Xn+1)2  V k∗−µ(Xn+1)
2 |k∗≤n
i
, where
µ(Xn+1) = EZ1:n

V k∗|k∗≤n, Xn+1

is the average length ﬁnite PI at Xn+1, marginalized over Z1:n.
• Average percent of inﬁnite PI: P(k∗= n + 1).
We estimate the above quantities with empirical estimates using D0. As in the previous section, we consider
the case where the function form V (X, Y ) is estimated by CV and Vi ←ˆV −i(X0
i , Y 0
i ). For example, we want
to construct the score function V (X, Y ) = |Y −f(X)| where f(X) is the mean prediction function. Then,
ˆV −i(X0
i , Y 0
i ) is calculated as
ˆV −i(X0
i , Y 0
i ) = |Y 0
i −ˆf −k(x)|,
28

where ˆf −k(.) is the learned mean function using data excluding fold k that includes sample i. We also
estimate the spreads and deﬁne the distance on D0 using the CV estimates.
Given the dissimilarity measure dij for any pair (X0
i , X0
j ), and thus Hij = exp(−dij
h ) for a given h, we
estimate the empirical loss for h ∈{h1, . . . , hm} as below:
• Estimation of average length and inﬁnite PI probability:
– We subsample ˜n = (n + 1) ∧n0 samples without replacement from D0, let the set be S and
construct PI for each sample i ∈S with a calibration set S \ {i}. Let Li be the scaled length for
the constructed PI (scaled by σ(X0
i )).
– The probability of having inﬁnite PI is estimated as C1(h) = #{i∈S,Li=∞}
˜n
, and the average ﬁnite
PI length is estimated as C2(h) =
P
i∈S,Li<∞Li
#{b:Lib<∞}∨1.
The above estimates can be repeated for multiple times when n0 is much larger than (n + 1).
• Estimation of conditional variability:
– Repeat B times the PI construction: for b = 1, . . . , B, we subsample n samples with replacement
from D0, and let the length of scaled PI of V at Z0
i be Lib for i = 1, . . . , n0.
– Calculate the ﬁnite conditional mean as µi =
P
b:Lib<∞Lib
#{b:Lib<∞}∨1.
– Calculate the conditional variance as si =
P
b:Lib<∞(Lib−µi)2
#{b:Lib<∞}∨1
.
– The average conditional variability for PI with ﬁnite length is estimated as C3,h =
q P
i(#{b:Lib<∞}×si)
#{(i,b):Lib<∞}
We take h from the candidate set to minimize the empirical objective:
h = arg
min
C1(h)≤δ (C2(h) + λC3(h)) .
29
