Byzantine-Resilient SGD in High Dimensions on
Heterogeneous Data∗
Deepesh Data and Suhas Diggavi
University of California, Los Angeles, USA
Email: deepesh.data@gmail.com, suhas@ee.ucla.edu
Abstract
We study distributed stochastic gradient descent (SGD) in the master-worker architecture under
Byzantine attacks. We consider heterogeneous data model, where diﬀerent workers may have diﬀerent
local datasets, and we do not make any probabilistic assumption on data generation. At the core of our
algorithm, we use the polynomial-time outlier-ﬁltering procedure for robust mean estimation proposed by
Steinhardt et al. (ITCS 2018) to ﬁlter-out corrupt gradients. In order to be able to apply their ﬁltering
procedure in our heterogeneous data setting where workers compute stochastic gradients, we derive a new
matrix concentration result, which may be of independent interest.
We provide convergence analyses for smooth strongly-convex and non-convex objectives. We derive our
results under the bounded variance assumption on local stochastic gradients and a deterministic condition
on datasets, namely, gradient dissimilarity; and for both these quantities, we provide concrete bounds in
the statistical heterogeneous data model. We give a trade-oﬀbetween the mini-batch size for stochastic
gradients and the approximation error. Our algorithm can tolerate up to 1
4 fraction Byzantine workers. It
can ﬁnd approximate optimal parameters in the strongly-convex setting exponentially fast, and reach
to an approximate stationary point in the non-convex setting with a linear speed, thus, matching the
convergence rates of vanilla SGD in the Byzantine-free setting.
We also propose and analyze a Byzantine-resilient SGD algorithm with gradient compression, where
workers send k random coordinates of their gradients. Under mild conditions, we show a d
k-factor saving in
communication bits as well as decoding complexity over our compression-free algorithm without aﬀecting
its convergence rate (order-wise) and the approximation error.
1
Introduction
Stochastic gradient descent (SGD) [RM51] is the main workhorse behind the optimization procedure in several
modern large-scale learning algorithms [Bot10]. In this paper, we consider a master-worker architecture,
where the training data is distributed across several machines (workers) and a central node (master) wants
to learn a machine learning model using SGD [DG08]. This setting naturally arises in the case of federated
learning [Kon17,KMRR16], where user devices are recruited to help build machine learning models. This
can also arise in a distributed setup, where data is partitioned and stored in many servers to speed up the
computation. In such scenarios, the recruited worker nodes may not be trusted with their computation,
either because of non-Byzantine failures, such as software bugs, noisy training data, etc., or because of
Byzantine attacks, where corrupt nodes may manipulate the information to their advantage [LSP82]. These
Byzantine adversaries may collaborate and arbitrarily deviate from their pre-speciﬁed programs. Training
machine learning models in the presence of Byzantine attacks has received attention lately [YCRB18,YCRB19,
AAL18,SX19,CSX17,BMGS17,BZAA19,CWCP18,RWCP19,DSD19a,DSD19b,DD19,GV19] and also in
the context of the Internet of Battleﬁeld Things (IoBT) [A+18]. See also [K+19, Section 5] for a detailed
∗This work was supported by the NSF grants #1740047, #1514731, and by the UC-NL grant LFR-18-548554.
1
arXiv:2005.07866v1  [stat.ML]  16 May 2020

Distributed SGD with Byzantine adversaries
M broadcasts x
M
bg(x) ≈
1
|H|
P
i∈H gi(x)?
W1
(D1, F1)
W2
(D2, F2)
W3
(D3, F3)
WR
(DR, FR)
g1(x)
eg2(x)
g3(x)
egR(x)
Figure 1 In the master-worker architecture for distributed
optimization, each of the R workers (denoted by Wi) stores
local datasets – worker r stores Dr with an associated local
loss function Fr. We are in a heterogeneous data setting,
where the local datasets Dr’s are arbitrary and are not
necessarily generated from the same distribution. Master
(denoted by M) wants to learn a machine learning model
through SGD which minimizes the average of local loss
functions; see (1). The adversarial nodes are denoted in red
color. Let H denote the set of honest workers. In any SGD
iteration, master broadcasts the current model parameter
vector x to all workers. Each honest worker i computes the
stochastic gradient gi(x) and sends it back to the master;
corrupt nodes may send arbitrary vectors. Master wants
to compute bg(x) ≈
1
|H|
P
i∈H gi(x) in order to update the
model parameter vector. Computing bg(x) and providing
convergence analyses for strongly-convex and non-convex
objectives is the subject of this paper.
survey on Byzantine-robustness in federated learning. The importance of this problem motivates us to study
Byzantine-resilient optimization algorithms that are suitable for large-scale learning problems. See Section 1.2
where we put our work in context with these works.
In this paper, we study distributed SGD in the presence of Byzantine adversaries for empirical risk
minimization. The training data is distributed across R diﬀerent workers, and master wants to iteratively
build a machine learning model using the gradients computed at the workers. All workers have potentially
diﬀerent local datasets, and we do not make any probabilisitic assumption on data generation. In our setup,
up to ϵR workers (where ϵ > 0 is a constant) may be under Byzantine attacks, and corrupt workers may
collaborate and report adversarially chosen gradients to the master; see Figure 1. See also Section 2.1 for
more details on our adversary model.
1.1
Our contributions
In our Byzantine-resilient SGD algorithm (see Algorithm 1), we use a non-trivial decoding at master to
ﬁlter-out corrupt gradients, which is inspired by the recent advances in high-dimensional robust mean
estimation [LRV16,DKK+16,SCV18]. In particular, we use the outlier-ﬁltering method developed by [SCV18].
Our algorithm can tolerate ϵ ≤1
4 fraction of corrupt worker nodes. Our results can be summarized as follows:
• We give convergence analyses for smooth strongly-convex and non-convex objectives. We assume these
functional properties (e.g., smoothness, convexity, etc.) only for the global objective function, and
the local objective functions (deﬁned by the local datasets) at diﬀerent workers may be arbitrary.
We identify a deterministic condition on datasets, namely, that the gradient dissimilarity at diﬀerent
workers is bounded, and we derive our convergence results under this condition. For SGD, we make a
standard assumption that the local variances (due to sampling stochastic gradients) at all workers are
bounded. We provide concrete upper bounds on the gradient dissimilarity as well as the local variances
in the statistical heterogeneous model under diﬀerent distributional assumptions (sub-exponential and
sub-Gaussian) on local gradients; see Theorem 6 and Theorem 7 in Section 6.
• In the strongly-convex case, our algorithm can ﬁnd optimal parameters within an approximation error
of O(κ2 + σ2
bR + σ2dϵ′
bR ) (where ϵ′ > ϵ is any constant and b is the mini-batch size for stochastic gradients)
“exponentially fast”; and in the non-convex case, it can ﬁnd an approximate stationary point within the
same error with “linear speed”, i.e., with a rate of 1/T. The κ2 term in the approximation error quantiﬁes
the gradient dissimilarity, and is equal to zero if all workers are assumed to have access to the same
dataset, which (i.e., κ = 0) had been the underlying assumption in most results in Byzantine-resilient
SGD; see, for example, [BMGS17,AAL18]. The second term σ2
bR is the standard SGD variance term
and the third term σ2dϵ′
bR
is due to Byzantine attacks. Note that both these terms can be made small by
2

taking a suﬃciently large mini-batch size of stochastic gradients. Note that when workers compute
full-batch gradients (i.e., σ = 0), the approximation error becomes O(κ2).1 See Theorem 1 for our
mini-batch SGD results and Theorem 2 for our full-batch GD results.
• In order to be able to apply the robust mean estimation result of [SCV18] to our heterogeneous data
setting where workers sample stochastic gradients from their local datasets, we derive a new matrix
concentration bound (stated in Theorem 3), by building upon a recent result from [CSV17], which was
developed to tackle several robust learning tasks. See Section 4.1 for more details.
• We extend the above results to a setting, where, instead of sending full gradients, workers send
compressed gradients to master. In particular, workers send random k coordinates of their d-dimensional
gradients to master; see Algorithm 2. Under a mild condition that ϵ is constant and k ≥(1 −ϵ)R,
i.e., the adversary corrupts a constant fraction of workers and k is more than the number of honest
workers, we show that the convergence rate and the approximation error achieved by this algorithm
match (order-wise) with its compression-free counterpart. This has two direct implications: ﬁrst, it
gives a saving of d
k factor in communication bits as well as in the decoding complexity, which could be as
large as Ω( d
R); and second, it shows that incorporating error-feedback cannot improve the convergence
of our algorithm, which is in sharp contrast with the Byzantine-free SGD with compression, where
error-feedback provably improves the convergence rate [SCJ18]. See Theorem 5 for our results with
gradient compression.
As far as we know, this is the ﬁrst paper that studies Byzantine-resilient distributed SGD under standard
SGD assumptions in the heterogeneous data model, where diﬀerent workers may have diﬀerent local datasets
with no probabilistic assumptions on data generation.2 We can control the approximation error (in our
convergence results) with the batch size of the mini-batch stochastic gradients, and can reduce it a point where
the approximation error only depends on the heterogeneity in the local datasets. Note that the approximation
error consists of two types of terms: one is a ﬁxed term that captures the gradient dissimilarity among local
datasets; and the other is a statistical error term, which comes from two sources, one from stochastic sampling
of gradients and the other from robustly estimating the true gradient. Reducing the ﬁrst type of statistical
error by increased mini-batch is standard, but an implication of our result is that we can even control the
second type of statistical error (which is due to Byzantine attacks) in the same way.
1.2
Related work
Byzantine-resilient distributed computing has a long history [LSP82] and is a very well studied topic, which
has received recent attention in the context of distributed learning. The approach taken to tackle Byzantine
attacks in literature can be broadly divided into two categories: One using statistical assumptions on the data
(e.g., all workers have data generated from the same distribution, or in the extreme case, all workers have the
same data) – detailed references are given below; and the other using coding-theoretic or redundancy-based
techniques [CWCP18,DSD19a,RWCP19].
Byzantine SGD has been studied under the assumption that all workers have access to the entire
dataset: [BMGS17] employed a heuristic to compute the smallest ball containing (1 −ϵ)R gradients, where
an ϵ fraction of the R workers are corrupt. They consider non-convex objectives and showed an almost
sure convergence of gradients under stringent assumptions. [AAL18] ﬁlter out corrupt workers based on
gradients received in diﬀerent iterations. Using martingale-based methods, they showed convergence under
an assumption that the stochastic error in gradients is bounded with probability 1 (instead of assuming
bounded variance). [GV19] studies linear regression only – it removes corrupt nodes based on norm-ﬁltering,
and achieves an error that scales with the number of data points. In contrast, our results in this paper are
for general smooth strongly-convex and non-convex objectives under a standard SGD assumption of bounded
1It is not surprising that when κ = 0, we reach to an exact optimum in full-batch GD – when κ = 0, all workers have the
same data, and master can decode the correct gradient by simply taking the majority vote of the received gradients.
2Note that Theorem 6 and Theorem 7 are just to demonstrate the behavior of the parameters if the data was generated by a
heterogeneous statistical model.
3

variance, where diﬀerent workers have diﬀerent local datasets and we assume no data distribution (thus,
deviating from the standard assumption in Byzantine-resilient SGD of all workers having access to the same
dataset [BMGS17,AAL18]). Our results are qualitatively diﬀerent from these papers, where we can control
the approximation error by the mini-batch size (larger the mini-batch size, better the accuracy), and with
suﬃciently large batch size, we can even get an error that depends only on the heterogeneity bound (which is
captured through the gradient dissimilarity).
There have been works in full-batch gradient descent against Byzantine attacks, where data at workers
is assumed to be drawn i.i.d. from a probability distribution, and the goal is to minimize the population
risk [YCRB18,YCRB19,SX19,CSX17]. In contrast, we consider a more realistic heterogeneous data setting,
where diﬀerent workers may have local datasets satisfying bounded gradient dissimilarity (the bound is κ2, as
stated in (4)), and our goal is to minimize the empirical risk. We give concrete bounds on κ2 for a statistical
heterogeneous model, where data points at diﬀerent workers are sampled from diﬀerent distributions, but i.i.d.
from the same distribution at any given worker. In this setting, we prove that κ2 = κ2
mean + O

d log(nd)
n

,
where κmean captures the diﬀerence between local and global population means and n is the number of data
samples at each worker. Note that κ2
mean = 0 in the above-mentioned works, as all workers sample data from
the same distribution. Though our main focus in this paper is on SGD, we also give our specialized results for
full-batch GD, with which we will compare the results in above works. With the above bound on κ2 (derived in
the statistical heterogeneous data model), the approximation error in our convergence results with full-batch
GD for both strongly-convex and non-convex objectives is bounded by e
O
 κ2
mean + d
n

.3 [YCRB18] employed
coordinate-wise median and trimmed median, and got an approximation error of e
O

d2
nR

for both convex
and non-convex objectives, which could be prohibitive in high-dimensional problems; [CSX17] and [SX19]
considered only strongly-convex objectives, where [CSX17] used decoding based on median-of-means and gave
an error of e
O
  ϵd
n

, and [SX19] improved it to e
O
  d
nR

for constant ϵ – observe that these papers only study
strongly-convex objectives, whereas, in addition, we study non-convex objectives too. The decoding algorithm
in [SX19] is taken from [SCV18], which is based on robust mean estimation, and we also use that algorithm
in our decoding. Note that [SX19] requires eΩ(d2) data points to succeed, whereas, we require eΩ(dR) data
points, which could be signiﬁcantly smaller when d ≫R. [YCRB19] proposed and analyzed an algorithm to
avoid saddle-point attacks in non-convex problems and provided second-order convergence guarantees. In the
high-dimensional setting, they also used the decoding algorithm of [SCV18] and gave an approximation error
of e
O
  d
nR

in their i.i.d. homogeneous data setting. Note that our bound e
O
 κ2
mean + d
n

on the approximation
error is in a more general heterogeneous data setting, and we believe that our results can also be extended to
combat the saddle-point attacks in non-convex problems, which we leave as part of the future work.
Apart from the heterogeneity in data, there are other technical diﬀerences between [SX19,YCRB19] and
our work, and we would like to point out one of them here. In order to use the decoding algorithm of [SCV18],
both these works derive a matrix concentration bound, the need of which arises because they minimize the
population risk. In this paper, since we minimize the empirical risk, we do not need such a result. However,
we do need to prove a matrix concentration bound (which is of a very diﬀerent nature than theirs, and we
use entirely diﬀerent tools to prove that), the need of which arises because the gradients are stochastic due to
SGD – if we work with full-batch deterministic gradients, we would not need any of such concentration bounds.
See also the discussion in Section 4.1 for more details and Theorem 3 for our new matrix concentration
result. Note that [SX19] analyzed full-batch gradient descent only for strongly-convex objectives in the i.i.d.
homogeneous data setting and left a few problems open, including analyzing the stochastic gradient descent,
convergence for non-convex objectives, and an algorithm with gradient compression. In this paper, we resolve
all these open problems (while minimizing the empirical risk) in a more general heterogeneous data setting,
and provide comprehensive analyses of Byzantine SGD and prove its convergence for both strongly-convex and
non-convex objectives. Note that [YCRB19] also provided a convergence analysis for non-convex objectives in
the same setting as that of [SX19], i.e., the i.i.d. homogeneous data setting while minimizing the population
risk with full-batch GD; in contrast, our results are in a heterogeneous data setting, and we minimize the
3The e
O and eΩnotations hide logarithmic factors.
4

empirical risk with SGD.
As far as we know, not much has been studied for Byzantine learning with gradient compression, except
for a few notable exceptions of [BZAA19,GMK+19]. Under the assumption that all workers have access to
the same data, [BZAA19] achieves compression using a 1-bit quantizer – sign of the gradient vector – and
performs a simple decoding at master node using the majority vote. They assume that each component of
the stochastic gradients has symmetric and unimodal distribution around its mean. Their algorithm can
only tolerate “blind multiplicative adversaries”, which restricts the adversary to multiply the gradient by any
vector of its choice, but it has to be decided before observing the gradient.4 Their convergence results only
hold under large mini-batch stochastic gradients, where the mini-batch size is equal to the total number
of iterations. [GMK+19] studies full batch gradient descent under the i.i.d. homogeneous data assumption.
Their setting and distributional assumptions are similar to [YCRB18], and they get an approximation error
of e
O

d2
nR

, which could be prohibitive in high-dimensional settings. Their results are for an arbitrary
compressor, but their decoding algorithm employs only norm ﬁltering; see the discussion in Section 4 on why
norm-based ﬁltering is not suﬃcient for getting good approximation guarantees in high-dimensional learning.
In contrast to the settings in both these papers, we study mini-batch distributed SGD on heterogeneous data
under standard SGD assumptions (with diﬀerent workers having diﬀerent datasets and no data distribution),
where we can control the approximation error by the mini-batch size, and thus obtain qualitatively diﬀerent
results.
1.3
Paper organization
We describe our problem setup in Section 2. We state our main convergence results in Section 3 and extend
them with gradient compression in Section 5. We describe the core part of our algorithm, the robust gradient
estimation, and our new matrix concentration result in Section 4. We instantiate our assumptions in the
statistical heterogeneous data model in Section 6. We conclude with a few open problems in Section 7.
1.4
Notation
For any n, n1, n2 ∈N such that n1 ≤n2, we denote the set {1, 2, . . . , n} by [n], and the set {n1, n1 +1, . . . , n2}
by [n1 : n2]. For any ﬁnite set K ⊂N, we write k ∈U K to denote that k is sampled uniformly at random
from K. We denote matrices with bold capital letters A, B, etc., and vectors with bold small letters x, y,
etc. All vector norms in this paper are ℓ2-norm, and, for simplicity, without explicitly writing ∥· ∥2, we will
just denote them by ∥· ∥. For a matrix A, we denote the matrix norm of A (induced by the ℓ2-norm on the
vector space) by ∥A∥(instead of explicitly writing ∥A∥2), which is equal to the largest singular value of A.
For a matrix A, we write A ⪰0 and A ≻0 to denote that A is positive semi-deﬁnite and positive deﬁnite,
respectively. For two matrices A, B, we write A ⪯B to denote that (B −A) is positive semi-deﬁnite. For a
square matrix A, we denote its largest eigenvalue by λmax(A).
2
Problem Setup
In the master-worker architecture that we consider in this paper, each of the R workers may have diﬀerent
datasets; see Figure 1. Let the dataset stored at the r’th worker be denoted by Dr = {zr,1, zr,2, . . . , zr,nr},
which is a collection of nr data points for some nr ∈N. We allow diﬀerent workers to have diﬀerent number
of data points. Let C ⊆Rd denote the parameter space. We can take C to be equal to Rd in the absence of
any constraints on parameters or a compact and convex set otherwise. Note that the dimension of the data
samples may be much smaller than d. For example, in the case of neural networks, the dimension of the data
samples is equal to the number of inputs in the ﬁrst layer, which may be much smaller than the dimension d
of the model learned.
4It is not hard to come up with a slightly more powerful adversarial attack that breaks their system.
5

Our goal is to learn a model x ∈C that minimizes the average loss F(x) :=
1
R
PR
r=1 Fr(x), where
F : Rd →R denotes the global loss function, and for each r ∈[R], Fr : Rd →R denotes the local loss function
at worker r. For r ∈[R], Fr is deﬁned as Fr(x) :=
1
nr
Pnr
i=1 Fr,i(x), where Fr,i(x) denotes the loss associated
with zr,i (the i’th data-point at worker r) with respect to (w.r.t.) the model x. Note that Fr(x) denotes the
average loss associated with the data-points in Dr w.r.t. the model x, and we want to ﬁnd an x ∈C that
minimizes the average loss 1
R
PR
r=1 Fr(x). Formally, we want to solve the following minimization problem:
arg min
x∈C
 
F(x) := 1
R
R
X
r=1
Fr(x)
!
.
(1)
All convergence results in this paper only require properties of the global loss function F; the local loss
functions Fr, r ∈[R] may be arbitrary. For example, in the smooth strongly-convex case, we only require F
to be smooth and strongly-convex, and we do not impose any condition on Fr’s. Similarly for the smooth
non-convex case.
When F is strongly-convex, let the minimization in (1) be attained at x∗and we assume that x∗∈C. In
the case of non-convex F, as standard in literature, we ﬁnd a stationary point where the gradient becomes
zero.
We can minimize (1) using distributed stochastic gradient descent (SGD), which is an iterative algorithm
that proceeds as follows: Initialize the model x0 := 0. At the t’th iteration, for t ≥0, master broadcasts xt;
each worker r ∈[R] sends gr(xt) := ∇Fr,rt(xt) to the master for a randomly chosen rt ∈U [nr], independent
of the choice of other workers; master updates the parameter vector according to the following update rule:
x0 := 0;
xt+1 = xt −η 1
R
R
X
r=1
gr(xt),
t = 0, 1, 2, . . .
(2)
Here, η denotes the learning rate. We make the following assumptions about distributed SGD.
Note that, for any r ∈[R], Ei∈U[nr][∇Fr,i(x)] = ∇Fr(x) holds for every x ∈Rd.
Assumption 1 (Bounded local variances). The stochastic gradient sampled from any local dataset is uniformly
bounded over C for all workers, i.e., there exists a ﬁnite σ, such that
Ei∈U[nr]∥∇Fr,i(x) −∇Fr(x)∥2 ≤σ2,
∀x ∈C, r ∈[R].
(3)
Assumption 2 (Bounded gradient dissimilarity). The diﬀerence between the local gradients ∇Fr(x), r ∈[R]
and the global gradient ∇F(x) =
1
R
PR
r=1 ∇Fr(x) is uniformly bounded over C for all workers, i.e., there
exists a ﬁnite κ, such that
∥∇Fr(x) −∇F(x)∥2 ≤κ2,
∀x ∈C, r ∈[R].
(4)
Though we assume that all local datasets have the same σ, κ, this is without loss of generality – in case
diﬀerent datasets have diﬀerent σr, κr, r ∈[R], since σ, κ are upper bounds in (3), (4), respectively, we can
take σ = maxr∈[R] σr and κ = maxr∈[R] κr to be the maximum of the corresponding local parameters.
Assumption 1 is standard in the SGD literature. In Assumption 2, κ quantiﬁes the deviation between the
local loss functions Fr, r ∈[R] and the global loss function F, and this assumption states that this deviation
is bounded. Note that when all workers have access to the same dataset, we have κ = 0, which has been the
standard assumption for Byzantine SGD in literature [AAL18,BMGS17]. Assumption 2 has been used earlier;
see, for example, [YJY19], which studies decentralized SGD with momentum (without Byzantine workers).
Remark 1. Observe that, in distributed algorithms in the presence of Byzantine adversaries, since we do
not know which subset of ϵR workers are corrupt, we have to make some assumption on the data to provide
relationships among gradients sampled at diﬀerent nodes for reliable decoding. Otherwise, since the adversary
can corrupt any subset of ϵR workers and we do not assume any relationship among gradients, there is no
way we can do reliable decoding at the master. Previous works created such relationships by assuming that
6

either the workers’ data is drawn i.i.d. from a probability distribution [YCRB18,YCRB19,SX19,CSX17] or all
workers can sample gradients from the same data [AAL18,BMGS17]. Other works have used redundancy-based
or coding-theoretic techniques to provide such relationships among gradients [CWCP18,DSD19a,RWCP19].
All these approaches fall short of in a distributed setup such as federated learning [KMRR16], where workers
have non-i.i.d. data and therefore cannot sample gradients from the same data, and it is infeasible to perform
data encoding across diﬀerent nodes. In this paper, we neither use coding/redundancy-based techniques, nor
do we make any probabilistic assumptions on the data generation; and we allow diﬀerent workers to have
diﬀerent arbitrary datasets. As argued above, since we have to make some assumption that correlates local
data, we assume uniform boundedness (4) of the deviation of local gradients from the global gradient, which is
a much weaker assumption that the above-mentioned ones.
The gradient dissimilarity bound in (4) can be seen as a deterministic condition on local datasets, under
which we derive our results. All results (matrix concentration and convergence) in this paper are given in terms
of the variance bound σ2 and the gradient dissimilarity bound κ2. In Section 6, we provide concrete bounds
on σ, κ in the statistical heterogeneous model under diﬀerent distributional assumptions (sub-exponential and
sub-Gaussian) on local gradients. In the statistical heterogeneous model, local datasets at diﬀerent workers are
generated from potentially diﬀerent distributions. This model is more suitable for federated learning [KMRR16]
than the statistical homogeneous model considered in literature [CSX17,YCRB18,SX19,YCRB19], where all
local datasets are generated from the same distribution. Note that we make distributional assumptions on
data generation only to derive bounds on σ, κ. Other than that, we do not make any distributional assumption
on the data and all results in this paper hold for arbitrary datasets satisfying (3), (4).
In the parameter update rule (2), once the workers send the local stochastic gradients to master, it
aggregates them by taking their average and updates the parameter vector according to (2). Observe that,
this simple aggregation rule (i.e., averaging) at master is vulnerable to Byzantine attacks, because, instead of
sending the true stochastic gradients, the corrupt workers may send adversarially chosen vectors to disrupt
the computation – it is known that even a single Byzantine worker can prevent the algorithm to converge,
even worse, it can cause the algorithm to converge to an adversarially chosen point [BMGS17]. Our adversary
model is described next.
2.1
Adversary model
We assume that an ϵ fraction of R workers are corrupt; as we see later, we can tolerate ϵ ≤1
4. The corrupt
workers can collaborate and arbitrarily deviate from their pre-speciﬁed programs: In any SGD iteration,
instead of sending the true stochastic gradients, corrupt workers can send adversarially chosen vectors (they
may not even send anything if they wish, in which case, the master can treat them as erasures and replace
them with a ﬁxed value). Note that, in the erasure case, master knows which workers are corrupt; whereas,
in the Byzantine problem, master does not have this information.
Our algorithms are also resilient against a more powerful adaptive and mobile adversary (which can
corrupt a diﬀerent set of ϵR workers in diﬀerent SGD iterations based on the knowledge it has gathered in the
past),5 as long as it does not change the set of corrupt workers after observing the gradients in any iteration;
otherwise, the gradients of honest workers may not remain independent, a property we need in order to derive
our matrix concentration result stated in Theorem 3. Note that since we allow a mobile adversary, we cannot
consider optimizing the expression in (1) with respect to the data stored only at the honest workers, as there
is no ﬁxed set of honest workers during the entire optimization procedure.
3
Our Results
We tackle the Byzantine behavior of corrupt workers by applying a non-trivial decoding algorithm at the
master in each SGD iteration. Our decoding algorithm is inspired by the recent breakthrough results in
5We do not allow a mobile adversary to contaminate local datasets of the compromised nodes; otherwise, after a certain
number of iterations, it can end up contaminating the entire data stored at all the workers, which renders solving the optimization
problem in (1) meaningless.
7

Algorithm 1 Byzantine-Resilient SGD
1: Initialize. Set x0 := 0. Fix a constant learning rate η and a mini-batch size b.
2: for t = 0 to T −1 do
3:
On Workers:
4:
for r = 1 to R do
5:
Receive xt from master. Take a mini-batch stochastic gradient gr(xt) ∈U F⊗b
r (xt).
6:
egr(xt) =
(
gr(xt)
if worker r is honest,
⋇
if worker r is corrupt,
where ⋇is an arbitrary vector in Rd.
7:
Send egr(xt) to master.
8:
end for
9:
At Master:
10:
Receive {egr(xt)}R
r=1 from the R workers.
11:
Apply the decoding algorithm RGE (described in Algorithm 3 in Appendix E) on {egr(xt)}R
r=1. Let
bg(xt) = RGE(eg1(xt), . . . , egR(xt)).
12:
Update the parameter vector:
bxt+1 = xt −ηbg(xt);
xt+1 = ΠC
 bxt+1
.
13:
Broadcast xt+1 to all workers.
14: end for
theoretical computer science for robust mean estimation [LRV16,DKK+16,SCV18]; see Section 4 for more
details.
Before stating our results, we need to formally deﬁne mini-batch SGD. Note that we can speed up the
convergence of distributed SGD by having each worker sample many data points (without replacement),
say, b ≥1 data points, and send the average gradients on these data points to the master. This is called
mini-batch SGD. To formalize this, for any x ∈Rd, r ∈[R], b ∈[nr], consider the following set
F⊗b
r (x) :=
(
1
b
X
i∈Hb
∇Fr,i(x) : Hb ∈
[nr]
b
)
.
(5)
Let Hb denote a random variable taking values in
 [nr]
b

with uniform distribution. We denote a mini-batch
stochastic gradient with batch size b by ∇Fr,Hb(x), which is a uniformly random element of F⊗b
r (x). It is not
hard to show that the mean of ∇Fr,Hb(x) remains unchanged and is equal to ∇Fr(x) =
1
nr
Pnr
i=1 ∇Fr,i(x),
and the variance reduces by a factor of b:
EHb [∇Fr,Hb(x)] = ∇Fr(x),
(6)
EHb ∥∇Fr,Hb(x) −∇Fr(x)∥2 ≤σ2
b .
(7)
Note that the variance bound in (7) trivially follows if we assume that the workers sample stochastic gradients
with replacement. In this paper, since workers sample stochastic gradients without replacement, we can in fact
show a stronger bound of EHb ∥∇Fr,Hb(x) −∇Fr(x)∥2 ≤
(nr−b)
b(nr−1)σ2. However, for simplicity of exposition,
we only use the weaker variance bound of (7) in this paper.
We present our Byzantine-resilient SGD algorithm in Algorithm 1. Our convergence results are for both
strongly-convex and non-convex smooth functions. Before stating them, we need some deﬁnitions ﬁrst.
8

• L-smoothness: A function F : C →R is called L-smooth over C ⊆Rd, if for every x, y ∈C, we have
∥∇F(x) −∇F(y)∥≤L∥x −y∥(this property is also known as L-Lipschitz gradients). This is also
equivalent to F(y) ≤F(x) + ⟨∇F(x), y −x⟩+ L
2 ∥x −y∥2.
• µ-strong convexity: A function F : C →R is called µ-strongly convex over C ⊆Rd, if for every
x, y ∈C, we have F(y) ≥F(x) + ⟨∇F(x), y −x⟩+ µ
2 ∥x −y∥2.
3.1
Convergence results
Theorem 1 (Strongly-convex and Non-convex). Suppose an ϵ > 0 fraction of R workers are adversarially
corrupt. For an L-smooth global objective function F : C →R, let Algorithm 1 generate a sequence of iterates
{xt}T
t=0 when run with a ﬁxed learning rate η, where in the t’th iteration, every honest worker r ∈[R] samples
a mini-batch stochastic gradient from F⊗b
r (xt), satisfying (6) and (7) (corrupt workers may send arbitrary
vectors). Fix an arbitrary constant ϵ′ > 0. If ϵ ≤1
4 −ϵ′, then with probability at least 1 −T exp(−ϵ′2(1−ϵ)R
16
),
we have the following convergence guarantees:
• Strongly-convex: If F is also µ-strongly convex and we take η =
µ
L2 , then we have
E∥xT −x∗∥2 ≤

1 −µ2
2L2
T
∥x0 −x∗∥2 + 2L2
µ4 Γ.
(8)
If we take T = log

µ4
L2Γ ∥x0 −x∗∥2
/log(
1
1−µ2/2L2 ), we get E∥xT −x∗∥2 ≤3L2
µ4 Γ.
• Non-convex: If we take η =
1
4L, then we have
1
T
T
X
t=0
E∥∇F(xt)∥2 ≤8L2
T ∥x0 −x∗∥2 + Γ,
(9)
If we take T = 8L2∥x0−x∗∥2
Γ
, we get
1
T
PT
t=0 E∥∇F(xt)∥2 ≤2Γ.
In both (8) and (9), expectation is taken over the sampling of mini-batch stochastic gradients. Here, Γ =
9σ2
(1−(ϵ+ϵ′))bR + 9κ2 + 9Υ 2 with Υ = O
 σ0
√
ϵ + ϵ′
, where σ2
0 = 24σ2
bϵ′

1 +
d
(1−(ϵ+ϵ′))R

+ 16κ2.
We prove the strongly-convex part of Theorem 1 in Appendix B.1 and the non-convex part in Appendix B.2.
Projection.
If the parameter space C is not equal to Rd, then our convergence analysis for non-convex
objectives requires a mild technical assumption on the size of C.
This assumption is only required to
ensure that the iterates xt always stay inside C without projection. Similar assumption has also been made
in [YCRB18] for the same purpose. This assumption streamlines our convergence analysis, as our focus in
this paper is on Byzantine-resilience.
Assumption 3 (Size of C). Suppose ∥∇F(x)∥≤M for all x ∈C. We assume that C contains the ℓ2 ball
{x ∈Rd : ∥x −x0∥≤2L
Γ (M + Γ1)∥x0 −x∗∥2}, where Γ =
9σ2
(1−(ϵ+ϵ′))bR + 9κ2 + 9Υ 2 and Γ1 = nmaxσ
b
+ κ + Υ,
where nmax = maxr∈[R] nr and other parameters are as deﬁned in Theorem 1 above.
Note the dependence of the size of C on nmax
b
, which is the maximum number of data samples at any
worker. This happens because we want a deterministic bound on the size of C (not in expectation) even
though we are doing stochastic sampling of data points for gradient computation. See the proof of Lemma 4
(and also Claim 4) in Appendix B.2 for more details.
9

3.2
Remarks about Theorem 1
In this section, we discuss some important aspects of our convergence results.
Analysis of the approximation error. In both parts of Theorem 1, the approximation error Γ consists
of three error terms: ﬁrst is Γ1 = O(σ2/(1−(ϵ+ϵ′))bR), which is the standard error arising due to the sampling
of stochastic gradients; second is Γ2 = O(κ2), which is due to dissimilarity in the local datasets; and third is
Γ3 = O

σ2
bϵ′

1 +
d
(1−(ϵ+ϵ′))R

+ κ2
(ϵ + ϵ′)

, which is due to Byzantine attacks. Observe that Γ1 decreases
with the mini-batch size b and the number of workers R, as desired. Note that Γ3 consists of two terms
Γ3,1 = σ2
bϵ′

1 +
d
(1−(ϵ+ϵ′))R

(ϵ + ϵ′) and Γ3,2 = κ2(ϵ + ϵ′), where we can make Γ3,1 small by taking a large
mini-batch size b. Note that the presence of Γ3,2 is inevitable, since κ captures the dissimilarity in diﬀerent
datasets, and that will always show up when bounding the deviation of the true “global” gradient from the
decoded one in the presence of Byzantine workers. See Figure 2 to get a pictorial intuition on the above
analysis of the approximation error.
Hence, by taking a suﬃciently large mini-batch size, we can reduce the error term to O(κ2), which, in
the statistical heterogeneous model described in Section 6, is equal to O

κ2
mean + d log(nd)
n

; see Theorem 6.
Here, κmean captures the diﬀerence between local and global population means (see Assumption 7) and
n is the number of data samples at each worker. In particular, if each worker has n = Ω(d log(nd)) data
points, and they take a suﬃciently large mini-batch size in each iteration of Algorithm 1, we can reduce the
approximation error to O(κ2
mean). Note that our heterogeneous data setting generalizes (as far as we know) the
only data settings studied in literature for Byzantine-resilient distributed optimization (see also Remark 1),
where workers either have i.i.d. homogeneous data (i.e., κmean = 0) [YCRB18,YCRB19,SX19,CSX17], or are
assumed to have access to the same data [AAL18,BMGS17] (i.e., κ = 0).
Convergence rates.
Note that, in the strongly-convex case, Algorithm 1 approximately ﬁnds optimal
parameters x∗(within Γ error, which could be a constant) “exponentially fast”; and in the non-convex case,
Algorithm 1 approximately ﬁnds a stationary point up to the same error with “linear speed”, i.e., with a rate
of 1/T. Thus, we recover the convergence rate of vanilla SGD (running in the Byzantine-free setting) for both
the objectives.
Corruption threshold. Our proposed algorithm can tolerate up to 1
4 fraction Byzantine workers, which
is away from the information-theoretically optimal 1
2 fraction. The 1
4 bound comes from the subroutine of
robust mean estimation (RME) that we use for robust gradient estimation (RGE), as explained in Section 4.
So, improved algorithms for RME that can be adapted to our setting will directly give an improved corruption
threshold for our algorithm.
Failure probability.
The failure probability of our algorithm is at most T exp(−ϵ′2(1−ϵ)R
16
), which is at
most δ, for any δ > 0, provided we run our algorithm for T ≤δ exp( ϵ′2(1−ϵ)R
16
) iterations. Though the error
probability scales linearly with T, it also goes down exponentially with the number of workers R. As a result,
in settings such as federated learning, where number of workers R could be very large (in tens of thousands,
or in millions), we can get a very small probability of error, say, 1/100, even if run our algorithm for a very
long time. Note that the probability of error is due to the stochastic sampling of gradients, and if we want a
“zero” probability of error, we can run full-batch gradient descent, which is described in the next section.
3.3
Convergence results for full-batch gradient descent
In this section, we provide our results for the setting where workers compute full-batch gradients, instead of
mini-batch stochastic gradients. This setting will simplify the approximation error in the solution produced
by Algorithm 1 on both strongly-convex and non-convex objectives as well as their convergence analyses.
Theorem 2. Suppose an ϵ > 0 fraction of R workers are adversarially corrupt. For an L-smooth global
objective function F : C →R, let Algorithm 1 generate a sequence of iterates {xt}T
t=0 when run with a ﬁxed
learning rate η, where in the t’th iteration, every honest worker r ∈[R] sends ∇Fr(xt) to the master (corrupt
10

workers may send arbitrary vectors). If ϵ ≤1
4, then with probability 1, we have the following convergence
guarantees (where ΓGD = 6κ2 + 6Υ 2
GD with ΥGD = O (κ√ϵ)):
• Strongly-convex: If F is also µ-strongly convex and we take η =
µ
L2 , then we have
∥xT −x∗∥2 ≤

1 −µ2
2L2
T
∥x0 −x∗∥2 + 2L2
µ4 ΓGD.
(10)
• Non-convex: If we take η =
1
4L, then we have
1
T
T
X
t=0
∥∇F(xt)∥2 ≤8L2
T ∥x0 −x∗∥2 + ΓGD.
(11)
Theorem 2 is proved in Appendix B.3.
As mentioned in Section 3.1, when C is a bounded set, our convergence analysis for non-convex objectives
requires a mild technical assumption on the size of C:
Assumption 4. Suppose ∥∇F(x)∥≤M for every x ∈C. We assume that C contains the ℓ2 ball {x ∈Rd :
∥x −x0∥≤
2L
ΓGD (M + Γ2)∥x0 −x∗∥2}, where ΓGD = 6κ2 + 6Υ 2
GD and Γ2 = κ + Υ, where the parameters are
as deﬁned in Theorem 2 above.
Note that, unlike in Assumption 3, Γ2 in Assumption 4 does not depend on the number of local samples
at workers. This is because the variance is zero when workers take full-batch gradients.
A note about the approximation error. Note that the approximation error term in both strongly-convex
and non-convex objectives is ΓGD = 6κ2 + O(κ2ϵ) = O(κ2), which only depends on the heterogeneity in the
data, and as argued in the error analysis paragraph of Section 3.2, is inevitable in the heterogeneous data setting.
In the statistical heterogeneous data model described in Section 6, this is equal to O

κ2
mean + d log(nd)
n

; see
the discussion in Section 3.2. A special case is where all workers have i.i.d. data (i.e., κmean = 0), which is
the setting considered in [YCRB18,YCRB19,SX19,CSX17]. These papers also studied full-batch gradient
descent, but to minimize the population risk, as opposed to minimizing the empirical risk, which is the focus
of this paper. See Section 1.2 for a detailed comparison of our approximation error with that in these works.
4
Robust Gradient Estimation (RGE)
We are given R gradient vectors eg1(x), . . . , egR(x) ∈Rd for an arbitrary x ∈Rd, where, egr(x) = gr(x) is a
uniform sample from F⊗b
r (x) if the r’th worker is honest, otherwise, egr(x) can be arbitrary. We want to
compute bg(x), an estimate of ∇F(x) = 1
R
PR
r=1 ∇Fr(x), where ∇Fr(x) is the mean of F⊗b
r (x), such that
∥bg(x)−∇F(x)∥is small for all x ∈Rd. In this section, building upon the recent advances in high-dimensional
robust mean estimation problem, we provide a polynomial-time decoding algorithm (in particular, we use the
outlier-ﬁltering algorithm proposed by [SCV18]) and derive a matrix concentration result in order to use that
algorithm in our setting.
First we describe the problem of robust mean estimation (RME). In RME, we are given R samples in
Rd (out of which an ϵ-fraction is corrupted) from an unknown distribution with unknown mean, and the
goal is to estimate its mean. Though our problem is more general than RME, it would be helpful to ﬁrst get
some perspective on what makes RME, and hence our problem, so diﬃcult. RME is a classic problem in
robust statistics [Tuk60,Hub64]. Until recently, all the solutions to this problem were either computationally
intractable or were very poor in terms of the quality of the estimator produced. The method of Tukey
median [Tuk75] solves this problem with dimension-independent error guarantees, but it is NP-hard to
compute in general [JP78]. On the other hand, solutions based on geometric-median, coordinate-wise median
are computationally tractable, but can only give dimension-dependent error guarantees, which scales with
√
d [LRV16]. Below we give an intuition on the fundamental diﬃculty of this problem.
11

Why is robust mean estimation in high dimensions such a diﬃcult problem? To understand this,
assume that gradients are distributed according to a high-dimensional Gaussian distribution N(0, I). It is
a well known fact that samples from such a distribution lie around the annulus at a distance
√
d from the
origin, w.h.p. So, it would not be in the adversary’s best interest to put the corrupt samples far from the
annulus, as they can be trivially ﬁltered out just based on the norm. However, the adversary can put the
corrupted samples in a concentrated form around the annulus, which cannot be detected just based on the
norm, but can shift the sample mean away from the true mean in an adversarially chosen direction. This
implies that ﬁltering based on individual sample-by-sample basis is not enough, and we have to ﬁlter the
outliers collectively, i.e., using all the samples at once. This makes devising computationally-eﬃcient decoding
algorithm that provide good approximation guarantees highly non-trivial. Recently, [LRV16] and [DKK+16]
in their breakthrough papers independently provided computationally eﬃcient algorithms for RME that
give dimension-independent error guarantees. Following these papers, there had been a ﬂurry of research
improving upon their results in various directions; see [DK19] and references therein.
Diﬃculty of our problem.
When all local datasets Dr, r ∈[R], are the same, we have that for every
x ∈Rd, all F⊗b
r (x), r ∈[R] are the same and so are ∇Fr(x), r ∈[R]. By letting ∇Fr(x) := ∇F(x) and
F⊗b
r (x) := F⊗b(x), we can map our problem to RME as follows: Let the R gradient samples come i.i.d. from
F⊗b(x) with a uniform distribution, out of which an ϵ fraction may be adversarially corrupt. Note that each
gradient sample is unbiased and has mean equal to ∇F(x) and has variance bounded by σ2
b (see (6), (7)),
and our goal is to estimate the mean ∇F(x). Note that not all results on RME are applicable to our setting,
as most of these results have been derived assuming particular distributions, e.g., Gaussian, from which the
samples are drawn; whereas, in this paper we only assume the variance bound on the gradients.
Note that our problem is more general than the one described above. In our setting, diﬀerent workers have
diﬀerent datasets, which adds further complications. In RME, all samples come from the same distribution,
whereas, in our problem, diﬀerent samples come from diﬀerent local distributions (which are all uniform but
over distinct supports, with potentially diﬀerent support sizes) – for any worker r ∈[R], the r’th gradient
sample gr(x) comes uniformly at random from F⊗b
r (x), where F⊗b
r (x)’s are diﬀerent for diﬀerent r ∈[R].
Note that we get one sample from each distribution, and we want to estimate ∇F(x) = 1
R
PR
r=1 ∇Fr(x), the
average of the local means. Observe that, if we do not have any correlation among the local datasets (e.g., a
probabilistic model for the data or assuming that all local datasets are the same), it would be impossible to
solve this problem using RME-type algorithms and get a meaningful result; see also Remark 1. To make this
tractable, we assume that the local datasets are not arbitrarily far from each other, in the sense that the true
local gradients (evaluated at an arbitrary point in the domain) at any worker are at most κ away from the
global gradient; see (4). As mentioned in Remark 1, all other papers in distributed optimization literature
that provide Byzantine-resilience for optimizing a generic objective function (1) under standard assumptions
either assume that the local data at diﬀerent workers is generated i.i.d. from the same distribution, or that
all workers can access to the same data.
Reducing the sample complexity by increased mini-batch size. It is known that, in the problem of
RME, in order to get a good estimator, the sample complexity (i.e., the number of samples required) for
robustly estimating the mean grows at least linearly with the dimension d [LRV16]. In a setting where all
local datasets are the same, this implies that to robustly estimating ∇F(x), the number of workers R should
grow linearly with the dimension d. In a distributed setup, since it is not practical to increase the number of
workers with the dimension of the problem, we address this issue by increasing the mini-batch size b. As
noted in (7), by increasing the mini-batch size b, the variance of the resulting gradients (which are samples
from F⊗b(x)) reduces by a factor of b, which implies that as we increase b, the resulting gradients become
closer to the mean ∇F(x); and as we see later, this will cut down the requirement of R growing linearly with
d. Observe that it is crucial that increasing the mini-batch size does not change the mean, as we want to
estimate ∇F(x) using F⊗b(x). As we show later, this argument, in fact, holds true in a more general setting
that we consider in this paper, where diﬀerent workers have diﬀerent datasets, and we get one gradient sample
from each F⊗b
r (x). In this case, the approximation error will inevitably be aﬀected by κ, which captures the
dissimilarity between the local datasets. Increasing the mini-batch size b will result in making local stochastic
gradients close to their corresponding local true gradients, and which, on average (after removing the eﬀect of
12

∇F1(x)
g1
∇F2(x)
g2
∇F3(x)
g3
σ
√
b
∇F4(x)
g4
∇F5(x)
g5
∇F6(x)
g6
∇F7(x)
g7
∇F8(x)
∇F9(x)
∇F(x)
κ
∇F[2:7](x)
g[2:7]
bg(x)
Figure 2 We have total 9 workers, out of which 2 workers
(numbered 8, 9) are Byzantine. Since diﬀerent workers
have diﬀerent datasets, their true local gradients (denoted
by ∇Fi(x)) are placed in diﬀerent locations. The blue
dashed circles (numbered 1 to 7) are centered at the true
local gradients of honest workers, and have their radius
equal to the standard deviation σ/
√
b, which implies that
their stochastic gradient samples gi may not lie inside
the blue circles. The red dashed circles correspond to the
Byzantine workers, and we do not have any control over
them. Let {g2, . . . , g7} be the subset S of uncorrupted
gradients ensured by the ﬁrst part of Theorem 3. Let the
robust gradient estimator in the second part of Theorem 3
outputs bg(x) as an estimate of g[2:7] := 1
6
P7
i=2 gi. To
bound the approximation error E∥bg(x) −∇F(x)∥, note
that E∥bg(x) −∇F(x)∥≤E∥bg(x) −g[2:7](x)∥+ E∥g[2:7] −
∇F[2:7](x)∥+ ∥∇F[2:7](x) −∇F(x)∥, where the ﬁrst term
can be bounded by O(σ0
√
ϵ + ϵ′), the second term can be
bounded by the square root of σ2/6b, which comes from the
variance bound for sampling, and the third term can be
bounded by κ, which is the gradient dissimilarity bound
from (4). Note that the κ term is inevitable because, in
the presence of a constant number of Byzantine workers,
intuitively, ∇F[2:7](x) will shift away from ∇F(x) by a
constant fraction of κ.
corrupt gradients), will be about κ distance away from the global gradient. See also Figure 2.
Our main result for robust gradient estimation is as follows:
Theorem 3 (Robust Gradient Estimation). Fix an arbitrary x ∈Rd. Suppose an ϵ fraction of workers are
corrupt and we are given R gradients eg1(x), . . . , egR(x) ∈Rd, where egr(x) = gr(x) is a uniform sample from
F⊗b
r (x) satisfying (6), (7) if the r’th worker is honest, otherwise can be arbitrary. Let egi := egi(x) for i ∈[R].
Then, for any constant ϵ′ > 0, we have the following:
1. Matrix concentration: With probability 1 −exp(−ϵ′2(1−ϵ)R
16
), there exists a subset S ⊂[R] of
uncorrupted gradients of size (1 −(ϵ + ϵ′))R such that
λmax
 
1
|S|
X
i∈S
(gi −gS) (gi −gS)T
!
≤24σ2
bϵ′

1 +
d
(1 −(ϵ + ϵ′))R

+ 16κ2,
(12)
where gS :=
1
|S|
P
i∈S gi, κ is from (4), and λmax denotes the largest eigenvalue.
2. Outlier-ﬁltering algorithm: If ϵ ≤1
4 −ϵ′, then we can ﬁnd an estimate bg of gS in polynomial-time
with probability 1, such that ∥bg −gS∥≤O
 σ0
√
ϵ + ϵ′
, where σ2
0 = 24σ2
bϵ′

1 +
d
(1−(ϵ+ϵ′))R

+ 16κ2.
Remark 2. The squared approximation error has two terms T1 = O

σ2
bϵ′

1 +
d
(1−(ϵ+ϵ′))R

(ϵ + ϵ′)

and
T2 = O(κ2(ϵ + ϵ′)). Here, the ﬁrst error term T1 appears due to the stochastic sampling of gradients, and is
equal to zero when there is no gradient sampling error (i.e., σ = 0), which would be the case, for instance,
when workers take full batch gradients w.r.t. their local datasets; see also the proof of Theorem 2 for a detailed
discussion for this case with an improved bound on the approximation error. The second error term T2
accounts for the dissimilarity κ among the local datasets at diﬀerent workers, and will be equal to zero when
all workers are assumed to have access to the same dataset.6
6Note that when σ = κ = 0, then the problem of robust gradient estimation becomes trivial, as this setting would correspond
to running full batch gradient descent in a distributed manner, where all honest workers have the same data, and, therefore, send
the same gradient to master, who can perform the majority vote to compute the correct gradient – in that case, less than 1/2
fraction of Byzantine workers can be tolerated.
13

The statement of Theorem 3 consists of two parts: First, it shows an existence of a large subset S
of uncorrupted gradients having bounded concentration around their sample mean, which is a matrix
concentration result; and second, it eﬃciently estimates the average of the gradients in S. We prove the ﬁrst
part in Section 4.1; and for the second part, we use the polynomial-time outlier-ﬁltering procedure of [SCV18],
which we describe in detail in Algorithm 3 in Appendix E, and prove the second part of Theorem 3 in
Appendix F by providing a comprehensive analysis of the outlier-ﬁltering procedure.
We also provide an intuition behind the outlier-ﬁltering procedure from [SCV18] in Appendix E.1 and
its running time analysis in Appendix E.2, where we show that Algorithm 3 performs at most O(R) SVD
computations of d×R matrices, which can be performed in O(dR2 min{d, R}) time in total; hence, Algorithm 3
runs in polynomial-time.
Note that the same ﬁltering procedure has also been used by [SX19,YCRB19] in the context of Byzantine-
robust full batch gradient descent, where data comes i.i.d. from a probability distribution, as opposed to
the stochastic gradient descent considered in this paper. In our setting, diﬀerent workers may have diﬀerent
datasets, and we do not make any probabilistic assumption on the data generation. Our results are derived
under standard SGD assumptions in the distributed setting.
4.1
Matrix concentration
Now we prove the ﬁrst part of Theorem 3. For that, we need to show an existence of a subset S of the R
gradients (out of which an ϵ fraction is corrupted) that has good concentration, as quantiﬁed in (12). We
want to point out that if workers send full-batch deterministic gradients (as in [SX19,YCRB19]), then we can
take S to be the set of all honest workers and get a deterministic bound; see Theorem 8 in Appendix B.3
for more details. However, when workers compute mini-batch stochastic gradients, showing an existence of
such a set is non-trivial. Note that, though [SX19,YCRB19] studied full-batch gradient descent, they also
proved a matrix concentration result, which they needed because they minimize the population risk, whereas,
we do not need such a result because, instead, we minimize the empirical risk in this paper. On the other
hand, we also prove a matrix concentration bound as stated in the ﬁrst part of Theorem 3, whose need arises
because of the stochasticity of gradients (due to SGD). This bound is of a very diﬀerent nature than theirs
and requires only the bounded variance assumption (3) of local gradients to prove, whereas, their bound
requires distributional assumptions (sub-exponential/sub-Gaussian) on local gradients.
In order to prove (12) in the ﬁrst part of Theorem 3, ﬁrst we show a separate matrix concentration bound
in the following lemma, and then we show how we can use that to prove our desired bound (12).
Lemma 1. Suppose there are m independent distributions p1, p2, . . . , pm in Rd such that Ey∼pi[y] = µi, i ∈
[m] and each pi has bounded variance in all directions, i.e., Ey∼pi[⟨y −µi, v⟩2] ≤σ2
pi holds for all unit vectors
v ∈Rd. Take an arbitrary ϵ′ ∈(0, 1]. Then, given m independent samples y1, y2, . . . , ym, where yi ∼pi, with
probability 1 −exp(−ϵ′2m/16), there is a subset S of (1 −ϵ′)m points such that
λmax
 
1
|S|
X
i∈S
(yi −µi) (yi −µi)T
!
≤4σ2
pmax
ϵ′

1 +
d
(1 −ϵ′)m

,
where σ2
pmax = max
i∈[m] σ2
pi.
Lemma 1 is a generalization of [CSV17, Proposition B.1], where the m samples y1, . . . , ym are drawn
independently from a single distribution p with mean µ and variance bound of σ2
p. Note that, in our setting,
diﬀerent yi’s may come from diﬀerent distributions, which may have diﬀerent means and variances. We
provide a proof of Lemma 1 in Appendix A.
Proof of the ﬁrst part of Theorem 3. In order to use Lemma 1 in our robust gradient estimation problem, for
every i ∈[R] corresponding to the honest worker, take pi to be a uniform distribution over F⊗b
i
(x), which
implies, using (6) and (7), that its associated mean and variance are µi = ∇Fi(x) and σ2
pi = σ2
b , respectively.
It is easy to see that the hypothesis of Lemma 1 is satisﬁed with yi = gi(x), µi = ∇Fi(x), σ2
pi = σ2
b :
E[⟨gi(x) −∇Fi(x), v⟩2]
(a)
≤E[∥gi(x) −∇Fi(x)∥2] · ∥v∥2
(b)
≤
σ2
b ,
14

where (a) follows from the Cauchy-Schwarz inequality and (b) uses (7) and ∥v∥≤1.
We are given R gradients, out of which at least (1 −ϵ)R are according to the correct distribution. By
considering only the uncorrupted gradients (i.e., taking m = (1 −ϵ)R), we have from Lemma 1 that there
exists a subset S of R gradients of size (1 −ϵ′)(1 −ϵ)R ≥(1 −(ϵ + ϵ′))R that satisﬁes
λmax
 
1
|S|
X
i∈S
(gi(x) −∇Fi(x)) (gi(x) −∇Fi(x))T
!
≤4σ2
bϵ′

1 +
d
(1 −(ϵ + ϵ′))R

.
(13)
Note that (13) is bounding the deviation of the points in S from their respective means ∇Fi(x). However, in
(12), we need to bound the deviation of the points in S from their sample mean gS(x) =
1
|S|
P
i∈S gi(x). Using
the gradient dissimilarity bound (4) together with some algebraic manipulations provided in Appendix A.1,
we can show that (13) implies λmax

1
|S|
P
i∈S (gi(x) −gS(x)) (gi(x) −gS(x))T 
≤24σ2
bϵ′

1 +
d
(1−(ϵ+ϵ′))R

+
16κ2, which completes the proof of the ﬁrst part of Theorem 3.
5
Gradient Compression
In this section, we make Algorithm 1 from Section 3 communication-eﬃcient by having the workers send
compressed gradients instead of full gradients. There are many ways to compress gradients: (i) sparsify
them by taking their top k entries, magnitude wise (denoted by topk) or random k entries (denoted by
randk) [SCJ18,AHJ+18]; (ii) quantize them to a small number of bits, either by using a deterministic quantizer
(e.g., taking the sign vector of gradients [BWAA18,KRSJ19]) or a randomized quantizer [AGL+17,SYKM17];
or (iii) using a combination of both [BDKD19]. In Section 5.1 below, ﬁrst we rule out the use of quantizers in
our setting, and then in Section 5.2, we show that the randk sparsiﬁer works well for our purpose. We state
our main results with gradient compression using the randk sparsiﬁer in Section 5.3.
5.1
Quantization does not suﬃce
First we argue that quantizers do not serve our purpose well. The reason being, since the variance bound
for any quantizer ensures that quantization does not blow up the length of the input vector by much, and
when applied to stochastic gradients, it turns out (and as shown below) that the variance of the quantized
mini-batch stochastic gradients does not decrease with increased mini-batch size in general. Note that the
reduction in variance with increased mini-batch size was crucial to our robust gradient estimation procedure
described in Section 4, and it played an instrumental role in our convergence results (see the discussion in
Section 3.2), where we could control the approximation error of our solutions by increasing the mini-batch
size of stochastic gradients. This important property of our solution will no longer hold if we use quantizers
for gradient compression, as explained below in more detail.
Take any unbiased quantizer, say, Qs from [AGL+17], which probabilistically maps each component of the
vector to one of s levels. It was shown in [AGL+17] that Qs is unbiased and has bounded variance, i.e., for
every vector v ∈Rd, we have EQ[Qs(v)] = v, and EQ∥Qs(v) −v∥2 ≤βd,s∥v∥2, where βd,s = min{
√
d/s, d/s2}.
Fix any worker r ∈[R]. By taking v to be a stochastic gradient ∇Fr,i(x) where i ∈U [nr], and applying
Qs on that, we get EQ,i[Qs(∇Fr,i(x))] = ∇Fr(x), where ∇Fr(x) is the true local gradient at worker r
w.r.t. the model x, and EQ,i∥Qs(∇Fr,i(x)) −∇Fr(x)∥2 ≤(1 + βd,s)Ei∥∇Fr,i(x)∥2. Under the assumption
that the local stochastic gradients have bounded second moment, i.e., Ei∥∇Fr,i(x)∥2 ≤G2 for some ﬁnite
G, we have EQ,i∥Qs(∇Fr,i(x)) −∇Fr(x)∥2 ≤(1 + βd,s)G2. Recall from (6), (7) that ∇Fr,Hb(x) denote
a uniform sample from F⊗b
r (x) and is a mini-batch stochastic gradient with batch size b. Even though
Var(∇Fr,Hb(x)) ≤σ2/b (see (7)), i.e., by taking the mini-batch SGD with batch size b, the variance reduces
by a factor of b; unfortunately, Var(Qs(∇Fr,Hb(x))) remains the same and does not reduce by any factor, i.e.,
Var(Qs(∇Fr,Hb(x))) = EQ,Hb∥Qs(∇Fr,Hb(x)) −∇Fr(x)∥2 ≤(1 + βd,s)G2. This is because when we take an
average of diﬀerent vectors, which are not far from each other (implied by the variance bound (3)) and all
have approximately the same length (implied by the bounded second moment assumption), the resulting
15

vector will also have similar length, which does not decrease with the number of vectors. Formally, we have
Er,Hb∥∇Fr,Hb(x)∥2 ≤Ei∥∇Fr,i(x)∥2 ≤G2,7 i.e., unlike variance, the second moment bound is not aﬀected
by taking a larger mini-batch.
Observe that, for Hb ∈U
 [nr]
b

, if we compute the variance of
1
b
P
i∈Hb Qs (∇Fr,i(x)) (instead of
Qs
  1
b
P
i∈Hb ∇Fr,i(x)

), we would get the desired reduction in variance by a factor of b. However, computing
b quantized gradients locally at each worker and then taking their average defeats the purpose of quantization,
as (in addition to being computationally expensive, because we would be quantizing b vectors separately
and then taking an average) the entries in the resulting vector may not have low precision. Note that
Qs
  1
b
P
i∈Hb ∇Fr,i(x)

may be far from 1
b
P
i∈Hb Qs (∇Fr,i(x)) in general, as quantizers are data-dependent,
and consequently, Var
  1
b
P
i∈Hb Qs (∇Fr,i(x))

could also be far from Var
 Qs
  1
b
P
i∈Hb ∇Fr,i(x)

, implying
that the results obtained for 1
b
P
i∈Hb Qs (∇Fr,i(x)) may not hold for Qs
  1
b
P
i∈Hb ∇Fr,i(x)

. Because of
these reasons, using quantization for gradient compression is undesirable for the purpose of this paper.
5.2
Using the randk sparsiﬁer
Now we show that the (properly scaled) randk sparsiﬁer has all the properties we want, i.e., unbiasedness,
bounded variance, and variance reduction in proportion to the mini-batch size. Recall that randk : Rd →Rd is
a randomized map, that takes a vector in Rd, selects its k entries uniformly at random, and set the remaining
(d −k) entires to zero. Formally, we deﬁne randk as follows: For any subset K ∈
 [d]
k

, deﬁne an operator
selectK : Rd →Rd such that for any vector v ∈Rd, selectK(v)i = vi when i ∈K and selectK(v)i = 0 when
i ∈[d] \ K. Note that randk(v) is equivalent to ﬁrst selecting K ∈U
 [d]
k

and then outputting selectK(v). Let
K and Hb be random variables respectively taking values in
 [d]
k

and
 [n]
b

with uniform distribution. It is
easy to show that EK←K[ d
k · selectK(v)] = v. The following lemma is proved in Appendix C.
Lemma 2. Fix any worker r ∈[R]. Suppose the local stochastic gradients at worker r have uniformly bounded
second moment, i.e., Ei∈U[nr]∥∇Fr,i(x)∥2 ≤G2, ∀x ∈C.8 Then we have
EK←K,Hb
d
k · selectK (∇Fr,Hb(x))

= ∇Fr(x)
(14)
EK←K,Hb

d
k · selectK (∇Fr,Hb(x)) −∇Fr(x)

2
≤d
k
G2
b .
(15)
In our Byzantine-resilient SGD algorithm with compressed gradients, if each worker selects random k
coordinates independent of each other, though master receives R gradients, the non-zero entries in compressed
gradients from diﬀerent workers may not conﬁne to the same k coordinates, and may spread across all over
d coordinates. So, in this case, to decode, master has to treat them as vectors in Rd, which will lead to
a squared approximation error (in the robust gradient estimator of Theorem 3) of O(eσ2
0(ϵ + ϵ′)), where
eσ2
0 = 24dG2
kbϵ′

1 +
d
(1−(ϵ+ϵ′))R

+ 16κ2 (by replacing σ2 by d
kG2 in Theorem 3). Observe that this scales as
d2
kbR, as opposed to
d
bR, which was achievable without gradient compression (see Theorem 3).
To mitigate this, we use a simple idea, where, instead of workers sampling k coordinates, master picks
k random coordinates and broadcast them, and dictate all the workers to restrict their entries to those
coordinates only. In addition to saving on communication, this will achieve two things:
1. Since all R gradients have their non-zero entries conﬁned to the same k coordinates, we can as-
sume that master receives R vectors in Rk.
With this, eσ2
0 in the approximation error becomes
24dG2
kbϵ′

1 +
k
(1−(ϵ+ϵ′))R

+ 16κ2, which, assuming k ≥(1 −(ϵ + ϵ′))R, scales as
d
bR, same as in the case
without gradient compression.
7The ﬁrst inequality Er,Hb∥∇Fr,Hb(x)∥2 ≤Ei∥∇Fr,i(x)∥2 follows from the Jensen’s inequality in case when Hb is a collection
of b elements drawn uniformly at random from [nr] with replacement. It is not crucial here, but we can show a similar bound
when Hb is a collection of b elements drawn uniformly at random from [nr] without replacement.
8This is a standard assumption in SGD literature with compressed gradient [SCJ18,BDKD19].
16

Algorithm 2 Byzantine-Resilient SGD with Gradient Compression
1: Initialize. Set x0 := 0. Fix a constant learning rate η and a mini-batch size b.
2: for t = 0 to T −1 do
3:
On Workers:
4:
for r = 1 to R do
5:
Receive xt and Kt ∈U
 [d]
k

from master.
6:
pr(xt) = 1
b
P
i∈Hb ∇Fr,i(xt), where Hb ∈U
 [nr]
b

, i.e., pr(xt) is a mini-batch stochastic gradient
with batch size b.
7:
gKt,r(xt) = d
k · selectKt(pr(xt)).
8:
egKt,r(xt) =
(
gKt,r(xt)
if worker r is honest,
⋇
if worker r is corrupt,
where ⋇is an arbitrary vector in Rd.
9:
Send egKt,r(xt) to master.
10:
end for
11:
At Master:
12:
Select a k-size subset Kt ∈U
 [d]
k

uniformly at random from [d] and broadcast to all the workers.
13:
Receive {egKt,r(xt)}R
r=1 from the R workers.
14:
Apply the decoding algorithm RGE on {egKt,r(xt)}R
r=1. Let
bgKt(xt) = RGE(egKt,1(xt), . . . , egKt,R(xt)).
15:
Update the parameter vector:
bxt+1 = xt −ηbgKt(xt);
xt+1 = ΠC
 bxt+1
.
16:
Broadcast xt+1 to all workers.
17: end for
2. Since the decoding algorithm now works on vectors in Rk (instead of Rd), the decoding complexity also
reduces: As mentioned on page 14 in Section 4, the outlier-ﬁltering procedure that we use from [SCV18]
takes O(dR2 min{d, R}) time with uncompressed gradients. When master receives compressed gradients,
which are vectors in Rk, master needs to perform at most O(R) SVD computations of k × R matrices,
which would take O(kR2 min{k, R}) time in total. So, the decoding complexity under compression
reduces by a factor of at least d
k, which could be as large as Ω( d
R) when k ≥(1 −(ϵ + ϵ′))R.
Our Byzantine-resilient SGD algorithm with compressed gradients is described in Algorithm 2.
5.3
Main results
To state our main results, we deﬁne a set of compressed mini-batch stochastic gradients for any K ∈
 [d]
k

as
follows, where r ∈[R].
F⊗b
K,r(x) =
(
d
k · selectK
1
b
X
i∈Hb
∇Fr,i(x)

: Hb ∈
[nr]
b
)
.
Note that a uniformly random sample from F⊗b
K,r(x) with K ∈U
 [d]
k

will satisfy (14) and (15).
As stated in Algorithm 2, each iteration of our algorithm needs to estimate the true gradient robustly
from the compressed gradients. For that, we need to prove a robust gradient estimation result (similar to
Theorem 3) for compressed gradients.
17

Theorem 4 (Robust Gradient Estimation with Compressed Gradients). Suppose the stochastic gradients at
all workers have bounded second moments, i.e., for every r ∈[R], we have Ei∈U[nr]∥∇Fr,i(x)∥2 ≤G2, ∀x ∈C.
Fix an arbitrary x ∈C. Let K ∈U
 [d]
k

. Suppose an ϵ fraction of workers are corrupt and we are given
R gradients egK,1(x), . . . , egK,R(x), where egK,r(x) = gK,r(x) is a uniform sample from F⊗b
K,r(x) if the r’th
worker is honest, otherwise can be arbitrary. Take an arbitrary constant ϵ′ > 0. If ϵ ≤1
4 −ϵ′, then with
probability 1 −exp(−ϵ′2(1−ϵ)R
16
), there exists a subset S of uncorrupted gradients of size (1 −(ϵ + ϵ′))R
(with gK,S(x) :=
1
|S|
P
i∈S gK,i(x) being its sample mean) and an estimate bgK(x) of gK,S(x) such that
∥bgK(x) −gK,S(x)∥≤O
 σ0
√
ϵ + ϵ′
, where σ2
0 = 24dG2
kbϵ′

1 +
k
(1−(ϵ+ϵ′))R

+ 16κ2.
Theorem 4 can be proved in the same way as we proved Theorem 3, except for the following changes:
Instead of using the variance bound (7) for uncompressed gradient, we use the variance bound (15) for
compressed gradient, and also the fact that master performs decoding on vectors in Rk instead of vectors in
Rd.
Our convergence results with compressed gradients are stated below.
Theorem 5. Suppose an ϵ > 0 fraction of R workers are adversarially corrupt and the stochastic gradients at
all workers have bounded second moments, i.e., for every r ∈[R], we have Ei∈U[nr]∥∇Fr,i(x)∥2 ≤G2, ∀x ∈C.
For an L-smooth global objective function F : C →R, let Algorithm 2 generate a sequence of updates {xt}T
t=0
when run with a ﬁxed learning rate η, where in the t’th iteration, master picks a random subset Kt ∈U
 [d]
k

and
broadcasts it, and every worker r ∈[R] samples a compressed mini-batch stochastic gradient from F⊗b
Kt,r(xt).
Fix an arbitrary ϵ′ > 0. If ϵ ≤1
4 −ϵ′, then with probability 1 −T exp(−ϵ′2(1−ϵ)R
16
), we have the following
guarantees:
• Strongly-convex: If F is also µ-strongly convex and we take η =
µ
L2 , then we have
E∥xT −x∗∥2 ≤

1 −µ2
2L2
T
∥x0 −x∗∥2 + 2L2
µ4 Γ,
(16)
• Non-convex: If we take η =
1
4L, then we have
1
T
T
X
t=0
E∥∇F(xt)∥2 ≤8L2
T ∥x0 −x∗∥2 + Γ,
(17)
In both (16) and (17), expectation is taken over the sampling of mini-batch stochastic gradients and also
the random coordinates for compression.
In both the expressions, Γ =
9dG2
(1−(ϵ+ϵ′))kbR + 9κ2 + 9Υ 2 with
Υ = O
 σ0
√
ϵ + ϵ′
, where σ2
0 = 24dG2
kbϵ′

1 +
k
(1−(ϵ+ϵ′))R

+ 16κ2.
Theorem 5 can be proved along the lines of the proof of Theorem 1, except for one change: Instead of
using (7) and Theorem 3 (which are for uncompressed gradients), we use (15) and Theorem 4, respectively.
Analysis of the approximation error. Similar to the discussion in Section 3.2, the approximation error
term Γ in (16) and (17) consists of three terms: Γ1 = O

dG2
(1−(ϵ+ϵ′))kbR

, which is the standard variance
term in SGD convergence, Γ2 = O(κ2), which captures the dissimilarity among diﬀerent local datasets,
and Γ3 = O

dG2
kbϵ′

1 +
k
(1−(ϵ+ϵ′))R

+ κ2
(ϵ + ϵ′)

, which is due to Byzantine attacks. Observe that, for
constant ϵ, we have Γ1 + Γ2 ≤Γ3 for all values of the mini-batch size b. This implies that, when a constant
fraction of workers are corrupt, the error due to combating Byzantine attacks subsumes the error due to
sampling stochastic gradients.
Furthermore, when k ≥(1 −(ϵ + ϵ′))R (which is satisﬁed when k is more than the number of
honest workers), we have Γ3 = O

dG2
ϵ′(1−(ϵ+ϵ′))bR + κ2
(ϵ + ϵ′)

, which gives the same dependence on
18

d, b, R we get from our compression-free algorithm; see Theorem 1, where the corresponding error term is
O

dσ2
ϵ′(1−(ϵ+ϵ′))bR + κ2
(ϵ + ϵ′)

. Thus, when a constant fraction of workers are corrupt (which implies
Γ1 + Γ2 ≤Γ3) and sparsity of the compressed gradients is more than the number of honest workers (which
implies Γ3 remains the same irrespective of whether we are working with full gradients or compressed
gradients), the approximation error term in the case of compressed gradients would be the same (order-wise)
as that in the case of full gradients. This essentially means that we get “compression for free”. On top of
that, we get a direct saving by a factor of at least d
k (which could be Ω( d
R)) in decoding complexity in each
iteration. Note that (ignoring term κ2 term) Γ3 can be made as small as possible by taking a suﬃciently
large mini-batch.
Error feedback does not help in getting faster convergence. It is known that compression (topk or
randk) without error feedback leads to slower/sub-optimal convergence. A remedy to this is to accumulate
the error (the gradient information that was not sent) into the memory and add it in subsequent iterations
[SCJ18,AHJ+18], so that eventually all the information is sent. One of the revealing implications of the above
analysis of the approximation error is that, when ϵ is constant and we select suﬃciently many coordinates
for gradient compression, then error feedback cannot help in speeding up the convergence of our Byzantine-
resilient SGD algorithm with gradient compression, as the approximation error remains the same irrespective
of whether we are working with compressed gradients or not. This is in sharp contrast with the Byzantine-free
SGD with compression, where error-feedback provably improves the convergence [SCJ18,AHJ+18].
6
Bounding the Local Variances and Gradient Dissimilarity in the
Statistical Heterogeneous Model
In this section, we bound the gradient dissimilarity κ2 (from (4)) and local variance σ2 (from (3)) in the
statistical model in heterogeneous setting, where diﬀerent workers may have local data generated from
potentially diﬀerent distributions. The purpose of this section is to provide upper bounds on κ and σ in the
statistical model.
Let q1, q2, . . . , qR denote the R probability distributions from which the local data samples at the workers
are drawn.
Speciﬁcally, the data samples at any worker r are drawn from qr in an i.i.d. fashion and
independently from other workers. For r ∈[R], let Qr denote the alphabet over which qr is distributed.
For r ∈[R], let fr : Qr × C →R denote the local loss function at worker r, where fr(z, x) is the loss
associated with the sample z ∈Qr w.r.t. the model parameters x ∈C ⊆Rd. Linear regression is a classic
example of this, where, if z = (w, y) denote the pair of a feature vector w ∈Rd and the response y ∈R, then
fr(z, x) = 1
2(⟨w, x⟩−y)2. For each worker r ∈[R], we assume that for any ﬁxed z ∈Qr, the local loss function
fr(z, x) is L-smooth w.r.t. x, i.e., for any z ∈Qr, we have ∥∇fr(z, x) −∇fr(z, y)∥≤L∥x −y∥, ∀x, y ∈C.
Let µr(x) := Ez∼qr[fr(z, x)] denote the expected value of fr(z, x), when z is sampled from Qr according
to qr. For any x ∈C, let µ(x) := 1
R
PR
r=1 µr(x) denote the average value of µr(x), r ∈[R].
We are given nr i.i.d. samples zr,1, zr,2, . . . , zr,nr at the r’th worker from qr. Fix an arbitrary parameter
vector x ∈C. Let ¯fr(x) :=
1
nr
Pnr
i=1 fr(zr,i, x) denote the average loss at worker r on the nr samples
zr,1, . . . , zr,nr w.r.t. x. Let ¯f(x) := 1
R
Pr
r=1 ¯fr(x) denote the average loss across all workers. The analogues
of (4) and (3) in this statistical heterogeneous model are the following:
∇¯fr(x) −∇¯f(x)
2 ≤κ2,
∀x ∈C,
(18)
Ei∈U[nr]
∇fr(zr,i, x) −∇¯fr(x)
2 ≤σ2,
∀x ∈C.
(19)
We need to ﬁnd good upper bounds on κ and σ that hold for all r ∈[R], x ∈C with high probability. We provide
two bounds on κ, one when the local gradients at workers are assumed to be sub-exponential random vectors,
and other when they are sub-Gaussian random vectors. We provide a bound on σ assuming that the local
gradients are sub-Gaussian random vectors. These are standard assumptions on gradients in statistical models,
where data at all workers are sampled from the same distribution in an i.i.d. fashion [CSX17,SX19,YCRB19],
19

which is in contrast to our heterogeneous data setting, where data at diﬀerent workers may be sampled from
diﬀerent distributions. Note that these works minimize the population risk with full batch gradient descent,
whereas, we minimize the empirical risk with stochastic gradient descent. In particular, [CSX17,SX19] make
sub-exponential gradient assumption and give convergence guarantees only for strong-convex objectives. On
the other hand, [YCRB19] gives convergence guarantees for non-convex objectives, but under a stricter
condition of sub-Gaussian distribution on gradients. In this paper, we provide convergence guarantees for
both strongly-convex and non-convex objectives. Moreover, as opposed to [CSX17, SX19, YCRB19], our
results are in a more general heterogeneous data model. Note that we need sub-Gaussian assumption only to
bound the variance, which occurs because workers sample stochastic gradients. In case of full batch gradient
descent, we only need sub-exponential assumption, as the variance is zero.
Now we state the distributional assumptions on local gradients.
Assumption 5 (Sub-exponential local gradients). For every x ∈C, the local gradient vectors at any worker
r ∈[R] are sub-exponential random vectors, i.e., there exist non-negative parameters (ν, α) such that
sup
v∈Rd:∥v∥=1
Ez∼qr [exp (λ ⟨∇fr(z, x) −∇µr(x), v⟩)] ≤exp
 λ2ν2/2

,
∀|λ| < 1
α.
(20)
Assumption 6 (Sub-Gaussian local gradients). For every x ∈C, the local gradient vectors at any worker
r ∈[R] are sub-Gaussian random vectors, i.e., there exists a non-negative parameter σg such that
sup
v∈Rd:∥v∥=1
Ez∼qr [exp (λ ⟨∇fr(z, x) −∇µr(x), v⟩)] ≤exp
 λ2σ2
g/2

,
∀λ ∈R.
(21)
Though, as stated above in both the assumptions, local gradients at all workers have the same parameters
((ν, α) for sub-exponential and σg for sub-Gaussian), this is without loss of generality. In case they have
diﬀerent parameters ((νr, αr), r ∈[R] for sub-exponential and σr, r ∈[R] for sub-Gaussian), we can take the
ﬁnal parameters to be the maximum of the respective local parameters – for sub-exponential, we can take
ν = maxr∈[R] νr and α = maxr∈[R] αr, and for sub-Gaussian, we can take σg = maxr∈[R] σr.
6.1
Bounding the gradient dissimilarity κ
In this section, we provide an upper bound on
∇¯fr(x) −∇¯f(x)
.
∇¯fr(x) −∇¯f(x)
 ≤
∇¯fr(x) −∇µr(x)
 + ∥∇µr(x) −∇µ(x)∥+
∇¯f(x) −∇µ(x)

≤
∇¯fr(x) −∇µr(x)
 + ∥∇µr(x) −∇µ(x)∥+ 1
R
R
X
r=1
∇¯fr(x) −∇µr(x)
 ,
(22)
where for the third term, we used ¯f(x) = 1
R
PR
r=1 ¯fr(x) and µ(x) = 1
R
PR
r=1 µr(x), and applied the triangle
inequality. It follows from (22) that in order to bound
∇¯fr(x) −∇¯f(x)
 uniformly over x ∈C, it suﬃces to
bound ∥∇µr(x) −∇µ(x)∥and
∇¯fr(x) −∇µr(x)
 , ∀r ∈[R] uniformly over x ∈C.
Bounding ∥∇µr(x) −∇µ(x)∥. Note that ∇µr(x) = Ez∼qr[∇fr(z, x)] is a property of the distribution qr
from which the data samples have been drawn and so is ∇µ(x) = 1
R
PR
r=1 ∇µr(x) the property of q1, . . . , qR.
Note that ∥∇µr(x) −∇µ(x)∥captures heterogeneity among distributions through their expected values, and
is equal to zero in the i.i.d. homogeneous data setting of [CSX17,YCRB18,SX19,YCRB19]. In order to get a
meaningful bound for κ, it is reasonable to assume that this heterogeneity is bounded. We assume a uniform
bound on the ∥∇µr(x) −∇µ(x)∥for every x ∈C.
Assumption 7. For every worker r ∈[R], the population mean of the local gradients has a uniformly bounded
deviation from the population mean of the global gradient, i.e.,
∥∇µr(x) −∇µ(x)∥≤κmean,
∀x ∈C.
(23)
20

Bounding
∇¯fr(x) −∇µr(x)
. Now we bound the diﬀerence between the sample mean and the true mean
under both sub-exponential and sub-Gaussian distributional assumptions on local gradients. For that we use
standard tools, such as concentration results for sum of independent sub-Gaussian/sub-exponential random
variables and ϵ-net arguments. We prove in Lemma 6 and Lemma 7, respectively, in Appendix D that under
both the assumptions, with high probability, our bounds are
∇¯fr(x) −∇µr(x)
 ≤O
q
d log(nrd)
nr

for
every x ∈C. Note that under the sub-exponential assumption, the bound holds only for suﬃciently large nr
such that nr = Ω(d log(nrd)), whereas, under the sub-Gaussian assumption, the bound holds for every nr.
Substituting these bounds in (22) yields the following result, which, for notational convenience, we state
for the case when all workers have the same number of data samples. Let D = max{∥x −x′∥: x, x′ ∈C} be
the diameter of C. Note that D = Ω(
√
d), and we assume that D can grow at most polynomially in d.
Theorem 6 (Gradient dissimilarity). Suppose n := nr, ∀r ∈[R], and Assumption 7 holds. Then, the gradient
dissimilarity bound under diﬀerent distributional assumptions is as follows:
1. [Sub-exponential] Suppose Assumption 5 holds. Let n ∈N be suﬃciently large such that n = Ω(d log(nd)).
Then, with probability at least 1 −
R
(1+nLD)d , the following bound holds for all r ∈[R]:
∇¯fr(x) −∇¯f(x)
 ≤κmean + O
 r
d log(nd)
n
!
,
∀x ∈C.
(24)
2. [Sub-Gaussian] Suppose Assumption 6 holds. For every n ∈N, with probability at least 1 −
R
(1+nLD)d ,
the following bound holds for all r ∈[R]:
∇¯fr(x) −∇¯f(x)
 ≤κmean + O
 r
d log(nd)
n
!
,
∀x ∈C.
(25)
Remark 3. Note that under Assumption 5 (sub-exponential), the gradient dissimilarity bound (24) holds
only when each worker has suﬃciently large number of samples n = Ω(d log(nd)). On the other hand, under
Assumption 6 (sub-Gaussian), the gradient dissimilarity bound (25) holds for every n ∈N.
6.2
Bounding the local variances
The local variance bound at the r’th worker is Ei∈U[nr]
∇fr(zr,i, x) −∇¯fr(x)
2 ≤σ2 (from (19)). We
simplify the LHS:
Ei∈U[nr]
∇fr(zr,i, x) −∇¯fr(x)
2 ≤2Ei∈U[nr] ∥∇fr(zr,i, x) −∇µr(x)∥2
+ 2Ei∈U[nr]
∇¯fr(x) −∇µr(x)
2
(a)
= 2 ∥∇fr(zr,1, x) −∇µr(x)∥2 + 2
∇¯fr(x) −∇µr(x)
2
(b)
≤4 ∥∇fr(zr,1, x) −∇µr(x)∥2
(26)
For the ﬁrst term on the RHS of (a), we used that zr,i, i ∈[nr] are i.i.d., and the second term follows because it
is independent of i ∈[nr]. Inequality (b) follows because
∇¯fr(x) −∇µr(x)
2 ≤∥∇fr(zr,1, x) −∇µr(x)∥2,
since the average of i.i.d. samples gives tighter concentration in comparison to if we use just one sample.
Note that bounding ∥∇fr(zr,1, x) −∇µr(x)∥is equivalent to bounding ∥∇fr(z, x) −∇µr(x)∥for a
random z ∼qr. We provide a uniform bound on ∥∇fr(z, x) −∇µr(x)∥for a random z ∼qr in Appendix D.3
using the sub-Gaussian gradient assumption. Below we state our ﬁnal bound on the local variances.
Theorem 7 (Variance bound). Suppose n := nr, ∀r ∈[R], and Assumption 6 holds. Then, with probability
at least 1 −
R
(1+nLD)d , the following bound holds for all r ∈[R]:
Ei∈U[n]
∇fr(zr,i, x) −∇¯fr(x)
2 ≤O (d log(d)) ,
∀x ∈C.
(27)
21

Remark 4 (Sub-Gaussian vs. sub-exponential assumption). Note that, we needed sub-Gaussian assumption
on local gradients because we wanted to uniformly bound Ei∈[nr] ∥∇fr(zr,i, x) −∇µr(x)∥2, which is the case
when we use only one data sample in each SGD iteration. In this paper, we use mini-batch SGD with a
variable batch size (to control the approximation error of our solution; see the approximation error analysis
in Section 3.2). So, when the batch-size b is suﬃciently large and satisﬁes b = Ω(d log(bd)), we can work
with the sub-exponential gradient assumption because the large batch size gives a concentration similar to
sub-Gaussian. This would give a bound of O

d log(bd)
b

on variance.
7
Future Work
We leave a few open problems for future research for Byzantine-resilient SGD on heterogeneous data:
Improving the Byzantine tolerance to beyond 1
4 fraction; obtaining second-order convergence guarantees for
non-convex objectives for SGD (note that [YCRB19] obtained such guarantees for full-batch GD on i.i.d.
homogeneous data); improving upon the decoding complexity at master node (note that the outlier-ﬁltering
procedure in this paper requires SVD computations, which could be expensive); studying local SGD with
Byzantine adversaries to improve communication eﬃciency; and study gradient compression with arbitrary
compressors (beyond the randk sparsiﬁer).
References
[A+18]
Tarek F. Abdelzaher et al. Will distributed computing revolutionize peace? the emergence of
battleﬁeld iot. In ICDCS 2018, pages 1129–1138, 2018.
[AAL18]
Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In Neural
Information Processing Systems (NeurIPS), pages 4618–4628, 2018.
[AGL+17]
D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. QSGD: communication-eﬃcient
SGD via gradient quantization and encoding. In NIPS, pages 1707–1718, 2017.
[AHJ+18]
D. Alistarh, T. Hoeﬂer, M. Johansson, N. Konstantinov, S. Khirirat, and C. Renggli. The
convergence of sparsiﬁed gradient methods. In NeurIPS, pages 5977–5987, 2018.
[BDKD19] Debraj Basu, Deepesh Data, Can Karakus, and Suhas N. Diggavi. Qsparse-local-sgd: Distributed
SGD with quantization, sparsiﬁcation and local computations. In NeurIPS, pages 14668–14679,
2019.
[BMGS17] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning
with adversaries: Byzantine tolerant gradient descent. In NIPS, pages 119–129, 2017.
[Bot10]
L. Bottou. Large-scale machine learning with stochastic gradient descent. In COMPSTAT, pages
177–186, 2010.
[BSS12]
Joshua D. Batson, Daniel A. Spielman, and Nikhil Srivastava. Twice-ramanujan sparsiﬁers. SIAM
J. Comput., 41(6):1704–1721, 2012.
[BWAA18] J. Bernstein, Y. Wang, K. Azizzadenesheli, and A. Anandkumar. SignSGD: compressed optimisa-
tion for non-convex problems. In ICML, pages 559–568, 2018.
[BZAA19]
Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd with
majority vote is communication eﬃcient and fault tolerant. In ICLR, 2019.
[CSV17]
Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In STOC,
pages 47–60, 2017.
22

[CSX17]
Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial
settings: Byzantine gradient descent. POMACS, 1(2):44:1–44:25, 2017.
[CWCP18] Lingjiao Chen, Hongyi Wang, Zachary B. Charles, and Dimitris S. Papailiopoulos. DRACO:
byzantine-resilient distributed training via redundant gradients. In ICML, pages 902–911, 2018.
[DD19]
Deepesh Data and Suhas N. Diggavi. Byzantine-tolerant distributed coordinate descent. In ISIT,
pages 2724–2728, 2019.
[DG08]
J. Dean and S. Ghemawat. Mapreduce: simpliﬁed data processing on large clusters. Commun.
ACM, 51(1):107–113, 2008.
[DK19]
Ilias Diakonikolas and Daniel M. Kane. Recent advances in algorithmic high-dimensional robust
statistics. CoRR, abs/1911.05911, 2019.
[DKK+16] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair
Stewart. Robust estimators in high dimensions without the computational intractability. In
FOCS, pages 655–664, 2016.
[DSD19a]
Deepesh Data, Linqi Song, and Suhas N. Diggavi. Data encoding for byzantine-resilient distributed
optimization. CoRR, abs/1907.02664, 2019.
[DSD19b]
Deepesh Data, Linqi Song, and Suhas N. Diggavi. Data encoding methods for byzantine-resilient
distributed optimization. In ISIT, pages 2719–2723, 2019.
[GMK+19] Avishek Ghosh, Raj Kumar Maity, Swanand Kadhe, Arya Mazumdar, and Kannan Ramchandran.
Communication-eﬃcient and byzantine-robust distributed learning. CoRR, abs/1911.09721, 2019.
[GV19]
Nirupam Gupta and Nitin H. Vaidya. Byzantine fault-tolerant parallelized stochastic gradient
descent for linear regression. In Allerton, pages 415–420, 2019.
[Hub64]
Peter J. Huber. Robust estimation of a location parameter. Ann. Math. Statist., 35(1):73–101, 03
1964.
[JP78]
David S. Johnson and Franco P. Preparata. The densest hemisphere problem. Theor. Comput.
Sci., 6:93–107, 1978.
[K+19]
Peter Kairouz et al. Advances and open problems in federated learning. CoRR, abs/1912.04977,
2019.
[KMRR16] Jakub Konecný, H. Brendan McMahan, Daniel Ramage, and Peter Richtárik. Federated op-
timization: Distributed machine learning for on-device intelligence. CoRR, abs/1610.02527,
2016.
[Kon17]
Jakub Konecný. Stochastic, distributed and federated optimization for machine learning. CoRR,
abs/1707.01155, 2017.
[KRSJ19]
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U. Stich, and Martin Jaggi. Error feedback
ﬁxes signsgd and other gradient compression schemes. In ICML, pages 3252–3261, 2019.
[LRV16]
Kevin A. Lai, Anup B. Rao, and Santosh S. Vempala. Agnostic estimation of mean and covariance.
In FOCS, pages 665–674, 2016.
[LSP82]
Leslie Lamport, Robert Shostak, and Marshall Pease. The byzantine generals problem. ACM
Trans. Program. Lang. Syst., 4(3):382–401, July 1982.
[RM51]
Herbert Robbins and Sutton Monro.
A stochastic approximation method.
The Annals of
Mathematical Statistics. JSTOR, 22, no. 3:400–407, 1951.
23

[RWCP19] Shashank Rajput, Hongyi Wang, Zachary B. Charles, and Dimitris S. Papailiopoulos. DETOX:
A redundancy-based framework for faster and more robust gradient aggregation. In NeurIPS,
pages 10320–10330, 2019.
[SCJ18]
S. U. Stich, J. B. Cordonnier, and M. Jaggi. Sparsiﬁed SGD with memory. In NeurIPS, pages
4452–4463, 2018.
[SCV18]
Jacob Steinhardt, Moses Charikar, and Gregory Valiant. Resilience: A criterion for learning in
the presence of arbitrary outliers. In ITCS, pages 45:1–45:21, 2018.
[SX19]
Lili Su and Jiaming Xu. Securing distributed gradient descent in high dimensional statistical
learning. POMACS, 3(1):12:1–12:41, 2019.
[SYKM17] A. Theertha Suresh, F. X. Yu, S. Kumar, and H. B. McMahan. Distributed mean estimation
with limited communication. In ICML, pages 3329–3337, 2017.
[Tuk60]
John W. Tukey.
A survey of sampling from contaminated distributions.
Contributions to
probability and statistics, 2:448–485, 1960.
[Tuk75]
John W. Tukey. Mathematics and picturing of data. In Proceedings of ICM, volume 6, pages
523–531, 1975.
[Ver10]
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. CoRR,
abs/1011.3027, 2010.
[YCRB18] Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter L. Bartlett.
Byzantine-robust
distributed learning: Towards optimal statistical rates. In ICML, pages 5636–5645, 2018.
[YCRB19] Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter L. Bartlett. Defending against saddle
point attack in byzantine-robust distributed learning. In ICML, pages 7074–7084, 2019.
[YJY19]
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication eﬃcient
momentum SGD for distributed non-convex optimization. In ICML, pages 7184–7193, 2019.
A
Proofs of Lemma 1 and the First Part of Theorem 3
Lemma (Restating Lemma 1). Suppose there are m independent distributions p1, p2, . . . , pm in Rd such that
Ey∼pi[y] = µi, i ∈[m] and each pi has bounded variance in all directions, i.e., Ey∼pi[⟨y −µi, v⟩2] ≤σ2
pi holds
for all unit vectors v ∈Rd. Take an arbitrary ϵ′ ∈(0, 1]. Then, given m independent samples y1, y2, . . . , ym,
where yi ∼pi, with probability 1 −exp(−ϵ′2m/16), there is a subset S of (1 −ϵ′)m points such that
λmax
 
1
|S|
X
i∈S
(yi −µi) (yi −µi)T
!
≤4σ2
pmax
ϵ′

1 +
d
(1 −ϵ′)m

,
where σ2
pmax = max
i∈[m] σ2
pi.
As mentioned in Section 4.1, Lemma 1 generalizes [CSV17, Proposition B.1], where the m samples
y1, . . . , ym are drawn independently from a single distribution p with mean µ and variance bound of σ2
p,
whereas, in our setting, diﬀerent yi’s may come from diﬀerent distributions, which may have diﬀerent means
and variances. Lemma 1 can be proved using similar arguments given in the proof of [CSV17, Proposition
B.1], and we provide a complete proof of this in this section.
Proof of Lemma 1 relies on the following claim.
Lemma 3. Let p be a distribution on Rd such that Ey∼p[y] = µ and Ey∼p[⟨y−µ, v⟩2] ≤σ2 for all unit vectors
v ∈Rd. Let M be a symmetric matrix such that 0 ≺M ≺cI for some constant c > 0 and tr
 (cI −M)−1
≤
1
4σ2prev , where σprev ≥σ. Take an arbitrary ϵ′ ∈(0, 1]. Then, for y ∼p, with probability at least 1 −ϵ′
2 , we
have
 M + ϵ′(y −µ)(y −µ)T 
≺(c + 4σ2)I and tr
 (c + 4σ2)I −(M + ϵ′(y −µ)(y −µ)T )
−1
≤
1
4σ2prev .
24

Before proving Lemma 3, ﬁrst we show how we can use it to prove Lemma 1.
Proof of Lemma 1. Initialize a matrix M := 0, a set S := ∅, and c := 4σ2
pmaxd. Note that the preconditioning
of Lemma 3 (i.e., 0 ≺M ≺cI and tr
 (cI −M)−1
≤
1
4σ2prev ) is satisﬁed with σprev = σpmax. Go through
the stream of m samples from y1 to ym. Note that σpmax ≥σpi holds for all i ∈[m]. For notational
convenience, let eyi = yi −µi for i = 1, 2, . . . , m. If (M + ϵ′ eyieyT
i ) satisﬁes the conclusion of Lemma 3, i.e.,
 M + ϵ′ eyieyT
i

≺(c + 4σ2
pi)I and tr
 (c + 4σ2
pi)I −(M + ϵ′ eyieyT
i )
−1
≤
1
4σ2pmax (which we know holds with
probability at least 1 −ϵ′
2 ), then update S ←S ∪{i}, M ←M + ϵ′ eyieyT
i , and c ←c + 4σ2
pi.9
Note that, in the next iteration, when we consider the sample yi+1, the preconditioning of Lemma 3 is
automatically satisﬁed: If the conclusion in the i’th step did not hold and we did not update S, M, c, then the
preconditioning of Lemma 3 in the (i + 1)’st iteration trivially holds, as it used to hold in the i’th iteration.
If the conclusion in the i’th step held and we updated S, M, c, then the preconditioning of Lemma 3 in the
(i + 1)’st iteration holds, as it is the same condition that we checked in the conclusion of the i’th iteration for
updating S, M, c.
When we have gone through the stream of m samples, in the end, we have c = 4σ2
pmaxd + P
i∈S 4σ2
pi ≤
4σ2
pmax(d + |S|) and M ≺
 4σ2
pmax(d + |S|)

I, which implies that λmax(M) ≤4σ2
pmax(d + |S|). Since M =
P
i∈S ϵ′ eyieyT
i , we have λmax

1
|S|
P
i∈S eyieyT
i

=
1
ϵ′|S|λmax (M) ≤
4σ2
pmax
ϵ′
(1 +
d
|S|). It only remains to show
that |S| ≥(1 −ϵ′)m holds with high probability.
By the above discussion, note that for each element i, we add i to S with probability at least 1 −ϵ′
2 .
Since the m samples yi, i ∈[m] are independent of each other, we have that the distribution of |S| is
lower-bounded by the sum of m independent indicator random variables, where each of them is equal to 1
with probability 1 −ϵ′
2 . So, by Chernoﬀbound, we have Pr[|S| ≤(1 −ϵ′)m] ≤exp(−ϵ′2m
16 ), which implies
that Pr[|S| ≥(1 −ϵ′)m] ≥1 −exp(−ϵ′2m
16 ).
We have shown that with probability 1−exp(−ϵ′2m
16 ), there exists a subset S of y1, . . . , ym such that |S| ≥
(1 −ϵ′)m and λmax

1
|S|
P
i∈S eyieyT
i

≤
4σ2
pmax
ϵ′

1 +
d
(1−ϵ′)m

. Substituting eyi = yi −µi for i = 1, 2, . . . , m
concludes the proof of Lemma 1.
Now we proceed to proving Lemma 3.
Proof of Lemma 3. A version of this lemma has appeared in [CSV17, Lemma B.2], which, in turn, is essentially
the same as [BSS12, Lemma 3.3]. Our proof is along the lines of the proof of [CSV17, Lemma B.2].
For simplicity of notation, let ey = y −µ. Instead of
 M −ϵ′ eyeyT 
, it will be helpful later to consider
 M −teyeyT 
for arbitrary t ∈[0, ϵ′].
By the Sherman-Morrison matrix inversion formula, we have that if a square matrix A ∈Rn×n is invertible
and u, v ∈Rn are column vectors such that (1 + vT A−1u) ̸= 0, then (A + uvT ) is invertible and its inverse
is equal to (A + uvT )−1 = A−1 −A−1uvT A−1
1+vT A−1u .
We want to apply this formula on
 ((c + 4σ2)I −M) −teyeyT −1 with A = ((c + 4σ2)I −M), u =
√
tey,
and v = −
√
tey. For that, we need to show two things: ﬁrst, that ((c + 4σ2)I −M) is invertible, and second,
that (1 −teyT ((c + 4σ2)I −M)−1ey) ̸= 0. For the ﬁrst requirement, note that M ≺(c + 4σ2)I, which follows
because M ≺cI (by assumption), and σ > 0. This implies that ((c + 4σ2)I −M) is invertible. It follows
from the analysis below (see the paragraph after (30)) that the second requirement also holds for every
t ∈[0, ϵ′] with probability at least 1 −ϵ′
2 . Now, applying the Sherman-Morrison matrix inversion formula on
 ((c + 4σ2)I −M) −teyeyT −1:
9Note that we only observe yi’s, not (yi −µi). In the context of distributed SGD, the yi’s correspond to the stochastic
gradients that the master receives from workers, and there, the master does not know the true local gradients at any worker
– the true local gradient at worker i corresponds to the mean µi here.
Yet, in each iteration i, we probabilistically add
ϵ′(yi −µi)(yi −µi)T to M. We can do that, because we just want to show an existence of a set S that satisﬁes the required
properties stated in Lemma 1. This is just for the purpose of analysis, and we are not giving an algorithm to construct S.
25

 ((c + 4σ2)I −M) −teyeyT −1 =
 (c + 4σ2)I −M
−1
+ t
 (c + 4σ2)I −M
−1eyeyT  (c + 4σ2)I −M
−1
1 −teyT  (c + 4σ2)I −M
−1ey
(28)
Taking trace on both sides gives
tr
 ((c + 4σ2)I −M) −teyeyT −1
= tr
 (c + 4σ2)I −M
−1
+
tr
 (c + 4σ2)I −M
−1eyeyT  (c + 4σ2)I −M
−1
1
t −eyT  (c + 4σ2)I −M
−1ey
.
Let Φc(M) = tr
 (cI −M)−1
. Using tr(AB) = tr(BA) on the last term and using the fact that trace of a
scalar is the scalar itself, we get
Φc+4σ2(M + teyeyT ) = Φc+4σ2(M) +
eyT  (c + 4σ2)I −M
−2ey
1
t −eyT  (c + 4σ2)I −M
−1ey
.
(29)
We are given Φc(M) ≤
1
4σ2prev , and we want to show Φc+4σ2(M + teyeyT ) ≤
1
4σ2prev . So, it suﬃces to prove that
Φc+4σ2(M + teyeyT ) ≤Φc(M). This, in light of (29), is equivalent to the condition
1
t
≥eyT  (c + 4σ2)I −M
−1ey + eyT  (c + 4σ2)I −M
−2ey
Φc(M) −Φc+4σ2(M)
=: Ψ,
(30)
which, as we show in the analysis below, will hold with probability at least 1 −ϵ′
2 for all t ∈[0, ϵ′]. (Assume
that (30) holds with probability at least 1−ϵ′
2 for all t ∈[0, ϵ′]. Note that Φc(M) > Φc+4σ2(M) (from Claim 2
below) and ((c+4σ2)I−M) ≻0 hold. Using these in (30) imply that 1
t > eyT  (c+4σ2)I−M
−1ey holds with
probability at least 1 −ϵ′
2 for all t ∈[0, ϵ′]. Thus the second requirement (1 −teyT ((c + 4σ2)I −M)−1ey) ̸= 0
also holds, which was necessary for applying the matrix inversion formula on
 ((c + 4σ2)I −M) −teyeyT −1
to write (28).)
Since Ψ is a scalar, we have tr(Ψ) = Ψ. Taking trace in (30), and using tr(eyT Aey) = tr(AeyeyT ), and then
taking expectation, we get
E [Ψ] = E
h
tr
 (c + 4σ2)I −M
−1eyeyT i
+
E
h
tr
 (c + 4σ2)I −M
−2eyeyT i
Φc(M) −Φc+4σ2(M)
.
(31)
Since
 (c+4σ2)I−M

≻0, we also have that
 (c+4σ2)I−M
−i ≻0, for i = 1, 2. Let A =
 (c+4σ2)I−M
−i
for any i ∈{1, 2}. Now we argue that E

tr
 AeyeyT 
≤σ2tr (A), where σ2 is such that Ey∼p[⟨ey, v⟩2] ≤σ2 for
all unit vectors v ∈Rd. Note that the last condition is equivalent to supv∈Rd:∥v∥=1 vT  Ey∼p[eyeyT ]

v ≤σ2,
which, in view (35), is equivalent to saying that λmax
 Ey∼p[eyeyT ]

≤σ2.
Claim 1. E

tr
 AeyeyT 
≤σ2tr (A).
Proof. The claim follows from the following set of inequalities.
E

tr
 AeyeyT  (a)
= E

X
i,j
Aij(eyeyT )ji

=
X
i,j
Aij
 E[eyeyT ]

ji
26

(b)
= tr
 AE[eyeyT ]
 (c)
≤
E
eyeyT  ∥A∥∗
(d)
≤σ2tr(A)
In (a) and (b), we used the deﬁnition of trace: tr(AB) = P
i(AB)ii = P
i,j AijBji. In (c), we used
tr(AB) = tr(BA) ≤∥B∥∥A∥∗(see Claim 7 in Appendix F), where ∥· ∥∗denotes the nuclear norm, which is
equal to the sum of singular values. In (d), we used two things, ﬁrst, since A ⪰0, we have ∥A∥∗= tr(A),
and second, that
E
eyeyT  ≤σ2, which follows because
E
eyeyT  = λmax
 Ey∼p[eyeyT ]

≤σ2.
Using Claim 1 in (31) gives
E [Ψ] ≤σ2

tr
 (c + 4σ2)I −M
−1
+
tr
 (c + 4σ2)I −M
−2
Φc(M) −Φc+4σ2(M)

.
(32)
Claim 2. Φc(M) −Φc+4σ2(M) ≥4σ2tr
 (c + 4σ2)I −M
−2
.
Proof. Since (cI−M) ≻0, let its eigen-decomposition be (cI−M) = P
i λiuiuT
i , where λi’s are the eigenvalues
of (cI−M) and ui’s are the corresponding eigenvectors. It follows that
 (cI −M) + 4σ2I

= P
i(λi+4σ2)uiuT
i .
These imply that (cI −M)−1 = P
i
1
λi uiuT
i and
 (cI −M) + 4σ2I
−1 = P
i
1
(λi+4σ2)uiuT
i .
Substituting the deﬁnition of Φc(M) = tr
 (cI −M)−1
, we have
Φc(M) −Φc+4σ2(M) = tr

(cI −M)−1 −
 ((c + 4σ2)I −M)
−1
= tr
 X
i
1
λi
uiuT
i −
X
i
1
(λi + 4σ2)uiuT
i
!
= 4σ2tr
 X
i
1
λi(λj + 4σ2)uiuT
i
!
(g)
= 4σ2 X
i
1
λi(λj + 4σ2)
(h)
≥4σ2 X
i
1
(λj + 4σ2)2
= 4σ2tr
 X
i
1
(λj + 4σ2)2 uiuT
i
!
= 4σ2tr
 (cI −M) + 4σ2I
−2
Here (g) follows from the fact that trace of a square matrix is equal to the sum of its eigenvalues and (h)
follows because 1
λ ≥
1
λ+4σ2 .
Substituting Φc+4σ2(M) = tr
 ((c + 4σ2)I −M)
−1
for the ﬁrst term in (32) and the bound from
Claim 2 for the second term gives E [Ψ] ≤σ2  Φc+4σ2(M) +
1
4σ2

. Note that Claim 2 trivially implies
Φc+4σ2(M) ≤Φc(M), where Φc(M) = tr
 (cI −M)−1
≤
1
4σ2prev (which follows from the hypothesis of
Lemma 3). So, we have
E [Ψ] ≤σ2

1
4σ2prev
+
1
4σ2
 (h)
≤σ2
 1
4σ2 +
1
4σ2

≤1
2,
(33)
where (h) follows from our assumption that σprev ≥σ.
27

Note that Ψ is a non-negative random variable (see (30)). So, by the Markov’s inequality, we have
Pr[Ψ ≥1
ϵ′ ] ≤E[Ψ]
1/ϵ′ ≤ϵ′
2 , which implies that Pr[Ψ ≤1
ϵ′ ] ≥1 −ϵ′
2 . Substituting the value of Ψ in (30) implies
that (30) holds with probability at least 1 −ϵ′
2 for all t ∈[0, ϵ′]. Note that the condition in (30) is equivalent
to the condition that Φc+4σ2(M + teyeyT ) ≤Φc(M), where Φc(M) ≤
1
4σ2prev . Thus, with probability at least
1 −ϵ′
2 , we have that tr
 ((c + 4σ2)I −(M + teyeyT ))
−1
= Φc+4σ2(M + teyeyT ) ≤
1
4σ2prev , for every t ∈[0, ϵ′].
It only remains to show that
 M+ϵ′ eyeyT 
≺(c+4σ2)I, which is equivalent to the condition that λmax
 M+
ϵ′ eyeyT 
< (c+4σ2). Suppose not, i.e., λmax
 M+ϵ′ eyeyT 
≥(c+4σ2). Note that we have λmax(M) < c (by the
hypothesis of Lemma 3). Since λmax
 M + teyeyT 
is a continuous function of t and λmax(M) < c, λmax
 M +
ϵ′ eyeyT 
≥(c + 4σ2), we have from the intermediate value theorem that there exists a t′ ∈[0, ϵ′] such that
λmax
 M + t′ eyeyT 
= (c + 4σ2). This implies that the matrix
 ((c + 4σ2)I −(M + t′ eyeyT ))
−1 is not invertible
(as
 (c + 4σ2)I −(M + t′ eyeyT )

has a zero eigenvalue), implying that tr
 ((c + 4σ2)I −(M + teyeyT ))
−1
is unbounded. But, we have already shown that tr
 ((c + 4σ2)I −(M + teyeyT ))
−1
≤
1
4σ2prev < ∞, for all
t ∈[0, ϵ′]. A contradiction!
This completes the proof of Lemma 3.
A.1
Remaining proof of the ﬁrst part of Theorem 3 from Section 4.1
We have from (13) that
λmax
 
1
|S|
X
i∈S
(yi −µi) (yi −µi)T
!
≤4σ2
bϵ′

1 +
d
(1 −(ϵ + ϵ′))R

,
(34)
where, for simplicity of notation, we replaced gi(x), ∇Fi(x) by yi, µi, respectively. From the alternate
deﬁnition of the largest eigenvalue of symmetric matrices, we have
For a symmetric matrix A ∈Rd×d, we have, λmax(A) =
sup
v∈Rd,∥v∥=1
vT Av.
(35)
Applying this with A =
1
|S|
P
i∈S (yi −µi) (yi −µi)T and using vT 
1
|S|
P
i∈S (yi −µi) (yi −µi)T 
v =
1
|S|
P
i∈S⟨yi −µi, v⟩2, we can equivalently write (34) as
sup
v∈Rd:∥v∥=1
 
1
|S|
X
i∈S
⟨yi −µi, v⟩2
!
≤bσ2
0,
(36)
where bσ2
0 = 4σ2
bϵ′

1 +
d
(1−(ϵ+ϵ′))R

.
Note that (36) is bounding the deviation of the points in S from their respective means µi, i ∈S. However,
in the ﬁrst part of Theorem 3, we need to bound the deviation of the points in S from their sample mean
1
|S|
P
i∈S yi. For that, deﬁne yS :=
1
|S|
P
i∈S yi to be the sample mean of the points in S. Take an arbitrary
v ∈Rd such that ∥v∥= 1.
1
|S|
X
i∈S
⟨yi −yS, v⟩2 = 1
|S|
X
i∈S
[⟨yi −µi, v⟩+ ⟨µi −yS, v⟩]2
≤2
|S|
X
i∈S
⟨yi −µi, v⟩2 + 2
|S|
X
i∈S
⟨µi −yS, v⟩2
(using (a + b)2 ≤2a2 + 2b2)
Using (36) to bound the ﬁrst term, we get
≤2bσ2
0 + 2
|S|
X
i∈S
*
µi −1
|S|
X
j∈S
yj, v
+2
28

= 2bσ2
0 + 2
|S|
X
i∈S

1
|S|
X
j∈S
⟨yj −µi, v⟩


2
≤2bσ2
0 + 2
|S|
X
i∈S
1
|S|
X
j∈S
⟨yj −µi, v⟩2
(using the Jensen’s inequality)
= 2bσ2
0 + 2
|S|
X
i∈S
1
|S|
X
j∈S
[⟨yj −µj, v⟩+ ⟨µj −µi, v⟩]2
≤2bσ2
0 + 2
|S|
X
i∈S
2
|S|
X
j∈S
⟨yj −µj, v⟩2 + 2
|S|
X
i∈S
2
|S|
X
j∈S
⟨µj −µi, v⟩2
(using (a + b)2 ≤2a2 + 2b2)
≤2bσ2
0 + 4
|S|
X
i∈S
1
|S|
X
j∈S
⟨yj −µj, v⟩2 + 4
|S|
X
i∈S
1
|S|
X
j∈S
∥µj −µi∥2
(using the Cauchy-Schwarz inequality and that ∥v∥≤1)
= 2bσ2
0 + 4
|S|
X
j∈S
⟨yj −µj, v⟩2 + 4
|S|
X
i∈S
1
|S|
X
j∈S
∥µj −µ + µ −µi∥2
where µ = 1
R
PR
i=1 µi. Using (36) for the 2nd term and ∥u + v∥2 ≤2∥u∥2 + 2∥v∥2 for the third term, we get
≤6bσ2
0 + 4
|S|
X
i∈S
1
|S|
X
j∈S
2∥µj −µ∥2 + 2∥µi −µ∥2
= 6bσ2
0 + 8
|S|
X
i∈S
1
|S|
X
j∈S
∥µj −µ∥2 + 8
|S|
X
i∈S
1
|S|
X
j∈S
∥µi −µ∥2
= 6bσ2
0 + 16
|S|
X
i∈S
∥µi −µ∥2
≤6bσ2
0 + 16κ2.
(37)
For the last inequality, ﬁrst we substituted µi = ∇Fi(x), µ = ∇F(x), and then used (4) to bound
∥µi −µ∥2 ≤κ2 for all i ∈S.
Note that (37) holds for every unit vector v ∈Rd. By substituting yi = gi(x), yS = gS(x) and the value
of bσ2
0, we get
sup
v∈Rd:∥v∥=1
 
1
|S|
X
i∈S
⟨gi(x) −gS(x), v⟩2
!
≤24σ2
bϵ′

1 +
d
(1 −(ϵ + ϵ′))R

+ 16κ2.
(38)
In view of (35), the above is equivalent to the matrix concentration bound (12) in the ﬁrst part of Theorem 3.
B
Convergence Proof of Theorem 1
We prove convergence results for the strongly-convex part of Theorem 1 in Appendix B.1 and for the
non-convex part in Appendix B.2.
B.1
Proof of Theorem 1 (strongly-convex)
Recall the update rule of our algorithm: bxt+1 = xt −ηbg(xt); xt+1 = ΠC
 bxt+1
, t = 1, 2, 3, . . .. Since xt+1 is
the projection of bxt+1 onto the convex set C, and by assumption x∗∈C, we have ∥xt+1 −x∗∥≤∥bxt+1 −x∗∥.
Now we proceed with the proof.
∥xt+1 −x∗∥2 ≤∥bxt+1 −x∗∥2
29

= ∥xt −x∗−η∇F(xt) + η(∇F(xt) −bg(xt))∥2
= ∥xt −x∗−η∇F(xt)∥2 + η2∥∇F(xt) −bg(xt)∥2
+ 2

xt −x∗−η∇F(xt), η(∇F(xt) −bg(xt))

(39)
First we bound the last term of (39). For this, we use a simple but very powerful trick for inner-products, that
allows us to get a contracting recurrence on ∥xt+1−x∗∥2. Let u = xt−x∗−η∇F(xt) and v = ∇F(xt)−bg(xt).
With this notation, the last term of (39) is equal to 2⟨u, ηv⟩.
2⟨u, ηv⟩= 2
rµη
2 u,
r2η
µ v
 (a)
≤µη
2 ∥u∥2 + 2η
µ ∥v∥2,
where in (a) we used the inequality 2⟨u, v⟩≤∥u∥2 + ∥v∥2. Substituting this with the values of u and v in
(39) gives
∥xt+1 −x∗∥2 ≤

1 + µη
4

∥xt −x∗−η∇F(xt)∥2 + η

η + 4
µ
 ∇F(xt) −bg(xt)
2 .
(40)
Now we bound the ﬁrst term on the RHS of (40):
∥xt −x∗−η∇F(xt)∥2 = ∥xt −x∗∥2 + η2∥∇F(xt)∥2 + 2η

x∗−xt, ∇F(xt)

(41)
We can bound the second term on the RHS of (41) using L-smoothness of F:
∥∇F(xt)∥2 = ∥∇F(xt) −∇F(x∗)∥2 ≤L2∥xt −x∗∥2,
(42)
where in the ﬁrst equality we used ∇F(x∗) = 0. To bound the third term of (41), we use µ-strong convexity
of F:
F(x∗) ≥F(xt) +

∇F(xt), x∗−xt
+ µ
2 ∥xt −x∗∥2
F(xt) ≥F(x∗) + µ
2 ∥xt −x∗∥2
In the second inequality we used ∇F(x∗) = 0. Adding the above two inequalities gives

∇F(xt), x∗−xt
≤−µ∥xt −x∗∥2
(43)
Substituting these bounds in (41) gives
∥xt −x∗−η∇F(xt)∥2 ≤(1 + η2L2 −2ηµ)∥xt −x∗∥2.
(44)
Substituting this in (40) and taking expectation w.r.t. sampling at the t’th iteration (while conditioning on
the past) gives:
Et∥xt+1 −x∗∥2 ≤

1 + µη
2

(1 + η2L2 −2ηµ)∥xt −x∗∥2 + η

η + 2
µ

Et
∇F(xt) −bg(xt)
2
(45)
Note that sampling at the t’th iteration does not aﬀect xt, and, therefore, does not aﬀect the ∥xt −x∗∥2
term in (45). Now we bound the last term of (45).
Claim 3. With probability at least 1 −exp(−ϵ′2(1−ϵ)R
16
), we have
Et
∇F(xt) −bg(xt)
2 ≤
3σ2
(1 −(ϵ + ϵ′))bR + 3κ2 + 3Υ 2,
where Υ = O
 σ0
√
ϵ + ϵ′
and σ2
0 = 24σ2
bϵ′

1 +
d
(1−(ϵ+ϵ′))R

+ 16κ2.
30

Proof. Let St denote the subset of uncorrupted gradients of size (1−(ϵ+ϵ′))R that our algorithm in Theorem 3
approximates at the t’th iteration. Note that St exists with probability at least 1 −exp(−ϵ′2(1−ϵ)R
16
). Let
gSt(xt) = P
r∈St gr(xt) denote the average gradients of workers in St.
Et
∇F(xt) −bg(xt)
2 = Et
 ∇F(xt) −∇FSt(xt)

+
 ∇FSt(xt) −gSt(xt)

+
 gSt(xt) −bg(xt)
2 ,
where FSt(xt) =
1
|St|
P
r∈St Fr(xt). Now using
Pk
i=1 ui

2
≤k Pk
i=1 ∥ui∥2, which holds for any positive
integer k and an arbitrary set of k vectors, we get
Et
∇F(xt) −bg(xt)
2 ≤3Et
∇F(xt) −∇FSt(xt)
2 + 3Et
∇FSt(xt) −gSt(xt)
2
+ 3Et
gSt(xt) −bg(xt)
2 .
(46)
Now we bound each of the three terms on the RHS of (46) separately.
Bounding the ﬁrst term of (46): Note that Fr(xt) for any r ∈[R], is a deterministic quantity w.r.t. the
randomness used in the t’th iteration.
Et
∇F(xt) −∇FSt(xt)
2 =
∇F(xt) −∇FSt(xt)
2
=

1
|St|
X
r∈St
 Fr(xt) −∇F(xt)


2
≤
1
|St|
X
r∈St
Fr(xt) −∇F(xt)
2
(using the Jensen’s inequality)
≤κ2
(47)
The last inequality follows from (4).
Bounding the second term of (46):
Et
gSt(xt) −∇FSt(xt)
2 =
1
|St|2 Et

X
r∈St
gr(xt) −
X
r∈St
∇Fr(xt)

2
(a)
=
1
|St|2
X
r∈St
Et
gr(xt) −∇Fr(xt)
2
(b)
≤
1
|St|2
X
r∈St
σ2
b ≤
σ2
(1 −(ϵ + ϵ′))bR
(48)
Since the local stochastic gradients are sampled independently across diﬀerent workers, we have that gr(xt), r =
1, . . . , R are independent random variables with Et [gr(xt)] = ∇Fr(xt), implying that Et
P
r∈St gr(xt)

=
P
r∈St ∇Fr(xt). Now (a) follows from the fact that if X and Y are independent random variables, then
V ar(X + Y ) = V ar(X) + V ar(Y ). In (b) we used the bounded local variance assumption (7), as gr(xt) is
sampled uniformly from F⊗b
r (xt). In the last inequality we used that |St| ≥(1 −(ϵ + ϵ′))R.
Bounding the third term of (46): It follows from Theorem 3 that
gSt(xt) −bg(xt)
2 ≤O
 σ2
0(ϵ + ϵ′)

,
(49)
where σ2
0 = 24σ2
bϵ′

1 +
d
(1−(ϵ+ϵ′))R

+ 16κ2.
Substituting the bounds from (47)-(49) in (46) proves Claim 3.
Substituting the bound from Claim 3 in (45) gives the following with probability at least 1 −exp(−ϵ′2(1−ϵ)R
16
):
Et∥xt+1 −x∗∥2 ≤

1 + µη
2

(1 + η2L2 −2ηµ)∥xt −x∗∥2
31

+ η

η + 2
µ
 
3σ2
(1 −(ϵ + ϵ′))bR + 3κ2 + 3Υ 2

.
Using η =
µ
L2 implies
 1 + µη
2

(1 + η2L2 −2ηµ) = (1 +
µ2
2L2 )(1 −µ2
L2 ) ≤1 −
µ2
2L2 for the ﬁrst term. Note that
L ≥µ, which implies η ≤1
µ. Using this, we can bound the coeﬃcient of the last term as η

η + 2
µ

≤
3
µ2 .
Substituting these, and taking expectation w.r.t. the entire process yield
E∥xt+1 −x∗∥2 ≤

1 −µ2
2L2

E∥xt −x∗∥2 + Γ
µ2 ,
(50)
where Γ :=
9σ2
(1−(ϵ+ϵ′))bR + 9κ2 + 9Υ 2. Solving the recurrence in (50) gives
E∥xT −x∗∥2 ≤

1 −µ2
2L2
T
∥x0 −x∗∥2 + 2L2
µ4 Γ.
(51)
If we take T ≥
log

µ4
L2Γ ∥x0−x∗∥2
log(
1
1−µ2/2L2 )
, we get E∥xT −x∗∥2 ≤3L2
µ4 Γ.
Error probability analysis.
Note that the recurrence in (50) holds with probability at least 1 −
exp(−ϵ′2(1−ϵ)R
16
). Since we apply this recurrence T times to get (51), it follows by the union bound that (51)
holds with probability at least 1 −T exp(−ϵ′2(1−ϵ)R
16
), which is at least (1 −δ), for any δ > 0, provided we
run our algorithm for T ≤δ exp( ϵ′2(1−ϵ)R
16
) iterations.
This completes our proof of the strongly-convex part of Theorem 1.
B.2
Proof of Theorem 1 (non-convex)
First we prove the result when the parameter space C = Rd. In this case we do not need projection of iterates
onto C. Recall the update rule of our algorithm: xt+1 = xt −ηbg(xt), t = 1, 2, 3, . . .
F(xt+1)
(a)
≤F(xt) + ⟨∇F(xt), xt+1 −xt⟩+ L
2 ∥xt+1 −xt∥2
= F(xt) −η⟨∇F(xt), bg(xt)⟩+ η2L
2 ∥bg(xt)∥2
= F(xt) −η⟨∇F(xt), bg(xt) −∇F(xt) + ∇F(xt)⟩+ η2L
2 ∥bg(xt) −∇F(xt) + ∇F(xt)∥2
(b)
≤F(xt) −η⟨∇F(xt), bg(xt) −∇F(xt)⟩−η∥∇F(xt)∥2
+ η2L
2
 2∥bg(xt) −∇F(xt)∥2 + 2∥∇F(xt)∥2
(c)
≤F(xt) + η
1
2∥∇F(xt)∥2 + 1
2∥bg(xt) −∇F(xt)∥2

−η(1 −ηL)∥∇F(xt)∥2
+ η2L∥bg(xt) −∇F(xt)∥2
= F(xt) −η
1
2 −ηL

∥∇F(xt)∥2 + η
1
2 + ηL

∥bg(xt) −∇F(xt)∥2
(52)
In (a) we used our assumption that F is L-smooth; in (b) we used the inequality ∥u + v∥2 ≤2(∥u∥2 + ∥v∥2);
in (c) we used the inequality 2⟨u, v⟩≤∥u∥2 + ∥v∥2. For η ≤1/4L, we have 1/2 −ηL ≥1/4 and 1/2 + ηL ≤3/4.
Substituting these in (52) and taking expectation w.r.t. the sampling at the t’th iteration (while conditioning
on the past) gives
Et[F(xt+1)] ≤F(xt) −η
4∥∇F(xt)∥2 + 3η
4 Et∥bg(xt) −∇F(xt)∥2.
(53)
32

As argued in Claim 3 in the proof of the strongly-convex part of Theorem 1 above, we can similarly bound
the last term of (53) as Et∥bg(xt) −∇F(xt)∥2 ≤
3σ2
(1−(ϵ+ϵ′))bR + 3κ2 + 3Υ 2, where Υ ≤O(σ0
√
ϵ + ϵ′), with
probability at least 1 −exp(−ϵ′2(1−ϵ)R
16
), which comes from Theorem 3. Using this in (53) gives
Et[F(xt+1)] ≤F(xt) −η
4∥∇F(xt)∥2 + 3η
4

3σ2
(1 −(ϵ + ϵ′))bR + 3κ2 + 3Υ 2

.
(54)
By taking a telescoping sum from t = 0 to T, and also taking expectation w.r.t. the sampling at all workers
throughout the process, we get
1
T
T
X
t=0
E∥∇F(xt)∥2 ≤4
ηT E[F(x0) −F(xT +1)] + Γ,
(55)
where Γ =
9σ2
(1−(ϵ+ϵ′))bR + 9κ2 + 9Υ 2.
Substituting E[F(xT +1))] ≥F(x∗) and using F(x0) −F(x∗) ≤
L
2 ∥x0 −x∗∥2 (which follows from L-smoothness of F) gives
1
T
T
X
t=0
E∥∇F(xt)∥2 ≤2L
ηT ∥x0 −x∗∥2 + Γ.
(56)
Note that the last term Γ in (56) is a constant. So, it would be best to take the learning rate η to be as
large as possible such that it satisﬁes η ≤1/4L (we used this bound to arrive at (53)). We take η = 1/4L.
Substituting this in (56) gives
1
T
T
X
t=0
E∥∇F(xt)∥2 ≤8L2
T ∥x0 −x∗∥2 + Γ.
(57)
If we run our algorithm for T = 8L2∥x0−x∗∥2
Γ
iterations, we get 1
T
PT
t=0 E∥∇F(xt)∥2 ≤2Γ.
This concludes our proof of the non-convex part of Theorem 1 when C = Rd.
When C is a bounded set. When C is bounded, we need projection in general. But, under Assumption 3,
we show that all iterates bxt, t = 1, . . . , T stay within C, i.e., we do not projection in the parameter update
rule. This is what we show in the following; and, as a result, the above convergence analysis (which we did
for C = Rd) suﬃces under Assumption 3.
Lemma 4. Suppose Assumption 3 holds. Then, in the setting of Theorem 1, with probability at least
1 −T exp(−ϵ′2(1−ϵ)R
16
), we have bxt ∈C for every t = 0, 1, 2, . . . , T.
Proof. Deﬁne T events E1, E2, . . . , ET as follows: For every t ∈[T], deﬁne Et := {bxt ∈C | bxi ∈C, ∀i ∈[t −1]}.
Note that
Pr[bxt ∈C, ∀t ∈[T]] = ΠT
t=1 Pr[bxt ∈C | bxi ∈C, ∀i ∈[t −1]] = ΠT
t=1 Pr[Et].
Now we calculate Pr[Et], t ∈[T].
Fix an arbitrary t ∈[T]. In Et, we are given that bxi ∈C, ∀i ∈[t −1], which, by Assumption 3, implies
that bxi = xi and ∥∇F(xi)∥≤M for all i ∈[t −1]. Now we show that bxk+1 ∈C holds with probability
at least 1 −exp(−ϵ′2(1−ϵ)R
16
).
For this, in view of Assumption 3, it suﬃces to show that ∥bxt −x0∥≤
2L
Γ (M + Γ1)∥x0 −x∗∥2
∥bxt −x0∥= ∥xt−1 −ηbg(xt−1) −x0∥
(Since bxi = xi, ∀i ∈[t −1])
≤∥xt−1 −x0∥+ η
 ∥∇F(xt−1)∥+ ∥bg(xt−1) −∇F(xt−1)∥

≤
t−1
X
i=0
η
 ∥∇F(xi)∥+ ∥bg(xi) −∇F(xi)∥

33

(a)
≤
t
4L (M + Γ1) ≤T
4L (M + Γ1)
(b)
≤2L
Γ (M + Γ1)∥x0 −x∗∥2.
In (a), we used Claim 4 to bound ∥bg(xk) −∇F(xk)∥≤Γ1, which holds with probability at least 1 −
exp(−ϵ′2(1−ϵ)R
16
). Note that, we used a deterministic bound on ∥bg(xk) −∇F(xk)∥, as opposed to a bound
which only holds in expectation (even though we are doing SGD). See the proof of Claim 4 for details. In (b),
we used T = 8L2∥x0−x∗∥2
Γ
.
We have shown that for every t ∈[T], we have Pr[Et] ≥1 −exp(−ϵ′2(1−ϵ)R
16
).
This implies that
Pr[bxt ∈C, ∀t ∈[T]] ≥

1 −exp(−ϵ′2(1−ϵ)R
16
)
T
≥1 −T exp(−ϵ′2(1−ϵ)R
16
).
Claim 4. For any x ∈C, with probability at least 1 −exp(−ϵ′2(1−ϵ)R
16
), we have
Γ1 := ∥∇F(x) −bg(x)∥≤nmaxσ
b
+ κ + Υ,
where nmax = maxr∈[R] nr, Υ = O
 σ0
√
ϵ + ϵ′
and σ2
0 = 24σ2
bϵ′

1 +
d
(1−(ϵ+ϵ′))R

+ 16κ2.
Proof. Let S denote the subset of uncorrupted gradients of size (1−(ϵ+ϵ′))R that our algorithm in Theorem 3
approximates, and let gS(x) denote their average gradient.
∥∇F(x) −bg(x)∥≤∥∇F(x) −∇FS(x)∥+ ∥∇FS(x) −gS(x)∥+ ∥gS(x) −bg(x)∥.
(58)
The ﬁrst and the third terms on the RHS of (58) can be bounded similarly as we bounded them in (47) and
(49), respectively.
∥∇F(x) −∇FS(x)∥≤κ
(59)
∥gS(x) −bg(x)∥≤O

σ0
p
(ϵ + ϵ′)

(60)
In (60), σ2
0 = 24σ2
bϵ′

1 +
d
(1−(ϵ+ϵ′))R

+ 16κ2.
Now we bound the second term of (58). Note that in (48) we bounded E ∥∇FS(x) −gS(x)∥2, where
expectation is taken over the sampling of the mini-batch stochastic gradients at workers in S. However, for
the second term of (58), we need a deterministic bound, which holds for every choice of the mini-batch at
workers in S. Fix arbitrary sets Hr
n ∈U
 [nr]
b

for every r ∈S.
∥gS(x) −∇FS(x)∥=

1
|S|
X
r∈S
(gr(x) −∇Fr(x))

≤1
|S|
X
r∈S
∥gr(x) −∇Fr(x)∥
= 1
|S|
X
r∈S

1
b
X
i∈Hr
b
(∇Fr,i(x) −∇Fr(x))

(Since gr(x) = 1
b
P
i∈Hr
b ∇Fr,i(x))
≤1
|S|
X
r∈S
1
b
X
i∈Hr
b
∥∇Fr,i(x) −∇Fr(x)∥
(a)
≤
1
|S|
X
r∈S
1
b nrσ ≤nmaxσ
b
(61)
34

In the last inequality we used nmax = maxr∈[R] nr. The explanation for (a) is as follows: We have from
Assumption 1 that the stochastic gradients at any worker have bounded variance, i.e., for every r ∈[R], x ∈C,
we have Ei∈U[nr]∥∇Fr,i(x)−∇Fr(x)∥2 ≤σ2. Using Jensen’s inequality gives Ei∈U[nr]∥∇Fr,i(x)−∇Fr(x)∥≤σ.
This is equivalent to
1
nr
Pnr
i=1 ∥∇Fr,i(x)−∇Fr(x)∥≤σ, which implies that P
i∈T ∥∇Fr,i(x)−∇Fr(x)∥≤nrσ
holds for every T ⊆[nr]. Since (61) holds for arbitrary Hr
b’s, it holds for all Hr
b ∈
 [nr]
b

, r ∈St.
Substituting the bounds from (59)-(61) in (58) proves Claim 4.
Error probability analysis.
Note that the recurrence in (54) holds with probability at least 1 −
exp(−ϵ′2(1−ϵ)R
16
).
We use this recurrence T times to get (55), it follows by the union bound that (55)
holds with probability at least 1 −T exp(−ϵ′2(1−ϵ)R
16
), which is at least (1 −δ), for any δ > 0, provided we
run our algorithm for T ≤δ exp( ϵ′2(1−ϵ)R
16
) iterations. Note that this is the same probability with which all
iterates bxt, t ∈[T] lie in C (see Lemma 4).
B.3
Proof of Theorem 2
In this section, we focus on the case when workers compute full-batch gradients (instead of computing
mini-batch stochastic gradients) and prove Theorem 2. Note that the robust gradient estimator of Theorem 3
(which is for stochastic gradients) is the main ingredient behind the convergence analyses of Algorithm 1 for
strongly-convex and non-convex objectives. So, in order to prove Theorem 2, ﬁrst we need to show a robust
gradient estimation result (similar to Theorem 3). Since we are working with full-batch gradients, we can
show an analogous result with a much simpliﬁed analysis.
Note that, to prove Theorem 3, we showed an existence of a subset of honest workers from which the
stochastic gradients are well concentrated, as stated in form of a matrix concentration bound (12) in the ﬁrst
part of Theorem 3. It turns out that for full-batch gradients, an analogous result can be proven directly (as
there is no randomness due to stochastic gradients); and below we provide and prove such a result. Note that
Theorem 3 is a probabilistic statement, where we show that with high probability, there exists a large subset
of honest workers whose stochastic gradients are well concentrated. In contrast, in the following result, we
can deterministically take the set of all honest workers to be that subset for which we can directly show the
concentration.
Theorem 8 (Robust Gradient Estimation for Full-Batch Gradient Descent). Fix an arbitrary x ∈Rd. Suppose
an ϵ fraction of workers are corrupt and we are given R full-batch gradients ∇eF1(x), . . . , ∇eFR(x) ∈Rd,
where ∇eFr(x) = ∇Fr(x) if the r’th worker is honest, otherwise can be arbitrary. Let S be the set of all
honest workers and ∇FS(x) :=
1
|S|
P
i∈S ∇Fi(x) be the sample average of uncorrupted gradients. We can
ﬁnd an estimate ∇bF(x) of ∇FS(x) in polynomial-time such that
∇bF(x) −∇FS(x)
 ≤O (κ√ϵ) holds with
probability 1.
Proof. First we prove that λmax

1
|S|
P
i∈S (∇Fi(x) −∇FS(x)) (∇Fi(x) −∇FS(x))T 
≤4κ2. In view of (35),
this is equivalent to showing that for every unit vector v ∈Rd, we have
1
|S|
P
i∈S⟨∇Fi(x)−∇FS(x), v⟩2 ≤4κ2.
1
|S|
X
i∈S
⟨∇Fi(x) −∇FS(x), v⟩2 = 1
|S|
X
i∈S
[⟨∇Fi(x) −∇F(x) + ∇F(x) −∇FS(x), v⟩]2
≤2
|S|
X
i∈S
⟨∇Fi(x) −∇F(x), v⟩2 + 2
|S|
X
i∈S
⟨∇FS(x) −∇F(x), v⟩2
≤2
|S|
X
i∈S
⟨∇Fi(x) −∇F(x), v⟩2 + 2 ⟨∇FS(x) −∇F(x), v⟩2
= 2
|S|
X
i∈S
⟨∇Fi(x) −∇F(x), v⟩2 + 2
"
1
|S|
X
i∈S
⟨∇Fi(x) −∇F(x), v⟩
#2
35

≤2
|S|
X
i∈S
⟨∇Fi(x) −∇F(x), v⟩2 + 2
|S|
X
i∈S
⟨∇Fi(x) −∇F(x), v⟩2
= 4
|S|
X
i∈S
⟨∇Fi(x) −∇F(x), v⟩2
≤4
|S|
X
i∈S
∥∇Fi(x) −∇F(x)∥2
≤4κ2
Now apply the second part of Theorem 3 with S being the set of honest workers, gi = ∇Fi(x), ϵ′ = 0, and
σ2
0 = 4κ2. We would get that we can ﬁnd an estimate ∇bF(x) of ∇FS(x) in polynomial-time, such that
∇bF(x) −∇FS(x)
 ≤O (κ√ϵ) holds with probability 1.
Now we proceed with proving Theorem 2.
Proof of Theorem 2. This can be proved along the lines of the proof of Theorem 1. Here we only write what
changes in those proofs.
Recall the update rule of our algorithm with the full-batch gradient descent: bxt+1 = xt−η∇bF(xt); xt+1 =
ΠC
 bxt+1
, t = 1, 2, 3, . . ..
B.3.1
Strongly-convex
Following that proof of the strongly-convex part of Theorem 1 given in Appendix B.1 until (45) gives
∥xt+1 −x∗∥2 ≤

1 + µη
2

(1 + η2L2 −2ηµ)∥xt −x∗∥2 + η

η + 2
µ
 ∇F(xt) −∇bF(xt)

2
(62)
Now we bound the last term of (62). For this, we can simplify the proof of Claim 3: Firstly, note that, with
full-batch gradients, the variance σ2 becomes zero; secondly, as shown in Theorem 8, the robust estimation of
gradients holds with probability 1. Following the proof of Claim 3 with these changes, we get
∇F(xt) −∇bF(xt)

2
≤2κ2 + 2Υ 2
GD,
(63)
where ΥGD = O(κ√ϵ). Substituting the bound from (63) in (62) and then following the proof given in
Appendix B.1 until (51) gives
∥xT −x∗∥2 ≤

1 −µ2
2L2
T
∥x0 −x∗∥2 + 2L2
µ4 ΓGD,
(64)
where ΓGD = 6κ2 + 6Υ 2
GD. Note that (64) holds with probability 1.
B.3.2
Non-convex
Using the bound from (63) and following the proof of the non-convex part of Theorem 1 given in Appendix B.2
exactly until (57) gives
1
T
T
X
t=0
∥∇F(xt)∥2 ≤8L2
T ∥x0 −x∗∥2 + ΓGD,
(65)
where ΓGD = 6κ2 + 6Υ 2
GD. Note that (65) holds with probability 1.
Projection.
When C is a bounded set, then under Assumption 4, we can show that the iterates bxt for
t = 0, 1, 2, . . . , T lie in C and we do not need projection. For this, we can show a result analogous to Lemma 4,
but which holds with probability 1:
36

Lemma 5. Suppose Assumption 4 holds. Then, in the setting of Theorem 2 (non-convex), with probability 1,
we have bxt ∈C for every t = 0, 1, 2, . . . , T.
Proof. We can prove this lemma along the lines of how we proved Lemma 4, except with one modiﬁcation. In
order to bound
∇F(x) −∇bF(x)
, we can put σ = 0 in Claim 4, and that gives
∇F(x) −∇bF(x)
 ≤κ+ΥGD.
Furthermore, with full-batch gradients, this bound holds with probability 1. This is because with full-batch
gradients, our robust gradient estimator from Theorem 8 succeeds with probability 1.
This concludes the proof of Theorem 2.
C
Proof of Lemma 2
Lemma (Restating Lemma 2). Fix any worker r ∈[R]. Suppose the local stochastic gradients at worker r
have bounded second moment, i.e., Ei∈U[nr][∥∇Fr,i(x)∥2] ≤G2. Then we have
EK←K,Hb
d
k · selectK (∇Fr,Hb(x))

= ∇Fr(x)
(66)
EK←K,Hb

d
k · selectK (∇Fr,Hb(x)) −∇Fr(x)

2
≤d
k
G2
b .
(67)
Proof. For notational convenience, in this proof, we suppress the explicitly dependence on r in nr, Fr,Hb(x), Fr(x),
and denote them simply by n, FHb(x), F(x), respectively.
Recall from (6) that EHb[∇FHb(x)] = ∇F(x). Now showing (66) is straightforward:
EK←K,Hb
d
k · selectK (∇FHb(x))

= EK←K
d
k · selectK (EHb [∇FHb(x)])

= EK←K
d
k · selectK (∇F(x))

= ∇F(x).
To show (67), observe that once we ﬁx K, selectK(v1+v2) = selectK(v1)+selectK(v2). This simple observation
is crucial for getting the variance reduction. Note that Hb is a collection of b elements drawn uniformly at
random from [n] without replacement. However, we show (67) when Hb is a collection of b elements drawn
uniformly at random from [n] with replacement. Observe that, the bound derived under the latter condition
is an upper-bound on the quantity of interest, which is under the former condition. Let Hb = (I1, I2, . . . , Ib),
where Ij’s are i.i.d. random variables taking values in [n] with uniform distribution.
EK←K,I1,...,Ib

d
k · selectK

1
b
b
X
j=1
∇FIj(x)

−∇F(x)

2
= EK←K

EI1,...,Ib

d
k · selectK

1
b
b
X
j=1
∇FIj(x)

−∇F(x)

2

= EK←K

EI1,...,Ib

1
b
b
X
j=1
d
k · selectK
 ∇FIj(x)

−∇F(x)

2

= EK←K,I1,...,Ib

1
b
b
X
j=1
d
k · selectK
 ∇FIj(x)

−∇F(x)

2
37

= 1
b2 · EK←K,I1,...,Ib

b
X
j=1
d
k · selectK
 ∇FIj(x)

−∇F(x)

2
(a)
= 1
b2 · EK←K,I1,...,Ib
b
X
j=1

d
k · selectK
 ∇FIj(x)

−∇F(x)

2
+ 1
b2 · EK←K,I1,...,Ib
X
j̸=k
d
k · selectK
 ∇FIj(x)

−∇F(x), d
k · selectK (∇FIk(x)) −∇F(x)

= 1
b2 ·
b
X
j=1
EK←K,Ij

d
k · selectK
 ∇FIj(x)

−∇F(x)

2
(b)
≤1
b2 ·
b
X
j=1
EK←K,Ij

d
k · selectK
 ∇FIj(x)

2
(c)
= 1
b2 · d2
k2
k
d
b
X
j=1
EIj
∇FIj(x)
2
(d)
≤d
k · G2
b .
The second term on the RHS of (a) is equal to 0, which follows from the fact that Ij and Ik for j ̸= k
are independent random variables and that EK←K,Ij
 d
k · selectK(∇FIj(x))

= ∇F(x). In (b) we used the
inequality that for any random vector X, we have E∥X −E[X]∥2 ≤E∥X∥2. In (c) we used that for any
vector v ∈Rd, we have Erandk∥randk(v)∥2 = k
d∥v∥2. In (d) we used the second moment bound assumption
Ei∥∇Fi(x)∥2 ≤G2. This completes the proof of Lemma 2.
D
Omitted Details from Section 6
In this section, we bound
∇¯fr(x) −∇µr(x)
 under both the sub-exponential and sub-Gaussian gradient
distributional assumptions. First we give some deﬁnitions.
Deﬁnition 1 (Sub-exponential distribution). A random variable Z with mean µ = E[Z] is sub-exponential
if there are non-negative parameters (ν, α) such that
E [exp (λ(Z −µ))] ≤exp
 λ2ν2/2

,
∀|λ| < 1
α.
A random vector Z with mean µ = E[Z] is sub-exponential if its projection on every unit vector is sub-
exponential, i.e., there are non-negative parameters (ν, α) such that
sup
v∈Rd:∥v∥=1
E [exp (λ⟨Z −µ, v⟩)] ≤exp
 λ2ν2/2

,
∀|λ| < 1
α.
Now we state a concentration inequality for sums of independent sub-exponential random variables.
Fact 1 (Sub-exponential concentration inequality). Suppose X1, X2, . . . , Xn are independent random variables,
where for every i ∈[n], Xi is sub-exponential with parameters (νi, αi) and mean µi. Then Pn
i=1 Xi is sub-
exponential with parameters (ν, α), where ν2 = Pn
i=1 ν2
i and α = max1≤i≤n αi. Moreover, we have
Pr
" n
X
i=1
(Xi −µi) ≥t
#
≤exp

−1
2 min
 t2
ν2 , t
α

,
∀t ≥0
(68)
38

Deﬁnition 2 (Sub-Gaussian distribution). A random variable Z with mean µ = E[Z] is sub-Gaussian if
there is a non-negative parameter σg such that
E [exp (λ(Z −µ))] ≤exp
 λ2σ2
g/2

,
∀λ ∈R.
A random vector Z with mean µ = E[Z] is sub-Gaussian if its projection on every unit vector is sub-Gaussian,
i.e., there is a non-negative parameter σg such that
sup
v∈Rd:∥v∥=1
E [exp (λ⟨Z −µ, v⟩)] ≤exp
 λ2σ2
g/2

,
∀λ ∈R.
Now we state a concentration inequality for sums of independent sub-Gaussian random variables.
Fact 2 (Sub-Gaussian concentration inequality). Suppose X1, X2, . . . , Xn are independent random variables,
where for every i ∈[n], Xi is sub-Gaussian with parameter σi > 0 and mean µi. Then Pn
i=1 Xi is sub-Gaussian
with parameter σg =
pPn
i=1 σ2
i . Moreover, we have
Pr
" n
X
i=1
(Xi −µi) ≥t
#
≤exp
 −t2/2σ2
g

,
∀t ≥0.
(69)
Let D = max{∥x −x′∥: x, x′ ∈C} be the diameter of C. Note that C is contained in Bd
D/2, which is the
Euclidean ball of radius D
2 in d dimensions that contains C. Note that D = Ω(
√
d), and we assume that D
can grow at most polynomially in d.
Now we state two lemmas (which will be used to prove Theorem 6), each of which uniformly bounds
∇¯fr(x) −∇µr(x)
 over all x ∈C under diﬀerent distributional assumptions on gradients. We prove these
one by one in subsequent subsections.
Lemma 6 (Sub-exponential gradients). Suppose Assumption 5 holds. Take an arbitrary r ∈[R]. Let nr ∈N
be suﬃciently large such that nr = Ω(d log(nrd)). Then, with probability at least 1 −
1
(1+nrLD)d , we have
∇¯fr(x) −∇µr(x)
 ≤3ν
s
8d log(1 + nrLD)
nr
,
∀x ∈C.
(70)
Lemma 7 (Sub-Gaussian gradients). Suppose Assumption 6 holds. Take an arbitrary r ∈[R]. For any
nr ∈N, with probability at least 1 −
1
(1+nrLD)d , we have
∇¯fr(x) −∇µr(x)
 ≤3σg
s
8d log(1 + nrLD)
nr
,
∀x ∈C.
(71)
Proof of Theorem 6. In order to prove Theorem 6, we need to show two bounds, one (stated in (24)) under
the sub-exponential gradient assumption, and the other (stated in (25)) under the sub-Gaussian assumption.
We can show (24) using Lemma 6 and (25) using Lemma 7. Here we only show (24); and (25) can be shown
similarly.
Using Assumption 7 (i.e., ∥∇µr(x) −∇µ(x)∥≤κmean, ∀x ∈C) in (22) gives
∇¯fr(x) −∇¯f(x)
 ≤
∇¯fr(x) −∇µr(x)
 + κmean + 1
R
R
X
r=1
∇¯fr(x) −∇µr(x)
 .
(72)
Note that (70) holds for any ﬁxed worker r ∈[R]. By the union bound, we have that with probability at
least 1 −
R
(1+nrLD)d , for every r ∈[R], we have
∇¯fr(x) −∇µr(x)
 ≤3ν
q
8d log(1+nrLD)
nr
, ∀x ∈C.
Let nr = n, ∀r ∈[R]. Using these in (72), we get that with probability at least 1 −
R
(1+nrLD)d , for every
worker r ∈[R], we have
∇¯fr(x) −∇¯f(x)
 ≤κmean + O
q
d log(nd)
n

, ∀x ∈C, which proves (24). This
completes the proof of Theorem 6.
39

D.1
Proof of Lemma 6 (sub-exponential gradients)
We prove Lemma 6 with the help of the following result, which holds for any ﬁxed x ∈C. Then we extend
this bound to all x ∈C using an ϵ-net argument. These are standard calculations and have appeared in
literature [CSX17,YCRB19].
Lemma 8. Suppose Assumption 5 holds. Take an arbitrary r ∈[R]. For any δ ∈(0, 1) and nr ∈N, deﬁne
∆=
√
2ν
q
d log 5+log(1/δ)
nr
. If nr is such that ∆≤ν2
α , then, for any ﬁxed x ∈C, with probability at least 1 −δ,
we have
∇¯fr(x) −∇µr(x)
 ≤2
√
2ν
s
d log 5 + log(1/δ)
nr
,
(73)
where randomness is due to the sub-exponential distribution of local gradients.
Proof. Let Bd = {v ∈Rd : ∥v∥≤1}. Let V = {v1, v2, . . . , vN1/2} denote an 1
2-net of Bd, which implies that
for every v ∈Bd, there exists a v′ ∈V such that ∥v −v′∥≤1
2. We have from [Ver10, Lemma 5.2] that
N1/2 = |V| ≤5d.
Fix an arbitrary x ∈C. Note that there exists a v∗∈Bd (namely, v∗=
∇¯
fr(x)−∇µr(x)
∥∇¯
fr(x)−∇µr(x)∥) such that
∇¯fr(x) −∇µr(x)
 =

∇¯fr(x) −∇µr(x), v∗
. By the property of V, there exists an index i∗∈[N1/2] such
that ∥v∗−vi∗∥≤1
2. Now we bound
∇¯fr(x) −∇µr(x)
.
∇¯fr(x) −∇µr(x)
 =

∇¯fr(x) −∇µr(x), v∗
=

∇¯fr(x) −∇µr(x), vi∗
+

∇¯fr(x) −∇µr(x), v∗−vi∗
≤

∇¯fr(x) −∇µr(x), vi∗
+
∇¯fr(x) −∇µr(x)
 ∥v∗−vi∗∥
≤

∇¯fr(x) −∇µr(x), vi∗
+ 1
2
∇¯fr(x) −∇µr(x)

≤max
v∈V

∇¯fr(x) −∇µr(x), v

+ 1
2
∇¯fr(x) −∇µr(x)

By collecting similar terms together, we get
∇¯fr(x) −∇µr(x)
 ≤2 max
v∈V

∇¯fr(x) −∇µr(x), v

(74)
Note that the RHS of (74) is a non-negative number (because LHS is). Note also that, since V ⊂Bd, for every
v ∈V, we have ∥v∥≤1. This implies that maxv∈V

∇¯fr(x) −∇µr(x), v

≤maxv∈V
D
∇¯fr(x) −∇µr(x),
v
∥v∥
E
.
Using this in (74), we get
∇¯fr(x) −∇µr(x)
 ≤2 max
v∈V

∇¯fr(x) −∇µr(x), v
∥v∥

.
(75)
Fix any v ∈V. It follows from Assumption 5 that
D
∇fr(z, x) −∇µr(x),
v
∥v∥
E
, where z ∼qr, is a sub-
exponential random variable (with mean zero) with parameters (ν, α). From Fact 1 (stated on page 38),
we have that Pnr
i=1
D
∇fr(zr,i, x) −∇µr(x),
v
∥v∥
E
(where zr,i ∼qr, i ∈[nr] are i.i.d.) is a sub-exponential
random variable with parameters (√nrν, α).
Now, apply the concentration bound from (68) with t = nr∆. Substituting this and the parameters
(√nrν, α), the bound becomes exp(−1
2 min{ n2
r∆2
nrν2 , nr∆
α })
(a)
= exp(−1
2
nr∆2
ν2 ), where (a) follows because ∆≤ν2
α .
This gives
Pr
" nr
X
i=1

∇fr(zr,i, x) −∇µr(x), v
∥v∥

≥nr∆
#
≤exp

−nr∆2
2ν2

.
(76)
40

Note that Pnr
i=1
D
∇fr(zr,i, x) −∇µr(x),
v
∥v∥
E
= nr
D
∇¯fr(x) −∇µr(x),
v
∥v∥
E
. Using this in (76) yields
Pr

∇¯fr(x) −∇µr(x), v
∥v∥

≥∆

≤exp

−nr∆2
2ν2

(77)
This implies that
Pr

max
v∈V

∇¯fr(x) −∇µr(x), v
∥v∥

≥∆

≤
X
v∈V
Pr

∇¯fr(x) −∇µr(x), v
∥v∥

≥∆

≤|V| exp

−nr∆2
2ν2

≤5d exp

−nr∆2
2ν2

= exp

−nr∆2
2ν2
+ d log 5

(78)
Together with (75), which implies that
Pr
∇¯fr(x) −∇µr(x)
 ≥t

≤Pr

2 max
v∈V

∇¯fr(x) −∇µr(x), v
∥v∥

≥t

holds for every t > 0, (78) gives
Pr
∇¯fr(x) −∇µr(x)
 ≥2∆

≤exp

−nr∆2
2ν2
+ d log 5

≤δ,
(79)
where in the last inequality we used ∆=
√
2ν
q
d log 5+log(1/δ)
nr
.
This completes the proof of Lemma 8.
Proof of Lemma 6. We have from Lemma 8 that for each ﬁxed x ∈C, with probability at least 1 −δ, we have
∇¯fr(x) −∇µr(x)
 ≤2ν
s
2d log 5 + 2 log(1/δ)
nr
.
(80)
To extend this argument uniformly over the entire set C, we use another covering argument. Recall that
D is the diameter of C. Note that C is contained in Bd
D/2, which is the Euclidean ball of radius D
2 in d
dimensions that contains C. For some δ0 > 0, let Cδ0 = {x0, x2, . . . , xNδ0} be the δ0-net of C. It follows
from [Ver10, Lemma 5.2] that Nδ0 ≤

1 + D
δ0
d
.
Applying the union bound in (80), we get that with probability at least 1 −δ, we have for all xi ∈Cδ0,
∇¯fr(xi) −∇µr(xi)
 ≤2ν
v
u
u
t2d log 5 + 2 log
 Nδ0
δ

nr
.
(81)
We want to bound
∇¯fr(x) −∇µr(x)
 for all x ∈C. Take any x ∈C. Since Cδ0 is a δ0-net of C, there exists
an x′ ∈Cδ0 such that ∥x −x′∥≤δ0.
∇¯fr(x) −∇µr(x)
 =
∇¯fr(x) −∇¯fr(x′) + ∇¯fr(x′) −∇µr(x) + ∇µr(x′) −∇µr(x′)

≤
∇¯fr(x) −∇¯fr(x′)

|
{z
}
=: T1
+ ∥∇µr(x) −∇µr(x′)∥
|
{z
}
=: T2
+
∇¯fr(x′) −∇µr(x′)

(82)
Now we bound each term on the RHS of (82).
41

T1 =

1
nr
nr
X
i=1
(∇fr(zr,i, x) −∇fr(zr,i, x′))
 ≤1
nr
nr
X
i=1
∥∇fr(zr,i, x) −∇fr(zr,i, x′)∥
≤L∥x −x′∥≤Lδ0
T2 = ∥Ez∼qr[∇fr(z, x) −∇fr(z, x; )]∥≤Ez∼qr ∥∇fr(z, x) −∇fr(z, x; )∥
≤Ez∼qrL∥x −x′∥≤Lδ0
Substituting the above bounds on T1, T2 in (82) and bounding the third term of (82) using (81) gives
∇¯fr(x) −∇µr(x)
 ≤2Lδ0 + 2ν
v
u
u
t2d log 5 + 2 log
 Nδ0
δ

nr
.
(83)
Note that Nδ0 ≤

1 + D
δ0
d
. Take δ = 1/

1 + D
δ0
d
. If we take δ0 =
1
nrL, which implies δ =
1
(1+nrLD)d , we
would get 2d log 5 + 2 log
 Nδ0
δ

≤4d + 4d log(1 + nrLD) ≤8d log(1 + nrLD). Substituting these in above
gives
∇¯fr(x) −∇µr(x)
 ≤2
nr
+ 2ν
√nr
p
8d log(1 + nrLD).
(84)
When nr ≥
1
2ν2d log(1+nrLD) (which is a very small number less than 1), with probability at least 1−
1
(1+nrLD)d ,
we have
∇¯fr(x) −∇µr(x)
 ≤3ν
s
8d log(1 + nrLD)
nr
,
∀x ∈C.
(85)
Lower bound on nr. Note that Lemma 8 requires ∆≤ν2
α , where ∆=
√
2ν
q
d log 5+log(1/δ)
nr
. Substituting
the value of δ =
1
(1+nrLD)d gives nr ≥2α2
ν2 (d log 5 + d log(1 + nrLD)), which is Ω(d log(nrLD)) for constant
α, ν. Treating the smoothness parameter L a constant, we get nr = Ω(d log(nrd)) to be requirement on the
sample size at the r’th worker for the bound in Lemma 6 to hold.
This completes the proof of Lemma 6.
D.2
Proof of Lemma 7 (sub-Gaussian gradients)
We prove Lemma 7 with the help of the following result, which holds for any ﬁxed x ∈C.
Lemma 9. Suppose Assumption 6 holds. Take an arbitrary r ∈[R]. For any δ ∈(0, 1) and nr ∈N, with
probability at least 1 −δ, we have for any ﬁxed x ∈C:
∇¯fr(x) −∇µr(x)
 ≤2
√
2σg
s
d log 5 + log(1/δ)
nr
,
(86)
where randomness is due to the sub-Gaussian distribution of local gradients.
Proof. Follow the proof of Lemma 8 exactly until (75). Then instead of the sub-exponential assumption, use
the sub-Gaussian assumption (Assumption 6) on local gradients. Then apply the concentration bound from
(69) with t = nr∆. This gives that for any ﬁxed v ∈V and any ∆≥0, we have
Pr

∇¯fr(x) −∇µr(x), v
∥v∥

≥∆

≤exp

−nr∆2
2σ2g

.
(87)
42

Now following the proof of Lemma 8 from (77) to (79) gives
Pr
∇¯fr(x) −∇µr(x)
 ≥2∆

≤exp

−nr∆2
2σ2g
+ d log 5

≤δ,
(88)
where in the last inequality we used ∆=
√
2σg
q
d log 5+log(1/δ)
nr
.
We can extend the bound from Lemma 9 to all x ∈C (and prove Lemma 7) using an ϵ-net argument
exactly in the same way as used in the proof of Lemma 6. So, to avoid repetition, we do not show this
extension here.
D.3
Bounding the local variances
In Section 6.2, we showed that in order to bound Ei∈U[nr]
∇fr(zr,i, x) −∇¯fr(x)
2 uniformly over all x ∈C,
it suﬃces to bound ∥∇fr(z, x) −∇µr(x)∥for a random z ∼qr uniformly over all x ∈C.
Bounding ∥∇fr(z, x) −∇µr(x)∥. To bound this, we need sub-Gaussian assumption on local gradients (we
can also bound this using sub-exponential assumption, but that will give a bound that scales as eΩ(d) as
opposed to eΩ(
√
d)). Note that Lemma 7 holds for any nr ∈N. In particular, it also holds for nr = 1. So,
under Assumption 6, with probability at least 1 −
1
(1+nrLD)d , we have
∥∇fr(z, x) −∇µr(x)∥≤3σg
p
8d log(1 + LD),
∀x ∈C,
(89)
where z ∼qr, and probability is over the randomness due to the sub-Gaussian distribution of local gradients.
So, with probability at least 1 −
1
(1+nrLD)d , we have
Ei∈U[nr]
∇fr(zr,i, x) −∇¯fr(x)
2 ≤288σ2
gd log(1 + LD),
∀x ∈C.
(90)
Note that (90) holds for a ﬁxed worker r ∈[R]. By taking the union bound over all workers r ∈[R] proves
Theorem 7.
E
Robust Mean Estimation
In this section, we present the robust mean estimation algorithm of [SCV18] that we use to ﬁlter-out the
corrupt gradients and estimate the average of the good gradients. The procedure is presented in Algorithm 3,
which was used in the second part of Theorem 3.
E.1
Intuition behind Algorithm 3
First we argue that (91) is a saddle-point optimization problem. This follows from the Von Neumann
Minimax theorem, for which we can verify that (i) Φ(W, Y) is a continuous function of its arguments, (ii)
Φ(W, Y) is convex in W (for a ﬁxed Y) and concave in Y (for a ﬁxed W), and (iii) the sets over which the
maximum/minimum are taken in (91) are compact convex sets. See the proof of Claim 6 in Appendix F
for more details. This implies that we can switch min and max in (91), i.e., maxY minW Φ(W, Y) =
minW maxY Φ(W, Y).
For a matrix Z, let ∥Z∥2 denote the matrix norm induced by the ℓ2-norm, which is equal to the
largest singular value of Z. We have the following identity (which we prove in Claim 10 in Appendix F):
∥G −GW∥2
2 = maxY⪰,tr(Y)≤1
PR
i=1(gi −Gwi)T Y(gi −Gwi), where G = [g1, g2, . . . , gR] ∈Rd×R. Using
this and the Minimax theorem, the optimization problem (91) in the ﬁrst iteration of the while-loop (when
A = [R] and ci = 1, ∀i ∈A) can be equivalently written as
minimizeW∈RR×RR
∥G −GW∥2
2
43

Algorithm 3 Robust Gradient Estimation (RGE) [SCV18]
1: Initialize. ci := 1 for all i ∈[R], α := (1 −˜ϵ) ≥3/4, A := {1, 2, . . . , R}; G := [g1, g2, . . . , gR] ∈Rd×R.
2: while true do
3:
Let W∗∈R|A|×|A| and Y∗∈Rd×d be the minimizer/maximizer of the saddle point problem:
max
Y⪰0,
tr(Y)≤1
min
0≤Wji≤
4−α
α(2+α)R ,
P
j∈A Wji=1,∀i∈A
Φ(W, Y),
(91)
where the cost function Φ(W, Y) is deﬁned as
Φ(W, Y) :=
X
i∈A
ci(gi −GAwi)T Y(gi −GAwi),
(92)
To avoid cluttered notation, we index the |A| rows/columns of W by the elements of A; GA denotes
the restriction of G to the columns in A; for i ∈A, wi denotes the column of W indexed by i.
4:
For i ∈A, let
τi = (gi −GAw∗
i )T Y∗(gi −GAw∗
i )
(93)
5:
if P
i∈A ciτi > 4Rσ2
0 then
6:
For i ∈A, ci ←

1 −
τi
τmax

ci, where τmax = maxj∈A τj.
7:
For all i with ci < 1
2, remove i from A.
8:
else
9:
Break while-loop
10:
end if
11: end while
12: return bg =
1
|A|
P
i∈A gi.
subject to
0 ≤Wji ≤
4 −α
α(2 + α)R, ∀i, j,
R
X
j=1
Wji = 1, ∀i = 1, . . . , R.
(94)
At the heart of Algorithm 3 is to solve (94), which can be eﬃciently solved using the singular value
decomposition (SVD); see [SCV18] for details. The idea behind (94) is to represent each gradient vector as a
weighted average of other α(2+α)
4−α R gradients, where α = 1 −˜ϵ. Since the set S in the ﬁrst part of Theorem 3
in Section 4 (also see Lemma 10 in Appendix F) is suﬃciently large and is well concentrated, the gradients in
the set S can be represented well using its empirical mean gS =
1
|S|
P
i∈S gi. So, gradients that cannot be
represented as such must be outliers/corrupted and cannot lie in S, and we must remove them. For removing
corrupted gradients, we do a soft removal, by maintaining weights ci’s, which are all initialized to 1. When
we ﬁnd that the reconstruction error is large, we reduce the weights of the gradients with high reconstruction
error (denoted by τi). We remove a gradient when its associated ci becomes less than 1/2. We maintain an
active set A consisting of all the gradients with ci ≥1/2.
We can describe Algorithm 3 informally as follows – for simplicity, we describe it w.r.t. the ﬁrst iteration
of the while-loop only, where A = [R] and ci = 1, ∀i ∈[R]:
1. Solve the optimization problem (94) (with taking into account the weights ci’s for the later iterations).
2. If the optimum value (which is equal to P
i ciτi (line 5 of Algorithm 3)) is bigger than 4Rσ2
0, then
down-weight the gradient vectors in proportion to their corresponding reconstruction error – larger
44

the reconstruction error, lesser the weight it gets. If the weight goes below 1
2 for any gradient vector,
discard it.
3. If the optimum value is less than 4Rσ2
0, then output the average of the remaining gradients.
E.2
Running time analysis of Algorithm 3
As noted earlier, we can ﬁnd the optimum value of (94) using SVD. Let W∗be a minimizer of (94). For
down-weighting the i’th point, we need to compute the reconstruction error τi, for which we have to ﬁnd an
optimum Y (satisfying Y ⪰0 and tr(Y) ≤1) that maximizes PR
i=1(gi−Gw∗
i )T Y(gi−Gw∗
i ). This can again
be found using SVD, as Y that attains the maximum in this expression is equal to the vvT , where v is the
principal eigenvector (i.e., eigenvector corresponding to the largest eigenvalue) of (G −GW∗)(G −GW∗)T
(see the discussion on the time complexity of ﬁnding the optimal Y below). This can be seen as follows:
R
X
i=1
(gi −Gw∗
i )T Y(gi −Gw∗
i ) = tr
 (G −GW∗)T Y(G −GW∗)

(a)
= tr
 (G −GW∗)(G −GW∗)T Y

(b)
≤∥(G −GW∗)(G −GW∗)T ∥∥Y∥∗
(c)
≤∥(G −GW∗)(G −GW∗)T ∥
Here (a) follows because tr(AB) = tr(BA) holds for any two dimension compatible matrices A, B. (b) follows
from tr(AB) ≤∥A∥∥B∥∗(which is proved in Claim 7 in Appendix F), where ∥· ∥is the matrix 2-norm
and ∥· ∥∗is the nuclear norm, i.e., sum of the singular values. (c) holds because for a positive semi-deﬁnite
matrix Y, we have ∥Y∥∗= tr(Y), which is at most 1 (due to the constraint). Note that, since Y ⪰0 with
tr(Y) ≤1, equality holds in (b) and (c) above when Y is a rank one matrix such that Y = vvT , where v is
the unit norm principal eigenvector of (G−GW∗)(G−GW∗)T . See also the proof of Claim 7 in Appendix F.
Time complexity of ﬁnding the optimal Y.
Since (G −GW∗)(G −GW∗)T is a d × d matrix, the
principal eigenvector v can be found in O(d3) time using SVD, which may be computationally very expensive,
as d could be very large. Note, however, that the principal eigenvector of (G −GW∗)(G −GW∗)T is the
same as the principal left singular vector (i.e., left singular vector corresponding to the largest singular value)
of G −GW∗.10 Since G −GW∗is a d × R matrix, its principal left singular vector can be computed in
O(dR min{d, R}) time using SVD.
Polynomial-time complexity of Algorithm 3.
Observe that, in each iteration of the while-loop of
Algorithm 3, we remove at least one gi (which corresponds to the τi for which τi = τmax) from A, which
means that the while-loop terminates after executing for at most O(R) iterations. Therefore, Algorithm 3 is
a repeated (and at most O(R)) number of applications of SVD. Since SVD of a d×R matrix can be performed
in O(dR min{d, R}) time, Algorithm 3 runs in O(dR2 min{d, R}) time; hence, runs in polynomial-time.
F
Comprehensive Analysis of Algorithm 3
In this section, we prove the second part of Theorem 3 by giving a comprehensive analysis of Algorithm 3.
For convenience, we state the second part of Theorem 3 as a separate lemma.
10This can be seen as follows: By doing SVD, we can write G −GW∗= UΣVT , where U, V are unitary matrices, and Σ is
the diagonal matrix whose entries are the singular values of G−GW∗. Note that (G−GW∗)(G−GW∗)T = UΣVT VΣUT =
UΣ2UT , which is the eigen-decomposition of (G −GW∗)(G −GW∗)T . This implies that the eigenvector corresponding to
the largest eigenvalue of (G −GW∗)(G −GW∗)T is the same as the left singular vector corresponding to the largest singular
value of G −GW∗.
45

Lemma 10 (Proposition 16 in [SCV18]). Suppose we are given R arbitrary vectors g1, . . . , gR ∈Rd with
the promise that there exists a subset S ⊂[R] of these vectors such that |S| = (1 −˜ϵ)R for some ˜ϵ > 0 and
S satisﬁes λmax

1
|S|
P
i∈S (gi −gS) (gi −gS)T 
≤σ2
0, where gS =
1
|S|
P
i∈S gi denotes the sample mean of
the vectors in S. Then, if ˜ϵ ≤1
4, Algorithm 3 can ﬁnd an estimate bg of gS in polynomial-time, such that
∥bg −gS∥≤O(σ0
√
˜ϵ).
The above lemma is from [SCV18] and has also been used in [SX19,YCRB19] in the context of Byzantine-
resilient full batch gradient descent, where workers have i.i.d. homogeneous data drawn from a probability
distribution. We apply it in the context of mini-batch stochastic gradient descent on heterogeneous data, where
diﬀerent workers may have diﬀerent local datasets and we do not assume any probabilistic assumption on
data. Our results are under the standard SGD assumption of bounded local variance (3) and also under the
bounded gradient dissimilarity bound (4). For completeness, we provide a comprehensive proof of Lemma 10
in this section. The proof details are along the lines of those from [SCV18,SX19].
To prove Lemma 10, we will use the techniques developed by [SCV18] and later used by [SX19]. Note that
the results in [SCV18] are stated for a general norm ∥·∥, and we use those results in this paper by specializing
them to the ℓ2-norm, but for simplicity, we write it as ∥· ∥. First we deﬁne a notion called (ϵ, δ)-resilience for
a given set S, which says that if S is resilient around a point µ (which need not be its mean), then dropping
some elements from S does not change the concentration of the resulting set around µ by much.
Deﬁnition 3 (Resilience, [SCV18]). A set S = {y1, y2, . . . , ym} of m points, each lying in Rd, is (ϵ, δ)-resilient
around a point µ ∈Rd, if every subset T ⊆S of cardinality at least (1−ϵ)m satisﬁes
 1
|T |
P
y∈T (y −µ)
 ≤σ.
The notion of resilience is useful for robust gradient estimation, because of the following reasons: (i) Since
the subset S in the statement of Lemma 10 is such that the vectors in S are concentrated around their mean,
this implies using Claim 11 (on page 53) that S is resilient around its sample mean gS. (ii) Algorithm 3 in
fact ﬁnds a suﬃciently large subset A ⊂[R] (see (97) in Lemma 11) and outputs its empirical mean bg. We
show in Claim 12 (on page 54) that A is also resilient around its mean bg. (iii) Since both S and A are large,
they must have a large intersection, and by resilience, their means must both be close to the mean of their
intersection, and hence to each-other, by the triangle inequality; see Claim 13 on page 55. This implies that
bg must be close to gS.
In the following discussion, we make these statements precise.
Lemma 11. Let α := 1 −˜ϵ. The following invariants are maintained in every iteration of the while-loop of
Algorithm 3:
X
i∈S∩A
ciτi ≤αRσ2
0
(95)
X
i∈S
(1 −ci) ≤α
4
R
X
i=1
(1 −ci)
(96)
|S ∩A| ≥α(2 + α)
4 −α
R
(97)
In Lemma 11, (95) ensures that the reconstruction error in the non-corrupted gradients is small; (96)
ensures that we reduce the weights of non-corrupted data at least 4 times slower than overall; and (97)
ensures that we do not remove too many non-corrupted points.
Proof of Lemma 11. Note that ci, τi, A get updated throughout the execution of the while-loop in Algorithm 3.
So, for convenience, we denote these quantities at the beginning of the t’th iteration by ci(t), τi(t), A(t). We
prove Lemma 11 by induction on (96) and (97). Note that induction is not on (95).
Base case, t = 1: Note that A(1) = [R] and ci(1) = 1, ∀i ∈A. So (96) trivially holds. For (97), note
that S ∩A(1) = S, and |S| = αR ≥α(2+α)
4−α R, where the ﬁrst inequality follows from hypothesis of Lemma 10
46

and the second inequality holds because 2+α
4−α ≤1 for α ≥3/4. We show that (95) holds for all t ≥1 in the
induction step below.
Induction step: Suppose the while-loop has not terminated in the t’th iteration, which means that we
update ci(t), A(t) to ci(t + 1), A(t + 1). This implies that the condition on line 5 of Algorithm 3 is satisﬁed,
i.e.,
X
i∈A(t)
ci(t)τi(t) > 4Rσ2
0.
(98)
By induction hypothesis, (96) and (97) hold at the t’th iteration, i.e.,
X
i∈S
 1 −ci(t)

≤α
4
R
X
i=1
 1 −ci(t)

(99)
|S ∩A(t)| ≥α(2 + α)
4 −α
R.
(100)
Before proving that they also hold at the (t + 1)’th iteration, we ﬁrst show that (95) holds at the t’th iteration
for all t ≥1.
Claim 5. For all t ≥1, we have P
i∈S∩A(t) ci(t)τi(t) ≤αRσ2
0.
Proof. First we deﬁne the sets over which we maximize/minimize the expression in (91) in the t’th iteration:
W(t) :=


W ∈R|A(t)|×|A(t)| :
X
j∈A(t)
Wji = 1, ∀i ∈A(t) and Wji ∈

0,
4 −α
α(2 + α)R


,
Y(t) :=

Y ∈Rd×d : Y ⪰0, tr(Y) ≤1
	
.
It is immediate from the deﬁnitions that W(t) and Y(t) are compact and convex sets. Fix any W ∈W(t), Y ∈
Y(t), and consider the cost function Φ from (92) in Algorithm 3:
Φ(W, Y) =
X
i∈A(t)
ci(t)(gi −GA(t)wi)T Y(gi −GA(t)wi).
(101)
It can be veriﬁed that Φ is a continuous function of its arguments. The following claim is proved at the end
of this section.
Claim 6. Φ is convex in W (for a ﬁxed Y) and concave in Y (for a ﬁxed W).
Given that Claim 6 holds, and W(t), Y(t) are compact and convex sets, and Φ is a continuous function, it
follows from the Von-Neumann Minimax theorem that
min
W∈W(t) max
Y∈Y(t) Φ
 W, Y

= max
Y∈Y(t)
min
W∈W(t) Φ
 W, Y

.
(102)
Furthermore, there is a saddle-point W∗(t), Y∗(t) (that achieves the optimum in (91) in the t’th iteration)
such that
max
Y∈Y(t) Φ
 W∗(t), Y

=
min
W∈W(t) Φ
 W, Y∗(t)

= Φ
 W∗(t), Y∗(t)

.
In particular, we have W∗(t) ∈arg minW∈W(t) Φ(W, Y∗(t)). Substituting the expression for Φ from (101),
we get
W∗(t) ∈arg
min
W∈W(t)
X
i∈A(t)
ci(t)
 gi −GA(t)wi
T Y∗(t)
 gi −GA(t)wi

.
(103)
47

Observe that the summation in (103) has |A(t)| terms, and wi (the i’th column of W) appears only in the
i’th summand. This implies that we can separately optimize for each column of W in (103). Deﬁne
W′(t) :=


w ∈R|A(t)| :
X
j∈A(t)
wj = 1 and wj ∈

0,
4 −α
α(2 + α)R



Let w∗
i (t) denote the i’th column of W∗(t). We can write
w∗
i (t) ∈arg
min
w∈W′(t)
 gi −GA(t)w
T Y∗(t)
 gi −GA(t)w

.
(104)
We have from (93) in Algorithm 3 that τi(t) =
 gi −GA(t)w∗
i (t)
T Y∗(t)
 gi −GA(t)w∗
i (t)

. Note that τi(t)
is equal to the expression on the RHS of (104) evaluated with its minimizer w∗
i (t). Deﬁne e
w(t) ∈R|A(t)|
such that e
w(t)j :=
1j∈S∩A(t)
|S∩A(t)| . Since |S ∩A(t)| ≥α(2+α)
4−α R (from the induction hypothesis (100)), it follows
that e
w(t) ∈W′(t). This, together with the deﬁnition of τi(t), implies that
τi(t) ≤
 gi −GA(t) e
wi(t)
T Y∗(t)
 gi −GA(t) e
wi(t)

(105)
Let GA(t) ∈Rd×|A(t)| be the matrix with gi, i ∈A(t) as its columns. We have
GA(t) e
w(t) =
1
|S ∩A(t)|
X
i∈S∩A(t)
gi =: gS∩A(t).
In order to prove Claim 5, we have to bound P
i∈S∩A(t) ci(t)τi(t):
X
i∈S∩A(t)
ci(t)τi(t) ≤
X
i∈S∩A(t)
ci(t)
 gi −GA(t) e
wi(t)
T Y∗(t)
 gi −GA(t) e
wi(t)

(From (105))
(a)
=
X
i∈S∩A(t)
ci(t)
 gi −gS∩A(t)
T Y∗(t)
 gi −gS∩A(t)

(b)
≤
X
i∈S∩A(t)
 gi −gS∩A(t)
T Y∗(t)
 gi −gS∩A(t)

(c)
≤
X
i∈S∩A(t)
 gi −gS
T Y∗(t)
 gi −gS

(d)
≤
X
i∈S
 gi −gS
T Y∗(t)
 gi −gS

(e)
≤

X
i∈S
 gi −gS
 gi −gS
T

(f)
=
max
v∈Rd,∥v∥=1
X
i∈S
vT [(gi −gS)(gi −gS)T ]v
=
max
v∈Rd,∥v∥=1
X
i∈S
⟨gi −gS, v⟩2
(g)
≤|S|σ2
0 = αRσ2
0.
In (a) we used gS∩A(t) = GA(t) e
w(t). In (b) we used that ci(t) ≤1 for every i ∈A(t). In (c) we used the fact
that gS∩A(t) is the minimizer of minu∈Rd P
i∈S∩A(t)(gi −u)T Y ∗(t)(gi −u). In (d) we used that Y∗(t) ⪰0,
48

which implies that the additional terms corresponding to i ∈S \ A(t) are non-zero. The reasoning for (e) is
given below. In (f) we used the deﬁnition of the matrix norm. In (g) we used the hypothesis in Lemma 10
about the set S.
X
i∈S
 gi −gS
T Y∗(t)
 gi −gS
 (a)
=
X
i∈S
tr
 gi −gS
T Y∗(t)
 gi −gS

(b)
=
X
i∈S
tr
 gi −gS
 gi −gS
T Y∗(t)

(c)
= tr
" X
i∈S
 gi −gS
 gi −gS
T
!
Y∗(t)
#
(d)
≤

X
i∈S
 gi −gS
 gi −gS
T
 ∥Y∗(t)∥∗
(e)
≤

X
i∈S
 gi −gS
 gi −gS
T

The equality (a) follows because trace of a scalar is the scalar itself. In (b) we used that tr(AB) = tr(BA)
for any two dimension compatible matrices A, B. In (c) we used that tr(A + B) = tr(A) + tr(B). In (d) we
used tr(AB) ≤∥A∥∥B∥∗(shown below in Claim 7), where ∥· ∥∗denotes the nuclear norm, which is equal to
the sum of singular values. Note that the matrix norm here is induced by the ℓ2 norm. (e) follows from the
fact that for a positive semi-deﬁnite matrix Y, we have ∥Y∥∗= tr(Y), which is at most 1. This completes
the proof of Claim 5.
Claim 7. For any two dimension compatible matrices A, B, we have tr(AB) ≤∥A∥∥B∥∗, where ∥· ∥is the
matrix norm induced by the ℓ2-norm, and ∥· ∥∗is the nuclear norm, which is equal to the sum of singular
values of B.
Proof. Let r = rank(B), and let σi, i = 1, 2, . . . , r denote the non-zero singular values of B.
By the
singular value decomposition, we have B = Pr
i=1 σiuivT
i , where ui, vi are the left and right singular vectors,
respectively, corresponding to the singular value σi. Note that ui, vi for every i = 1, . . . , r are unit norm
vectors.
tr(AB) = tr(A
r
X
i=1
σiuivT
i )
=
r
X
i=1
σitr(AuivT
i )
(Since tr(A +B) = tr(A) + tr(B))
=
r
X
i=1
σitr(vT
i Aui)
(Since tr(AB) = tr(BA))
=
r
X
i=1
σivT
i Aui
(Since vT
i Aui is a scalar)
(a)
≤
r
X
i=1
σi∥vi∥∥A∥∥ui∥
= ∥A∥
r
X
i=1
σi
(Since ∥ui∥= 1, ∥vi∥= 1, for every i = 1, . . . , r)
= ∥A∥∥B∥∗
49

In (a), ﬁrst we used the Cauchy-Schwarz inequality to write vT
i Aui ≤∥vi∥∥Aui∥and then used the deﬁnition
of matrix norm to write ∥Aui∥≤∥A∥∥ui∥. Note that if A, B are positive semi-deﬁnite, then equality holds
in (a) above if and only if B is a multiple of uuT , where u is the eigenvector corresponding to the largest
eigenvalue of A.
This completes the proof of Claim 7.
Now we are ready to prove the inductive step for (96) and (97).
X
i∈S
 1 −ci(t + 1)

=
X
i∈S∩A(t)
 1 −ci(t + 1)

+
X
i∈S\A(t)
 1 −ci(t + 1)

=
X
i∈S∩A(t)
 1 −ci(t + 1)

+
X
i∈S\A(t)
 1 −ci(t)

(Since ci(t + 1) = ci(t), ∀i /∈A(t))
=
X
i∈S
 1 −ci(t)

+
X
i∈S∩A(t)
 ci(t) −ci(t + 1)

(106)
The ﬁrst term in (106) can be bounded using the induction hypothesis (99): P
i∈S
 1 −ci(t)

≤α
4
PR
i=1
 1 −
ci(t)

. The second term is equal to
1
τmax(t)
P
i∈S∩A(t) τi(t)ci(t) (see line 6 of Algorithm 3). It follows from
(98) and Claim 5 that P
i∈S∩A(t) ci(t)τi(t) < α
4
P
i∈A(t) ci(t)τi(t). Substituting these in (106), we get
X
i∈S
 1 −ci(t + 1)

< α
4
R
X
i=1
 1 −ci(t)

+
α
4τmax(t)
X
i∈A(t)
ci(t)τi(t)
= α
4


X
i∈[R]\A(t)
 1 −ci(t)

+
X
i∈A(t)
 1 −ci(t)

+
X
i∈A(t)
ci(t) τi(t)
τmax(t)


= α
4


X
i∈[R]\A(t)
 1 −ci(t + 1)

+
X
i∈A(t)




1 −

1 −
τi(t)
τmax(t)

ci(t)
|
{z
}
= ci(t+1)







(Since ci(t + 1) = ci(t), ∀i /∈A(t))
= α
4
R
X
i=1
 1 −ci(t + 1)

.
(107)
This proves that (96) holds throughout the execution of the while-loop of Algorithm 3. Now we prove (97).
From (107), we have P
i∈S
 1 −ci(t + 1)

=
α
4
P
i∈S
 1 −ci(t + 1) + P
i∈[R]\S
 1 −ci(t + 1)

.
By
rearranging terms, we get P
i∈S
 1 −ci(t + 1)

≤
α
4−α
P
i∈[R]\S
 1 −ci(t + 1)

. Since ci(t + 1) ≥0 for all i, t,
and |S| ≥αR, we have
X
i∈S
 1 −ci(t + 1)

≤α(1 −α)
4 −α
R.
(108)
It follows from (108) that the number of i’s in S for which ci(t + 1) <
1
2 is at most 2α(1−α)
4−α
R. So, at
most 2α(1−α)
4−α
R elements of S can be removed from A(t) in total in line 7 of Algorithm 3. This implies
|S \ A(t + 1)| ≤2α(1−α)
4−α
R. So, |S ∩A(t + 1)| = |S| −|S \ A(t + 1)| ≥αR −2α(1−α)
4−α
R = α(2+α)
4−α R. This proves
that (97) is preserved throughout the execution of the while-loop of Algorithm 3.
This concludes the proof of Lemma 11.
Claim 8. Let W be the minimizer of (91) in Algorithm 3, when the while-loop terminates. Let W1 be the
result of zeroing out all singular values of W that are greater than 0.9. Then W −W1 is a rank one matrix.
50

Proof. Note that the constraints in (91) imply that ∥W∥2
F ≤
4−α
α(2+α). This can be seen as follows:
∥W∥2
F =
X
i,j∈A
W 2
ji
≤
X
i.j∈A
Wji · max
k,l∈A Wkl
≤
X
i∈A

X
j∈A
Wji

·
4 −α
α(2 + α)R
(Since maxk,l∈A Wkl ≤
4−α
α(2+α)R; see (91))
=
4 −α
α(2 + α).
(Since P
j∈A Wji = 1,
∀i ∈A; see (91))
Let σ1, σ2, . . . , σ|A| denote the singular values of W, arranged in non-increasing order. Since ∥W∥2
F =
P
i∈A σ2
i , and that,
4−α
α(2+α) ≤52/33 < 2 × 0.92 for α ≥3/4, we have that at most one singular value of W can
be greater than 0.9. Since P
j∈A Wji = 1,
∀i ∈A, i.e., W is a column stochastic matrix, we know that its
largest singular value is at least 1. Together, we have that σ1 ≥1 and σi < 0.9, ∀i = 2, 3, . . . , |A|. Since W1
is the result of zeroing out all singular values of W that are greater than 0.9, as a consequence, W −W1 is a
rank one matrix.
Claim 9. Let W0 = (W −W1)(I −W1)−1. Then W0 is a column stochastic matrix of rank one.
Proof. First we show that W0 is a column stochastic matrix, i.e., 1T W0 = 1T .
1T W0 = 1T (W −W1)(I −W1)−1
= (1T −1T W1)(I −W1)−1
(Since (11W = 1T ) ⇐⇒(P
j∈A Wji = 1, ∀i ∈A); see (91))
= 1T (I −W1)(I −W1)−1
= 1T .
Now we show that rank(W0) = 1.
Since for any two dimension-compatible matrices A, B, we have
rank(AB) ≤min{rank(A), rank(B)}.
As a consequence, since W0 = (W −W1)(I −W1)−1, where
rank(W −W1) = 1 (from Claim 8), we have that rank(W0) ≤1. Since W0 is a column stochastic matrix, its
largest singular value is at least one, implying that rank(W0) ≥1. As a result, we have that rank(W0) = 1.
Since W0 is a column stochastic matrix of rank one, it follows that W0 = u1T for some vector u ∈R|A|.
Recall that GA is a d×|A| matrix whose columns are the gradients gi, i ∈A. Deﬁne g′ := GAu. This implies
that all the columns of GAW0 are identical and equal to g′: GAW0 = GAu1T = g′1T = [g′ g′ . . . g′].
Lemma 12.
GA −g′1T  ≤20
√
2Rσ0.
Proof. First we show
GA(I −W)
p
diag(cA)
 ≤2
√
Rσ0, where
p
diag(cA) denotes a |A| × |A| diagonal
matrix, whose entries are √ci, i ∈A. For this, we use the following identity:
Claim 10. For any matrix Z with entries from R, we have
∥Z∥2 =
max
Y⪰0,tr(Y)≤1 tr(ZT YZ).
Proof. Take any Y ⪰0 such that tr(Y) ≤1. First we show that tr(ZT YZ) ≤∥Z∥2.
tr(ZT YZ)
(a)
= tr(ZZT Y)
(b)
≤∥ZZT ∥∥Y∥∗
(c)
≤∥ZZT ∥
(d)
= ∥Z∥2.
Here (a) follows because tr(AB) = tr(BA) for any A, B. The inequality (b) follows from tr(AB) ≤∥A∥∥B∥∗
(which is proved in Claim 7), where ∥· ∥∗denotes the nuclear norm, and ∥B∥∗is equal to sum of the singular
51

values of B. (c) follows from the fact that for a positive semi-deﬁnite matrix Y, we have ∥Y∥∗= tr(Y),
which is at most 1. (d) follows because ∥Z∥= ∥ZT ∥and that the largest singular value σmax(Z) of Z is equal
to
p
λmax(ZT Z), where λmax(ZT Z) = ∥ZT Z∥is the largest eigenvalue of ZT Z.
Taking maximum over all Y ⪰0 such that tr(Y) ≤1, we get
max
Y⪰0,tr(Y)≤1 tr(ZT YZ) ≤∥Z∥2.
(109)
Now we show that maxY⪰0,tr(Y)≤1 tr(ZT YZ) ≥∥Z∥2. This, together with (109), proves Claim 10.
max
Y⪰0,tr(Y)≤1 tr(ZT YZ)
(e)
≥
max
v:∥v∥=1 tr(ZT vvT Z)
=
max
v:∥v∥=1 tr(vT ZZT v)
=
max
v:∥v∥=1 vT ZZT v
=
max
v:∥v∥=1 ∥ZT v∥2
(g)
= ∥ZT ∥2 (h)
= ∥Z∥2
In (e), we restrict to those Y’s which are of the form Y = vvT for a unit norm vector v. It is immediate
that such Y’s are positive semi-deﬁnite and their trace is equal to one. Note that the vector v that attains
the maximum in the RHS of (g) is the eigenvector corresponding to the largest eigenvalue of ZZT . In (g) we
used the deﬁnition of the matrix norm ∥A∥= maxv:∥v∥=1 ∥Av∥, and in (h) we used the fact that transpose
does not change the matrix norm.
By letting Z = [z1 z2 . . . zm], where {zi}m
i=1 are the columns of Z, we have tr(ZT YZ) = Pm
i=1 zT
i Yzi.
Applying this identity to Z = GA(I −W)
p
diag(cA) (which implies zi = √ci(gi −GAwi)) gives
∥GA(I −W)
p
diag(cA)∥2
2 =
max
Y⪰0,tr(Y)≤1
X
i∈A
[√ci(gi −GAwi)]T Y [√ci(gi −GAwi)]
=
max
Y⪰0,tr(Y)≤1
X
i∈A
ci(gi −GAwi)T Y(gi −GAwi)
(a)
=
X
i∈A
ciτi
(b)
≤4Rσ2
0.
(110)
For (a), note that W is the optimal weight matrix when the while-loop of Algorithm 3 terminates, and τi is
deﬁned w.r.t. the optimal W, Y. In (b) we used the fact that the while-loop terminates when the condition
in line 5 of Algorithm 3 is violated, i.e., P
i∈A ciτi ≤4Rσ2
0.
Now, we are ready to prove Lemma 12.
GA −g′1T  = ∥GA −GAW0∥
(Since GAW0 = g′1T )
=
GA(I −W1)(I −W1)−1 −GA(W −W1)(I −W1)−1
=
GA[(I −W1) −(W −W1)](I −W1)−1
=
GA(I −W)(I −W1)−1
≤∥GA(I −W)∥
(I −W1)−1
≤10 ∥GA(I −W)∥
(Since the largest singular value of W1 is at most 0.9)
(a)
≤10
√
2
GA(I −W)
p
diag(cA)

(Since ci ≥1
2 for all i ∈A)
52

≤20
√
2Rσ0.
(Using (110))
In (a), we used the fact that ci ≥1
2 for all i ∈A; and
p
diag(cA) denotes a |A| × |A| diagonal matrix, whose
entries are √ci, i ∈A.
This completes the proof of Lemma 12.
Claim 11. The set S in the hypothesis of Lemma 10 is (2σ0√ϵ1, ϵ1)-resilient around its empirical mean gS
for any ϵ1 ∈[0, 1
2).
Proof. Take an arbitrary ϵ1 ∈[0, 1
2). We need to show that for any subset T ⊆S such that |T | ≥(1 −ϵ1)|S|,
we have ∥gT −gS∥≤2σ0√ϵ1, where gT =
1
|T |
P
i∈T gi. For any subset T ⊆[R], we write GT to denote the
d × |T | matrix whose columns are gi, i ∈T .
∥gT −gS∥=

1
|T |
X
i∈T
(gi −gS)

=

 1
|T | −1
|S|
 X
i∈T
(gi −gS) + 1
|S|
X
i∈T
(gi −gS)

(a)
=

 1
|T | −1
|S|
 X
i∈T
(gi −gS) −1
|S|
X
i∈S\T
(gi −gS)

=

 1
|T | −1
|S|

[GT −gS1T ]1 −1
|S|[GS\T −gS1T ]1

≤
 1
|T | −1
|S|
 [GT −gS1T ]1
 + 1
|S|
[GS\T −gS1T ]1

≤
 1
|T | −1
|S|
 GT −gS1T  ·
1|T |
 + 1
|S|
GS\T −gS1T  ·
1|S\T |

=
 
|S| −|T |
p
|T ||S|
!
GT −gS1T  +
p
|S \ T |
|S|
GS\T −gS1T 
(b)
≤
 
|S| −|T |
p
|T ||S|
+
p
|S \ T |
|S|
!
GS −gS1T 
(111)
In the above inequalities, we denote the all 1’s vector by 1; its dimension is not explicitly written, and is taken
what makes the operation dimension compatible. In (a) we used the fact that P
i∈S(gi −gS) = 0 and that
P
i∈S(gi −gS) = P
i∈T (gi −gS) + P
i∈S\T (gi −gS). The inequality (b) follows from the fact that for any
matrix A, if we consider another matrix A′ whose column set is a subset of the column set of A, then we have
∥A′∥≤∥A∥. This can be seen as follows: Let P denote the projection matrix that selects columns of A to
construct A′ such that A′ = AP; note that P is a diagonal matrix with Pii = 1 if and only if the i’th column
of A is selected. Since the matrix norm ∥· ∥of a matrix is equal to its largest singular value, and the largest
singular value of P is equal to 1, we have ∥P∥= 1. Now (b) follows from ∥A′∥= ∥AP∥≤∥A∥· ∥P∥= ∥A∥,
where we used ∥AB∥≤∥A∥· ∥B∥which holds for any two dimension-compatible matrices A, B.
In order to bound (111), ﬁrst we bound the coeﬃcient

|S|−|T |
√
|T ||S|

as follows:
 
|S| −|T |
p
|T ||S|
+ |S \ T |
|S|
!
≤

ϵ1
√1 −ϵ1
+ √ϵ1

1
p
|S|
≤2√ϵ1
p
|S|
,
(112)
where in the ﬁrst inequality we used |T | ≥(1 −ϵ1)|S| and |S \ T | ≤ϵ1|S|; the second inequality holds because
ϵ1 < 1/2.
53

Now we bound
GS −gS1T :
GS −gS1T  (c)
=

 GS −gS1T T 
(d)
=
r
λmax

(GS −gS1T ) (GS −gS1T )T 
(e)
=
v
u
u
tλmax
 X
i∈S
(gi −gS)(gi −gS)T
!
(f)
≤
p
|S|σ0.
(113)
Here, (c) follows from the fact that the matrix norm ∥· ∥of a rectangular matrix is equal to the matrix norm
of its transpose, i.e., for any matrix A, we have ∥A∥= ∥AT ∥. (d) follows from the fact that for any matrix
A, its matrix norm ∥A∥is equal to its largest singular value, which is equal to ∥A∥=
p
λmax(AT A). (e)
uses the identity
 GS −gS1T   GS −gS1T T = P
i∈S(gi −gS)(gi −gS)T . (f) follows from the hypothesis
of Lemma 10 that λmax

1
|S|
P
i∈S (gi −gS) (gi −gS)T 
≤σ2
0.
Substituting the bounds from (112) and (113) back in (111) completes the proof of Claim 11.
Note that using an iterative procedure, Algorithm 3 ﬁnds a subset A ⊂[R] and outputs bg =
1
|A|
P
i∈A gi.
Now we show that A is resilient around its sample mean bg.
Claim 12. The set A produced when Algorithm 3 terminates is (80σ0√ϵ2, ϵ2)-resilient around its sample
mean bg for any ϵ2 ∈[0, 1
2).
Proof. Take an arbitrary ϵ2 ∈[0, 1
2). We need to show that for any subset T ⊆A such that |T | ≥(1 −ϵ2)|A|,
we have ∥gT −bg∥≤80σ0√ϵ2, where gT =
1
|T |
P
i∈T gi. Let G = [g1, . . . , gR] ∈Rd×R.
∥gT −bg∥=

1
|T |
X
i∈T
gi −1
|A|
X
i∈A
gi

=

1
|T |
X
i∈T
(gi −g′) −1
|A|
X
i∈A
(gi −g′)

(where g′ = GAu and u is such that W0 = u1T )
=

 1
|T | −1
|A|
 X
i∈T
(gi −g′) −1
|A|
X
i∈A\T
(gi −g′)

=

 1
|T | −1
|A|

[GT −g′1T ]1 −1
|A|[GA\T −g′1T ]1

≤
 1
|T | −1
|A|
 [GT −g′1T ]1
 + 1
|A|
[GA\T −g′1T ]1

≤
 1
|T | −1
|A|
 GT −g′1T  ·
1|T |
 + 1
|A|
GA\T −g′1T  ·
1|A\T |

=
 
|A| −|T |
p
|T ||A|
!
GT −g′1T  +
p
|A \ T |
|A|
GA\T −g′1T 
≤
 
|A| −|T |
p
|T ||A|
+
p
|A \ T |
|A|
!
GA −g′1T 
(114)
In the last inequality, we used that
GA −g′1T  ≥
GA′ −g′1T  for any subset A′ ⊆A. This can be
shown along the lines of the reasoning given for (111). In order to bound the RHS of (114), ﬁrst we bound
54

the coeﬃcient

|A|−|T |
√
|T ||A| +
√
|A\T |
|A|

and then we bound
GA −bg1T . To bound the coeﬃcient, we can use
the steps used for obtaining (112), which gives
 
|A| −|T |
p
|T ||A|
+
p
|A \ T |
|A|
!
≤2√ϵ2
p
|A|
.
(115)
We need to lower-bound |A|, which we do as follows: |A| ≥|A∩S|
(a)
≥α(α+2)
4−α R
(b)
≥33R
52 > R
2 , where (a) follows
from Lemma 11 and (b) follows by substituting α = (1 −˜ϵ) ≥3/4. Substituting this in (115) and using the
resulting bound in (114) gives:
∥gT −bg∥≤2
r
2ϵ2
R
GA −g′1T 
(116)
Substituting the bound
GA −g′1T  ≤20
√
2Rσ0 from Lemma 12 gives ∥gT −bg∥≤80√ϵ2σ0, which
completes the proof of Claim 12.
Now we show that Claim 11 and Claim 12 imply ∥bg −gS∥≤O(σ0
√
˜ϵ), proving Lemma 10.
Claim 13. ∥bg −gS∥≤O(σ0
√
˜ϵ), where ˜ϵ ≤1/4.
Proof. We have from Claim 11 that for any subset T1 ⊆S such that |T1| ≥(1 −ϵ1)|S|, where ϵ1 ∈[0, 1
2), we
have ∥gT1 −gS∥≤2σ0√ϵ1. We have from Claim 12 that for any subset T2 ⊆A such that |T2| ≥(1 −ϵ2)|A|,
where ϵ2 ∈[0, 1
2), we have ∥gT2 −bg∥≤80σ0√ϵ2.
We take T1 = T2 = S ∩A. For that we need to show |S ∩A| ≥(1 −ϵ1)|S| and |S ∩A| ≥(1 −ϵ2)|A| hold
for some ϵ1, ϵ2 ∈[0, 1
2). We show these below.
We have from Lemma 11 that |S ∩A| ≥
α(α+2)
4−α R, where α = 1 −˜ϵ ≥
3
4.
It can be veriﬁed that
α(α+2)
4−α
≥1 −5
3(1 −α) (which is equivalent to the fact that (1 −α)2 ≥0). Substituting this and that
R ≥max{|S|, |A|}, we get |S ∩A| ≥
 1 −5
3(1 −α)

max{|S|, |A|}. Note that 5
3(1 −α) < 1
2. We have shown
that if we take T1 = T2 = S ∩A and ϵ1 = ϵ2 =
5
3(1 −α) <
1
2, we have |T1| ≥
 1 −5
3(1 −α)

|S| and
|T2| ≥
 1 −5
3(1 −α)

|A|. These give
∥gS∩A −gS∥≤2
r
5
3σ0
√
1 −α.
∥gS∩A −bg∥≤80
r
5
3σ0
√
1 −α
Using the triangle inequality and substituting ˜ϵ = 1 −α gives ∥bg −gS∥≤82
q
5
3σ0
√
˜ϵ = O(σ0
√
˜ϵ). This
completes the proof of Claim 13.
This concludes the proof of Lemma 10.
Claim (Restating Claim 6). The cost function Φ from (101) is convex in W (for a ﬁxed Y) and concave in
Y (for a ﬁxed W).
Proof. Recall the deﬁnition of Φ from (101):
Φ(W, Y) =
X
i∈A(t)
ci(t)(gi −GA(t)wi)T Y(gi −GA(t)wi).
55

In order to prove this claim, ﬁrst we simplify some notation. Since ci(t) ≥0 for all i ∈A(t), we can safely
ignore them. We write GA(t) simply as G. We also suppress the explicit dependence on t. With these
simpliﬁcations, the modiﬁed cost function, denoted by eΦ can be written as:
eΦ(W, Y) =
X
i∈A
(gi −Gwi)T Y(gi −Gwi).
For i ∈A, deﬁne zi = gi −Gwi. Let Z be a matrix whose columns are zi’s. Note that Z = (G −GW) =
G(I −W). With this notation, we have P
i∈A(gi −Gwi)T Y(gi −Gwi) = P
i∈A zT
i Yzi = tr(ZT YZ). Now
eΦ can be written as
eΦ(W, Y) = tr
 G(I −W)
T Y
 G(I −W)

= tr
 I −WT )GT YG(I −W)

= tr
 GT YG + WT GT YGW −WT GT YG −GT YGW

(a)
= tr(GT YG) + tr(WT GT YGW) −tr(WT GT YG) −tr(GT YGW)
(b)
= tr(GT YG) + tr(WWT GT YG) −tr(WT GT YG) −tr(WGT YG

= tr
 GT YG + WWT GT YG −WT GT YG −WGT YG

= tr
 (I + WWT −WT −W)GT YG

= tr
 (I −W)(I −WT )GT YG

(117)
In (a) we used the property that trace is linear in its arguments (i.e., tr(A + B) = tr(A) + tr(B) for all
A, B). In (b), we used the property that tr(AB) = tr(BA). In order to prove that for a ﬁxed Y, eΦ is convex
in W, it suﬃces to show that, for any λ ∈[0, 1] and W1W2 ∈W(t), we have eΦ(λW1 + (1 −λ)W2, Y) ≤
λeΦ(W1, Y) + (1 −λ)eΦ(W2, Y).
eΦ(λW1 + (1 −λ)W2, Y)
≤tr
h
I −
 λW1 + (1 −λ)W2

I −
 λW1 + (1 −λ)W2
T 
GT YG
i
= tr
h
λ
 I −W1

+ (1 −λ)
 I −W2

λ
 I −W1

+ (1 −λ)
 I −W2
T
GT YG
i
= tr
h
λ2 I −W1
 I −W1
T + λ(1 −λ)
 I −W1
 I −W2
T 
GT YG
i
+ tr
h
λ(1 −λ)
 I −W2
 I −W1
T + (1 −λ)2 I −W2
 I −W2
T 
GT YG
i
(a)
= tr
h λ2 −λ
 I −W1
 I −W1
T + λ(1 −λ)
 I −W1
 I −W2
T 
GT YG
i
+ tr
h
λ(1 −λ)
 I −W2
 I −W1
T +
 (1 −λ)2 −(1 −λ)
 I −W2
 I −W2
T 
GT YG
i
+ tr
h
λ
 I −W1
 I −W1
T + (1 −λ)
 I −W2
 I −W2
T 
GT YG
i
= tr
h
λ(1 −λ)
 I −W1
 I −W2
T −
 I −W1
T 
GT YG
i
+ tr
h
λ(1 −λ)
 I −W2
 I −W1
T −
 I −W2
T 
GT YG
i
+ tr
h
λ
 I −W1
 I −W1
T + (1 −λ)
 I −W2
 I −W2
T 
GT YG
i
= tr
h
λ(1 −λ)
 I −W1

−
 I −W2
 I −W2
T −
 I −W1
T 
GT YG
i
+ tr
h
λ
 I −W1
 I −W1
T + (1 −λ)
 I −W2
 I −W2
T 
GT YG
i
56

= tr
h
−

λ(1 −λ)
 W1 −W2
 W1 −W2
T 
GT YG
i
+ tr
h
λ
 I −W1
 I −W1
T + (1 −λ)
 I −W2
 I −W2
T 
GT YG
i
(b)
≤tr
h
λ
 I −W1
 I −W1
T + (1 −λ)
 I −W2
 I −W2
T 
GT YG
i
= λtr
h I −W1
 I −W1
T 
GT YG
i
+ (1 −λ)tr
h I −W2
 I −W2
T 
GT YG
i
= λeΦ(W1, Y) + (1 −λ)eΦ(W2, Y)
In (a) we added and subtracted tr
h
λ
 I −W1
 I −W1
T + (1 −λ)
 I −W2
 I −W2
T 
GT YG
i
. In
(b) we used that tr
h W1 −W2
 W1 −W2
T GT YG
i
= tr
h W1 −W2
T GT YG
 W1 −W2
i
≥0, as
 W1 −W2
T GT YG
 W1 −W2

⪰0 (recall that, by assumption, Y ⪰0), implying that all its eigenvalues
are greater than or equal to zero. Since trace is equal to the sum of eigenvalues, the trace is also bigger than
or equal to zero. This completes the proof of Claim 6.
57
