Data Encoding for Byzantine-Resilient Distributed Optimization∗
Deepesh Data1, Linqi Song2, and Suhas Diggavi1
1University of California, Los Angeles, USA
1{deepeshdata, suhasdiggavi}@ucla.edu
2City University of Hong Kong, Hong Kong
2linqi.song@cityu.edu.hk
Abstract
We study distributed optimization in the presence of Byzantine adversaries, where both data and
computation are distributed among m worker machines, t of which may be corrupt. The compromised
nodes may collaboratively and arbitrarily deviate from their pre-speciﬁed programs, and a designated
(master) node iteratively computes the model/parameter vector for generalized linear models. In this
work, we primarily focus on two iterative algorithms: Proximal Gradient Descent (PGD) and Coordinate
Descent (CD). Gradient descent (GD) is a special case of these algorithms. PGD is typically used in
the data-parallel setting, where data is partitioned across diﬀerent samples, whereas, CD is used in the
model-parallelism setting, where data is partitioned across the parameter space.
At the core of our solutions to both these algorithms is a method for Byzantine-resilient matrix-vector
(MV) multiplication; and for that, we propose a method based on data encoding and error correction
over real numbers to combat adversarial attacks.
We can tolerate up to t ≤⌊m−1
2
⌋corrupt worker
nodes, which is information-theoretically optimal. We give deterministic guarantees, and our method
does not assume any probability distribution on the data. We develop a sparse encoding scheme which
enables computationally eﬃcient data encoding and decoding.
We demonstrate a trade-oﬀbetween
the corruption threshold and the resource requirements (storage, computational, and communication
complexity). As an example, for t ≤m
3 , our scheme incurs only a constant overhead on these resources,
over that required by the plain distributed PGD/CD algorithms which provide no adversarial protection.
To the best of our knowledge, ours is the ﬁrst paper that connects MV multiplication with CD and
designs a speciﬁc encoding matrix for MV multiplication whose structure we can leverage to make CD
secure against adversarial attacks.
Our encoding scheme extends eﬃciently to (i) the data streaming model, in which data samples come
in an online fashion and are encoded as they arrive, and (ii) making stochastic gradient descent (SGD)
Byzantine-resilient. In the end, we give experimental results to show the eﬃcacy of our proposed schemes.
1
Introduction
Map-reduce architecture [DG08] is implemented in many distributed learning tasks, where there is one
designated machine (called the master) that computes the model iteratively, based on the inputs from
the worker machines at each iteration, typically using descent techniques, like (proximal) gradient descent,
coordinate descent, stochastic gradient descent, the Newton’s method, etc. The worker nodes perform the
required computations using local data, distributed to the nodes [ZWLS10]. Several other architectures,
including having no hierarchy among the nodes have been explored [LZZ+17].
∗This paper was presented in parts at the IEEE Allerton 2018 (as an invited talk) [DSD18], and ISIT 2019 [DSD19,DD19].
1
arXiv:1907.02664v2  [cs.DC]  4 Nov 2020

In several applications of distributed learning, including the Internet of Battleﬁeld Things (IoBT) [A+18],
federated optimization [Kon17], the recruited worker nodes might be partially trusted with their computa-
tion. Therefore, an important question is whether we can reliably perform distributed computation, taking
advantage of partially trusted worker nodes. These Byzantine adversaries can collaborate and arbitrarily
deviate from their pre-speciﬁed programs. The problem of distributed computation with Byzantine adver-
saries has a long history [LSP82], and there has been recent interest in applying this computational model
to large-scale distributed learning [BMGS17,CWCP18,CSX17].
In this paper, we study Byzantine-tolerant distributed optimization to learn a regularized generalized
linear model (GLM) (e.g., linear/ridge regression, logistic regression, Lasso, SVM dual, constrained mini-
mization, etc.). We consider two frameworks for distributed optimization: (i) data-parallelism architecture,
where data points are distributed across diﬀerent worker nodes, and in each iteration, they all parallelly com-
pute gradients on their local data and master aggregates them to update the parameter vector using gradient
descent (GD) [BT89,Bot10,DCM+12]; and (ii) model-parallelism architecture, where data points are parti-
tioned across features, and several worker nodes work in parallel, updating diﬀerent subsets of coordinates of
the model/parameter vector through coordinate descent (CD) [BKBG11,Wri15,RT16]. Note that GD requires
full gradients to update the parameter vector; and if full gradients are too costly to compute, we can reduce
the per-iteration cost by using CD,1 which also has been shown to be very eﬀective for solving generalized lin-
ear models, and is particularly widely used for sparse logistic regression, SVM, and Lasso [BKBG11]. Given
its simplicity and eﬀectiveness, CD can be chosen over GD in such applications [Nes12]. Computing gradients
in the presence of Byzantine adversaries has been recently studied [BMGS17, CSX17, CWCP18, YCRB18,
AAL18,SX19,XKG19,YCRB19,GV19,RWCP19,LXC+19,GHYR19,YLR+19,DD20b,DD20a,HKJ20], and
we discuss them in detail Section 3 where we also put our work in context. However, as far as we know,
making CD robust to Byzantine adversaries has not received much attention, and to the best of our knowl-
edge, ours is the ﬁrst paper that studies CD against Byzantine attacks and provides an eﬃcient solution for
that.
1.1
Our Contributions
We propose Byzantine-resilient distributed optimization algorithms both for PGD and CD based on data
encoding and error correction (over real numbers). As mentioned above, there have been several papers that
provide diﬀerent methods for gradient computation in the presence of Byzantine adversaries, however, our
proposed algorithm diﬀers from them in one or more of the following aspects: (i) it does not make statistical
assumptions on the data or Byzantine attack patterns; (ii) it can tolerate up to a constant fraction (< 1/2) of
the worker nodes being Byzantine, which is information-theoretically optimal; and (iii) it enables a trade-oﬀ
(in terms of storage and computation/communication overhead at the master and the worker nodes) with
Byzantine adversary tolerance, without compromising the eﬃciency at the master node. We give the same
guarantees for CD also.
First we design a coding scheme for distributed matrix-vector (MV) multiplication, speciﬁcally, for op-
erating in the presence of Byzantine adversaries, and use that in both our algorithms for PGD and CD to
learn GLMs. Note that the connection of MV multiplication with gradient computation is straightforward
and has been known for some time (see, for example, [LLP+18,DCG16]), however, it is not clear whether we
can use MV multiplication methods for CD also. Indeed, since each CD update has a diﬀerent requirement
than that of gradient computation, a general-purpose algorithm for MV multiplication may not be applicable
for CD. One distinction is that in gradient computation, we only need to encode the data to compute the
MV multiplication, whereas, in CD, in addition to data encoding, since workers update few coordinates of
diﬀerent parts of the parameter vector in parallel, we need to encode the parameter vector as well for master
to be able to decode that. In this paper, we design our encoding matrix for MV multiplication in such a
way that it is sparse and has a regular structure of non-zero entries (see (11) for the encoding matrix for
any worker), which makes it applicable for CD too. This leads to eﬃcient solutions for both PGD and CD,
1Alternatively, we can also use SGD to reduce the per-iteration cost, and we give a method for making SGD Byzantine-
resilient in Section 6.1.
2

which are our main focus in this paper.
Inspired from the real-error correction (or sparse reconstruction) problem [CT05], we develop eﬃcient
encoding/decoding procedures for MV multiplication, where we encode the data matrix and distribute it to
the m worker nodes, and to recover the MV product at the master, we reduce the decoding problem to the
sparse reconstruction or real-error correction problem [CT05]. Note that in PGD, we only need to encode
the data, whereas, in CD, we also need to encode the parameter vector, and our coding scheme should
facilitate the requirement that the update on a small fraction of the encoded parameter vector should aﬀect
only a small fraction of the original parameter vector. This is a non-trivial requirement, and our coding
scheme for MV multiplication is designed in such a way that it supports this requirement in an eﬃcient
manner; see Section 2.2 for a description on plain distributed CD, Section 2.5 for our approach to making
CD robust to Byzantine attacks, and Section 5 for a complete solution for Byzantine-resilient CD. In the
context of PGD/CD, for decoding, the master node processes the inputs from the worker nodes, either to
compute the true gradient in the case of PGD or to facilitate the computation at the worker nodes in the
case of CD. We take a two-round approach in each iteration of both these algorithms. Our main results are
summarized in Theorem 1 (on page 9) for PGD and Theorem 2 (on page 11) for CD, and demonstrate a
trade-oﬀbetween the Byzantine resilience (in terms of the number of adversarial nodes) and the resource
requirement (storage, computational, and communication complexity).
As an example, for t ≤
m
3 , our
scheme incurs only a constant overhead on these resources, over that required by the plain distributed PGD
and CD algorithms which provide no adversarial protection. Our coding schemes can handle both Byzantine
attacks and missing updates (e.g., caused by delay or asynchrony of worker nodes). Our encoding process is
also eﬃcient. Though data encoding is a one-time process, it has to be eﬃcient to harness the advantage of
distributed computation. We design a sparse encoding process, based on real-error correction, which enables
eﬃcient encoding, and the worker nodes encode data using the sparse structure. This allows encoding with
storage redundancy2 of
2m
m−2t (which is a constant, even if t is a constant (< 1
2) fraction of m), and a one-time
total computation cost for encoding is O((1 + 2t)nd). Note that the time for data encoding is a factor of
(1 + 2t) (where t is the corruption threshold) more than the time required for plain data distribution which
is O(nd), the size of the data matrix.
We extend our encoding scheme in a couple of important ways: ﬁrst, to make the stochastic gradient
descent (SGD) algorithm Byzantine-resilient without compromising much on the resource requirements; and
second, to handle streaming data eﬃciently, where data points arrives one by one (and we encode them
as they arrive), rather than being available at the beginning of the computation; we also give few more
applications of our method. For the streaming model, more speciﬁcally, our encoding requires the same
amount of time, irrespective of whether we encode all the data at once, or we get data points one by one
(or in batches) and we encode them as they arrive. This setting encompasses a more realistic scenario, in
which we design our coding scheme with the initial set of data points and distribute the encoded data among
the workers. Later on, when we get some more samples, we can easily incorporate them into our existing
encoded setup. See Section 6 for details on these extensions.
1.2
Paper Organization
We present our problem formulation, description of the plain distributed PGD and CD algorithms, and the
high-level ideas of our Byzantine-resilient algorithms for both PGD and CD along-with our main results
in Section 2.
We give detailed related work in Section 3.
We present our full coding schemes for MV
multiplication and also for gradient computation for PGD along-with a complete analysis of their resource
requirements in Section 4. In Section 5, we provide a complete solution to CD. In Section 6, we show how
our method can be extended to SGD and to the data streaming model. We also discuss applicability of our
method to a few more important applications in that section. In Section 7, we show numerical results of our
method: we show the eﬃciency of our method for both gradient descent (GD) and coordinate descent (CD)
by running them to solve linear regression on two datasets (moderate and large) and plotting the running
time with varying number of corrupt worker nodes (up to <1/2 fraction).
2Storage redundancy is deﬁned as the ratio of the size of the encoded matrix and the size of the raw data matrix.
3

1.3
Notation
We denote vectors by bold small letters (e.g., x, y, z, etc.)
and matrices by bold capital letters (e.g.,
A, F, S, X, etc.).
We denote the amount of storage required by a matrix X by |X|.
For any positive
integer n ∈N, we denote the set {1, 2, . . . , n} by [n]. For n1, n2 ∈N, where n1 ≤n2, we write [n1 : n2]
to denote the set {n1, n1 + 1, . . . , n2}. For any vector u ∈Rn and any set S ⊂[n], we write uS to denote
the |S|-length vector, which is the restriction of u to the coordinates in the set S. The support of a vector
u ∈Rn is deﬁned by supp(u) := {i ∈[n] : ui ̸= 0}. We say that a vector u ∈Rn is t-sparse if |supp(u)| ≤t.
While stating our results, we assume that performing the basic arithmetic operations (addition, subtraction,
multiplication, and division) on real numbers takes unit time.
2
Problem Setting and Our Results
Given a dataset consisting of n labelled data points (xi, yi) ∈Rd × R, i ∈[n], we want to learn a
model/parameter vector w ∈Rd, which is a minimizer of the following empirical risk minimization problem:
min
w∈Rd
  1
n
n
X
i=1
fi(w)

+ h(w)
!
,
(1)
where fi(w), i = 1, 2, . . . , n, denotes the risk associated with the i’th data point with respect to w and
h(w) denotes a regularizer. We call f(w) := 1
n
Pn
i=1 fi(w) the average empirical risk associated with the
n data points with respect to w. Our main focus in this paper is on generalized linear models (GLM),
where fi(w) = ℓ(⟨xi, w⟩; yi) for some diﬀerentiable loss function ℓ. Here, each fi : Rd →R is diﬀerentiable,
h : Rd →R is convex but not necessarily diﬀerentiable, and ⟨xi, w⟩is the dot product of xi and w. We do not
necessarily need each fi to be convex, but we require f(w) to be a convex function. Note that f(w) + h(w)
is a convex function. In the following we study diﬀerent algorithms for solving (1) to learn a GLM.
2.1
Proximal Gradient Descent
We can solve (1) using Proximal Gradient Descent (PGD). This is an iterative algorithm, in which we choose
an arbitrary/random initial w0 ∈Rd, and then update the parameter vector according to the following
update rule:
wt+1 = proxh,αt(wt −αt∇f(wt)),
t = 1, 2, 3, . . .
(2)
where αt is the step size or the learning rate at the t’th iteration, determining the convergence behaviour.
There are standard choices for it; see, for example, [BV04, Chapter 9]. For any h and α, the proximal
operator proxh,α : Rd →R is deﬁned as
proxh,α(w) = arg min
z∈Rd
1
2α∥z −w∥2
2 + h(z).
(3)
Observe that if h = 0, then proxh,α(w) = w for every w ∈Rd, and PGD reduces to the classical gradient
descent (GD). This encompasses several important optimization problems related to learning, for which prox
operator has a closed form expression; some of these problems are given below.
• Lasso. Here fi(w) = 1
2(⟨xi, w⟩−yi)2 and h(w) = λ∥w∥1. It turns out that proxh,α(z) for Lasso is equal
to the soft-thresholding operator Sλα(z) [Tib15], which, for j ∈[d], is deﬁned as
(Sλα(z))j =







zj + λα
if zj < −λα,
0
if −λα ≤zj ≤λα,
zj −λα
if zj > λα.
4

• SVM dual. Jaggi [Jag13] showed an equivalence between the dual formulation of Support Vector Machines
(SVM) and Lasso. Hence, SVM dual is also a special case of (1).
• Constrained optimization. We want to solve a constrained minimization problem minw∈C f(w), where
C ⊆Rd is a closed, convex set. Deﬁne an indicator function IC for C as follows: IC(w) := 0, if w ∈C; and
IC(w) := ∞, otherwise. Now, observe the following equivalence
min
w∈C f(w) ⇐⇒min
w∈Rd f(w) + IC(w).
If we solve the RHS using PGD, then it can be easily veriﬁed that the corresponding proximal operator
is equal to the projection operator onto the set C [Tib15]. So, the proximal gradient update step is to
compute the usual gradient and then project it back onto the set C.
• Logistic regression. Here fi is the logistic function, deﬁned as
fi(w) = −yi log

1
1 + e−ui

−(1 −yi) log

e−ui
1 + e−ui

,
where ui = ⟨xi, w⟩, and h = 0. As noted earlier, since h = 0, PGD reduces to GD for logistic regression.
• Ridge regression. Here fi(w) = 1
2(⟨xi, w⟩−yi)2 and h(w) = λ
2 ∥w∥2
2. Since fi’s and h are diﬀerentiable,
we can alternatively solve this simply using GD.
Let X ∈Rn×d denote the data matrix, whose i’th row is equal to the i’th data point xi. For simplicity,
assume that m divides n, and let Xi denote the
n
m × d matrix, whose j’th row is equal to x(i−1) n
m +j. In
a distributed setup, all the data is distributed among m worker machines (worker i has Xi) and master
updates the parameter vector using the update rule (2). At the t’th iteration, master sends wt to all the
workers; worker i computes the gradient (denoted by ∇if(wt)) on its local data and sends it to the master;
master aggregates all the received m local gradients to obtain the global gradient
∇f(wt) = 1
m
m
X
i=1
∇if(wt).
(4)
Now, master updates the parameter vector according to (2) and obtains wt+1. Repeat the process until
convergence.
If full gradients are too costly to compute. Updating the parameter vector in each iteration of PGD
according to (2) requires computing full gradients. This may be prohibitive in large-scale applications, where
each machine in a distributed framework has a lot of data, and computing full gradients at local machines
may be too expensive and becomes the bottleneck. In such scenarios, there are two alternatives to reduce this
per-iteration cost: (i) Coordinate Descent (CD), in which we pick a few coordinates (at random), compute the
partial gradient along those, and descent along those coordinates only, and (ii) Stochastic Gradient Descent
(SGD), in which we sample a data point at random, compute the gradient on that point, and descent along
that direction. These are discussed in Section 2.2 and Section 6.1, respectively.
2.2
Coordinate Descent
For the clear exposition of ideas, we focus on the non-regularized empirical risk minimization from (1) (i.e.,
taking h = 0) for learning a generalized linear model (GLM). This can be generalized to objectives with
(non-)diﬀerentiable regularizers [BKBG11, ST11]. Let X ∈Rn×d denote the data matrix and y ∈Rn the
corresponding label vector. To make it distinct from the last section, we denote the objective function by φ
and write it as φ(Xw; y) to emphasize that we want to learn a GLM, where the objective function depends
5

on the data points only through their inner products with the parameter vector. Formally, we want to
optimize3
min
w∈Rd
 
φ(Xw; y) :=
n
X
i=1
ℓ(⟨xi, w⟩; yi)
!
.
(5)
For U ⊆[d], we write ∇Uφ(Xw; y) to denote the gradient of φ(Xw; y) with respect to wU, where wU denotes
the |U|-length vector obtained by restricting w to the coordinates in U. To make the notation less cluttered,
let φ′(Xw; y) denote the n-length vector, whose i’th entry is equal to ℓ′(⟨xi, w⟩; yi) :=
∂
∂uℓ(u; yi)|u=⟨xi,w⟩.
Note that ∇φ(Xw; y) = XT φ′(Xw; y) and that ∇Uφ(Xw; y) = XT
Uφ′(Xw; y), where XU denotes the n×|U|
matrix obtained by restricting the column indices of X to the elements in U.
Coordinate descent (CD) is an iterative algorithm, where, in each iteration, we choose a set of coordinates
and update only those coordinates (while keeping the other coordinates ﬁxed). In distributed CD, we take
advantage of the parallel architecture to improve the running time of (centralized) CD. In the distributed
setting, we divide the data matrix vertically into m parts and store the i’th part at the i’th worker node.
Concretely, assume, for simplicity, that m divides d. Let X = [X1 X2 . . . Xm] and w = [wT
1 wT
2 . . . wT
m]T ,
where each Xi is an n ×
d
m matrix and each wi is a length
d
m vector. Each worker i stores Xi and is
responsible for updating (a few coordinates of) wi – hence the terminology, model-parallelism. We store the
label vector y at the master node. In coordinate descent, since we update only a few coordinates in each
round, there are a few options on how to update these coordinates in a distributed manner:
Subset of workers: Master picks a subset S ⊂[m] of workers and asks them to update their wi’s [RT16].
This may not be good in the adversarial setting, because if only a small subset of workers are updating their
parameters, the adversary can corrupt those workers and disrupt the computation.
Subset of coordinates for all workers:
All the worker nodes update only a subset of the coordinates
of their local parameter vector wi’s. Master can (deterministically or randomly) pick a subset U (which
may or may not be diﬀerent for all workers) of f ≤d/m coordinates and asks each worker to updates only
those coordinates. If master picks U deterministically, it can cycle through and update all coordinates of the
parameter vector in ⌈d/mf⌉iterations.
In Algorithm 1, we give the distributed CD algorithm with the second approach, where all worker
nodes update the coordinates of their local parameter vectors for a single subset U. We will adopt this
approach in our method to make the distributed CD Byzantine-resilient. Let r =
d
m. For any i ∈[m], let
wi = [wi1 wi2 . . . wir]T and Xi = [Xi1 Xi2 . . . Xir], where Xij is the j’th column of Xi. For any i ∈[m]
and U ⊆[r], let wiU denote the |U|-length vector that is obtained from wi by restricting its entries to the
coordinates in U; similarly, let XiU denote the n × |U| matrix obtained by restricting the column indices of
Xi to the elements in U.
In Algorithm 1, for each worker i to update wi according to (6), where the partial gradient of φ with
respect to wiU is equal to ∇iUφ(Xw; y) = XT
iUφ′(Pm
j=1 Xjwj; y) and worker i has only (Xi, wi), every
other worker j sends Xjwj to the master, who computes φ′(Pm
j=1 Xjwj; y)5 and sends it back to all the
workers. Observe that, even if one worker is corrupt, it can send an adversarially chosen vector to make
the computation at the master deviate arbitrarily from the desired computation, which may adversely aﬀect
the update at all the worker nodes subsequently.6 Similarly, corrupt workers can send adversarially chosen
information to aﬀect the stopping criterion.
3Here we are not optimizing the average of loss functions – since n is a ﬁxed number, this does not aﬀect the solution space.
4After the 1st iteration, worker i need not multiply Xi with wi to obtain Xiwi in every iteration; as only a few coordinates
of wi are updated, it only needs to multiply those columns of Xi that corresponds to the updated coordinates of wi.
5Note that even after computing Xw, master needs access to the labels yi, i = 1, 2, . . . , n to compute φ′(Xw; y). Since
y ∈Rn is just a vector, we can either store that at master, or, alternatively, we can encode y distributedly at the workers and
master can recover that using the method developed in Section 4 for Byzantine-resilient distributed matrix-vector multiplication,
where the matrix is an identity matrix and vector is equal to y.
6Speciﬁcally, suppose the i’th worker is corrupt and the adversary wants master to compute φ′(Xw + e; y) for any arbitrary
vector e ∈Rn of its choice, then the i’th worker can send Xiwi + e to the master.
6

Algorithm 1 Distributed Coordinate Descent
1: Initialize.
Each worker i ∈[m] starts with an arbitrary/random wi ∈Rr, where r =
d
m and, for
simplicity, we assume that m divides d.
2: while (until the stopping criteria at master is not satisﬁed) do
3:
On each worker i ∈[m], do in parallel:
4:
Worker i computes Xiwi and sends it to the master node.4
5:
Worker i receives (U ⊆[r], φ′(Xw; y)) from the master node.
6:
Worker i updates its local parameter vector as (where ∇iUφ(Xw; y) = XT
iUφ′(Xw; y))
wiU ←wiU −α∇iUφ(Xw; y)
(6)
while keeping the other coordinates of wi unchanged, and sends the updated wi to the master.
7:
At Master:
8:
Master receives {Xiwi}i∈[m] from the m workers.
9:
Master ﬁrst computes Xw = Pm
i=1 Xiwi and then computes φ′(Xw; y).
10:
Master picks U ⊆[r] (where U can be picked either randomly or in a round-robin fashion) and sends
(U ⊆[r], φ′(Xw; y)) to all workers.
11: end while
2.3
Adversary Model
We want to perform the distributed computation described in Section 2.1 and Section 2.2 under adversarial
attacks, where the corrupt nodes may provide erroneous vectors to the master node. Our adversarial model
is described next.
In our adversarial model, the adversary can corrupt at most t < m
2 worker nodes7, and the compromised
nodes may collaborate and arbitrarily deviate from their pre-speciﬁed programs. If a worker is corrupt,
then instead of sending the true vector, it may send an arbitrary vector to disrupt the computation. We
refer to the corrupt nodes as erroneous or under the Byzantine attack. We can also handle asynchronous
updates, by dropping the straggling nodes beyond a speciﬁed delay, and still compute the correct gradient
due to encoding.
Therefore we treat updates from these nodes as being “erased”.
We refer to these as
erasures/stragglers. For every worker i that sends a message to the master, we can assume, without loss of
generality, that the master receives ui+ei, where ui is the true vector and ei is the error vector, where ei = 0
if the i’th node is honest, otherwise can be arbitrary. We assume that at most t nodes can be adversarially
corrupt and at most s nodes can be stragglers, where s and t are some constants less than 1
2 that we will
decide later. Note that the master node does not know which t worker nodes are corrupted (which makes
this problem non-trivial to solve), but knows t. We propose a method that mitigates the eﬀects of both of
these anomalies.
Remark 1. A well-studied problem is that of asynchronous distributed optimization, where the workers can
have diﬀerent delays in updates [DB13]. One mechanism to deal with this is to wait for a subset of responses,
before proceeding to the next iteration, treating the others as missing (or erasures) [KSDY17]. Byzantine
attacks are quite distinct from such erasures, as the adversary can report wrong local gradients, requiring
the master node to create mechanisms to overcome such attacks. If the master node simply aggregates the
collected updates as in (4), the computed gradient could be arbitrarily far away from the true one, even with
a single adversary [MGR18].
7Our results also apply to a slightly diﬀerent adversarial model, where the adversary can adaptively choose which of the
t worker nodes to attack at each iteration. However, in this model, the adversary cannot modify the local stored data of the
attacked node, as otherwise, over time, it can corrupt all the data, making any defense impossible.
7

2.4
Our Approach to Gradient Computation
Recall that fi(w) = ℓ(⟨xi, w⟩; yi) for some diﬀerentiable loss function ℓ, and the gradient of fi at w is equal
to ∇fi(w) = (xi)T ℓ′(⟨xi, w⟩; yi), where ℓ′(⟨xi, w⟩; yi) :=
∂
∂uℓ(u; yi)|u=⟨xi,w⟩. Note that ∇fi(w) ∈Rd is a
column vector. Let f ′(w) denote the n-length vector whose i’th entry is equal to ℓ′(⟨xi, w⟩; yi). With this
notation, since f(w) = 1
n
Pn
i=1 fi(w), we have ∇f(w) = 1
nXT f ′(w). Since n is a constant, it is enough to
compute XT f ′(w). So, for simplicity, in the rest of the paper we write
∇f(w) = XT f ′(w),
∀w ∈Rd.
(7)
A natural approach to computing the gradient ∇f(w) is to compute it in two rounds: (i) compute
f ′(w) in the 1st round by ﬁrst multiplying X with w and then master locally computes f ′(w) from Xw
(master can do this locally, because Xw is an n-dimensional vector whose i’th entry is equal to ⟨xi, w⟩and
(f ′(w))i = ℓ′(⟨xi, w⟩; yi));8 and then (ii) compute ∇f(w) = XT f ′(w) in the 2nd round by multiplying XT
with f ′(w). So, the task of each gradient computation reduces to two matrix-vector (MV) multiplications,
where the matrices are ﬁxed and vectors may be diﬀerent each time. To combat against the adversarial
worker nodes, we do both of these MV multiplications using data encoding and real-error correction; see
Figure 1 on page 17 for a pictorial description of our approach.
A two-round approach for gradient computation has been proposed for straggler mitigation in [LLP+18],
but our method for MV multiplication diﬀers from that fundamentally, as we have to provide adversarial
protection. Note that in the case of stragglers/erasures we know who the straggling nodes are, but this infor-
mation is not known in the case of adversarial nodes, and master needs to decode without this information
in the context of Byzantine adversaries. This is slightly diﬀerent from the standard error correcting codes
(over ﬁnite ﬁelds) as the matrix entries in machine learning applications are from reals. In this case, we use
ideas from real-error correction (or sparse reconstruction) from the compressive sensing literature [CT05],
and using which we develop an eﬃcient decoding at master, which also gives rise to our sparse encoding
matrix; see Section 4 for more details. For decoding eﬃciently, we crucially leverage the block error pattern
and design a decoding method at master, which, interestingly, requires just one application of the sparse
recovery method on a vector of size m, the number of workers, which may be much smaller than the data
dimensions n and d, thereby making the decoding computationally eﬃcient. Our encoding matrix (given in
(11), designed for MV multiplication) is very sparse and has a regular pattern of non-zero entries, which also
makes it applicable for making coordinate-descent (CD) Byzantine-resilient. We emphasize that a general-
purpose code for MV multiplication may not be applicable for CD, as each CD iteration requires updating
only a few coordinates of the parameter vector, which makes it fundamentally diﬀerent (and arguably more
complicated to robustify) than GD iterations; see Section 3.2 and Section 5 for more details. Since iterative
algorithms (such as GD and CD) require repeated parameter updates, it is crucial to have a method that
has low computational complexity, both at the worker nodes as well as at the master node, and our coding
solutions for both GD and CD achieve that, in addition to being highly storage eﬃcient; see Theorem 1 for
GD and Theorem 2 for CD.
Coming back to our two-round approach for gradient computations using MV multiplications, for the
1st round, we encode X using a sparse encoding matrix S(1) = [(S(1)
1 )T , . . . , (S(1)
m )T ]T and store S(1)
i X
at the i’th worker node; and for the 2nd round, we encode XT using another sparse encoding matrix
S(2) = [(S(2)
1 )T , . . . , (S(2)
m )T ]T , and store S(2)
i XT at the i’th worker node. Now, in the 1st round of the
gradient computation at w, the master node broadcasts w and the i’th worker node replies with S(1)
i Xw
(a corrupt worker may report an arbitrary vector); upon receiving all the vectors, the master node applies
error-correction procedure to recover Xw and then locally computes f ′(w) as described above. In the 2nd
round, the master node broadcasts f ′(w) and similarly can recover XT f ′(w) (which is equal to the gradient)
at the end of the 2nd round. So, it suﬃces to devise a method for multiplying a vector v to a ﬁxed matrix A
in a distributed and adversarial setting. Since this is a linear operation, we can apply error correcting codes
over real numbers to perform this task. We describe it brieﬂy below.
8Note that even after computing Xw, master needs access to the labels yi, i = 1, 2, . . . , n to compute f′(w). See Footnote 5
for a discussion on how master can get access to the labels.
8

A trivial approach. Take a generator matrix G of any real-error correcting linear code. Encode A as
AT G =: B. Divide the columns of B into m groups as B = [B1 B2 . . . Bm], where worker i stores Bi.
Master broadcasts v and each worker i responds with vT Bi + eT
i , where ei = 0 if the i’th worker is honest,
otherwise can be arbitrary. Note that at most t of the ei’s can be non-zero. Responses from the workers
can be combined as vT B + eT . Since every row of B is a codeword, vT B = vT AT G is also a codeword.
Therefore, one can take any oﬀ-the-shelf decoding algorithm for the code whose generator matrix is G and
obtain vT AT . For example, we can use the Reed-Solomon codes (over real numbers) for this purpose, which
only incurs a constant storage overhead and tolerates optimal number of corruptions (up to < 1
2). Note
that we need fast decoding, as it is performed in every iteration of the gradient computation by the master.
As far as we know, any oﬀ-the-shelf decoding algorithm “over real numbers” requires at least a quadratic
computational complexity, which leads to Ω(n2 + d2) decoding complexity per gradient computation, which
could be impractical.
The trivial scheme does not exploit the block error pattern which we crucially exploit in our coding scheme
to give a ∼O((n + d)m) time decoding per gradient computation, which could be a signiﬁcant improvement
over the trivial scheme, since m typically is much smaller than n and d for large-scale problems. In fact,
our coding scheme enables a trade-oﬀ(in terms of storage and computation/communication overhead at the
master and the worker nodes) with Byzantine adversary tolerance, without compromising the eﬃciency at the
master node. We also want encoding to be eﬃcient (otherwise it defeats the purpose of data encoding) and
our sparse encoding matrix achieves that. Our main result for the Byzantine-resilient distributed gradient
computation is as follows, which is proved in Section 4:
Theorem 1 (Gradient Computation). Let X ∈Rn×d denote the data matrix. Let m denote the total number
of worker nodes. We can compute the gradient exactly in a distributed manner in the presence of t corrupt
worker nodes and s stragglers, with the following guarantees, where ϵ > 0 is a free parameter.
• (s + t) ≤
j
ϵ
1+ϵ · m
2
k
.
• Total storage requirement is roughly 2(1 + ϵ)|X|.
• Computational complexity for each gradient computation:
– at each worker node is O((1 + ϵ) nd
m ).
– at the master node is O((1 + ϵ)(n + d)m).
• Communication complexity for each gradient computation:
– each worker sends
 (1 + ϵ) n+d
m

real numbers.
– master broadcasts (n + d) real numbers.
• Total encoding time is O

nd

ϵ
1+ϵm + 1

.
Remark 2. The statement of Theorem 1 allows for any s and t as long as (s + t) ≤
j
ϵ
1+ϵ · m
2
k
. As we are
handling both erasures and errors in the same way9 the corruption threshold does not have to handle s and
t separately. To simplify the discussion, for the rest of the paper, we consider only Byzantine corruption,
and denote the corrupted set by I ⊂[m] with |I| ≤t, with the understanding that this can also work with
stragglers.
In Theorem 1, ϵ is a design choice and a free parameter that can take any value in the interval [0, m−1],
where ϵ = 0 implies no corruption and ϵ = m −1 implies that corruption threshold t can be anything up to
m−1
2 . If we want to tolerate t corrupt workers, then ϵ must satisfy ϵ ≥
2t
m−2t.10
9When there are only stragglers, one can design an encoding scheme where both the master and the worker nodes oper-
ate oblivious to encoding, while solving a slightly altered optimization problem [KSDY17], in which gradients are computed
approximately, leading to more eﬃcient straggler-tolerant GD.
10We could have written everything in terms of t, m, n, d, but we chose to introduce another variable ϵ which, in our opinion,
clearly brings out the tradeoﬀbetween the corruption threshold and the resource requirements without cluttering the expressions.
9

Remark 3 (Comparison with the plain distributed PGD). We compare the resource requirements of our
method with the plain distributed PGD (which provides no adversarial protection), where all the data points
are evenly distributed among the m workers. In each iteration, master sends the parameter vector w to all
the workers; upon receiving w, all workers compute the gradients on their local data in O( nd
m ) time (per
worker) and send them to the master; master aggregates them in O(md) time to obtain the global gradient
and then updates the parameter vector using (2).
In our scheme (i) the total storage requirement is O(1 + ϵ) factor more;11 (see also Remark 4) (ii) the
amount of computation at each worker node is O(1 + ϵ) factor more; (iii) the amount of computation at the
master node is O((1 + ϵ)(1 + n
d )) factor more, which is comparable in cases where n is not much bigger than
d; (iv) master broadcasts (1 + n
d ) factor more data, which is comparable if n is not much bigger than d; and
(v) each worker sends O

(1 + ϵ) 1+n/d
m

factor more data, which is O(1 + ϵ) – a constant factor – as long as
n = O(dm).
Remark 4. Let m be an even number. Note that we can get the corruption threshold t to be any number less
than m/2, but at the expense of increased storage and computation. For any δ > 0, if we want to get δ close
to m/2, i.e., t = m/2 −δ, then we must have (1 + ϵ) ≥m/2δ. In particular, at ϵ = 2, we can tolerate up to
m/3 corrupt nodes, with constant overhead in the total storage as well as on the computational complexity.
Note that when δ is a constant, i.e., t is close to
m−1
2 , then ϵ grows linearly with m; for example,
if t = m−1
2 , then ϵ = m −1. In this case, our storage redundancy factor is O(m). In contrast, the trivial
scheme (see “trivial approach” on page 9) does better in this regime and has only a constant storage overhead,
but at the expense of an increased decoding complexity at the master, which is at least quadratic in the problem
dimensions d and n, whereas, our decoding complexity at the master always scales linearly with d and n. If
we always want a constant storage redundancy for all values of the corruption threshold t, we can use our
coding scheme if t ≤c · m−1
2 , where c < 1 is a constant, and use the trivial scheme if t is close to m−1
2 .
Our encoding is also eﬃcient and requires O

nd

ϵ
1+ϵm + 1

time. Note that O(nd) is equal to the time
required for distributing the data matrix X among m workers (for running the distributed gradient descent
algorithms without the adversary); and the encoding time in our scheme (which results in an encoded matrix
that provides Byzantine-resiliency) is a factor of (2t + 1) more.
Remark 5. Our scheme is not only eﬃcient (both in terms of computational complexity and storage re-
quirement), but it can also tolerate up to ⌊m−1
2 ⌋corrupt worker nodes (by taking ϵ = m −1 in Theorem 1).
It is not hard to prove that this bound is information-theoretically optimal, i.e., no algorithm can tolerate
⌈m
2 ⌉corrupt worker nodes, and at the same time correctly computes the gradient.
2.5
Our Approach to Coordinate Descent
We use data encoding and add redundancy to enlarge the parameter space. Speciﬁcally, we encode the data
matrix X using an encoding matrix R = [R1 R2 . . . Rm], where each Ri is a d×p matrix (with pm ≥d), and
store XRi at the i’th worker. Deﬁne eXR := XR. Now, instead of solving (5), we solve the encoded problem
arg minv∈Rpm φ( eXRv; y) using Algorithm 1 (together with decoding at the master); see Figure 2 on page 25
for a pictorial description of our algorithm. We design the encoding matrix R such that at every iteration
of our algorithm, updating any (small) subset of coordinates of vi’s (let v = [vT
1 vT
2 . . . vT
m]) automatically
updates some (small) subset of coordinates of w; and, furthermore, by updating those coordinates of vi’s,
we can eﬃciently recover the correspondingly updated coordinates of w, despite the errors injected by the
adversary. In fact, at any iteration t, the encoded parameter vector vt and the original parameter vector wt
satisﬁes vt = R+wt, where R+ := RT (RRT )−1 is the Moore-Penrose pseudo-inverse of R, and wt evolves
in the same way as if we are running Algorithm 1 on the original problem.
11For example, by taking ϵ = 2, our method can tolerate m/3 corrupt worker nodes. So, we can tolerate linear corruption
with a constant overhead in the resource requirement, compared to the plain distributed gradient computation which does not
provide any adversarial protection.
10

We will be eﬀectively updating the coordinates of the parameter vector w in chunks of size (m −2t) or
its integer multiples (where t is the number of corrupt workers). In particular, if each worker i updates k
coordinates of vi, then k(m −2t) coordinates of w will get updated. For comparison, Algorithm 1 updates
km coordinates of the parameter vector w in each iteration, if each worker updates k coordinates in that
iteration.
As described in Algorithm 1 for the Byzantine-free CD, in order to update its local parameter vector wi
according to (6), worker i needs access to φ′(Xw; y), which master computes after receiving {Xjwj}j∈[m]
from the workers. In our Byzantine-resilient algorithm for CD also master will need to compute Xw in every
CD iteration, and for this purpose, we employ the same encoding-decoding procedure for MV multiplication
that we used in the ﬁrst round of gradient computation, as described in Section 2.4. In particular, to make
the notation distinct from gradient computation, in order to compute Xw, we encode X using an encoding
matrix L = [LT
1 LT
2 . . . LT
m]T , where each Li is a p′×n matrix (with p′m ≥n) and worker i stores eXL
i = LiX.
Note that in order to compute Xw, in the ﬁrst round of gradient computation as described in Section 2.4,
master broadcasts w to all the workers and each worker i computes eXL
i w and sends it the the master (corrupt
workers may report arbitrary vectors), who then decodes and obtains Xw. However, in coordinate descent,
though master wants to compute Xw in each CD iteration, we can signiﬁcantly improve the computation
required at each worker: since only a few coordinates of the original parameter vector w are updated in
each CD iteration, master needs to send only those updated coordinates, and workers need to preform MV
multiplication with a much smaller matrix, whose number of columns is equal to the number of updated
coordinates of w that they receive from master. Thus, the computational complexity in each CD iteration
at worker is proportional to the number of coordinates updated in each CD iteration, as desired.
Our main result for the Byzantine-resilient distributed coordinate descent is stated below, which is proved
in Section 5.
Theorem 2 (Coordinate Descent). Under the setting of Theorem 1, our Byzantine-resilient distributed CD
algorithm has the following guarantees, where ϵ > 0 is a free parameter.
• (s + t) ≤
j
ϵ
1+ϵ · m
2
k
.
• Total storage requirement is roughly 2(1 + ϵ)|X|.
• If each worker i updates τ coordinates of vi, then
–
τm
1+ϵ coordinates of the corresponding w gets updated.
– the computational complexity in each iteration
∗at each worker node is O(nτ).
∗at the master node is O((1 + ϵ)nm + τm2).
– the communication complexity in each iteration
∗each worker sends
 τ + (1 + ϵ) n
m

real numbers.
∗master broadcasts

τm
1+ϵ + n

real numbers.
• Total encoding time is O

nd

ϵ
1+ϵm + 1

.
Remark 6 (Comparison with the plain distributed CD). We compare the resource requirements of our
method with the plain distributed CD described in Algorithm 1 that does not provide any adversarial protec-
tion. Let ϵ be any number in the interval [0, m −1] – for illustration, we can take ϵ = 2, which means t ≤m
3
workers are corrupt. In Algorithm 1, if each worker i updates
τ
1+ϵ coordinates of wi (in total τm
1+ϵ coordinates
of w) in each iteration, then (i) each worker requires O( nτ
1+ϵ) time to multiply Xi with the updated part of
wi; (ii) master requires O(nm) time to compute Pm
i=1 Xiwi from {Xiwi}i∈[m]; (iii) each worker sends n real
numbers (required for Xiwi) to master; and (iv) master broadcasts n real numbers (required for φ′(Xw; y)).
11

In our scheme (i) the total storage requirement is O(1+ϵ) factor more; (ii) the amount of computation at
each worker node is O(1+ϵ) factor more; (iii) the amount of computation at the master node is O((1+ϵ)+ τm
n )
factor more – typically, since τ is a constant and number of workers is much less than n, this again could be
O(1 + ϵ); (iv) master broadcasts

1 +
τm
(1+ϵ)n

factor more data, which could be a constant if τm is smaller
than (1 + ϵ)n; and (v) each worker sends

τ
n + (1+ϵ)
nm

factor more data, where the 1st term is much smaller
than 1 as τ is typically a constant, and the 2nd term is close to zero as (1 + ϵ) is always upper-bounded by
m.
Remark 7 (Comparison with the replication-based strategy). One simple way to make Algorithm 1 Byzantine-
resilient is using repetition code, where we ﬁrst divide the set of m workers into
m
2t+1 groups of size (2t + 1)
each and also divide the data matrix as X = [X1 X2 . . . X
m
2t+1 ] (assume, for simplicity, that (2t+1) divides
m). Now, store the i’th block Xi at the (2t + 1) workers in the i’th group of workers. Let the parameter
vector be divided as w = [wT
1 wT
2
. . . wTm
2t+1 ]T . In each CD iteration, the local parameter updates in any
wi is replicated at (2t + 1) diﬀerent workers in the i’th group of workers, and since at most t workers are
corrupt, master can do a majority vote for decoding. Note that the total storage and the computation at
workers in this scheme grow linearly by a factor of (2t+1), where t is the number of corruption, which could
be signiﬁcant. In contrast, the method that we propose can tolerate linear corruption, say, t = m
3 , with a
constant overhead in storage and computational complexity.
The Remarks 2, 4, 5 are also applicable for Theorem 2.
3
Related Work
There has been a signiﬁcant recent interest in using coding-theoretic techniques to mitigate the well-known
straggler problem [DB13], including gradient coding [TLDK17,RTDT18,CP18,HRSH18], encoding compu-
tation [LLP+18, DCG16, DCG19], and data encoding [KSDY17, KSDY19]. However, one cannot directly
apply the methods for straggler mitigation to the Byzantine attacks case, as we do not know which up-
dates are under attack. Distributed computing with Byzantine adversaries is a richly investigated topic
since [LSP82], and has received recent attention in the context of large-scale distributed optimization and
learning [BMGS17, CSX17, CWCP18, YCRB18, AAL18, SX19, XKG19, YCRB19, GV19, RWCP19, LXC+19,
GHYR19, YLR+19, DD20b, DD20a, HKJ20].
These can be divided into three categories: (i) One which
assume explicit statistical models for data across workers (e.g., data drawn i.i.d. from a probability distri-
bution) and analyze gradient descent [CSX17, YCRB18, SX19, YCRB19, GHYR19]. (ii) Other set of works
make no probabilistic assumption on data, and optimize through stochastic methods (e.g., stochastic gra-
dient descent) [BMGS17, AAL18, GV19, XKG19, LXC+19, RWCP19, DD20a, DD20b, HKJ20] and also with
deterministic methods (e.g., gradient descent) [DD20a, DD20b]. Note that none of these two sets of works
do data encoding and work with data as it is, and provide Byzantine resilience by applying some robust
aggregation procedures (e.g., geometric median, coordinate-wise median, outlier-ﬁltering, etc.) at the mas-
ter for aggregating gradients. (iii) Another line of work which is most relevant to ours provide Byzantine
resiliency using redundant computations, either by encoding the gradients [CWCP18] or by encoding the
data itself [YLR+19]. Note that [RWCP19] combines both redundant computations and do a hierarchical
robust aggregation and not is directly comparable to ours.
Note that the statistical nature of data/analysis in the ﬁrst two sets of works leads to a statistical
approximation error in the convergence rates, which is also intensiﬁed by the inaccuracy of the robust gradient
aggregation procedure. One of the main focuses in these works is typically on obtaining faster convergence
(where the goal is to match the convergence rate of plain SGD/GD) and as good an approximation error
as possible. Note that the approximation error in all these works scales at least as Ω(
√
d), where d is the
dimension of the model parameter vector, which may be signiﬁcant in high-dimensional settings. Moreover,
in all these works, since we are not allowed to pre-process the data (such as, doing data encoding, etc.), we
need to make some assumptions on the data, and furthermore, master has to apply a non-trivial decoding
12

for gradient aggregation, which requires signiﬁcantly more time than what our decoding requires.
For
example, ﬁltering-based decoding [SX19, DD20a, DD20b], median-based decoding [CSX17, YCRB18], and
heuristic approaches [BMGS17], all have a super-linear complexity in m – in fact, the ﬁltering-based method
as in [SX19, DD20a, DD20b] (which is the most eﬀective in terms of the approximation error) requires
O(m3d) time. In contrast, our decoding has a linear dependence on both m and d. Note that, unlike the
ﬁrst two categories, the third line of work (to which ours also belongs) gives deterministic guarantees and
work with arbitrary datasets, with no probabilistic assumptions; we elaborate on these and do a detailed
comparison with ours below. We skip the comparison with the ﬁrst two categories, as it would not be a
fair comparison because the underlying setting is diﬀerent – results in the ﬁrst two categories are based on
statistical assumptions on data/algorithm and inaccurate gradient recovery, whereas, results in the third
category make no assumption on the data/algorithm and allow exact gradient recovery.
We want to emphasize that all these works use gradient descent (GD) or stochastic gradient descent
(SGD) as their optimization algorithm, which is a data-parallelization method; in this paper, additionally,
we also use coordinate descent (CD) algorithm for optimization, which is a model-parallelization method
and is preferred over GD in some applications; see Section 1 for more details on this. As will be evident
from Section 5, making CD secure against Byzantine attacks is arguably more intricate than securing GD.
We divide this section into three categories: ﬁrst we compare the redundancy-based methods for GD in
Section 3.1, and then CD in Section 3.2. Since we use matrix-vector (MV) multiplication as a core subroutine
for both GD and CD, we also compare related work on this in Section 3.3.
3.1
Gradient Descent (GD)
In this section, we do a detailed comparison with [CWCP18] and [YLR+19], which are the closest related
works that also combat Byzantine adversaries using redundant computations.
For the sake of comparison, assume that t ≤m−1
2
workers are corrupt. The coding scheme of Chen et
al. [CWCP18], which they called Draco, requires repetition of each data point (2t + 1) times, storing each
copy at diﬀerent workers. This gives the storage redundancy factor of (2t + 1) in Draco, whereas, our
coding method requires storage redundancy factor of 2(1 + ϵ) =
2m
m−2t, which is a constant even if t is a
constant (< 1
2) fraction of m.12 Since each worker in Draco is doing (2t + 1)-factor more computation for
each GD iteration (than simply computing the gradients as in plain distributed GD), the computational cost
at workers also grows by the same factor, which is a signiﬁcant downside of their scheme. In contrast, our
scheme only requires O(
m
m−2t) more computation at worker, which is a constant even if t is a constant (< 1
2)
fraction of m. This signiﬁcantly reduces the computation time at the worker nodes in our scheme compared
to Draco, without sacriﬁcing much on the computation time required by the master node – the decoding at
master in Draco takes O(md) time, whereas, our scheme requires O(
m
m−2t(n + d)m) time, which is a factor
of O(
m
m−2t(1 + n
d )) more than Draco. In high-dimensional settings, where n is not much bigger than d, and
t is a constant (< 1
2) fraction of m, this overhead is constant. Overall, for a constant fraction of corruption,
say, t = m
3 , Draco requires Ω(t) times more storage and computation at workers than our scheme (which
could be signiﬁcant in large-scale settings), and requires Ω(1 + n
d ) times less computation at master. Note
that the computation time at workers scales at least as Ω( nd
m ), which dominates the time taken by master
(since n, d are typically much larger than m), so our scheme will be faster than Draco with respect to the
overall running time. Note that the coding in Draco is restricted to data replication redundancy, as they
encode the gradient as done in [TLDK17], enabling application to (non)-convex problems; in contrast, we
encode the data enabling signiﬁcantly smaller redundancy, and apply it to learn generalized linear models,
and is also applicable to MV multiplication.
12To highlight the storage redundancy gain of our method over that of Draco, consider the following two concrete scenarios,
where the data matrix X ∈Rn×d consists of nd real numbers: (i) In a large setup with m = 1000 worker nodes, if we want
resiliency against t = 100 corrupt nodes (1/10 nodes are corrupt), our method requires redundancy of 2.5, whereas Draco
requires redundancy of 201 (i.e., we need to store only 2.5 × nd real numbers, whereas Draco stores 201 × nd real numbers), a
multiplicative-factor of > 80 more than ours. (ii) In a moderate setup with m = 150 and t = 50 (1/3 nodes are corrupt), the
redundancy of our method is 6, whereas Draco requires redundancy of 101, a multiplicative-factor of ≈17 more than ours.
13

Yu et al. [YLR+19] (which is a concurrent work13) proposes Lagrange coded computing in a distributed
framework to compute any multivariate polynomial of the input data and simultaneously provides resilience
against stragglers, security against adversaries, and privacy of the dataset against collusion of workers. They
leverage the Lagrange polynomial to create computation redundancy among workers, and using standard
Reed-Solomon decoding, they can tolerate both erasures/stragglers and errors/adversaries. Their method
provide privacy by adding random elements from the ﬁeld (which in the case of gradient computation is the
ﬁeld of all matrices of a certain dimension) while doing the polynomial interpolation. This is a standard
method in Shamir secret sharing scheme [Sha79] that is widely used in information-theoretically secure MPC
protocols [CDN15] to provide privacy of users’ data. For the sake of comparison of the resource requirements
of our scheme and the one in [YLR+19], consider the task of linear regression (the concrete machine learning
application studied in [YLR+19]). In the following, we assume that m−1
2
−δ workers are corrupt, which
corresponds to ϵ =
m
1+2δ −1 in our setting; here δ can take any value in [0 : m−1
2 ]. (i) The storage overhead
of our scheme is
m
δ+1/2, whereas, in [YLR+19], it is
m
δ+1, which is roughly the same as ours. For example,
to tolerate m
3 corrupt workers (i.e., δ = m−3
6 ), the storage overhead of our scheme and of [YLR+19] is a
multiplicative factor of 6 and
6
1+3/m ≈6, respectively. (ii) The encoding time complexity of our scheme is
O(nd(m−2δ)), whereas, it is O(m log2(m) nd
δ+1) in [YLR+19]. Note that for constant δ (i.e., corruption close
to 1/2), the encoding time of our scheme is much less (by a factor of O(m log2(m))) than that of [YLR+19],
whereas, for corruption cm, where c <
1
2, the scheme of [YLR+19] takes O(
m
log2(m))-factor less time in
encoding than ours. (iii) The computation time at each worker per gradient computation in both our scheme
and [YLR+19] is roughly the same – ours requires O(
nd
1+2δ) time and [YLR+19] requires O( nd
1+δ) time. (iv) The
decoding time complexity per gradient computation in [YLR+19] is O(m log2(m)d), whereas, ours requires
O((1 + ϵ)(n + d)m) time. Note that when n is not much bigger than d and we want a constant fraction of
corruption, say, m
3 corruption, then their decoding complexity is worse than ours by a logarithmic factor.
Also note that our decoding algorithm is arguably simpler than theirs. (v) For per gradient computation,
each worker respectively sends
n+d
1+2δ and d real numbers in ours and the scheme in [YLR+19]. Note that
if n ≤dm and to tolerate a constant fraction of corruption, say, m
3 corruption, each worker sends roughly
O(m) less data in our scheme than that of [YLR+19]. Overall, if we want tolerance against m
3 corrupt worker
nodes, then both our scheme and the one in [YLR+19] have similar resource requirements, except for that
our scheme has a much better communication complexity (by a factor of O(m)) from workers to the master,
whereas, the encoding time complexity (which is a one-time process) of [YLR+19] is better than ours by a
factor of O(
m
log2(m)).
3.2
Coordinate Descent (CD)
Even for the straggler problem, we are only aware of one work by Karakus et al. [KSDY19] that, in addition
to distributed GD, also studies distributed CD, and that for quadratic problems (e.g., linear/ridge regression)
only. It also does data encoding and achieves low redundancy and low complexity, by allowing convergence to
an approximate rather than exact solution. As far as we know, ours is the ﬁrst work that studies distributed
CD under Byzantine attacks and provides an eﬃcient solution, much better than the replication-based
solution (see Remark 7).
At the heart of our solution for CD is the matrix-vector (MV) multiplication
procedure that we develop in this paper; and it is the speciﬁc regular structure of our encoding matrix
(given in (11), designed for the MV multiplication) that allows for partially updating the coordinates of the
parameter vector in each CD iteration. Note that a general-purpose encoding matrix for MV multiplication
may not be applicable for the CD algorithm.
It has been observed earlier in several works (see, for example, [LLP+18,DCG16]) that gradient compu-
tation in GD for linear regression can be reduced to MV multiplication, and any general-purpose code for
MV multiplication can be used to provide a solution for gradient computation. As far as we know, ours is
the ﬁrst paper that makes the connection of CD and MV multiplication, and provides an eﬃcient solution
13Yu et al. [YLR+19] is concurrent to our conference versions in Allerton 2018 [DSD18] and ISIT 2019 [DSD19, DD19], on
which this paper is based.
14

for CD (which is also resilient to Byzantine attacks) for learning generalized linear models. Note that, unlike
GD, not any general-purpose code for MV multiplication can be used for CD: the main challenge in CD
comes from the fact that we only update a small number of coordinates of the parameter vector in each
CD iteration; when we encode the data and iteratively update some coordinates of the (encoded) parameter
vector using the encoded data, we need to make sure that this update in the encoded parameter vector
is reconciled with the update in the original parameter vector. This is fundamentally diﬀerent from GD
iterations. See Section 5 for more details.
3.3
Matrix-Vector Multiplication
For the task of a more fundamental problem of matrix-vector (MV) multiplication in the presence of Byzan-
tine adversaries, which is at the core of the optimization algorithms in this paper, we are only aware of two
concurrent works [YLR+19] (see Footnote 13) and [DCG19]14 that provide (coding-theoretic) solutions to
this problem. In the following, we do a detailed comparison of our solution with both of these works and
also discuss the (dis)similarities.
We have already done a detailed comparison with Yu et al. [YLR+19] (concurrent work, see Footnote 13)
with respect to gradient descent in Section 3.1. For the problem of MV multiplication, the storage require-
ment, computation time per worker, and communication complexity to/from workers is the same in both
ours and [YLR+19]. The comparison of encoding time complexity is same as above; however, for a constant
corruption, say, m
3 corrupt workers, our method outperforms the one in [YLR+19] in terms of the decoding
time complexity by a factor of O(log2(m)). Note that, unlike [YLR+19], we make a fundamental connection
of handling Byzantine errors with the sparse reconstruction (or the real-error correction) problem from the
compressive sensing literature [CT05].
Dutta et al. [DCG19] (concurrent work, see Footnote 14) focuses on matrix-vector (MV) multiplication.
Though their main focus is on providing resilience against stragglers, they also mention that handling
stragglers is very diﬀerent than handling errors, as it requires to correct errors over real numbers, and,
unlike stragglers, we do not know which workers are corrupt. Similar to our observation, they also note
that since the matrices and vectors have entries from real numbers, the decoding problem reduces to the
sparse reconstruction problem from the compressive sensing literature [CT05] and they also provide such a
reduction. Apart from these similarities, our solution for MV multiplication diﬀers from that of [DCG19] in
several important ways: (i) [DCG19] provides a detailed solution to the distributed MV multiplication for the
straggler problem for the case when the number of rows in the matrix is smaller than the number of workers
nodes. As mentioned in [DCG19], this method can be easily generalized to the more general case when
the matrix is of arbitrary dimension, in which case, ﬁrst we can divide the rows of the matrix into several
sub-matrices, each having number of rows smaller than the number of workers, and then apply the above
method independently to each sub-matrix. This simple extension may work (without losing eﬃciency) for
the straggler/erasure problem, however, leads to a highly ineﬃcient solution for the adversary/error problem.
The reason being that, in the presence of Byzantine workers, if we solve the sparse reconstruction problem
for each sub-matrix separately, this would be ineﬃcient, as the decoding would then be computationally
expensive. To remedy this, we exploit the block error pattern and use a simple idea of linearly combining the
response vectors from each worker using coeﬃcients drawn from an absolutely continuous distribution, so
that we only need to do just one computation for solving the sparse construction problem. This signiﬁcantly
reduces the decoding complexity; see Section 4.1 for details.
(ii) [DCG19] only shows a connection to
the sparse recovery problem, whereas, we provide a complete solution, with a concrete sparse recovery (or
real-error correction) matrix and resource (encoding/decoding time, storage, communication) requirement
analysis. (iii) Our encoding matrix (given in (11)) to encode data matrices of arbitrary dimensions is very
sparse and highly structured which allows us to apply that construction to CD algorithm, which, as far we
know, has not been connected with MV multiplication before. Also, ours is the ﬁrst paper that provides a
non-trivial and eﬃcient (data encoding) solution to CD in the presence of a Byzantine adversary. (iv) We
14The conference version [DCG16] only studies the straggler problem, and the journal version [DCG19] brieﬂy mentions how
their results from [DCG16] can be extended to handle adversarial nodes, and we describe that in this section.
15

also want to mention that the focus in [DCG19] is on making the encoded matrix sparse (at the expense
of increased computation at workers) so that workers need to compute shorter dot products, whereas, in
this paper, we make the encoding matrix sparse (much sparser than the encoded matrix of [DCG19]) to get
eﬃcient encoding/decoding.
4
Our Solution to Gradient Computation
In this section, we describe the core technical part of our two-round approach for gradient computation
described in Section 2.4 – a method for performing matrix-vector (MV) multiplication in a distributed
manner in the presence of a malicious adversary who can corrupt at most t of the m worker nodes. Here,
the matrix is ﬁxed and we want to right-multiply a vector with this matrix.
Given a ﬁxed matrix A ∈Rnr×nc and a vector v ∈Rnc, we want to compute Av in a distributed manner
in the presence of at most t corrupt worker nodes; see Section 2.3 for details on our adversary model. Our
method is based on data encoding and error correction over real numbers, where the matrix A is encoded and
distributed among all the worker nodes, and the master node recovers the MV product Av using real-error
correction; see Figure 1. We will think of our encoding matrix as S = [ST
1 ST
2 , . . . , ST
m], where each Si is
a p × nr matrix and pm ≥nr. We will derive the matrix S in Section 4.2. For the value of p, looking
ahead, we will set p = ⌈
n
m−2t⌉, which is a constant multiple of n
m even if t is a constant (< 1
2) fraction of m
(e.g., if t = m
3 , we would have p = 3n
m ). For i ∈[m], we store the matrix SiA at the i’th worker node. As
described in Section 2, the computation proceeds as follows: The master sends v to all the worker nodes and
receives {SiAv + ei}m
i=1 back from them. Let ei = [ei1, ei2, . . . , eip]T for every i ∈[p]. Note that ei = 0 if
the i’th node is honest, otherwise can be arbitrary. In order to ﬁnd the set of corrupt worker nodes, master
equivalently writes {SiAv + ei}m
i=1 as p systems of linear equations.
˜hi(v) = ˜SiAv + ˜ei,
i ∈[p]
(8)
where, for every i ∈[p], ˜ei = [e1i, e2i, . . . , emi]T , and ˜Si is an m × nr matrix whose j’th row is equal to
the i’th row of Sj, for every j ∈[m]. Note that at most t entries in each ˜ei are non-zero. Observe that
{SiAv + ei}m
i=1 and {˜SiAv + ˜ei}p
i=1 are equivalent systems of linear equations, and we can get one from the
other.
Note that ˜Si’s constitute the encoding matrix S, which we have to design. In the following, we will
design these matrices ˜Si’s (which in turn will determine the encoding matrix S), with the help of another
matrix F, which will be used to ﬁnd the error locations, i.e., identities of the compromised worker nodes. We
will design the matrix F (of dimension k × m, where k < m – here k is determined by the error-correction
capability, and we will set k = 2t; see Section 4.4 for more details) and the matrices ˜Si’s such that
C.1 F˜Si = 0 for every i ∈[p].
C.2 For any t-sparse u ∈Rm, we can eﬃciently ﬁnd all the non-zero locations of u from Fu.
C.3 For any T ⊂[m] such that |T | ≥(m −t), let ST denote the |T |p × nr matrix obtained from S by
restricting it to all the Si’s for which i ∈T . We want ST to be of full column rank.
If we can ﬁnd such matrices, then we can recover the desired MV multiplication Av exactly: brieﬂy, C.1
and C.2 will allow us to locate the corrupt worker nodes; once we have found them, we can discard all the
information that the master node had received from them. This will yield ST Av, where ST is the |T |p × nr
matrix obtained from S by restricting it to Si’s for all i ∈T , where T is the set of all honest worker nodes.
Now, by C.3, since ST is of full column rank, we can recover Av from ST Av exactly. Details follow.
Suppose we have matrices F and ˜Si’s such that C.1 holds. Now, multiplying (8) by F yields
fi := F˜hi(v) = F˜ei,
(9)
for every i ∈[p], where ∥˜ei∥0 ≤t. In Section 4.1, we give our approach for ﬁnding all the corrupt worker
nodes with the help of any error locator matrix F. Then, in Section 4.2, we give a generic construction for
16

w
M broadcasts w
M
Dec
W1
S(1)
1 X
W2
S(1)
2 X
W3
S(1)
3 X
Wm
S(1)
m X
S(1)
1 Xw
S(1)
2 Xw + e2
S(1)
3 Xw
S(1)
m Xw + em
Xw
Compute
f ′(w)
f ′(w)
M broadcasts f ′(w)
M
Dec
W1
S(2)
1 XT
W2
S(2)
2 XT
W3
S(2)
3 XT
Wm
S(2)
m XT
S(2)
1 XTf ′(w)
S(2)
2 XTf ′(w)
S(2)
3 XTf ′(w) + e3
S(2)
m XTf ′(w) + em
∇f(w) = XTf ′(w)
w ←−proxh,α(w −α∇f(w))
Figure 1 This ﬁgure shows our 2-round approach to the Byzantine-resilient distributed gradient descent to optimize (1) for
learning a generalized linear model. Since the gradient at w is equal to ∇f(w) = XT f′(w) (see (7)), we compute it in 2 rounds,
using a matrix-vector (MV) multiplication as a subroutine in each round. In the 1st round, ﬁrst we compute Xw, and then
compute f′(w) from Xw – since the j’th entry of Xw is equal to ⟨xj, w⟩, we can compute f′(w) from Xw (see Section 2.4).
In the 2nd round we compute XT f′(w) – which is equal to ∇f(w) – using another application of MV multiplication. For a
matrix A and a vector v, to make our distributed MV multiplication Av Byzantine-resilient, we encode A using a sparse matrix
S = [ST
1 ST
m . . . ST
m]T and distribute SiA to worker i (denoted by Wi). Note that in the ﬁrst round, we have A = X, v = w,
and we encode X using S(1), and in the second round, we have A = XT , v = f′(w), and encode XT using S(2). The adversary
can corrupt at most t workers (the compromised ones are denoted in red color), potentially diﬀerent sets of t workers in diﬀerent
rounds. The master node (denoted by M) broadcasts v to all the workers. Each worker performs the local MV product and
sends it back to M. If Wi is corrupt, then it can send an arbitrary vector. Once the master has received all the vectors (out of
which t may be erroneous), it sends them to the decoder (denoted by Dec), which outputs the correct MV product Av.
designing ˜Si’s (and, in turn, our encoding matrix S) such that C.1 and C.3 hold. In Section 4.3, we show
how to compute the desired matrix-vector product Av eﬃciently, once we have discarded all the data from
the corrupt works nodes. Then, in Section 4.4, we will give details of the error locator matrix F that we use
in our construction.
Remark 8. As we will see in Section 4.2, the structure of our encoding matrix S is independent of our error
locator matrix F. Speciﬁcally, the repetitive structure of the non-zero entries of S as well as their locations
will not change irrespective of what the F matrix is. This makes our construction very generic, as we can
choose whichever F suits our needs the best (in terms of how many erroneous indices it can locate and with
what decoding complexity), and it won’t aﬀect the structure of our encoding matrix at all – only the non-zero
entries might change, neither their repetitive format, nor their locations!
4.1
Finding The Corrupt Worker Nodes
Observe that supp(˜ei) may not be the same for all i ∈[p], but we know, for sure, that the non-zero locations
in all these error vectors occur within the same set of t locations. Let I = Sp
i=1 supp(˜ei), which is the set
of all corrupt worker nodes. Note that |I| ≤t. We want to ﬁnd this set I eﬃciently, and for that we note
the following crucial observation. Since the non-zero entries of all the error vectors ˜ei’s occur in the same
set I, a random linear combination of ˜ei’s has support equal to I with probability one, if the coeﬃcients
of the linear combination are chosen from an absolutely continuous probability distribution. This idea has
appeared before in [ME08] in the context of compressed sensing for recovering arbitrary sets of jointly sparse
17

signals that have been measured by the same measurement matrix.
Deﬁnition 1. A probability distribution is called absolutely continuous, if every event of measure zero occurs
with probability zero.
It is well-known that a distribution is absolutely continuous if and only if it can be represented as an
integral over an integrable density function [Bil95, Theorem 31.8, Chapter 6]. Since Gaussian and uniform
distributions have an explicit integrable density function, both are absolutely continuous. Conversely, dis-
crete distributions are not absolutely continuous. Now we state a lemma from [ME08] that shows that a
random linear combination of the error vectors (where coeﬃcients are chosen from an absolutely continuous
distribution) preserves the support with probability one.
Lemma 1 ([ME08]). Let I = Sp
i=1 supp(˜ei), and let ˆe = Pp
i=1 αi˜ei, where αi’s are sampled i.i.d. from an
absolutely continuous distribution. Then with probability 1, we have supp(ˆe) = I.
From (9) we have fi = F˜ei for every i ∈[p]. Take a random linear combination of fi’s with coeﬃcients
αi’s chosen i.i.d. from an absolutely continuous distribution, for example, the Gaussian distribution. Let
˜f = αi (Pp
i=1 fi) = αi (Pp
i=1 F˜ei) = F (Pp
i=1 αi˜ei) = F˜e, where ˜e = Pp
i=1 αi˜ei. Note that, with probability
1, supp(˜e) is equal to the set of all corrupt worker nodes, and we want to ﬁnd this set eﬃciently. In other
words, given F˜e, we want to ﬁnd supp(˜e) eﬃciently. For this, we need to design a k × m matrix F (where
k < m) such that for any sparse error vector e ∈Rm, we can eﬃciently ﬁnd supp(e) from f = Fe. Many
such matrices have been known in the literature that can handle diﬀerent levels of sparsity with varying
decoding complexity. We can choose any of these matrices depending on our need, and this will not aﬀect
the design of our encoding matrix S. In particular, we will use a k × m Vandermonde matrix along with the
Reed-Solomon type decoding, which can correct up to k/2 errors and has decoding complexity of O(m2);
see Section 4.4 for details.
Time required in ﬁnding the corrupt worker nodes.
The time taken in ﬁnding the corrupt worker
nodes is equal to the sum of the time taken in the following 3 tasks. (i) Computing F˜ei for every i ∈[p]:
Note that we can get F˜ei by multiplying (8) with F. Since F is a k × m matrix, and we compute F˜hi(v) for
p systems, this requires O(pkm) time. (ii) Taking a random linear combination of p vectors each of length
m, which takes O(pm) time. (iii) Applying Lemma 2 (in Section 4.4) once to ﬁnd the error locations, which
takes O(m2) time. Since p is much bigger than m, the total time complexity is O(pkm).
4.2
Designing The Encoding Matrix S
Now we give a generic construction for designing ˜Si’s such that C.1 and C.3 hold. Fix any k × m matrix F
such that we can eﬃciently ﬁnd e from Fe, provided e is suﬃciently sparse. We can assume, without loss
of generality, that F has full row-rank; otherwise, there will be redundant observations in Fe that we can
discard and make F smaller by discarding the redundant rows. Let N(F) ⊂Rm denote the null-space of
F. Since rank(F) = k, dimension of N(F) is q = (m −k). Let {b1, b2, . . . , bq} be a basis of N(F), and let
bi = [bi1 bi2 . . . bim]T , for every i ∈[q]. We set bi’s the columns of the following matrix F⊥:
F⊥=


b11
b21
. . .
bq1
b12
b22
. . .
bq2
...
...
...
...
b1m
b2m
. . .
bqm


m×q
(10)
The following property of F⊥will be used for recovering the MV product in Section 4.3.
Claim 1. For any subset T ⊂[m], such that |T | ≥(m −t), let F⊥
T be the |T | × q matrix, which is equal to
the restriction of F⊥to the rows in T . Then F⊥
T is of full column rank.
18

Proof. Note that q = m −k, where k = 2t. So, if we show that any q rows of F⊥are linearly independent,
then, this in turn will imply that for every T ⊂[m] with |T | ≥(m −t), the sub-matrix F⊥
T will have full
column rank. In the following we show that any q rows of F⊥are linearly independent. To the contrary,
suppose not; and let T ′ ⊂[m] with |T ′| = q be such that the q ×q matrix F⊥
T ′ is not a full rank matrix. This
implies that there exists a non-zero c′ ∈Rq such that F⊥
T ′c′ = 0. Let b = F⊥c′. Note that b ̸= 0 (because
columns of F⊥are linearly independent) and also that ∥b∥0 ≤m −q = k. Now, since FF⊥= 0, we have
Fb = 0, which contradicts the fact that any k columns of F are linearly independent.
Now we design ˜Si’s. For i ∈[p], we set ˜Si as follows:
˜Si =


0
. . .
0
b11
b21
. . .
bl1
0
. . .
0
0
. . .
0
b12
b22
. . .
bl2
0
. . .
0
...
...
...
...
...
...
...
...
...
...
0
. . .
0
b1m
b2m
. . .
blm
0
. . .
0


where l = q if i < p; otherwise l = nr −(p −1)q. The ﬁrst (i −1)q and the last nr −[(i −1)q + l] columns
of ˜Si are zero. This also implies that the number of rows in each Si is p = ⌈nr/q⌉.
Claim 2. For every i ∈[p], we have F˜Si = 0.
Proof. By construction, the null-space of F is N(F) = span{b1, b2, . . . , bq}, which implies that Fbi = 0, for
every i ∈[q]. Since all the columns of ˜Si’s are either 0 or bj for some j ∈[q], the claim follows.
The above constructed matrices ˜Si’s give the following encoding matrix Si for the i’th worker node:
Si =


b1i . . . bqi
...
b1i . . . bqi
b1i . . . bli


p×nr
(11)
All the unspeciﬁed entries of Si are zero. The matrix Si is for encoding the data for worker i. By stacking
up the Si’s on top of each other gives us our desired encoding matrix S.
To get eﬃcient encoding, we want S to be as sparse as possible. Since S is completely determined by
F⊥, whose columns are the basis vectors of N(F), it suﬃces to ﬁnd a sparse basis for N(F). It is known
that ﬁnding the sparsest basis for the null-space of a matrix is NP-hard [CP86]. Note that we can always
ﬁnd the basis vectors of N(F) by reducing F to its row-reduced-echelon-form (RREF) using the Gaussian
elimination [HK71]. This will result in F⊥whose last q rows forms a q × q identity matrix. Note that
q = m −k, where k = 2t. So, if the corruption threshold t is very small as compared to m, the F⊥that
we obtain by the RREF will be very sparse – only the ﬁrst 2t rows may be dense. Since computing S is
equivalent to computing F⊥, and we can compute F⊥in O(k2m) time using the Gaussian elimination, the
time complexity of computing S is also O(k2m).
Now we prove an important property of the encoding matrix S that will be crucial for recovery of the
desired matrix-vector product.
Claim 3. For any T ⊂[m] such that |T | ≥(m −t), let ST denote the |T |p × nr matrix obtained from S by
restricting it to all the blocks Si’s for which i ∈T . Then ST is of full column rank.
Proof. For i ∈[p −1], let Bi = [(i −1)q + 1 : iq] and Bp = [(p −1)q + 1 : nr −(p −1)q], where we see Bi’s as
a collection of some column indices. Consider any two distinct i, j ∈[p]. It is clear that for any two vectors
u1 ∈Bi, u2 ∈Bj, we have supp(u1) ∩supp(u2) = φ, which means that all the columns in distinct Bi’s are
linearly independent. So, to prove the claim, we only need to show that the columns within the same Bi’s
are linearly independent. Fix any i ∈[p], and consider the |T |p × q sub-matrix S(i)
T
of ST , which is obtained
19

by restricting ST to the columns in Bi. There are precisely |T | non-zero rows in S(i)
T , which are equal to the
rows of the matrix F⊥
T deﬁned in Claim 1. We have already shown in the proof of Claim 1 that F⊥
T is of full
column rank. Therefore, S(i)
T
is also of full column rank. This concludes the proof of Claim 3.
Since ST is of full column rank, in principle, we can recover any vector u ∈Rnr from ST u. In the next
section, we show an eﬃcient way for this recovery.
4.3
Recovering The Matrix-Vector Product Av
Once the master has found the set I of corrupt worker nodes, it discards all the data received from them.
Let T = [m] \ I = {i1, i2, . . . , if} be the set of all honest worker nodes, where f = (m −|I|) ≥(m −t). Let
r = [rT
1 rT
2 . . . rT
m], where ri = SiAv + ei. All the ri’s from the honest worker nodes can be written as
rT = ST Av,
(12)
where ST is as deﬁned in Claim 3, and rT is also deﬁned analogously and equal to the restriction of r to all
the ri’s for which i ∈T . Since ST has full column rank (by Claim 3), in principle, we can recover Av from
(12). Next we show how to recover Av eﬃciently, by exploiting the structure of S.
Let ˜rj = [ri1j, ri2j, . . . , rif j]T , for every j ∈[p]. The repetitive structure of Si’s (see (11)) allows us to
write (12) equivalently in terms of p smaller systems.
˜rj = Fj(Av)Bj,
for j ∈[p],
(13)
where, for j ∈[p −1], Bi = [(i −1)q + 1 : iq] and Fj = F⊥
T , and Bp = [(p −1)q + 1 : nr −(p −1)q] and
Fp is equal to the restriction of F⊥
T to its ﬁrst (nr −(p −1)q) columns. Since F⊥
T has full column rank (by
Claim 1), we can compute (Av)Bi for all i ∈[p], by multiplying (13) by F+
j = (FT
j Fj)−1FT
j , which it called
the Moore-Penrose inverse of Fj. Since Av = [(Av)T
B1, (Av)T
B2, . . . , (Av)T
Bp)]T , we can recover the desired
MV product Av.
Time Complexity analysis. The task of obtaining Av from ST Av reduces to (i) computing F+
j = (F⊥
T )+
once, which takes O(q2|T |) time naïvely; (ii) computing F+
p once, which takes at most O(q2|T |) time naïvely;
and (iii) computing the MV products F+
j ˜rj for every j ∈[p], which takes O(pq|T |) time in total. Since p is
much bigger than q, the total time taken in recovering Av from ST Av is O(pq|T |) = O(pm2).
4.4
Designing The Error Locator Matrix F
In this section, we design a k × m matrix F (where k < m) such that for any sparse error vector e ∈Rm, we
can uniquely and eﬃciently recover e (and, therefore, supp(e)) from the under-determined system of linear
equations f = Fe ∈Rk. This is related to the sparse representation problem, where one would like to ﬁnd
the sparsest representation of f in terms of the linear combination of the columns of F, i.e., minimizing
∥e∥0 subject to the constraint that f = Fe. This problem is of combinatorial nature and is known to be
NP-hard [CT05]. To make this problem computationally tractable, Candes and Tao [CT05] showed that if F
satisﬁes a certain regularity condition (which they named the restricted isometry property (RIP)), then the
sparsest reconstruction problem can be reduced to minimizing ∥e∥1 := Pm
i=1 |ei| subject to the constraint
that f = Fe, which can be eﬃciently solved using a linear program.
They also showed that a random
Gaussian matrix satisﬁes the RIP condition. A common problem with such random constructions is that
they may not work with small block-lengths (in our setting, m is the number of workers which may not be a
big number), and can only correct a constant fraction of errors, where the constant is very small. We need
a deterministic construction that can handle a constant fraction (ideally up to 1/2) of errors and that works
with small block-lengths.
Akçakaya and Tarokh [AT08] proposed an eﬃcient solution to the sparse representation problem using
Vandermonde matrices. To construct them, take m distinct non-zero elements z1, z2, . . . , zm from R, and
20

consider the following k × m Vandermonde matrix F.
F =


1
1
1
. . .
1
z1
z2
z3
. . .
zm
z2
1
z2
2
z2
3
. . .
z2
m
...
...
...
...
...
zk−1
1
zk−1
2
zk−1
3
. . .
zk−1
m


k×m
(14)
For the above F, it was shown in [AT08] that, if |supp(e)| ≤k/2, then the Reed-Solomon type decoding
can be used for exact reconstruction of e from f = Fe.15 Furthermore, their decoding algorithm is eﬃcient
and runs in O(m2) time. The results in [AT08] are given for complex vector spaces, and they hold over real
numbers also. Below we state the sparse recovery result (specialized to reals) from [AT08].
Lemma 2 ([AT08]). Let F be the k × m matrix as deﬁned in (14). Let e ∈Rm be an arbitrary vector with
|supp(e)| ≤k/2. We can exactly recover the vector e from f = Fe in O(m2) time.
Note that F is a k × m matrix, where k < m. Choosing k is in our hands, and larger the k, more the
number of errors we can correct (but at the expense of increased storage and computation); see Section 4.5
for more details.
4.5
Resource Requirement Analysis
In this section, we analyze the total amount of resources (storage, computation, and communication) required
by our method for computing gradients in the presence of t (out of m) adversarial worker nodes and prove
Theorem 1. Fix an ϵ > 0. Let the corruption threshold t satisfy t ≤⌊(ϵ/(1 + ϵ)) · (m/2)⌋.
As described earlier in Section 2.4, we compute the gradient ∇f(w) = XT f ′(w) in two-rounds; and in
each round we use the Byzantine-tolerant MV multiplication, which we have developed in Section 4, as a
subroutine; see Figure 1 for a pictorial representation of our scheme. We encode X to compute f ′(w) in
the 1st round: ﬁrst compute Xw using MV multiplication and then locally compute f ′(w). To compute
XT f ′(w) (which is equal to the gradient) in the 2nd round, we encode XT and compute XT f ′(w). Let S(1)
and S(2) be the encoding matrices of dimensions p1m × n and p2m × d, respectively, to encode X and XT ,
respectively. Here, p1 = ⌈n/q⌉and p2 = ⌈d/q⌉, where q = m −k. Since k = 2t (by Lemma 2), we have
q = (m −k) ≥m/(1 + ϵ).
4.5.1
Storage Requirement
Each worker node i stores two matrices S(1)
i X and S(2)
i XT . The ﬁrst one is a p1 × (d + 1) matrix, and
the second one is a p2 × n matrix. So, the total amount of storage at all worker nodes is equal to storing
(p1(d + 1) + p2n) × m real numbers. Since p1 ≤⌈(1 + ϵ) n
m⌉and p2 ≤⌈(1 + ϵ) d
m⌉, the total storage is
 p1(d + 1) + p2n

m = p1m(d + 1) + p2mn
< [(1 + ϵ)n + m](d + 1) + [(1 + ϵ)d + m]n
= (1 + ϵ)n(2d + 1) + m(n + d + 1).
where the ﬁrst term is roughly equal to a 2(1 + ϵ) factor more than the size of X. Note that the second term
does not contribute much to the total storage as compared to the ﬁrst term, because the number of worker
nodes m is much smaller than both n and d. In fact, if m −k divides both n and d, then the second term
vanishes. Since |X| is an n × d matrix, the total storage at each worker node is almost equal to 2(1 + ϵ) |X|
m ,
which is a constant factor of the optimal, that is, |X|
m , and the total storage is roughly equal to 2(1 + ϵ)|X|.
15Note that, since any k columns of F (which is the Vandermonde matrix) are linearly independent, if there exists a vector
e such that |supp(e)| ≤k/2 and e satisﬁes f = Fe for a ﬁxed f, then e is unique.
21

4.5.2
Computational Complexity
We can divide the computational complexity of our scheme as follows:
• Encoding the data matrix. Since, for every i ≤k and j > k, the total number of non-zero entries in S(1)
i
and S(1)
j
are at most n and p1, respectively (see Section 4.2 for details), the computational complexity for
computing S(1)
i X for each i ≤k, and S(1)
j X for each j > k, is O(nd) and O(p1d), respectively. So, the
encoding time for computing S(1)X is equal to O (k(nd) + (m −k)(p1d)) = O

(
ϵ
1+ϵm + 1)nd

. Similarly,
we can show that the encoding time for computing S(2)XT is also equal to O

(
ϵ
1+ϵm + 1)nd

. Note that
computing S(1) and S(2) take O(k2m) time each, which is much smaller, as compared to the encoding
time. So, the total encoding time is O

(
ϵ
1+ϵm + 1)nd

. Note that this encoding is to be done only once.
• Computation at each worker node. In the ﬁrst round, upon receiving w from the master node, each worker
i computes (S(1)
i X)w, and reports back the resulting vector. Similarly, in the second round, upon receiving
f ′(w) from the master node, each worker i computes (S(2)
i XT )f ′(w), and reports back the resulting vector.
Since S(1)
i X and S(2)
i XT are p1 × (d + 1) and p2 × n matrices, respectively, each worker node i requires
O(p1d + p2n) = O((1 + ϵ) nd
m ) time.
• Computation at the master node. The total time taken by the master node in both the rounds is the sum
of the time required in (i) ﬁnding the corrupt worker nodes in the 1st and 2nd rounds, which requires
O(p1km) and O(p2km) time, respectively (see Section 4.1), (ii) recovering Xw from S(1)
T Xw in the 1st
round, which requires O(p1m2) time, (iii) computing f ′(w) from Xw, which takes O(n) time, and (iv)
recovering XT f ′(w) from S(2)
T XT f ′(w) in the 2nd round, which requires O(p2m2) time (see Section 4.3).
Since k < m, the total time is equal to O((p1 + p2)m2) = O((1 + ϵ)(n + d)m).
4.5.3
Communication Complexity
In each gradient computation, (i) master broadcasts (n + d) real numbers, d in the ﬁrst round and n in the
second round; and (ii) each worker sends
 (1 + ϵ) n+d
m

real numbers to master, (1 + ϵ) n
m in the ﬁrst round
and (1 + ϵ) d
m in the second round.
5
Our Solution to Coordinate Descent
In this section, we give a solution to the distributed coordinate descent (CD) under Byzantine attacks and
prove Theorem 2. To make our notation simpler, we remove the dependence on the label vector y in the
problem expression (5) and rewrite it as follows (this is without loss of generality in the light of Footnote 5
and Algorithm 1):
arg min
w∈Rd φ(Xw) :=
n
X
i=1
ℓ(⟨xi, w⟩).
(15)
We want to optimize (15) using distributed CD, described in Section 2.2. As outlined in Section 2.5, we use
data encoding and error correction over real numbers for that. To combat the eﬀect of adversary, we add
redundancy to enlarge the parameter space. Let eXR = XR, where R = [R1 R2 . . . Rm] ∈Rd×pm with
pm ≥d, and each Ri is a p × d matrix. We will determine the encoding matrix R later, after describing
what properties we want from it. For the value of p, looking ahead, when t is the number of corrupt workers,
we will choose p =
d
m−2t, which is a constant multiple of
d
m even if t is a constant fraction (< 1
2) of m (e.g.,
for t = m
3 , we have p = 3d
m ). We consider R’s which are of full row-rank. Let R+ := RT (RRT )−1 denote its
Moore-Penrose inverse such that RR+ = Id, where Id is the d × d identity matrix. Note that R+ is of full
22

column-rank. Let v = R+w be the transformed vector, which lies in a larger (than d) dimensional space.
Let R+ = [(R+
1 )T (R+
2 )T . . . (R+
m)T ]T , where each R+
i := (R+)i is a p × d matrix. With this, by letting
v = [vT
1 vT
2
. . . vT
m]T , we have that vi = R+
i w for every i ∈[m]. Now, consider the following modiﬁed
problem over the encoded data.
arg min
v∈Rpm φ( eXRv).
(16)
Observe that, since R is of full row-rank, minw∈Rd φ(Xw) is equal to minv∈Rpm φ( eXRv); and from an
optimal solution to one problem we can obtain an optimal solution to the other problem. We design an
encoding/decoding scheme such that when we optimize the encoded problem (16) using Algorithm 1, the
vector v that we get in each iteration is of the form v = R+w for some vector w ∈Rd.16 In fact, our
encoding/decoding will ensure that the w for which v = R+w would be equal to the original parameter
vector in that iteration if we had run Algorithm 1 to solve (15). We need this property because in any CD
iteration t, we need access to the original parameter vector wt (such that vt = R+wt) to facilitate the local
parameter updates of vt
1, . . . , vt
m at the workers. See the paragraph after (18) for more details.
Now, instead of solving (15), we solve its encoded form (16) using Algorithm 1 (with decoding at the
master), where each worker i stores eXR
i = XRi and is responsible for updating (some coordinates of) vi.
In the following, let U ⊆[p] be a ﬁxed arbitrary subset of [p]. Let v0 := R+w0 for some w0 at time t = 0.
Suppose, at the beginning of the t’th iteration, we have vt = R+wt for some wt, and each worker i updates
vt
iU according to
vt+1
iU
= vt
iU −αt∇iUφ( eXRvt),
(17)
where ∇iUφ( eXRvt) = ( eXR
iU)T φ′( eXRvt). Recall that each Ri is a d × p matrix, and each R+
i := (R+)i is a
p×d matrix. We denote by RiU the d×|U| matrix obtained by restricting the columns of Ri to the elements
of U. Analogously, we denote by R+
iU := (R+)iU the |U| × d matrix obtained by restricting the rows of R+
i
to the elements of U. With this, we can write eXR
iU = XRiU. Now, (17) can be equivalently written as
vt+1
iU
= vt
iU −αtRT
iUXT φ′( eXRvt).
(18)
In order to update vt
iU, worker i requires φ′( eXRvt), where eXRvt = Pm
j=1 eXR
j vt
j and worker i has only
( eXR
i , vt
i). Since vt = R+wt, we have eXRvt = XRvt = Xwt. So, it suﬃces to compute Xwt at the master
node – once master has Xwt, it can locally compute φ′(Xwt) and send it to all the workers. Computing
Xwt is the distributed matrix-vector (MV) multiplication problem, where the matrix X is ﬁxed and we
want to compute Xwt for any vector wt in the presence of an adversary. In Section 4, we give a method
for performing distributed MV multiplication in the presence of an adversary. Now we give an overview,
together-with an improvement on its computational complexity.
We encode X using an encoding matrix L ∈R(p′m)×n. Let L = [LT
1 LT
2 . . . LT
m]T , where each Li is a
p′ × n matrix with p′ = ⌈
n
m−2t⌉. Each Li has p′ rows and n columns, and has the same structure as that
of Si from (11). Worker i stores eXL
i = LiX. To compute Xw, master sends w to all the workers; worker i
responds with LiXw+ei, where ei = 0 if the i’th worker is honest, otherwise can be arbitrary; upon receiving
{LiXw + ei}m
i=1, where at most t of the ei’s can be non-zero, master applies the decoding procedure and
recovers Xw back. We can improve the computational complexity of this method signiﬁcantly by observing
that, in each iteration of our distributed CD algorithm, only a few coordinates of w get updated and the
rest of the coordinates remain unchanged. (Looking ahead, when each worker updates viU’s according to
(17), it automatically updates wf(U) according to (6) – for a speciﬁc function f as deﬁned in (21) – where
v and w satisfy v = R+w.) This implies that for computing Xw, master only needs to send the updated
coordinates to the workers and keeps the result from the previous MV product with itself. This signiﬁcantly
reduces the local computation at the worker nodes, as now they only need to perform a local MV product
of a matrix of size p′ × |f(U)| and a vector of length |f(U)|. See Section 4 for details.
16If such a w exists, then it is unique. This follows from the fact that R+ is of full column-rank.
23

Our goal in each iteration of CD is to update some coordinates of the original parameter vector w;
instead, by solving the encoded problem, we are updating coordinates of the transformed vector v. We
would like to design an algorithm/encoding such that it has exactly the same convergence properties as if
we are running the distributed CD on the original problem without any adversary. For this, naturally, we
would like our algorithm to satisfy the following property:
Update on any (small) subset of coordinates of w should be achieved by updating some (small) subset of
coordinates of vi’s; and, by updating those coordinates of vi’s, we should be able to eﬃciently recover the
correspondingly updated coordinates of w. Furthermore, this should be doable despite the errors injected by
the adversary in every iteration of the algorithm.
Note that if each coordinate of v depends on too many coordinates of w, then updating a few coordinates
of v may aﬀect many coordinates of w, and it becomes information-theoretically impossible to satisfy the
above property (even without the presence of an adversary).17 This imposes a restriction that each row of R+
must have few non-zero entries, in such a way that updating vt
iU’s, for any choice of U ⊆[p], will collectively
update only a subset (which may potentially depend on U) of coordinates of the original parameter vector
wt, and we can uniquely and eﬃciently recover those updated coordinates of wt, even from the erroneous
vectors {vt+1
iU +eiU}m
i=1, where at most t out of m error vectors {eiU}m
i=1 are non-zero and may have arbitrary
entries. In order to achieve this, we will design a sparse encoding matrix R+ (which in turn determines R),
that satisﬁes the following properties:
P.1 R+ has structured sparsity, which induces a map f : [p] →P([d]) (where P([d]) denotes the power set
of [d]) such that
(a) {f(i) : i ∈[p]} partitions {1, 2, . . . , d}, i.e., for every i, j ∈[p], such that i ̸= j, we have f(i)∩f(j) =
∅and that Sp
i=1 f(i) = [d].
(b) |f(i)| = |f(j)| for every i, j ∈[p −1], and |f(p)| ≤|f(i)|, for any i ∈[p −1].
(c) For any U ⊆[p], deﬁne f(U) := ∪j∈Uf(j). If we update vt
iU, ∀i ∈[m], according to (18), it
automatically updates wt
f(U) according to
wt+1
f(U) = wt
f(U) −αtXT
f(U)φ′(Xwt).
(19)
If we set vt+1
iU
:= vt
iU and wt+1
f(U) := wt
f(U), then vt+1 = R+wt+1, i.e., our invariant holds.
Note that (19) is the same update rule if we run the plain CD algorithm to update wf(U). In fact, our
encoding matrix satisﬁes a stronger property, that vt+1
iU
= R+
iU,f(U)wt+1
f(U) holds for every i ∈[m], U ⊆[p],
where R+
iU,f(U) denotes the |U| × |f(U)| matrix obtained from R+
iU by restricting its column indices to the
elements in f(U).
P.2 We can eﬃciently recover wt+1
f(U) from the erroneous vectors {vt+1
iU
+ eiU}m
i=1, where at most t of eiU’s
are non-zero and may have arbitrary entries. Since vt+1
iU
= R+
iU,f(U)wt+1
f(U), for every i ∈[m], U ⊆[p],
this property requires that not only R+, but its sub-matrices also have error correcting capabilities.
Remark 9. Note that P.1 implies that for every i ∈[p], we have |f(i)| ≤d/p. As we see later, this will be
equal to m/(1 + ϵ) for some ϵ > 0 which is determined by the corruption threshold. This means that in each
iteration of the CD algorithm running on the modiﬁed encoded problem, we will be eﬀectively updating the
coordinates of the parameter vector w in chunks of size m/(1 + ϵ) or its integer multiples. In particular, if
each worker i updates k coordinates of vi, then km/(1+ϵ) coordinates of w will get updated. For comparison,
Algorithm 1 updates km coordinates of the parameter vector w in each iteration, if each worker updates k
coordinates in that iteration.
17To see this, consider the case when each worker i updates only the ﬁrst coordinate of vi and no worker is corrupt. Master
receives m linear equations vi1 = R+
i1w, i = 1, 2, . . . , m, where R+
i1 is the ﬁrst row of R+
i
for every i ∈[m]. Assume, for
simplicity, that these m equations are linearly independent. When m is smaller than d (which is always the case), there are
inﬁnite solutions to this system of linear equations, unless at most m elements of w are involved in the m linear equations
(i.e., the number of unknowns are at most the number of equations), which is equivalent to saying that the rows R+
i1 for
i = 1, 2, . . . , m are sparse. Our encoding matrix will satisfy this property; see Section 5.1 for more detail.
24

¯¯wt
f(U′)
M broadcasts ¯¯wt
f(U′)
M
Dec
W1
L1X
W2
L2X
W3
L3X
Wm
LmX
L1X ¯¯wt
f(U′)
L2X ¯¯wt
f(U′) + e2
L3X ¯¯wt
f(U′)
LmX ¯¯wt
f(U′) + em
Xwt
Compute
φ′(Xwt)
φ′(Xwt)
M broadcasts φ′(Xwt)
M
Dec
W1
XR1
W2
XR2
W3
XR3
Wm
XRm
vt+1
1U
vt+1
2U
vt+1
3U + e3U
vt+1
mU + emU
wt+1
f(U)
t ←t + 1; U′ ←U; ¯¯wt
f(U′) := wt−1
f(U′) −wt
f(U′)
Figure 2 This ﬁgure shows our 2-round approach to the Byzantine-resilient distributed coordinate descent (CD) for solving
(15) using data encoding and real-error correction. We encode X with the encoding matrix [R1 . . . Rm] ∈Rd×p2m and store
eXR
i
:= XRi at the i’th worker and solve (16) over an enlarged parameter vector v ∈Rp2m. At the t’th iteration, for some
U ⊆[p2], the update at the i’th worker is vt+1
iU
= vt
iU −αtRT
iUXT φ′( eXRvt), which requires φ′( eXRvt), where eXRvt = Xwt.
The ﬁrst part of the ﬁgure is for providing φ′(Xwt) to every worker in each iteration so that they can update vt
iU’s. For this,
we encode X using the encoding matrix [LT
1
. . . LT
m]T ∈Rp1m×n and store eXL
i := LiX at worker i. The encoding has the
property that we can recover Xwt from the erroneous vectors { eXL
i wt + ei}m
i=1, where at most t of the ei’s are non-zero and
can be arbitrary. We can make it computationally more eﬃcient at the workers’ side by observing that, in each iteration, only
a subset of coordinates of w are being updated: suppose we updated vt
iU′’s in the t’th iteration, which automatically updated
wt
f(U′). Since wt
[d]\f(U′) remain unchanged, we need to send only wt
f(U′) to the workers – in the ﬁgure, to take care of a
technicality, we let master broadcast ¯¯wt
f(U′) := wt−1
f(U′) −wt
f(U′), each worker i computes eXi ¯¯wt
f(U′) and sends it backs to the
master. Since master keeps Xwt−1 from the previous iteration with itself, it can compute Xwt. The set of corrupt workers
may be diﬀerent in diﬀerent rounds – the corrupt ones are shown in red color and they can send arbitrary outcomes to master.
Once master has recovered Xwt, it computes φ′(Xwt) and broadcasts it; upon receiving it worker i updates vt+1
iU
and sends it
back. By P.1, this reﬂects an update on wt+1
f(U) according to (19); and by P.2, the master can recover wt+1
f(U).
Now we design an encoding matrix R+ and a decoding method that satisfy P.1 and P.2.
5.1
Encoding and Decoding
In this section, we ﬁrst design an encoding matrix R+ that satisﬁes P.1.
R+ will be such that it has
orthonormal rows, so, R is easy to compute, R = (R+)T . For simplicity, we denote R+ by S. We show that
the encoding matrix that we design for the MV multiplication in Section 4 satisﬁes all the properties that
we want.18 In the MV multiplication, we had a ﬁxed matrix A and the master node wants to compute Aw
for any vector w of its choice. In the solution presented in Section 4, we encode A and store SiA at the i’th
worker node. Now, the master sends w to all the worker nodes, and each worker i responds with SiAw +ei,
where ei = 0 if worker i is honest, otherwise can be arbitrary. Once master receives {SiAw + ei}m
i=1, it can
run the error correcting procedure to recover Aw. To apply this in our setting, we take A to be the identity
matrix, such that SiA = Si, and the master can recover w from {ri = Siw + ei}m
i=1, if at most t of the ei’s
are non-zero. For convenience, we rewrite the encoding matrix Si for the i’th worker node from Section 4.2
18The encoding and decoding of this section is based on the corresponding algorithms from Section 4.
25

below:
Si =


b1i . . . bqi
...
b1i . . . bqi
b1i . . . bli


p×d
(20)
Here q = (m −2t) and l = d −(p −1)q, where p = ⌈d
q ⌉. Note that 1 ≤l < q, and if q divides d, then
l = q. All the unspeciﬁed entries of Si are zero. By stacking up the Si’s gives us our desired encoding matrix
S = [ST
1 ST
2 . . . ST
m]T . Note that b1i, b2i, . . . , bqi are such that if we let bi = [bi1 bi2 . . . bim]T for every i ∈[q],
then {b1, b2, . . . , bq} is a set of orthonormal vectors. This implies that S is orthonormal, and, therefore,
S+ = ST . By taking R = ST , we have R+ = S. Now we show that S satisﬁes P.1-P.2.
Our Encoding Satisﬁes P.1.
We need to show a map f : [p] →P([d]) that satisﬁes P.1. Let us deﬁne
the function f as follows, where (q = m −2t) and p = ⌈d
q ⌉:
f(i) :=
(
[(i −1) ∗q + 1 : i ∗q]
if 1 ≤i < p,
[(p −1) ∗q + 1 : d]
if i = p,
(21)
and for any U ⊆[p], we deﬁne f(U) := ∪i∈Uf(i). It is clear from the deﬁnition of f that (i) {f(i) : i ∈[p]}
partitions [d]; (ii) for every i ∈[p −1] we have |f(i)| = q, and that |f(p)| ≤q. Recall that q = m −2t. For
the 3rd property, note that, for any U ⊆[p], all the columns of SiU whose indices belong to [d] \ f(U) are
identically zero, which implies that we have
SiUw = SiU,f(U)wf(U),
for every w ∈Rd,
(22)
which in turn implies that
SiUXT = SiU,f(U)XT
f(U).
(23)
Since S+ = ST , we have S+
iU = ST
iU for every i ∈[m] and every U ⊆[p]. With these, our update rule
vt+1
iU
= SiUwt −αtSiUXT φ′(Xwt)19 can equivalently be written as
vt+1
iU
= SiU,f(U)wt+1
f(U),
(24)
where
wt+1
f(U) = wt
f(U) −αtXT
f(U)φ′(Xwt).
(25)
Observe that (25) is the same update rule as (19), which implies that if each worker i updates viU according
to the CD update rule, then the collective update at all the worker nodes automatically updates wf(U)
according the CD update rule. Now we show that our invariant vt+1 = Swt+1 is maintained. We show this
by induction. Base case v0 = Sw0 holds by construction. For the inductive case, assume that vt = Swt
holds at time t and we show vt+1 = Swt+1 holds at time t + 1.
Deﬁne U := [p] \ U and f(U) := [d] \ f(U). Since we did not update vt
iU’s, we have vt+1
iU
= vt
iU for every
i ∈[m]. This, together with the inductive hypothesis (i.e., vt = Swt), implies that
vt+1
iU
= SiUwt.
(26)
19We emphasize that we used S+ = ST crucially to equivalently write our update rule vt+1
iU
= R+
iUwt −αRT
iUXT φ′(Xwt)
from (18) as vt+1
iU
= SiUwt −αtSiUXT φ′(Xwt). This follows because S+ = ST and we take R+ = S, which together imply
that R+
iU = RT
iU = SiU.
26

Since f(U) = f(U), we have from (22) that
SiUwt = SiU,f(U)wt
f(U).
(27)
It is clear from (25) that wt
f(U) did not get an update when we updated vt
iU’s, which implies that wt+1
f(U) =
wt
f(U). Substituting this in (27) gives SiUwt = SiU,f(U)wt+1
f(U), which, by (22), yields SiUwt = SiUwt+1.
This, together with (26), implies
vt+1
iU
= SiUwt+1.
(28)
We already have from (22) and (24) that
vt+1
iU
= SiUwt+1.
(29)
Since (28) and (29) hold for every i ∈[m], we have vt+1 = Swt+1. Hence, the invariant is maintained.
Our Encoding Satisﬁes P.2. If we let
v[m]U := [vT
1U vT
2U . . . vT
mU]T ,
S[m]U,f(U) := [ST
1U,f(U) ST
2U,f(U) . . . ST
mU,f(U)]T ,
then the collective update (24) from all the workers can be written as
vt+1
[m]U = S[m]U,f(U)wt+1
f(U).
(30)
It is easy to verify that for every choice of U ⊆[p], S[m]U,f(U) is a full column-rank matrix, which implies
that we can in principle recover the updated wt+1
f(U) from vt+1
[m]U = S[m]U,f(U)wt+1
f(U). Now we show that not
only can we recover wt+1
f(U) from {SiU,f(U)wt+1
f(U)}m
i=1, but also eﬃciently recover wt+1
f(U) from the erroneous
vectors {SiU,f(U)wt+1
f(U) + eiU}m
i=1, where at most t out of m error vectors {eiU}m
i=1 are non-zero and may
have arbitrary entries. Let U = {j1, j2, . . . , j|U|}, and for every i ∈[m], let eiU = [eij1eij2 . . . eij|U|]T . Master
equivalently writes {SiU,f(U)wt+1
f(U) + eiU}m
i=1 as |U| systems of linear equations.
˜hi(wt+1
f(U)) = ˜Si,f(U)wt+1
f(U) + ˜ei,
i ∈U,
(31)
where, for every i ∈U, ˜ei = [e1i, e2i, . . . , emi]T and ˜Si,f(U) is an m × |f(U)| matrix whose j’th row is equal
to the i’th row of SjU, for every j ∈[m]. Note that at most t entries in each ˜ei are non-zero. Observe that
{SiU,f(U)wt+1
f(U) + eiU}m
i=1 and {˜Si,f(U)wt+1
f(U) + ˜ei}i∈U are equivalent systems of linear equations, and we can
get one from the other. Observe that (31) is similar to (8): ˜Si,f(U) is equal to ˜Si (for the same i) with some
of its zero columns removed; and adding zero columns to ˜Si,f(U) will not change the value of ˜hi(wt+1
f(U)).
Now, using the machinery developed in Section 4 we can recover wt+1
f(U) from (31) in O(|U|m2) time.
5.2
Resource Requirement Analysis
In this section, ﬁrst we give our algorithm developed for distributed coordinate descent in the presence of t
(out of m) adversarial worker nodes, whose pictorial description is given in Figure 2.
We use two encoding matrices L ∈R(p1m)×n and R ∈Rd×(p2m).
Let L = [LT
1 LT
2
. . . LT
m]T and
R = [R1 R2 . . . Rm], where each Li is a p1 × n matrix with p1 = ⌈
n
m−2t⌉and each Ri is a d × p2 matrix
with p2 = ⌈
d
m−2t⌉. Worker i stores both eXL
i = LiX and eXR
i = XRi. Roughly, L is used to recover Xw
from the erroneous {LiXw + ei}m
i=1, and R is used to update the parameter vector reliably despite errors.
Here L is a full column-rank matrix and R is a full row-rank matrix. Initialize with an arbitrary w0 and let
v0 = R+w0. Repeat the following until convergence:
27

1. At iteration t, master sends (wt−1
f(U) −wt
f(U))20 to all the workers (at t = 0, master sends w0), where
U ⊆[p2] is the set of indices used for updating vt−1
iU ’s in the previous iteration, which in turn updated
wt−1
f(U); see (24) and (25) in Section 5.1.
2. Worker i computes eXL
i (wt−1
f(U)−wt
f(U)) = LiX(wt−1
f(U)−wt
f(U)) and sends it to the master.21 Upon receiving
{ eXL
i (wt−1
f(U) −wt
f(U)) + ei}m
i=1, where at most t of the ei’s are non-zero and may have arbitrary entries,
the master applies the decoding procedure of Section 4 and recovers X(wt−1
f(U) −wt
f(U)).
We assume
that master keeps Xwt−1 from the previous iteration (which is equal to 0 if t = 0), it can compute
Xwt = Xwt−1 −X(wt−1
f(U) −wt
f(U)). Note that if t = 0, this is equal to Xw0.
3. After obtaining Xwt, master computes φ′(Xwt), picks a subset U ⊆[p2] (randomly or in a round robin
fashion to cover [p2] in a few iterations), and sends (φ′(Xwt), U) to all the workers.
4. Each worker node i ∈[m] updates vt+1
iU
←vt
iU −αt∇iUφ( eXvt) = vt
iU −αt( eXR
iU)T φ′(Xwt), while keep-
ing the other coordinates of vt
i unchanged.
Worker i sends vt+1
iU
to the master.
Note that vt+1
iU
=
R+
iU,f(U)wt+1
f(U), where wt+1
f(U) = [wt
f(U) −αXT
f(U)φ′(Xwt)]; see (24) and (25) in Section 5.1.
5. Upon receiving {vt+1
iU
+ eiU}m
i=1, where at most t of the {eiU}m
i=1’s are non-zero and may have arbitrary
entries, master applies the decoding procedure (since our encoding satisﬁes P.2) and recovers wt+1
f(U).
Now we analyze the total amount of resources (storage, computation, and communication) required by
the above algorithm and prove Theorem 2.
Fix an ϵ > 0.
Let the corruption threshold t satisfy t ≤
⌊(ϵ/(1 + ϵ)) · (m/2)⌋.
5.2.1
Storage Requirement:
By a similar analysis done in Section 4.5, we can show that the total storage at all worker nodes is roughly
equal to 2(1 + ϵ)|X|.
5.2.2
Computational Complexity:
We can divide the computational complexity of our scheme as follows:
• Encoding the data matrix. By a similar analysis done in Section 4.5, we can show that the total encoding
time is O

(
ϵ
1+ϵm + 1)nd

. Note that this encoding is to be done only once.
• Computation at each worker node. Suppose that in each iteration of our algorithm, all the workers update
τ coordinates of vi’s.
Fix an iteration t and assume that at iteration (t −1), workers updated the
coordinates in the set U ⊆[p2], where |U| = τ. Recall from P.1 that updating τ = |U| coordinates of each
vt−1
i
automatically updates wt−1
f(U). Upon receiving (wt−1
f(U) −wt
f(U)) from the master node, each worker
i computes eXL
i (wt−1
f(U) −wt
f(U)), and reports back the resulting vector. Note that (wt−1
f(U) −wt
f(U)) has
at most |f(U)| =
τm
1+ϵ non-zero elements, which together with that eXL
i is a p1 × d matrix, implies that
computing eXL
i (wt−1
f(U) −wt
f(U)) takes O(p1 · |f(U)|) = O(nτ) time.22 In the second round, given φ′(Xwt),
since ( eXR
iU)T is of dimension n × τ, updating vt
iU requires O(nτ) time, where τ = |U|. So, the total time
taken by each worker is O(nτ).
20Observe that master need not send the locations f(U), because workers can compute those by themselves, as they know
both U and the function f.
21With some abuse of notation, when we write Xwf(U), we implicitly assume that wf(U) is a length d vector, which has 0’s
in the indices that lie in f(U).
22Note that in the very ﬁrst iteration, master sends w0, which may be a dense length d vector, and computing eXiLw0 at
the i’th worker can take O(p1d) = O((1 + ϵ) nd
m ) time. This is only for the ﬁrst iteration.
28

• Computation at the master node.
Once master receives {LiX(wt−1
f(U) −wt
f(U)) + ei}m
i=1, applying the
decoding procedure of Section 4 to obtain X(wt−1
f(U) −wt
f(U)) from these erroneous vectors requires
O(p1m2) = O((1 + ϵ)nm) time. After that obtaining Xwt takes another O(n) time. Given Xwt, comput-
ing φ′(Xwt) takes O(n) time, assuming that computing ℓ′(⟨xi, wt⟩; yi) requires unit time, where ⟨xi, wt⟩
is equal to the i’th entry of Xwt. Upon receiving {vt+1
iU
+ eiU}m
i=1, where vt+1
iU
= R+
iU,f(U)wt+1
f(U), for
all i ∈[m], recovering wt+1
f(U) requires O(τm2) time.
So, the total time taken by the master node is
O((1 + ϵ)nm + τm2).
5.2.3
Communication Complexity:
Suppose workers update τ coordinates of vi’s in each iteration. Then (i) master broadcasts

τm
1+ϵ + n

real
numbers,
τm
1+ϵ in the ﬁrst round to represent wt
f(U) and n in the second round to represent φ′(Xwt); and
(ii) each worker sends
 τ + (1 + ϵ) n
m

real numbers, (1 + ϵ) n
m in the ﬁrst round for computing Xwt at the
master node and τ in the second iteration to represent vt
iU.
6
Extensions
In this section, we give a few important extensions of our coding scheme developed earlier in Section 4. First
we give a Byzantine-resilient and communication-eﬃcient method for stochastic gradient descent (SGD).
Second we show how to exploit the speciﬁc structure of our encoding matrix to eﬃciently extend our coding
technique to the streaming data model. In the end, we give a few more important applications, where our
method can be applied constructively.
6.1
Stochastic Gradient Descent
Stochastic gradient descent (SGD) [HM51] is another alternative if full gradients are too costly to compute.
In each iteration of SGD, we sample a data point uniformly at random, compute a gradient on that sample,
and update the parameter vector based on that.
We start with an arbitrary/random parameter vector
w0 ∈Rd and update it according the following update rule:
wt+1 = wt −αt∇frt(wt),
t = 1, 2, 3, . . .
(32)
where rt is sampled uniformly at random from {1, 2, . . . , n}. This ensures that the expected value of the
gradient is equal to the true gradient. Due to its simplicity and remarkable empirical performance, SGD has
become arguably the most widely-used optimization algorithm in many large-scale applications, especially
in deep learning [Bot10, RSS12, DCM+12].
We want to run SGD in a distributed setup, where data is
distributed among m worker nodes and at most t of them can be corrupt; see Section 2.3 for details on our
adversary model.
Our solution.
In the plain SGD, we sample a data point randomly and compute its gradient. So, we
give a method in which, at any iteration t, master picks a random number rt in {1, 2, . . . , n}, broadcasts it,
and recovers the rt’th data point xrt. Once the master has obtained xrt, it can compute a gradient on it
and updates the parameter vector. Since master recovers the data points, we can optimize for non-convex
problems also; essentially, we could optimize anything that the plain SGD can. Our method is described
below.
We encode XT using the ⌈d/(m −2t)⌉× d encoding matrix S(2), which has been deﬁned in Section 4.5.
For simplicity, we denote S(2) by S. Let S = [ST
1 ST
2 . . . ST
m]T . Note that the j’th worker stores SjXT . Let
eX := SXT , which is a ⌈d/(m −2t)⌉×n matrix, whose i’th column is the encoding exi := Sxi of the i’th data
point xi. Using the method developed in Section 4, given {Sjxi + ej}m
j=1, where ej = 0 if the j’th worker is
honest, otherwise can be arbitrary, master can recover xi exactly in O((1 + ϵ)md) time. Our main theorem
is stated below, a proof of which trivially follows from Section 4.
29

Theorem 3 (Stochastic Gradient Descent). Let X ∈Rn×d denote the data matrix. Let m denote the total
number of worker nodes. We can compute a stochastic gradient in a distributed manner in the presence of t
corrupt worker nodes and s stragglers, with the following guarantees, where ϵ > 0 is a free parameter.
• (s + t) ≤
j
ϵ
1+ϵ · m
2
k
.
• Total storage requirement is roughly (1 + ϵ)|X|.
• Computational complexity for each stochastic gradient computation:
– at each worker node is O((1 + ϵ) d
m).
– at the master node is O((1 + ϵ)dm).
• Communication complexity for each stochastic gradient computation:
– each worker sends
 (1 + ϵ) d
m

real numbers.
– master broadcasts ⌈log n⌉bits.
• Total encoding time is O

nd

ϵ
1+ϵm + 1

.
Observe the distributed gain of our method in the communication exchanged between the workers and
the master: (i) master only broadcasts an index in {1, 2, . . . , n}, which only takes ⌈log n⌉bits; and (ii) each
worker sends roughly 1+ϵ
m
fraction of the total dimension d. Hence, this method is particularly useful in
distributed settings with communication-constrained and band-limited links. The Remarks 2, 4, 5 are also
applicable for Theorem 3.
Remark 10 (One-round vs. two-round approach). Unlike the two-round approach taken for gradient com-
putation in PGD and also for CD, we give a one-round approach for each iteration of SGD. This is because
in each SGD iteration we need to compute the gradient on only one data point (not the entire dataset, as in
the case for each PGD iteration). Because of this, recovering a (random) data-point itself at the master and
then computing a gradient on it locally (which is what we do) would be far more eﬃcient than computing
gradient on a single data point in a distributed manner. This is in contrast to each gradient computation
for PGD, which requires computation of the full gradient (which is the summation of gradients on all n data
points). In principle, we can use the one-round for each PGD iteration also in which ﬁrst we recover all the
n data points at master and then compute the full gradient locally, but this approach would defeat the purpose
of distributed computation both in terms of storage and computational complexity. Note that our two-round
approach for PGD is signiﬁcantly more eﬃcient than this.
The reason behind taking the two-round approach for CD is because in order to update the local parameter
vectors in the t’th iteration, workers need access to the MV multiplication eXRvt = Xwt (see the paragraph
after (18) for more details), and in order to provide that we use an extra round – the ﬁrst round is used for
computing Xw and the second round is used for updating the local parameter vectors. Again, for CD also,
we could adopt a one-round approach where master recovers all the n data points and then do the parameter
update, but that would be highly ineﬃcient and defeat the purpose of distributed computation.
One of the main advantages of the one-round approach for SGD is that since we are recovering the data
point itself at the master, we can use it to optimize any function, both convex and non-convex. This is in
contrast to the two-round approach, which can only be used for generalized linear models only.
6.2
Encoding in The Streaming Data Model
An attractive property of our encoding scheme is that it is very easy to update with new data points. More
speciﬁcally, our encoding requires the same amount of time, irrespective of whether we get all the data at
once, or we get each sample point one by one, as in the online/streaming model. This setting encompasses
a more realistic scenario, in which we design our coding scheme with the initial set of data points and
30

distribute the encoded data among the workers. Later on, when we get some more samples, we can easily
incorporate them into our existing encoded data. We show that updating (m −2t) new data points in Rd
requires O ((m −2t) ((2t + 1)d)) time in total, i.e., O ((2t + 1)d) amortized-time per data point. This is the
best one can hope for, since the oﬄine encoding of n data points requires O ((2t + 1)nd) total time. At the
end of the update, the ﬁnal encoded matrix that we get is the same as the one we would have got had we
had all the n + 1 data points in the beginning. Therefore, the decoding is not aﬀected by this method at
all. Note that we use the same encoding matrices both for gradient computation as well as for coordinate
descent. So, it suﬃces to prove our result in the streaming model for any one of them, and we show it for
gradient computation below.
Theorem 4. The total time complexity in encoding all the data points at once, i.e., when encoding is done
oﬄine, is the same as the total time complexity in encoding the data points one by one as they come in the
streaming model, i.e., when encoding is done online.
Proof. Let S(1) and S(2) denote the encoding matrices for encoding X and XT , respectively; see Section 4.2.
For convenience, we copy over the corresponding encoding matrices S(1)
i
and S(2)
i
from (11) for the i’th
worker node in Figure 3.
S(1)
i
=


b1i . . . bqi
...
b1i . . . bqi
b1i . . . bl1i


p1×n
(a)
S(2)
i
=


b1i . . . bqi
...
b1i . . . bqi
b1i . . . bl2i


p2×d
(b)
Figure 3 Figure 3a depicts the encoding matrix for the i’th worker node for encoding X, which is used in the ﬁrst round of the
gradient computation. Here p1 = ⌈n/q⌉, where q = (m −k) and k is equal to the number of rows in the error recovery matrix
F in (14), and l1 = n −(p1 −1)q. Figure 3b depicts the encoding matrix for the i’th worker node for encoding XT , which is
used in the second round of the gradient computation. Here p2 = ⌈d/q⌉and l2 = d −(p2 −1)q. All the unspeciﬁed entries in
both the matrices are zero.
Suppose at some point of time we have encoded n data points each lying in Rd and distributed the
encoded data among the m worker nodes. Now a new data sample x ∈Rd comes in. We will show how to
incorporate it in the existing scheme in O ((2t + 1)d) time on average.
Updating the encoding matrices.
Fix an arbitrary worker i ∈[m]. Note that the new data matrix X
has dimension (n + 1) × d. So, the new encoding matrix S(1)
i
should have (n + 1) columns, and we have to
add one more column to S(1)
i . By examining the repetitive structure of S(1)
i , it is obvious which column to
add: if l1 < q, then we add the p1-dimensional vector [0, 0, . . . , 0, b(l1+1)i]T as the last column; otherwise, if
l1 = q, then we add the (p1 + 1)-dimensional vector [0, 0, . . . , 0, b1i]T as the last column. In the second case,
the number of rows of S(1)
i
increases by one – the last row has all zeros, except for the last element, which
is equal to b1i. Note that S(2)
i
does not change at all. Observe that if the i’th worker performs this update,
then it does not have to store its entire encoding matrix S(1)
i , it only needs to store n, q = (m −k), and the
31

q real numbers b1i, b2i, . . . , bqi, where q = m −k, which could be much smaller as compared to n and d, and
are enough to deﬁne S(1)
i
and S(2)
i .
Updating the encoded data.
Now we show how to update the encoded data with the new sample x.
We need to update both S(1)
i X as well as S(2)
i XT for every worker i ∈[m].
• Updating S(1)
i X. If l1 < q, then we add b(l1+1)ixT to the last row of S(1)
i X; otherwise, if l1 = q, then
we add b1ix as a new row in S(1)
i X.
In the ﬁrst case, the resulting matrix still has p1 rows, whose
ﬁrst p1 −1 rows are same as before, and the last row is the sum of the previous row and b(l1+1)ixT .
In the second case, the resulting matrix has (p1 + 1) rows, whose ﬁrst p1 rows are the same as before
and the last row is equal to b1ixT . Note that each row of S(1)
i
for i ≤2t, has at most (m −2t) non-
zero elements; whereas, for i > 2t, each row of S(1)
i
has exactly one non-zero entry.
Since there are
p1 = ⌈n/(m −2t)⌉rows in each S(1)
i , updating S(1)
i X for every i ≤2t takes O(d) time; and for i > 2t,
update in S(1)
i X happens only once in (m −2t) new data points (whenever the second case occurs and
the resulting S(1)
i
has (p1 + 1) rows). So, updating (m −2t) data points at all m worker nodes require
O (2t ∗(m −2t)d + (m −2t) ∗d) = O((m −2t)(2t + 1)d) time, i.e., O ((2t + 1)d) time per data point.
• Updating S(2)
i XT . Note that XT is a d×(n+1) matrix whose last column is equal to the new data sample
x. Now, to update S(2)
i XT , we add S(2)
i x as an extra column. The resulting matrix is of size p2 × (n + 1),
whose ﬁrst n columns are the same as before and the last column is equal to S(2)
i x. Since total number
of non-zero entries in S(2)
i
is equal to d if i ≤2t and equal to p2 = ⌈d/(m −2t)⌉if i > 2t, the total time
required to update a new data point is O(2t ∗d + (m −2t) ∗p2) = O ((2t + 1)d).
Observe that at the end of this local update at each worker node, the ﬁnal encoded matrix that we get is
the same as the one we would have got had we had all the n + 1 data points in the beginning. The decoding
is not aﬀected by this method at all. This completes the proof of Theorem 4.
Remark 11 (Updating the encoded data eﬃciently with new features). Observe that since we encode both
X and XT in an analogous fashion, it follows by symmetry that we can not only update eﬃciently upon
receiving a new data sample, but can also update eﬃciently if we decide to enlarge the dimension d of each of
the n data samples at some point of time – maybe we ﬁgure out some new features of the data to get a more
accurate model to overcome under-ﬁtting. In these situations, we don’t need to encode the entire dataset all
over again, just a simple update is enough to incorporate the changes.
Remark 12 (What allows our encoding to be eﬃcient for streaming data?). The eﬃcient update property of
our coding scheme is made possible by the repetitive structure of our encoding matrix (see Figure 3), together
with the fact that this structure is independent of the number of data points n and the dimension d – it
only depends on the number of worker nodes m and the corruption threshold t. We remark that other data
encoding methods in literature, even for weaker models, do not support eﬃcient update. For example, the
encoding of [KSDY17], which was designed for mitigating stragglers, depends on the dimensions n and d of
the data matrix. So, it may not eﬃciently update if a new data point comes in.
6.3
More Applications.
There are many iterative algorithms, other than the gradient descent for learning GLMs, which use repeated
MV multiplication. Some of them include (i) the power method for computing the largest eigenvalue of a
diagonalizable matrix, which is used in Google’s PageRank algorithm [ISW06], Twitter’s recommendation
system [GGL+13], etc.; (ii) iterative methods for solving sparse linear systems [Saa03]; (iii) many graph algo-
rithms, where the graph is represented by a ﬁxed adjacency matrix, [KG11]. In large-scale implementation of
these systems, where Byzantine faults are inevitable, the method described in this paper can be of interest.
32

In most of these applications, the underlying matrix A is generally sparse, which is exploited to gain
computational eﬃciency. So, it is desired not to lose sparsity even if we want resiliency against Byzantine
attacks. Fortunately, our encoding matrix S is sparse (see (11)), which ensures that the encoded matrix SA
will not lose the sparsity of A: Each of the ﬁrst pk rows of S has at most (m −k) (where k = 2t) non-zero
elements, and each of the remaining rows has exactly one 1. Since m is the number of worker nodes, which
may be small, and we can take t to be up to ⌊m−1
2 ⌋, we may have a few non-zero entries in each row of
S (in the extreme case when 2t = m −1, each row of S has only one non-zero entry). In a sense, we are
getting Byzantine-resiliency almost for free without compromising the computational eﬃciency that is made
possible by the sparsity of the matrix.
7
Numerical Experiments
In this section, we validate the eﬃcacy of our proposed methods by numerical experiments. We run dis-
tributed gradient descent (GD) and coordinate descent (CD) for linear regression arg minw∈Rd ∥Xw−y∥2
2. As
mentioned in Section 2.1, for linear regression (which is equal to ridge regression when h = 0), the projected
gradient descent (PGD) reduces to gradient descent (GD). Since we are doing exact computation (computing
the gradients exactly in the case of GD and updating the coordinates exactly in the case of CD), (i) there
is no need to check the convergence, and (ii) our algorithm will perform exactly the same whether we are
working with synthetic datasets or real datasets, hence, we will work with a synthetic dataset. We run our al-
gorithms23 with m = 15 worker nodes on two datasets: (n = 10, 000, d = 250) and (n = 20, 000, d = 22, 000).
For both the datasets, we generate (X, y) by sampling X ←N(0, I) and y = Xθ + z, where θ ∈Rd has
d/3 non-zero entries, all of them are i.i.d. according to N(0, 4), and each entry of z ∈Rn is sampled from
N(0, 1) i.i.d. In each round of the gradient computation, the adversary picks t worker nodes uniformly at
random, and adds independent random vectors of appropriate length as errors, whose entries are sampled
from N(0, σ2) i.i.d. with σ = 100, to the true vectors.
Figure 4 We run our algorithms (CD and GD) with 15 worker nodes on a dataset with n = 10, 000, d = 250. This plot reports
how the total time taken (in seconds) for updating diﬀerent number of coordinates in each CD iteration changes with varying
number of corrupt worker nodes from t = 1 to t = 7. In the ﬁgure, we plot the total time taken (per iteration) for updating
the γ-fraction of d coordinates for γ = 0.1, 0.25, 0.5, 1. Note that CD with γ = 1 is equivalent to full gradient computation as
in GD.
23We implement our algorithm in Python, and run it on an iMac machine with 3.8 GHz Quad-Core Intel Core i5 processor
and 16 GB 2400 MHz DDR4 memory.
33

CD(0.1d)
CD(0.25d)
CD(0.5d)
GD ≡CD(d)
Worker
Master
Worker
Master
Worker
Master
Worker
Master
t = 1
0.0020
0.0120
0.0073
0.0182
0.0122
0.0199
0.0493
0.0214
t = 2
0.0044
0.0187
0.0092
0.0212
0.0188
0.0277
0.0953
0.0393
t = 3
0.0054
0.0201
0.0118
0.0242
0.0269
0.0324
0.1213
0.0561
t = 4
0.0063
0.0253
0.0159
0.0327
0.0488
0.0468
0.1602
0.0610
t = 5
0.0107
0.0342
0.0328
0.0460
0.0776
0.0738
0.2943
0.0826
t = 6
0.0205
0.0717
0.0764
0.0833
0.1330
0.1088
0.8929
0.1227
Figure 5 We run our algorithms (CD and GD) with 15 worker nodes on a dataset with n = 20, 000, d = 22, 000, and separately
report the maximum time taken by any single worker and the master per iteration against varying number of corrupt worker
nodes from t = 1 to 6. For CD, we run our algorithm for updating diﬀerent number of coordinates. The ﬁrst two columns
correspond to the case when updating 0.1-fraction of d coordinates, the next two columns for 0.25-fraction, and so on. The last
two columns correspond to updating all the coordinates, which is equivalent to full gradient computation as in GD.
7.1
n = 10, 000, d = 250, m = 15
In Figure 4, we plot the total time taken (which is the sum of the maximum time taken by any single worker
node and the time taken by the master node in both rounds) for updating diﬀerent number of coordinates
in one CD iteration, with varying number of corrupt worker nodes from t = 1 to t = 7. We plot the time
needed for updating γ-fraction of d coordinates for four diﬀerent values of γ (i.e., γ = 0.1, 0.25, 0.5, 1) and we
denote it by CD(γd) for γ = 0.1, 0.25, 0.5, 1. Recall that CD(d) is equivalent to full gradient computation as
in the case of GD. Note that, when t = 7, we have ϵ = m −1, which is the main cause behind the signiﬁcant
increment in time for t = 7.
7.2
n = 20, 000, d = 22, 000, m = 15
In Figure 5, we report separately, the maximum time taken by any single worker node and the time taken
by the master node (together in both the rounds) in one CD iteration for updating diﬀerent number of
coordinates and also for GD, with varying number of corrupt worker nodes from t = 1 to t = 6. As in the
above case, we report the time needed for updating γ-fraction of d coordinates for four diﬀerent values of
γ. Observe that the time taken by the master node is orders of magnitude less than the time taken by the
worker nodes. We can also observe that with the running time in a worker node per iteration for CD(0.1d)
is 95% less than that for GD, while this time saving in the master node is more than 40%.
Acknowledgements
The work of Deepesh Data and Suhas Diggavi was partially supported by the Army Research Laboratory
under Cooperative Agreement W911NF-17-2-0196, by the UC-NL grant LFR-18-548554, and by the NSF
award 1740047. The work of Linqi Song was partially supported by the NSF awards 1527550, 1514531,
by the City University of Hong Kong grant 7200594, and by the Hong Kong RGC ECS 21212419. The
views and conclusions contained in this document are those of the authors and should not be interpreted
as representing the oﬃcial policies, either expressed or implied, of the Army Research Laboratory or the
U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation here on.
References
[A+18]
Tarek F. Abdelzaher et al. Will distributed computing revolutionize peace? the emergence of
battleﬁeld iot. In ICDCS 2018, pages 1129–1138, 2018.
34

[AAL18]
Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In Neural
Information Processing Systems (NeurIPS), pages 4618–4628, 2018.
[AT08]
Mehmet Akçakaya and Vahid Tarokh. A frame construction and a universal distortion bound
for sparse representations. IEEE Trans. Signal Processing, 56(6):2443–2450, 2008.
[Bil95]
P. Billingsley. Probability and Measure. Wiley Series in Probability and Statistics. Wiley, 1995.
[BKBG11] Joseph K. Bradley, Aapo Kyrola, Danny Bickson, and Carlos Guestrin.
Parallel coordinate
descent for l1-regularized loss minimization. In ICML, pages 321–328, 2011.
[BMGS17] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learn-
ing with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information
Processing Systems, NIPS 2017, 4-9 December 2017, Long Beach, CA, USA, pages 118–128,
2017.
[Bot10]
L. Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010. Physica-Verlag HD, 2010.
[BT89]
Dimitri P. Bertsekas and John N. Tsitsiklis. Parallel and Distributed Computation: Numerical
Methods. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1989.
[BV04]
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press,
New York, NY, USA, 2004.
[CDN15]
Ronald Cramer, Ivan Damgård, and Jesper Buus Nielsen. Secure Multiparty Computation and
Secret Sharing. Cambridge University Press, 2015.
[CP86]
Thomas F. Coleman and Alex Pothen. The null space problem I. complexity. SIAM Journal on
Algebraic Discrete Methods, 7(4):527–537, 1986.
[CP18]
Zachary B. Charles and Dimitris S. Papailiopoulos. Gradient coding using the stochastic block
model. In 2018 IEEE International Symposium on Information Theory, ISIT 2018, Vail, CO,
USA, June 17-22, 2018, pages 1998–2002, 2018.
[CSX17]
Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial
settings: Byzantine gradient descent. POMACS, 1(2):44:1–44:25, 2017.
[CT05]
Emmanuel J. Candès and Terence Tao. Decoding by linear programming. IEEE Trans. Infor-
mation Theory, 51(12):4203–4215, 2005.
[CWCP18] Lingjiao Chen, Hongyi Wang, Zachary B. Charles, and Dimitris S. Papailiopoulos. DRACO:
byzantine-resilient distributed training via redundant gradients. In Proceedings of the 35th Inter-
national Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden,
July 10-15, 2018, pages 902–911, 2018.
[DB13]
Jeﬀrey Dean and Luiz André Barroso. The tail at scale. Commun. ACM, 56(2):74–80, February
2013.
[DCG16]
Sanghamitra Dutta, Viveck R. Cadambe, and Pulkit Grover. Short-dot: Computing large linear
transforms distributedly using coded short dot products. In Advances in Neural Information
Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016,
December 5-10, 2016, Barcelona, Spain, pages 2092–2100, 2016.
[DCG19]
Sanghamitra Dutta, Viveck R. Cadambe, and Pulkit Grover. "short-dot": Computing large linear
transforms distributedly using coded short dot products. IEEE Trans. Inf. Theory, 65(10):6171–
6193, 2019.
35

[DCM+12] Jeﬀrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z.
Mao, Marc’Aurelio Ranzato, Andrew W. Senior, Paul A. Tucker, Ke Yang, and Andrew Y. Ng.
Large scale distributed deep networks. In Advances in Neural Information Processing Systems
(NIPS), pages 1232–1240, 2012.
[DD19]
Deepesh Data and Suhas N. Diggavi. Byzantine-tolerant distributed coordinate descent. In IEEE
International Symposium on Information Theory (ISIT), pages 2724–2728, 2019.
[DD20a]
Deepesh Data and Suhas N. Diggavi. Byzantine-resilient high-dimensional SGD with local iter-
ations on heterogeneous data. CoRR, abs/2006.13041, 2020.
[DD20b]
Deepesh Data and Suhas N. Diggavi. Byzantine-resilient SGD in high dimensions on heteroge-
neous data. CoRR, abs/2005.07866, 2020.
[DG08]
Jeﬀrey Dean and Sanjay Ghemawat. Mapreduce: Simpliﬁed data processing on large clusters.
Commun. ACM, 51(1):107–113, January 2008.
[DSD18]
Deepesh Data, Linqi Song, and Suhas N. Diggavi. Data encoding for byzantine-resilient dis-
tributed gradient descent. In Allerton Conference on Communication, Control, and Computing,
Allerton 2018, pages 863–870, 2018.
[DSD19]
Deepesh Data, Linqi Song, and Suhas N. Diggavi. Data encoding methods for byzantine-resilient
distributed optimization.
In IEEE International Symposium on Information Theory (ISIT),
pages 2719–2723, 2019.
[GGL+13] Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Zadeh. Wtf:
The who to follow service at twitter. In Proceedings of the 22Nd International Conference on
World Wide Web, WWW ’13, pages 505–514, New York, NY, USA, 2013. ACM.
[GHYR19] Avishek Ghosh, Justin Hong, Dong Yin, and Kannan Ramchandran. Robust federated learning
in a heterogeneous environment. CoRR, abs/1906.06629, 2019.
[GV19]
Nirupam Gupta and Nitin H. Vaidya. Byzantine fault-tolerant parallelized stochastic gradient
descent for linear regression. In 57th Annual Allerton Conference on Communication, Control,
and Computing, Allerton 2019, Monticello, IL, USA, September 24-27, 2019, pages 415–420.
IEEE, 2019.
[HK71]
Kenneth M Hoﬀman and Ray Kunze. Linear algebra. Prentice-Hall, Englewood Cliﬀs, NJ, 1971.
[HKJ20]
Lie He, Sai Praneeth Karimireddy, and Martin Jaggi. Byzantine-robust learning on heterogeneous
datasets via resampling. CoRR, abs/2006.09365, 2020.
[HM51]
Robbins Herbert and Sutton Monro. A stochastic approximation method. The Annals of Math-
ematical Statistics. JSTOR, www.jstor.org/stable/2236626., vol. 22, no. 3, pp. 400–407, 1951.
[HRSH18] Wael Halbawi, Navid Azizan Ruhi, Fariborz Salehi, and Babak Hassibi. Improving distributed
gradient descent using reed-solomon codes. In 2018 IEEE International Symposium on Informa-
tion Theory, ISIT 2018, Vail, CO, USA, June 17-22, 2018, pages 2027–2031, 2018.
[ISW06]
Ilse Ipsen and Rebecca S. Wills. Mathematical properties and analysis of google’s pagerank.
Boletín de la Sociedad Española de Matemática Aplicada, 34:191–196, 01 2006.
[Jag13]
Martin Jaggi.
An equivalence between the lasso and support vector machines.
CoRR,
abs/1303.1152, 2013.
[KG11]
Jeremy Kepner and John Gilbert. Graph Algorithms in the Language of Linear Algebra. Society
for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2011.
36

[Kon17]
Jakub Konecný. Stochastic, Distributed and Federated Optimization for Machine Learning. PhD
thesis, University of Edinburgh, 2017.
[KSDY17] Can Karakus, Yifan Sun, Suhas N. Diggavi, and Wotao Yin. Straggler mitigation in distributed
optimization through data encoding. In In Advances in Neural Information Processing Systems,
NIPS 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5440–5448, 2017.
[KSDY19] Can Karakus, Yifan Sun, Suhas N. Diggavi, and Wotao Yin. Redundancy techniques for straggler
mitigation in distributed optimization and learning. J. Mach. Learn. Res., 20:72:1–72:47, 2019.
[LLP+18]
Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris S. Papailiopoulos, and Kannan
Ramchandran. Speeding up distributed machine learning using codes. IEEE Trans. Information
Theory, 64(3):1514–1529, 2018.
[LSP82]
Leslie Lamport, Robert Shostak, and Marshall Pease. The byzantine generals problem. ACM
Trans. Program. Lang. Syst., 4(3):382–401, July 1982.
[LXC+19]
Liping Li, Wei Xu, Tianyi Chen, Georgios B. Giannakis, and Qing Ling.
RSA: byzantine-
robust stochastic aggregation methods for distributed learning from heterogeneous datasets. In
Conference on Artiﬁcial Intelligence (AAAI), pages 1544–1551, 2019.
[LZZ+17]
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? A case study for decentralized parallel stochas-
tic gradient descent. In Advances in Neural Information Processing Systems, NIPS 2017, 4-9
December 2017, Long Beach, CA, USA, pages 5336–5346, 2017.
[ME08]
M. Mishali and Y. C. Eldar.
Reduce and boost: Recovering arbitrary sets of jointly sparse
vectors. IEEE Transactions on Signal Processing, 56(10):4692–4702, Oct 2008.
[MGR18]
El Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Rouault.
The hidden vulnerability
of distributed learning in byzantium. In Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages
3518–3527, 2018.
[Nes12]
Yurii Nesterov. Eﬃciency of coordinate descent methods on huge-scale optimization problems.
SIAM Journal on Optimization, 22(2):341–362, 2012.
[RSS12]
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. In Proceedings of the 29th International Conference on
Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012, 2012.
[RT16]
Peter Richtárik and Martin Takáč. Parallel coordinate descent methods for big data optimization.
Mathematical Programming, 156(1):433–484, Mar 2016.
[RTDT18] Netanel Raviv, Rashish Tandon, Alex Dimakis, and Itzhak Tamo. Gradient coding from cyclic
MDS codes and expander graphs. In Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 4302–
4310, 2018.
[RWCP19] Shashank Rajput, Hongyi Wang, Zachary B. Charles, and Dimitris S. Papailiopoulos. DETOX:
A redundancy-based framework for faster and more robust gradient aggregation. In NeurIPS,
pages 10320–10330, 2019.
[Saa03]
Y. Saad. Iterative Methods for Sparse Linear Systems. Society for Industrial and Applied Math-
ematics, Philadelphia, PA, USA, 2nd edition, 2003.
[Sha79]
Adi Shamir. How to share a secret. Commun. ACM, 22(11):612–613, 1979.
37

[ST11]
Shai Shalev-Shwartz and Ambuj Tewari. Stochastic methods for l1-regularized loss minimization.
Journal of Machine Learning Research, 12:1865–1892, 2011.
[SX19]
Lili Su and Jiaming Xu. Securing distributed gradient descent in high dimensional statistical
learning. POMACS, 3(1):12:1–12:41, 2019.
[Tib15]
Ryan Tibshirani.
Convex optimization lecture notes.
http://www.stat.cmu.edu/~ryantibs/
convexopt-S15/scribes/08-prox-grad-scribed.pdf, 2015.
[TLDK17] Rashish Tandon, Qi Lei, Alexandros G. Dimakis, and Nikos Karampatziakis. Gradient coding:
Avoiding stragglers in distributed learning. In Proceedings of the 34th International Conference
on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 3368–3376,
2017.
[Wri15]
Stephen J. Wright. Coordinate descent algorithms. Math. Program., 151(1):3–34, 2015.
[XKG19]
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent
with suspicion-based fault-tolerance. In International Conference on Machine Learning (ICML),
pages 6893–6901, 2019.
[YCRB18] Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett.
Byzantine-robust dis-
tributed learning: Towards optimal statistical rates. In Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-
15, 2018, pages 5636–5645, 2018.
[YCRB19] Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter L. Bartlett. Defending against saddle
point attack in byzantine-robust distributed learning. In ICML, pages 7074–7084, 2019.
[YLR+19] Qian
Yu,
Songze
Li,
Netanel
Raviv,
Seyed
Mohammadreza
Mousavi
Kalan,
Mahdi
Soltanolkotabi, and Amir Salman Avestimehr. Lagrange coded computing: Optimal design for
resiliency, security, and privacy. In International Conference on Artiﬁcial Intelligence and Statis-
tics (AISTATS), pages 1215–1225, 2019.
[ZWLS10] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient
descent. In Advances in neural information processing systems, pages 2595–2603, 2010.
38
