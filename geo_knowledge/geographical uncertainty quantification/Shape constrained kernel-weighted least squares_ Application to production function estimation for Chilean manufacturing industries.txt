arXiv:1604.06003v5  [stat.AP]  18 Jan 2018
Shape constrained kernel-weighted least
squares: Estimating production functions for
Chilean manufacturing industries ∗
Daisuke Yagi1, Yining Chen2, Andrew L. Johnson1,3 and Timo
Kuosmanen4
1Texas A&M University
2London School of Economics and Political Science
3Osaka University
4Aalto University
January 19, 2018
Abstract
In this paper we examine a novel way of imposing shape constraints on a local
polynomial kernel estimator. The proposed approach is referred to as Shape Con-
strained Kernel-weighted Least Squares (SCKLS). We prove uniform consistency of
the SCKLS estimator with monotonicity and convexity/concavity constraints and es-
tablish its convergence rate. In addition, we propose a test to validate whether shape
constraints are correctly speciﬁed. The competitiveness of SCKLS is shown in a com-
prehensive simulation study. Finally, we analyze Chilean manufacturing data using
the SCKLS estimator and quantify production in the plastics and wood industries.
The results show that exporting ﬁrms have signiﬁcantly higher productivity.
Keywords: Local Polynomials, Kernel Estimation, Multivariate Convex Regression, Non-
parametric regression, Shape Constraints.
∗We thank two anonymous reviewers and the Associate Editor for providing useful suggestions that
helped improve this manuscript. We also thank Chris Parmeter, JeﬀRacine and Qi Li for their helpful
comments.
1

1
Introduction
Nonparametric regression methods, such as the local linear (LL) estimator, avoid functional
form misspeciﬁcation. To model production with a production or a cost function, the ﬂex-
ible nature of nonparametric methods can cause diﬃculties in interpreting the results.
Fortunately, microeconomic theory provides additional structure in the form of shape con-
straints. Recently several nonparametric shape constrained estimators have been proposed
that combine the advantage of avoiding parametric functional speciﬁcation with improved
small sample performance relative to unconstrained nonparametric estimators. Neverthe-
less, the existing methods have limitations regarding either estimation performance or
computational feasibility. In this paper, we propose a new estimator that imposes shape
restrictions on local kernel weighting methods. By combining local averaging with shape
constrained estimation, we improve ﬁnite sample performance by avoiding overﬁtting.
Work on shape-constrained regression ﬁrst started in the 1950s with Hildreth (1954),
who studied the univariate regressor case with a least squares objective subject to mono-
tonicity and concavity/convexity constraints. See also Brunk (1955) and Grenander (1956)
for alternative shape constrained estimators. Under the concavity/convexity constraint,
properties such as consistency, rate of convergence, and asymptotic distribution have been
shown by Hanson and Pledger (1976), Mammen (1991), and Groeneboom et al. (2001), re-
spectively. In the multivariate case, Kuosmanen (2008) developed the characterization of
the least squares estimator subject to concavity/convexity and monotonicity constraints,
which we will refer to as Convex Nonparametric Least Squares (CNLS) throughout this
paper. Furthermore, consistency of the least squares estimator was shown independently
by Seijo and Sen (2011) and Lim and Glynn (2012).
Regarding the nonparametric estimation implemented using kernel based methods,
Birke and Dette (2007), Carroll et al. (2011), and Hall and Huang (2001) investigated the
univariate case and proposed smooth estimators that can impose derivative-based con-
straints including monotonicity and concavity/convexity. Du et al. (2013) proposed Con-
strained Weighted Bootstrap (CWB) by generalizing Hall and Huang’s method to the
multivariate regression setting. Beresteanu (2007) developed a similar type of estimator
2

but for use with spline based estimators. Finally, we mention the work of Li et al. (2016),
which extended Hall and Huang’s method to use the k-nearest neighbor approach subject
to the monotonicity constraint.
In this paper, Shape Constrained Kernel-weighted Least Squares (SCKLS) estimator
is described, which optimizes a local polynomial kernel criterion while estimating a mul-
tivariate regression function with shape constraints.
Under the monotonicity and con-
vex/concavity constraints, we prove uniform consistency and establish the convergence
rate of the SCKLS estimator. Kuosmanen (2008), Seijo and Sen (2011) and Lim and Glynn
(2012) emphasize the potential advantage that CNLS does not require the selection of tun-
ing parameters. Our proposed SCKLS estimator sheds further light on this issue: in the
SCKLS framework, CNLS can be seen as the zero bandwidth estimator; we argue that,
compared to unrestricted kernel methods, the SCKLS estimator is relatively robust to the
bandwidth selected and is able to alleviate well-known issues such as boundary inconsis-
tency faced by the CNLS estimator.
Note that with n observations, CNLS imposes O(n2) concavity/convexity constraints,
which can lead to computational diﬃculties. The number of constraints and the number of
variables in the SCKLS estimator do not depend on the number of observations, but rather
the number of evaluation points which is arbitrarily deﬁned by the modeler, thereby bring
the computational complexity of the estimator largely under control of the modeler. In
this paper, we implement an iterative algorithm that reduces the number of constraints by
building on the ideas in Lee et al. (2013) to further improve the computational performance.
We then validate the performance of the SCKLS estimator via Monte Carlo simulations.
For a variety of parameter settings, we ﬁnd performance of SCKLS to be better or at
least competitive with CNLS, CWB, and the local linear estimators. We provide the ﬁrst
simulation study of CWB with global concavity constraints. We also investigate the use of
variable bandwidth methods that are a function of the data density 1 and propose variants
of a uniform grid as practical ways to further improve the performance of SCKLS.
Crucially, we also investigate the behavior of SCKLS when the shape constraints are
misspeciﬁed and propose a hypothesis test to validate the shape constraints imposed. Hav-
1A variable bandwidth method allows the bandwidth associated with a particular regressor to vary with
the density of the data.
3

ing a test that validates the shape constraints is critical because otherwise our estimation
procedure would lead to inconsistent estimates.
Finally, we apply the SCKLS estimator empirically on Chilean manufacturing data from
the Chilean Annual Industrial Survey. The estimation results provide a concise descrip-
tion of the supply-side of the Chilean plastic and wood industries as we report marginal
productivity, marginal rate of substitution and most productive scale size. We also investi-
gate the impact of exporting on productivity by including additional predictors of output
in a semi-parametric model. We ﬁnd that exporting correlates with higher productivity,
thus supporting international trade theories that high productivity ﬁrms are more likely to
compete in international markets.
Our focus on production functions guides our selection of the polynomial function used
in estimation, the data generation processes (DGP) in the Monte Carlo simulations. For
the application analyzing the Chilean manufacturing data, we are interested in monotonic
and concave shape constraints and use a local linear kernel function. These assumptions
are motivated by standard economic theory for production functions (Varian, 1984). How-
ever, the methods proposed in the paper are general and applicable for other applications
with higher order polynomial functions or alternative shape restrictions, as discussed in
Appendix A.
The remainder of this paper is as follows. Section 2 describes the model framework and
presents our estimator, SCKLS. Section 3 contains the statistical properties of the estima-
tor, and Section 4 discusses the behavior of SCKLS under misspeciﬁcation, as well as a
test for concavity and monotonicity. Monte Carlo simulation results under several diﬀerent
experimental settings are shown in Section 5. Section 6 applies the SCKLS estimator to
estimate a production function for both the Chilean plastics and wood industries. Sec-
tion 7 concludes and suggests future research directions. Appendix A provides extensions
to SCKLS and a comparison to CNLS and CWB. Appendix B contains all the technical
proofs and Appendix C describes a test for aﬃnity. Appendix D states the details of the
iterative algorithm for SCKLS, and Appendix E presents a more extensive set of simulation
results. Appendix F describes the details of the partially linear model, and Appendix G
gives further details about the application to the Chilean manufacturing data.
4

2
Model Framework and Methodology
2.1
Model
Suppose we observe n pairs of input and output data, {Xj, yj}n
j=1, where for every j =
1, . . . , n, Xj = (Xj1, . . . , Xjd)′ ∈Rd is a d-dimensional input vector, and yj ∈R is an
output. Consider the following regression model
yj = g0(Xj) + ǫj,
for j = 1, . . . , n,
where ǫj is a random variable satisfying E(ǫj|Xj) = 0. Assume that the regression function
g0 : Rd →R belongs to a class of functions, G, that satisﬁes certain shape restrictions.
Here our estimator can impose any shape restriction that can be modeled as a lower or
upper bound on a derivative. Examples are supermodularity, convexity, monotonicity, and
quasi-convexity. For purposes of concreteness, and in view of the application to production
functions, we focus on imposing monotonicity and global convexity/concavity, speciﬁcally,
g0 is concave if:
λg0(x1) + (1 −λ)g0(x2) ≤g0(λx1 + (1 −λ)x2),
∀x1, x2 ∈Rd and ∀λ ∈[0, 1]
Furthermore, saying g0 is monotonically increasing means that
if x1 ≤x2, then g0(x1) ≤g0(x2),
where the inequality of x1 ≤x2 means that every component of x2 is greater than or equal
to the corresponding component of x1. Here we denote G2 as the set of functions satisfying
these constraints.
5

2.2
Shape Constrained Kernel-weighted Least Squares (SCKLS)
with Local Linear
Given observations {Xj, yj}n
j=1, we state the (multivariate) local linear kernel estimator
developed by Stone (1977) and Cleveland (1979) as
min
a,b
n
X
j=1
(yj −a −(Xj −x)′b)2K
Xj −x
h

,
(1)
where a is a functional estimate, and b is an estimate of the slope of the function at x with
x being an arbitrary point in the input space, K

Xj−x
h

denotes a product kernel, and
h is a vector of bandwidths (see Racine and Li (2004) for more detail). We note that the
objective function uses kernel weights, so more weight is given to the observations that are
closer to the point x.
We introduce a set of m points, x1, . . . , xm, for evaluating constraints, which we call
evaluation points, and impose shape constraints on the local linear kernel estimator. In the
spirit of local linear kernel estimator, we deﬁne Shape Constrained Kernel-weighted Least
Squares (SCKLS) estimator, for the case of monotonicity and concavity, to be the function
ˆgn : Rd →R such that
ˆgn(x; ˆa, ˆb) =
min
i∈{1,...,m}
n
ˆai + (x −xi)′ˆbi
o
(2)
for any x ∈Rd, where ˆa = (ˆa1, . . . , ˆam)′ and ˆb = (ˆb′
1, . . . , ˆb′
m)′ are the solutions to the
following optimization problem
min
a,b
m
X
i=1
n
X
j=1
(yj −ai −(Xj −xi)′bi)2K
Xj −xi
h

subject to
ai −al ≥b′
i(xi −xl),
i, l = 1, . . . , m
bi ≥0,
i = 1, . . . , m.
(3)
The ﬁrst set of constraints in (3) imposes concavity and the second set of constraints
imposes non-negativity of bi at each evaluation point xi. For more details see Kuosmanen
6

(2008). Note that (2) implies the functional estimate is constructed by taking the minimum
of linear interpolations between the evaluation points. This makes SCKLS a globally shape
constrained function although it is a non-smooth piece-wise linear function.
The SCKLS estimator requires the user to specify the number and the locations of the
evaluation points. A standard method for determining the location of evaluation points,
{xi}m
i=1, is to construct a uniform grid, where each dimension is divided using equal spacing.
However, we can address the skewness of input variable distributions common in manufac-
turing survey data by using a non-uniform grid method, speciﬁcally percentile gridding, to
specify evaluation points.
Alternatively, we can deal with the input skewness by applying the k-nearest neighbor
(k-NN) approach, Li et al. (2016). The k-NN approach uses a smaller bandwidth in dense
data regions and a larger bandwidth when the data is sparse. The analysis in Section 6
uses both a percentile grid and k-NN approach to deﬁne the kernel function. For details of
these extensions, see Appendix A.
As the density of the evaluation points increases, the estimated function potentially
has more hyperplane components and is more ﬂexible; however, the computation time
typically increases. If a smooth functional estimate is preferred, see Nesterov (2005) and
Mazumder et al. (2015), where methods for smoothing are provided. In practice, we pro-
pose to select the bandwidth vector h via the leave-one-out cross-validation based on the
unconstrained estimator. See Section 5 for the details.
Appendix A proposes several alternative implementations of the SCKLS estimator: (1)
SCKLS with Local Polynomial approximation, (2) a k-nearest neighbor (k-NN) approach
and (3) non-uniform grid method.
3
Theoretical Properties of SCKLS
For mathematical concreteness, we next consider the statistical properties of SCKLS under
monotonicity and concavity constraints. Recall that G2 is the class of functions which are
monotonically increasing and globally concave, and g0 is the truth to be estimated from n
pairs of observations. We make the following assumptions:
7

Assumption 1.
(i) {Xj, yj}∞
j=1 are a sequence of i.i.d. random variables with yj = g0(Xj) + ǫj.
(ii) g0 ∈G2 and is twice-diﬀerentiable.
(iii) Xj follows a distribution with continuous density function f and support S. Here S
is a convex, non-degenerate and compact subset of Rd. Moreover,
min
x∈S f(x) > 0.
(iv) The conditional probability density function of ǫj, given Xj, denoted as p(e|x), is
continuous with respect to both e and x, with the mean function
µ(·) = E(ǫj|Xj = ·) = 0
and the variance function
σ2(·) = Var(ǫj|Xj = ·)
bounded away from 0 and continuous over S. Moreover, supx∈S E

ǫ4
j
Xj = x

< ∞.
(v) K(·) is a non-negative, Lipschitz second order kernel with a compact and convex sup-
port. For simplicity, we set the bandwidth associated with each explanatory variable,
hk, for k = 1, . . . , d, to be h1 = · · · = hd = h.
(vi) h = O(n−1/(4+d)) as n →∞.
Here (i) states that the data are i.i.d.; (ii) says that the constraints we impose on
the SCKLS estimator are satisﬁed by the true function; (iii) makes a further assumption
on the distribution of the covariates; (iv) states that the noise can be heteroscedastic in
certain ways, but requires the change in the variance to be smooth; (v) is rather standard
in local polynomial estimation to facilitate the theoretical analysis; and (vi) assures the
bandwidths become suﬃciently small as n →∞so that both the bias and the variance
from local averaging go to zero. For details of the consistency of local linear estimator
8

and a discussion of some of these conditions, see Masry (1996), Li and Racine (2007) and
Fan and Guerre (2016).
We consider two scenarios: let the number of evaluation points (denoted by m) grow
with n, or ﬁx the number of evaluation points a priori. For simplicity, we also assume that
the evaluation points are drawn independent of {Xj, yj}n
j=1.
Assumption 2.
(i) The number of evaluation points m →∞as n →∞. For simplicity, we assume
that the empirical distribution of {x1, . . . , xm} converges to a distribution Q that
has support S (i.e. as deﬁned in Assumption 1(iv))) and a continuous diﬀerentiable
density function q : S →R satisfying minx∈S q(x) > 0.
(ii) The number of evaluation points m is ﬁxed. All the evaluation points lie in the interior
of S. Moreover,
supx∈S mini=1,...,m ∥x −xi∥
mini̸=j;i,j∈{1,...,m} ∥xj −xi∥≤κ
for some κ ≥1 (i.e. {x1, . . . , xm} are reasonably well spread across S).
Our main results are summarized below. A short discussion on our proof strategy and
the proofs are available in Appendix B.
Theorem 1. Suppose that Assumption 1(i)-1(vi) and Assumption 2(i) or 2(ii) hold. Then,
1
m
m
X
i=1
{ˆgn(xi) −g0(xi)}2 = O(n−4/(4+d) log n)
.
Theorem 2.
1. (The case of an increasing m) Suppose that Assumption 1(i)-1(vi) and Assumption
2(i) hold. Let C be any ﬁxed closed set that belongs to the interior of S. Then with
probability one, as n →∞, the SCKLS estimator satisﬁes
sup
x∈C
ˆgn(x) −g0(x)
 →0.
9

2. (The case of a ﬁxed m) Suppose that Assumption 1(i)-1(vi) and Assumption 2(ii)
hold. Then, as n →∞, with probability one, the estimates from SCKLS satisfy
ˆai →g0(xi)
and
ˆbi
→∂g0
∂x (xi)
for all i = 1, . . . , m.
Note that this convergence rate is nearly optimal (diﬀering only by a factor of log n).
However, in the above, we only manage to show that the SCKLS estimator converges at the
evaluation points or in the interior of the domain. It is known that shape-constrained esti-
mators tend to suﬀer from bad boundary behaviors. For instance, the quantity supS
ˆgCNLS
n
(x)−
g0(x)
 does not converge to zero in probability, where ˆgCNLS
n
is the CNLS estimator.
Though for SCKLS, if we let the number of evaluation points, m, grow at a rate slower
than n, we argue that we can both alleviate the boundary inconsistency and improve the
computational eﬃciency.
Assumption 3. The number of evaluation points m = o(n2/(4+d)/ log n) as n →∞.
Theorem 3. Suppose that Assumption 1(i)-1(vi), Assumption 2(i) and Assumption 3 hold.
Then, with probability one, as n →∞, the SCKLS estimator satisﬁes
sup
x∈S
ˆgn(x) −g0(x)
 →0.
We also note that CNLS can be viewed as a special case of SCKLS when we let the set
of evaluation points be {X1, . . . , Xn} and the bandwidth vector ∥h∥→0. See Appendix A
for the proof of the relationship between CNLS and SCKLS, together with more discussions
on the relationship between SCKLS and alternative shape constrained estimators such as
CWB.
10

4
Shape Misspeciﬁcation: Theory and Testing
4.1
Misspeciﬁcation of the shape restrictions
So far we have assumed in our estimation procedures that g0 ∈G2, where G2 is the class
of functions which are monotonically increasing and globally concave. To understand the
behavior of SCKLS, we are interested in its performance when g0 /∈G2.
Let Q be a distribution on S (as in Assumption 2(i)) and deﬁne g∗: S →R as
g∗
0 := argmin
g∈G2
Z
S
{g(x) −g0(x)}2Q(dx).
The existence and Q-uniqueness of g∗
0 follows from the well-known results about the pro-
jection onto a cone in the Hilbert space. When g0 ∈G2, it is easy to check that g∗
0 = g0.
See also Lim and Glynn (2012). The following result can be viewed as a generalization of
Theorem 2.
Theorem 4.
Suppose that Assumption 1(i), 1(iii)-1(vi) and Assumption 2(i) hold.
Furthermore,
suppose that g0 is twice-diﬀerentiable. Let C be any compact set that belongs to the interior
of S. Then with probability one, as n →∞, the SCKLS estimator satisﬁes
sup
x∈C
ˆgn(x) −g∗
0(x)
 →0.
Theorem 4 assures us that the SCKLS estimator converges uniformly on a compact
set to the function g∗
0 that is closest in L2 distance to the true function g0 for which our
estimator is misspeciﬁed. Consequently, as long as g0 is not too far away from G2, our
estimator can still be used as a reasonable approximation to the truth, especially when the
sample size is moderate. See Appendix E for a numerical demonstration.
11

4.2
Hypothesis Testing for the Shape
Admittedly, the SCKLS estimator can be inappropriate if the shape constraints are not
fulﬁlled by g0. Thus, we propose a procedure based on the SCKLS estimators for testing
H0 :
{g0 : S →R} ∈G2
against
H1 :
{g0 : S →R} /∈G2.
Denote by
˜r2
{Xj, yj}n
j=1, {xi}m
i=1

= min
a,b
m
X
i=1
n
X
j=1
(yj −ai −(Xj −xi)′bi)2K
Xj −xi
h

;
the value of the objective function that is minimized by the local linear kernel estimator.
And denote by
ˆr2
{Xj, yj}n
j=1, {xi}m
i=1

= min
a,b
m
X
i=1
n
X
j=1
(yj −ai −(Xj −xi)′bi)2K
Xj −xi
h

,
subject to
ai −al ≥b′
i(xi −xl) and bi ≥0, i, l = 1, . . . , m.
Here ˆr2(·, ·) is the value of the objective function that is minimized by SCKLS.
We focus on the test statistic
Tn := T

{Xj, yj}n
j=1, {xi}m
i=1

=
h
1
mnhd
n
ˆr2
{Xj, yj}n
j=1, {xi}m
i=1

−˜r2
{Xj, yj}n
j=1, {xi}m
i=1
oi1/2
,
which is a re-scaled version of the diﬀerence between the values of the same objective func-
tion (with the same bandwidth h), optimized either with or without the shape constraints.
Intuitively, the value of this statistic should be small if g0 ∈G2. This statistic can also be
viewed as a smoothed and re-scaled version of the goodness-of-ﬁt statistic.
Here we focus on the boundary case when g0 is constant (i.e. g0 = 0) because it is
hardest to evaluate the null hypothesis when g0 is both non-increasing and non-decreasing
and both concave and convex, intuitively and theoretically and it allows us to control the
size of our test statistic. Since the noise here might be non-homogeneous, we use the wild
bootstrap to approximate the distribution of the test statistic under H0. See Wu (1986),
12

Liu (1988), Mammen (1993) and Davidson and Flachaire (2008) for an overview of the wild
bootstrap procedure.
Our testing procedure has three steps:
1. Estimate the error at each Xj by ˜ǫj = yj −˜gn(Xj) for j = 1, . . . , n, where ˜g is the un-
constrained local linear estimator with kernel and bandwidth satisfying Assumptions
1(v)–(vi).
2. The wild bootstrap method is used to construct a critical region for Tn. Let B be the
number of Monte Carlo iterations. For every k = 1, . . . , B, let uk = (u1k, . . . , unk)′
be a random vector with components sampled independently from the Rademacher
distribution, i.e. P(ujk = 1) = P(ujk = −1) = 0.5. Furthermore, let yjk = ujk ˜ǫj.
Then, the wild bootstrap test statistic is
Tnk = T

{Xj, yjk}n
j=1, {xi}m
i=1

.
3. Deﬁne the Monte Carlo p-value as2
pn = 1
B
B
X
k=1
1{Tn≤Tnk}.
For a test of size α ∈(0, 1), we reject H0 if pn < α.
A few remarks are in order.
First, here we conveniently implemented the simplest wild bootstrap scheme to simplify
our analysis, in line with the work of Davidson and Flachaire (2008). Instead of imposing
the Rademacher distribution on ukj, we can also use any distribution with zero-mean and
unit-variance. One popular choice suggested by Mammen (1993) is
ujk =



−
√
5−1
2
with probability 5+
√
5
10
√
5+1
2
with probability 5−
√
5
10
.
2Since we underestimate the level of the errors in Step 1 by a factor of roughly n−2/(4+d), for the theoret-
ical development, we address this bias issue by modifying the p-value to be pn = 1
B
PB
k=1 1{Tn≤Tnk+∆n},
where ∆n = O(n−2/(4+d) log n).
Note that if we ﬁx m and pick h = O(n−η) for η ∈(
1
4+d, 1
d), then
∆n/Tnk = op(1) as n →∞, i.e. this correction has a negligible eﬀect. Indeed, our experience suggests that
this modiﬁcation oﬀers little improvement in terms of ﬁnite sample performance in our simulation study.
13

Second, note that the deﬁnition of yjk in Step 2 makes this a test of the residuals, i.e.,
when drawing bootstrap samples, we use yjk = ujk ˜ǫj instead of yjk = ˆgn(Xj)+ujk ˜ǫj. From
this perspective, our test is similar to the univariate monotonicity test in Hall and Heckman
(2000). One reason behind this choice is to avoid the boundary inconsistency of the boot-
strap procedure. See Andrews (2000) and Cavaliere et al. (2017) who addressed this issue
in a much simpler setup. Generally speaking, testing the null hypothesis becomes harder
when g0 is on the boundary of G2. In practice, we could use yjk = ˆgn(Xj)+ujk ˜ǫj in certain
scenarios (e.g. when testing g0 is a strictly increasing and strictly concave function against
g0 /∈G2), and slight improvements are observed in terms of ﬁnite-sample performance.
We now look into the theoretical properties of our procedure under both H0 and H1.
See Appendix B for the proof.
Theorem 5. Suppose that Assumptions 1(i),(iii)–(v) and 2(i) hold, and the conditional
error distribution (i.e. ǫj|Xj) is symmetric. Furthermore, assume that g0 is continuously
twice-diﬀerentiable and let h = O(n−η) for some ﬁxed η ∈(
1
4+d, 1
d). Let B := B(n) →∞
as n →∞. Then, for any given α ∈(0, 1),
– Type I error: for any g0 ∈G2, lim supn→∞P(pn < α) ≤α;
– Type II error: for any g0 /∈G2, lim supn→∞
n
1 −P(pn < α)
o
= 0.
In addition, if we replace Assumption 2(i) by Assumption 2(ii), the same conclusions hold
for suﬃciently large m.
See also Section 5 for the ﬁnite-sample performance of our test in a simulation study,
where we demonstrate that the proposed test controls both Type I and Type II errors
reasonably well.
Additionally, Appendix C describes our procedure for testing aﬃnity
using SCKLS.
14

5
Simulation study
5.1
Numerical experiments on estimation
5.1.1
The setup
We now examine the ﬁnite sample performance and robustness of the proposed estimator
through Monte Carlo simulations. We run our experiments on a computer with Intel Core2
Quad CPU 3.00 GHz and 8GB RAM. We compare the performance of SCKLS is compared
with that of CNLS and LL. See Appendix E for a comparisons of SCKLS with CWB. For
the SCKLS and the CNLS estimator, we solve the quadratic programming problems with
MATLAB using the built-in quadratic programming solver, quadprog. We run two sets of
experiments varying the number of observations (n), the number of evaluation points (m),
and the number of the inputs (d). We also run additional experiments to show the robust
performance of the SCKLS estimator under alternative conditions. See Appendix E for the
results.
We measure the estimator’s performance using Root Mean Squared Errors (RMSE)
based on two criteria: the distance from the estimated function to the true function mea-
sured 1) at the observed points and 2) at the evaluation points constructed on an uniform
grid , respectively. As CNLS estimates hyperplanes at observation points, we use linear in-
terpolation to obtain the RMSE of CNLS3. We replicate each scenario 10 times and report
the average and standard deviation.
5.1.2
Choosing of the tuning parameters
For the SCKLS estimator, we use the Gaussian kernel function K(·) and leave-one-out
cross-validation (LOOCV) for bandwidth selection. LOOCV is a data-driven method, and
has been shown to perform well for unconstrained kernel estimators such as local linear
(Stone, 1977). We apply LOOCV procedure on unconstrained estimates (i.e. local linear) to
select the bandwidth for SCKLS to reduce the computational burden and because SCKLS is
3The CNLS estimates include the second stage linear programming estimation procedure described in
Kuosmanen and Kortelainen (2012) to ﬁnd the minimum extrapolated production function.
15

relatively insensitive to the bandwidth choice (see for example Section 5.1.3.1). For further
computational improvements, we apply the iterative algorithm described in Appendix D.
5.1.3
Results
5.1.3.1
Fixed number of evaluation points
Experiment 1. We consider a Cobb–Douglas production function with d-inputs and one-
output, g0(x1, . . . , xd) = Qd
k=1 x
0.8
d
k . For each pair (Xj, yj), each component of the input,
Xjk, is randomly and independently drawn from uniform distribution unif[1, 10], and the
additive noise, ǫj, is randomly sampled from a normal distribution, N(0, 0.72). We consider
15 diﬀerent scenarios with diﬀerent numbers of observations (100, 200, 300, 400 and 500)
and input dimensions (2, 3 and 4). The structure and data generation process of Experiment
1 follows Lee et al. (2013). We ﬁx the number of evaluation points at approximately 400
and locate them on a uniform grid.
For this experiment, we compare the following four estimators: SCKLS, CNLS, Local
Linear Kernel (LL), and parametric Cobb–Douglas estimator. The latter estimator serves
as a baseline because it is correctly speciﬁed parametric form. Tables 1 and 2 show for Ex-
periment 1 the RMSE measured on observation points and evaluation points, respectively.
The number in parentheses is the standard deviation of RMSE values computed by 10
replications. Note the standard derivations are generally small compared to the parameter
estimates, which indicates low variability even after only 10 replications. A more extensive
set of results for this experiment is summarized in Appendix E. The SCKLS estimator has
the lowest RMSE in most scenarios even when RMSE is measured on observation points
(note that the SCKLS estimator imposes the global shape constraints via evaluation points
in Equation (3)). Also as expected, the performance of SCKLS estimator improves as the
number of observation points increases. Moreover, the SCKLS estimator performs better
than the LL estimator particularly in higher dimensional functional estimation. This pro-
vides empirical evidence that the shape constraints in SCKLS are helpful in improving the
ﬁnite sample performance as compared to LL. Note that LL appears to have larger RMSE
values on evaluation points which are located in input space regions with sparse observa-
tions. This implies that the SCKLS estimator has more robust out-of-sample performance
16

than the LL estimator due to the shape constraints. We also observe that the performance
of the CNLS estimator measured at the evaluation points is worse than that measured at
the observations. CNLS often has ill-deﬁned hyperplanes which are very steep/shallow at
the edge of the observed data, and this over-ﬁtting leads to poor out-of-sample performance.
In contrast, the SCKLS estimator performs similarly for both the observation points and
evaluation points, because the construction of the grid that completely covers the observed
data makes the SCKLS estimator more robust.
We also conduct simulations with diﬀerent bandwidths to analyze the sensitivity of each
estimator to bandwidths. We compare SCKLS and LL with bandwidth h ∈[0, 10] with an
increment by 0.01 for the 1-input setting, and we use bandwidth h ∈[0, 5] × [0, 5] with an
increment by 0.25 in each coordinate for the 2-input setting. We simulate 100 datasets to
compute the RMSE for each bandwidth as well as for the bandwidth via LOOCV. Figure
1 displays the average RMSE of each estimator. The histogram shows the distribution of
bandwidths selected by LOOCV. The instances when SCKLS and LL provide the lowest
RMSE are shown in light gray and dark gray respectively. For the one-input scenario, the
SCKLS estimator performs better than the LL estimator for bandwidth between 0.25 -
Table 1. RMSE on observation points for Experiment 1.
Average of RMSE on observation points
Number of observations
100
200
300
400
500
2-input
SCKLS
0.193
0.171
0.141
0.132
0.118
(0.053)
(0.047)
(0.032)
(0.029)
(0.017)
CNLS
0.229
0.163
0.137
0.138
0.116
(0.042)
(0.037)
(0.010)
(0.027)
(0.016)
LL
0.212
0.166
0.149
0.152
0.140
(0.079)
(0.042)
(0.028)
(0.028)
(0.028)
Cobb–Douglas
0.078
0.075
0.048
0.039
0.043
3-input
SCKLS
0.230
0.187
0.183
0.152
0.165
(0.050)
(0.026)
(0.032)
(0.019)
(0.031)
CNLS
0.294
0.202
0.189
0.173
0.168
(0.048)
(0.035)
(0.020)
(0.014)
(0.020)
LL
0.250
0.230
0.235
0.203
0.181
(0.068)
(0.050)
(0.052)
(0.050)
(0.021)
Cobb–Douglas
0.104
0.089
0.070
0.047
0.041
4-input
SCKLS
0.225
0.248
0.228
0.203
0.198
(0.038)
(0.020)
(0.037)
(0.042)
(0.028)
CNLS
0.315
0.294
0.246
0.235
0.214
(0.039)
(0.027)
(0.024)
(0.029)
(0.015)
LL
0.256
0.297
0.252
0.240
0.226
(0.044)
(0.057)
(0.056)
(0.060)
(0.038)
Cobb–Douglas
0.120
0.073
0.091
0.067
0.063
17

Table 2. RMSE on evaluation points for Experiment 1.
Average of RMSE on evaluation points
Number of observations
100
200
300
400
500
2-input
SCKLS
0.219
0.189
0.150
0.147
0.128
(0.053)
(0.057)
(0.034)
(0.030)
(0.021)
CNLS
0.350
0.299
0.260
0.284
0.265
(0.082)
(0.093)
(0.109)
(0.119)
(0.078)
LL
0.247
0.182
0.167
0.171
0.156
(0.101)
(0.053)
(0.030)
(0.030)
(0.034)
Cobb–Douglas
0.076
0.076
0.049
0.040
0.043
3-input
SCKLS
0.283
0.231
0.238
0.213
0.215
(0.072)
(0.033)
(0.030)
(0.029)
(0.034)
CNLS
0.529
0.587
0.540
0.589
0.598
(0.112)
(0.243)
(0.161)
(0.109)
(0.143)
LL
0.336
0.340
0.360
0.326
0.264
(0.085)
(0.093)
(0.108)
(0.086)
(0.042)
Cobb–Douglas
0.116
0.098
0.080
0.052
0.046
4-input
SCKLS
0.321
0.357
0.329
0.308
0.290
(0.046)
(0.065)
(0.049)
(0.084)
(0.044)
CNLS
0.845
0.873
0.901
0.827
0.792
(0.188)
(0.137)
(0.151)
(0.235)
(0.091)
LL
0.482
0.527
0.483
0.495
0.445
(0.115)
(0.125)
(0.146)
(0.153)
(0.074)
Cobb–Douglas
0.146
0.091
0.115
0.081
0.080
2.25 as shown in (a). For the two-input scenario, the SCKLS estimator performs better
for most of the LOOCV values as shown by the majority of the histogram colored in light
gray. This indicates that LOOCV, calculated using the unconstrained estimator, provides
bandwidths that work well for the SCKLS estimator. Importantly, the SCKLS estimator
does not appear to be very sensitive to the bandwidth selection method since, heuristically,
the shape constraints help reduce the variance of the estimator.
Finally, we note that
similar results can be obtained in experimental settings with lower signal-to-noise level, or
with non-uniform input. See Appendix E for more details.
5.1.3.2
Diﬀerent numbers of evaluation points
Experiment 2. The setting is the same as Experiment 1.
However, now we consider 9
diﬀerent scenarios with diﬀerent numbers of evaluation points (100, 300 and 500) and
input dimensions (2, 3 and 4). We ﬁx the number of observed points at 400.
We show the performance of SCKLS. Table 3 and 4 shows for Experiment 2 the RMSE
measured on observations and evaluation points respectively. Both tables show that em-
pirically even if we increase the number of evaluation points, the RMSE value does not
18

(a) One-input
(b) Two-input
Figure 1. The histogram shows the distribution of bandwidths selected by LOOCV. The
curves show the relative performance of each estimator.
change signiﬁcantly. This has important implications for the running time. Speciﬁcally,
we can reduce the calculation time by using a rough grid without sacriﬁcing too much in
terms of RMSE performance of the estimator.
Table 3. RMSE on observation points for Experiment 2.
Average of RMSE on observation points
Number of evaluation points
100
300
500
2-input
SCKLS
0.142
0.141
0.141
3-input
SCKLS
0.198
0.203
0.197
4-input
SCKLS
0.239
0.207
0.206
Table 4. RMSE on evaluation points for Experiment 2.
Average of RMSE on evaluation points
Number of evaluation points
100
300
500
2-input
SCKLS
0.181
0.164
0.158
3-input
SCKLS
0.304
0.267
0.257
4-input
SCKLS
0.383
0.296
0.270
19

5.2
Numerical experiments on testing the imposed shape
Experiment 3. We test monotonicity and concavity for data generated from the following
single-input and single-output DGP:
g0(x) = xp
(4)
and
g0(x) =
1
1 + exp(−5 log(2x)).
(5)
With n observations, for each pair (Xj, yj), each input, Xj, is randomly and indepen-
dently drawn from uniform distribution unif[0, 1]. In this simulation, we use the following
multiplicative noise to validate whether the wild bootstrap can handle non-homogeneous
noise.
yj = g0(Xj) + (Xj + 1) · ǫj,
where ǫj, is randomly and independently sampled from a normal distribution, N(0, σ2).
We use three diﬀerent DGP scenarios A, B and C. For scenarios A and B, we use function
(4) where the exponent parameter p deﬁnes whether the function g0 is an element of the
class of functions G2 or not. We use p = {0, 2} for scenarios A and B respectively, where
g0 ∈G2 if p = 0, and g0 /∈G2 if p = 2 since g0 is strictly convex. For scenario C, we
consider an “S”-shape function deﬁned by (5) which violates both global concavity and
convexity. We consider diﬀerent sample sizes n = {100, 300, 500} and standard deviation
of the noise σ = {0.1, 0.2}, and perform 500 simulations to compute the rejection rate for
each scenario. We assume that we do not know the distribution of the noise in advance
and use the wild bootstrap procedure described in Section 4.2 with B = 200.
Table 5 shows the rejection rate for each DGP. For high signal-to-noise ratio scenarios
(σ = 0.1), the test works well even with a small sample size. Our test is able to control the
Type I error, as illustrated in scenario A. In addition, the Type II error of our test is small
for the scenarios B and C where shape constraints are violated by the DGP. Furthermore,
for low signal-to-noise ratio scenarios (σ = 0.2), the rejection rate for scenarios B and
C signiﬁcantly improves when the sample size is increased from 100 to 300. Indeed, for
20

larger noise scenarios more data is required for the test to have power. Thus, our test
seems informative enough to guide users to avoid imposing shape constraints on the data
generated from misspeciﬁed functions.
Table 5. Rejection rate (%) of the test for monotonicity and concavity
Sample size
DGP Scenario
Power of the Test (α)
0.05
0.01
0.05
0.01
(n)
σ = 0.1
σ = 0.2
100
A (H0)
5.8
2.0
8.0
2.6
B (H1)
98.6
94.6
55.0
36.2
C (H1)
98.6
94.4
42.6
24.2
300
A (H0)
6.8
1.8
6.6
3.0
B (H1)
100.0
100.0
92.0
83.2
C (H1)
100.0
100.0
97.0
86.8
500
A (H0)
5.4
1.6
5.6
1.4
B (H1)
100.0
100.0
99.4
97.2
C (H1)
100.0
100.0
99.8
99.4
21

6
Application
We apply the proposed method to estimate the production function for two large industries
in Chile: plastic (2520) and wood manufacturing (2010) where the values inside the paren-
theses indicate the CIIU3 industry code. There are some existing studies which analyze
the productivity of Chilean data, see for example Pavcnik (2002), who analyzed the ef-
fect of trade liberalization on productivity improvements. Other researchers have analyzed
the productivity of Chilean manufacturing including Benavente (2006), Alvarez and G¨org
(2009) and Levinsohn and Petrin (2003). However, the above-cited work use strong para-
metric assumptions and older data. Most studies use the Cobb–Douglas functional form
which restricts the elasticity of substitution to be 1. When diminishing marginal produc-
tivity of inputs characterizes the data, the Cobb–Douglas functional form imposes that
the most productive scale size is at the origin. We relax the parametric assumptions and
estimate a shape constrained production function nonparametrically using data from 2010.
We examine the marginal productivity, marginal rate of substitution, and most productive
scale size (MPSS) to analyze the structure of the industries. We also investigate how pro-
ductivity diﬀers between exporting and non-exporting ﬁrms, as exporting has become an
important source of revenue in Chile4. See Appendix G for the details of estimation and
comparison across diﬀerent estimators.
6.1
The census of Chilean manufacturing plants
We use the Chilean Annual Industrial Survey provided by Chile’s National Institute of
Statistics5. The survey covers manufacturing establishments with ten or more employees.
We deﬁne Capital and Labor as the input variables and Value Added as the output variable
of the production function6. Capital and Value Added are measured in millions of Chilean
4Note that ﬁrms’ decisions, i.e., selecting labor and capital levels with considerations for productivity
levels or whether to export, are potentially endogenous. Solutions to this issue are to instrument or build
a structural model based on timing assumptions. Our estimator can be embedded within the estimation
procedures such as those described in Ackerberg et al. (2015) to address this issue.
5The data are available at http://www.ine.cl/estadisticas/economicas/manufactura.
6The deﬁnition of Labor includes full-time, part-time, and outsourced labors.
Capital is de-
ﬁned as a sum of the ﬁxed assets balance such as buildings, machines, vehicles, furniture, and
technical software.
Value added is computed
by subtracting the cost of raw materials and
22

peso while Labor is measured as the total man-hours per year. We use cross sectional data
from the plastic and the wood industries.
Many researchers have found positive eﬀects of exporting for other countries using
parametric models. See for instance, De Loecker (2007) and Bernard and Jensen (2004).
Here we use SCKLS to relax the parametric assumption for the production function. To
capture the eﬀects of exporting, we use a semi-parametric modeling extension of SCKLS.
The partially linear model is represented as follows:
yj = Z′
jγ + g0(Xj) + ǫj,
(6)
where Zj = (Zj1, Zj2)′ denotes contextual variables and γ = (γ1, γ2)′ is the coeﬃcient of
contextual variables. We model exporting with two variables: a dummy variable indicating
the establishments that are exporting and the share of output being exported. For more
details see Appendix F.
Table 6 presents the summary of statistics for each industry by exporter/non-exporter.
We ﬁnd that exporters are typically larger than non-exporter in terms of labor and capital.
Input variables are positively skewed, indicating there exist many small and few large es-
tablishments. Since SCKLS with variable bandwidth (k-nearest neighbor) and non-uniform
grid performed the best in our simulation scenarios with non-uniform input data (as in-
dicated in Appendix E), we use these options. We choose the smoothing parameter k via
leave-one-out cross validation. Appendix A explains the details of our implementation of
K-NN for the SCKLS estimator.
Figure 2 is a plot of labor and capital for each industry and shows input data is sparse
for large establishments. Beresteanu (2005) proposed to include shape constraints only
for the evaluation points that are close to the observations. Thus, in addition to using a
percentile grid of evaluation points, we propose to use the evaluation points that are inside
the convex hull of observed input {Xj}n
j=1. See Appendix G for details.
We begin by testing if the Cobb–Douglas production function is appropriate for our
data. We use the hypothesis test for correct parametric speciﬁcation described in Henderson and Parmeter
intermediate
consumption from
the
total amount
produced.
Further
details
are available at
http://www.ine.cl/estadisticas/economicas/manufactura.
23

Table 6. Statistics of Chilean manufacturing data.
Plastic
(2520)
Non-exporters (n = 173)
Exporters (n = 72)
Labor
Capital
(million)
Value
Added
(million)
Labor
Capital
(million)
Value
Added
(million)
Share of
Exports
mean
92155
725.85
546.93
240890
2859
1733.9
0.147
median
55220
258.41
247.05
180330
1329.1
1054.9
0.0524
std
106530
1574
1068.1
212480
3840.2
1678.8
0.201
skewness
3.301
5.2052
5.9214
1.3681
2.4594
1.0678
-0.303
Wood
(2010)
Non-exporters (n = 97)
Exporters (n = 35)
Labor
Capital
Value
Added
Labor
Capital
Value
Added
Share of
Exports
mean
76561
364.93
334.83
501470
3063.4
4524.1
0.542
median
44087
109.48
115.39
378000
2195.4
2673.5
0.648
std
78057
702.35
555.87
436100
2510.3
4466.3
0.355
skewness
2.243
3.5155
3.432
0.81454
0.63943
1.0556
-0.303
Labor
×105
0
1
2
3
4
5
6
7
8
9
10
Capital
×104
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Plastic
(a) Plastic (2520)
Labor
×105
0
2
4
6
8
10
12
14
16
Capital
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
Wood
(b) Wood (2010)
Figure 2. Labor and Capital of each industry.
24

(2015)7. The resulting p-value is 0.092 for the plastic industry and 0.007 for the wood in-
dustry, respectively. Therefore, the Cobb–Douglas parametric speciﬁcation is likely to be
wrong, particularly applied to the wood industry.
Next, we apply the test proposed in Section 4.2 to determine if imposing global concavity
and monotonicity shape constraints is appropriate. We estimate a p-value of 0.302 for the
plastic industry and 0.841 for the wood industry, respectively. For both industries, the
estimated p-value is not small enough to reject H0, which means that the observed data is
likely to satisfy the shape constraints imposed.
6.2
Estimated production function and interpretation
We estimate a semi-parametric model with a nonparametric shape constrained production
function, a linear model for exporting share of sales, and a dummy variable for exporting.
Table 7 shows the goodness of ﬁt (R2) of the production function: 71.1% of variance is
explained in the plastic industry while 43.8% of variance is explained in the wood industry.
Table 7. SCKLS ﬁtting statistics for cross sectional data.
Industry
Number of
observations
R2
Plastic
245
71.1%
Wood
132
43.8%
Table 8 reports additional information characterizing the production function: the
marginal productivity and the marginal rates of substitution at the 10, 25, 50, 75 and
90 percentiles are reported for both measures. Here, the rate of substitution indicates how
much labor is required to maintain the same level of output when we decrease a unit of
capital. When comparing the two industries, we ﬁnd that the wood industry has a larger
marginal rate of substitution than the plastic industry. This indicates that capital is more
critical in the wood industry than the plastic industry.
We also compare the estimated production function by the local linear and the SCKLS
estimators. Figure 3 and Figure 4 show the estimated production function within the convex
hull of observations for plastic and wood industries, respectively. Visually, the production
7We apply a Cobb–Douglas OLS to the second stage data {Xj, yj −Zjγ}n
j=1 which removes the eﬀect
of contextual variables from observed output. See Appendix F for details.
25

-2000
2
10
0
2000
1.5
Value Added
Plastic
4000
6000
Labor
×105
Capital
×104
1
5
8000
0.5
0
0
(a) Local Linear
-2000
2
10
0
2000
1.5
Value Added
Plastic
4000
6000
×104
Capital
×105
Labor
5
1
8000
0.5
0
0
(b) SCKLS
Figure 3. Production function estimated by LL and SCKLS for the plastic industry (2520)
function estimated by the LL estimator is diﬃcult to interpret and the values of important
economic quantities such as marginal products and marginal rates of substitution are also
hard to interpret. In particular, it is not possible to identify most productive scale size.
Table 8. Characteristics of the production function.
Plastic (2520)
Marginal Productivity
Marginal Rate of Substitution
Labor (= bl)
Capital (= bk)
(= bk/bl)
(million peso/man hours)
(peso/peso)
10th percentile
0.00396
0.111
23.3
25th percentile
0.00523
0.139
23.9
50th percentile
0.00579
0.139
24.0
75th percentile
0.00579
0.139
35.3
90th percentile
0.00579
0.260
44.8
Wood (2010)
Marginal Productivity
Marginal Rate of Substitution
Labor (= bl)
Capital (= bk)
(= bk/bl)
10th percentile
1.46×10−18
0.816
760
25th percentile
8.55×10−16
0.816
760
50th percentile
0.00133
1.01
760
75th percentile
0.00133
1.01
9.73×1014
90th percentile
0.00133
1.01
5.59×1017
Table 9 reports the estimated coeﬃcients for the exporting variables. In the plastic
industry, the dummy variable for exporting is signiﬁcant and positive while exports’ share
of sales is not. This indicates that the plants that export tend to produce more output than
plants that do not export regardless of the export quantity. In contrast, the coeﬃcient on
the exports’ share of sales is signiﬁcant and positive in the wood industry while the dummy
26

-5000
2
10000
0
5000
1.5
Wood
Value Added
10000
Labor
Capital
×106
5000
1
15000
0.5
0
0
(a) Local Linear
-5000
2
10000
0
5000
1.5
Wood
Value Added
10000
Labor
Capital
×106
5000
1
15000
0.5
0
0
(b) SCKLS
Figure 4. Production function estimated by LL and SCKLS shape constraints for the wood
industry (2010)
variable for exporting is not signiﬁcant, indicating that establishments in the wood industry
tend to be more productive the more they export. Thus, in both industries we ﬁnd evidence
of increased productivity for exporting ﬁrms.
Table 9. Coeﬃcient of contextual variables from a 2-stage model.
Plastic (2520)
Wood (2010)
Dummy of
exporting
Share of exporting
in sales
Dummy of
exporting
Share of exporting
in sales
Point estimate
334.5
303.7
-763.0
4114
95% lower bound
148.7
-334.3
-1944
2568
95% upper bound
520.3
941.8
417.7
5660
p-value
4.70×10−4
0.3493
0.2033
5.64×10−7
Table 10 reports the most productive scale size for the 10, 25, 50, 75, 90 percentiles of
Capital/Labor ratio distribution of observed input. In both industries, the observed value
added output is the largest for establishments with high capital to labor ratios, indicating
that capital-intensive establishments have increased actual output. Furthermore, labor-
intensive establishments have smaller most productive scale size in both industries. This
is consistent with the theory of the ﬁrm, i.e. ﬁrms grow and become more capital intensive
over time by automating processes with capital and using less labor.
27

Table 10. Most productive scale size for each capital/labor ratio.
Plastic (2520)
MPSS Labor
MPSS Capital
Output
Capital/Labor percentile
(Value added)
10th percentile
619580
519.1
3290
25th percentile
529980
1344
3010
50th percentile
529980
2604
3185
75th percentile
529980
5617
3602
90th percentile
529980
10270
4248
Wood (2010)
MPSS Labor
MPSS Capital
Output
Capital/Labor percentile
(Value added)
10th percentile
2531100
741.6
1659
25th percentile
1045000
1200
2142
50th percentile
867250
2712
3470
75th percentile
662700
4179
4682
90th percentile
458150
5644
5893
7
Conclusion
This paper proposed the SCKLS estimator that imposes shape constraints on a local poly-
nomial estimator. We show the consistency and convergence rate of this new estimator
under monotonicity and concavity constraints, as well as its relationship with CNLS and
CWB. We also illustrate how to use SCKLS to validate the imposed shape constraints. In
applications where out-of-sample performance is less critical and the boundary behavior is
of less concern, such as regulation applications, the CNLS estimator may be preferable be-
cause of its simplicity. In contrast, in cases where out-of-sample performance is important,
such as survey data, the SCKLS estimator appears to be more robust. Simulation results
reveal the SCKLS estimator outperforms CNLS and LL in most scenarios. We propose
and validate the usefulness of several extensions, including variable bandwidth and non-
uniform griding, which are important to estimate functions with non-uniform input data
set which is common in manufacturing survey and census data. We also propose a test for
the imposed shape constraints based on SCKLS. Finally, we demonstrate the SCKLS esti-
mator empirically using Chilean manufacturing data. We compute marginal productivity,
marginal rate of substitution, most productive scale size and the eﬀects of exporting, and
provide several economic insights.
One limitation of the proposed SCKLS estimator is its computation eﬃciency due to the
28

large number of constraints. The algorithm we proposed for reducing constraints performs
well, and we demonstrate the ability to solve large problems instances within a reasonable
time.
Furthermore, our simulation results show good functional estimates even with a
rough grid. Consequently, we can make use of the ﬂexibility of the evaluation points to
reduce the computational time of the estimator.
Potential future research could focus on the bandwidth selection methods. Typically,
optimal bandwidth selection methods without shape constraints try to trade bias and vari-
ance to ﬁnd the best estimator in terms of RMSE. Since the imposed shape restrictions
already constrain the variance of the estimator to some extent, we expect that the optimal
bandwidth in the SCKLS estimator will be smaller than the optimal unconstrained estima-
tor. Further, if systematic ineﬃciency is present in the data, deconvoluting the residuals
following the stochastic frontier literature would allow the investigation of a production
frontier.
SUPPLEMENTARY MATERIAL
Appendix: The document contains: (A) extensions and the relationship between esti-
mators; (B) technical proofs of the theoretical results; (C) a test of aﬃnity using
SCKLS; (D) an algorithm for SCKLS computational performance; (E) comprehensive
results of existing and additional numerical experiments; (F) semiparametric model
to integrate contextual variable; and (G) details of the application to the Chilean
manufacturing data.
29

Appendix
This appendix includes:
• Extensions to SCKLS and a description of the relationship between SCKLS, CNLS
and CWB (Appendix A),
• Technical proofs of the theoretical results (Appendix B).
• A test of aﬃnity based on SCKLS (Appendix C)
• An algorithm for SCKLS computational performance (Appendix D).
• Comprehensive results of existing and additional numerical experiments (Appendix
E).
• Description of a semiparametric partially linear model to integrate contextual variable
(Appendix F).
• Details about the application to the Chilean manufacturing data (Appendix G)
30

A
More on SCKLS, CNLS and CWB
In this section, we ﬁrst give details on the extensions and practical considerations to SCKLS.
We then mention some recently proposed estimators that are related to SCKLS, and make
connections and comparisons among these methods.
A.1
More on practical considerations and extensions to SCKLS
A.1.1
SCKLS with general constraints
We focus on global concavity/convexity and monotonicity constraints in the main manuscript.
But the SCKLS estimator can handle any types of shape constrained by imposing con-
straints on decision variables {ai, bi}m
i=1. We re-deﬁne the SCKLS estimator as
min
a,b
m
X
i=1
n
X
j=1
(yj −ai −(Xj −xi)′bi)2K
Xj −xi
h

subject to
l(xi) ≤ˆg(s)(xi|a, b) ≤u(xi),
i = 1, . . . , m
(A.1)
where a = (a1, . . . , am)′ and b = (b′
1, . . . , b′
m)′. l(·) and u(·) represent lower and upper
bounds at each evaluation point respectively. s denotes the order of partial derivative to
each evaluation point xi.
A.1.2
SCKLS with Local Polynomial
With the proposed estimator in (A.1), we are only able to impose the constraints by
using the functional estimate and/or ﬁrst partial derivatives. For constraints involving a
higher order of derivatives, we need to formulate SCKLS estimator with a higher order
local polynomial function. For the multivariate local polynomial, we borrow the following
31

notation from Masry (1996).
r = (r1, . . . , rd),
r! = r1! × · · ·rd!,
¯r =
d
X
k=1
rk,
xr = xr1
1 × · · · xrd
d ,
X
0≤¯r≤p
=
p
X
k=0
k
X
r1=0
· · ·
k
X
rd=0
,
and
(Drg) (x) =
∂rg(x)
∂xr1
1 · · · ∂xrd
d
With this notation, we can approximate any function g : Rd →R locally (around any x)
using a multivariate polynomial of total order p, given by
g(z) :=
X
0≤¯r≤p
1
r! (D ¯rg) (x) (z −x)¯r .
(A.2)
We now deﬁne the SCKLS estimator with a local polynomial function of order p as follows:
min
bi
m
X
i=1
n
X
j=1
 
yj −
X
0≤¯r≤p
b′
i(Xj −xi)¯r
!2
K
Xj −xi
h

subject to
l(xi) ≤ˆg(s)(xi|b) ≤u(xi),
i = 1, . . . , m
(A.3)
where bi is the functional or derivative estimates at each evaluation points and b =
(b′
1, . . . , b′
m)′.
When we select p = 1, then the problem becomes exactly same as the
proposed estimator in (A.1). This extension allows us to make the proposed methods more
general and applicable for other applications of shape restricted functional estimation in
which higher order derivative restricts may be required. From a computational complexity
point of view, it is still optimizing a quadratic objective function within a convex solution
space, and thus, the problem is still typically solvable within polynomial time.
As demonstrated in Li and Racine (2007), the rate of convergence of local polynomial
estimator is the same for p = 1 and p = 2. From a theoretical perspective, one could
attempt to select a polynomial estimator with p ≥3 to improve its convergence performance
(at least theoretical). But that would require much stronger assumption on the smoothness
of g0, and would lead to additional computational burden8. Our experience suggests that
8While the optimization problem is still polynomial time solvable, the number of decision variables
32

SCKLS inherits these properties from the local polynomial method. Therefore, in practice,
with only monotonicity and concavity/convexity constraints, we feel that it suﬃces to
consider SCKLS with p = 1 (i.e. local linear).
A.1.3
SCKLS with k-nearest neighbor
Our primary application of interest is production functions estimated for census manufac-
turing data where the input distributions are often highly skewed meaning there are many
small establishments, but relatively few large establishments9. To address this issue, we
propose to use a k-nearest neighbor (k-NN) approach in SCKLS which we will refer to as
SCKLS k-NN which is in spirit similar to the extension to the CWB-type estimator pro-
posed by Li et al. (2016). The k-NN approach uses a smaller bandwidth for smoothing in
dense data regions and a larger bandwidth when the data is sparse. For a further descrip-
tion of the method, see for example Li and Racine (2007). For any given k, the formulation
of SCKLS k-NN with monotonicity and concavity constraints leads to a diﬀerent weighting
scheme in the objective function, as illustrated in the following.
min
ai,bi
m
X
i=1
n
X
j=1
(yj −ai −(Xj −xi)′bi)2w
∥Xj −xi∥
Rxi

subject to
ai −al ≥b′
i(xi −xl),
i, l = 1, . . . , m
bi ≥0,
i = 1, . . . , m
(A.4)
where w(·) is a general weight function, ∥·∥is the Euclidean norm and Rxi denotes the Eu-
clidean distance between xi and k-th nearest neighbor of xi among the set of all covariates
{Xj}n
j=1. In practice, k can be chosen by leave-one-out cross validation (LOOCV).
would increase and the constraint matrix would become signiﬁcantly more dense, lending to computational
challenges.
9An establishment is deﬁned as a single physical location where business is conducted or where services
or industrial operations are performed.
33

A.1.4
SCKLS with non-uniform grid
As noted in the paper, the SCKLS estimator requires the user to specify the number
and locations of the evaluation points.
We can also address the input skewness issue
by constructing the evaluation points diﬀerently, using a non-uniform grid method. To
do so, we ﬁrst use kernel density estimation to estimate the density function for each
input dimension. Then we take the equally spaced percentiles of the estimated density
function and construct non-uniform grid. Figure A.1 demonstrates how the non-uniform
grid are constructed for the 2-dimensional case. In this example, we set the minimum and
maximum of the observed inputs (with respect to each coordinate) as the edge of the grid,
and compute equally spaced percentile. When the support of the covariates is non-regular
(e.g. not a hyperrectangle), we shall limit ourselves to evaluation points inside the convex
hull of {Xj}n
j=1.
x1
x2
Figure A.1. Example of non-uniform grid with kernel density estimation.
34

A.2
Some related work
A.2.1
Convex Nonparametric Least Squares (CNLS)
Kuosmanen (2008) extends Hildreth’s least squares approach to the multivariate setting
with a multivariate input vector, and coins the term “Convex Nonparametric Least Squares”
(CNLS)10. CNLS builds upon the assumption that the true but unknown production func-
tion g0 belongs to the class of monotonically increasing and globally concave functions,
denoted by G2 in this paper. Given the observations {Xj, yj}n
j=1, a set of unique ﬁtted
values, ˆyj = ˆαj + ˆβjXj, can be found by solving the quadratic programming (QP) problem
min
α,β
n
X
j=1
(yj −(αj + β′
jXj))2
subject to
αj + β′
jXj ≤αl + β′
lXj,
j, l = 1, . . . , n
βj ≥0,
j = 1, . . . , n
(A.5)
where αj and βj deﬁne the intercept and slope parameters that characterize the estimated
set of hyperplanes.
The inequality constraints in (A.5) can be interpreted as a system
of Afriat inequalities (Afriat, 1972; Varian, 1984) to impose concavity constraints.
We
emphasize that CNLS does not assume or restrict the domain G2 to only piece-wise aﬃne
functions. We also note that the functional estimates resulting from (A.5) is unique only
at the observed data points.
In addition, when d = 1, Chen and Wellner (2016) and
Ghosal and Sen (2016) proved that the CNLS-type estimator attains n−1/2 pointwise rate
of convergence if the true function is piece-wise linear.
Finally, we remark that CNLS is related to the method of sieves (Grenander, 1981;
Chen and Qiu, 2016) in the following way. The estimator could be rewritten as
ˆgn ∈argmin
g∈Gn
1
n
n
X
j=1
(yj −g(Xj))2,
where Gn = {g : Rd →R | g(x) = minj∈{1...,n}(αj + β′
jx), with βj ≥0 for j = 1, . . . , n}.
10A related maximum likelihood formulation was proposed by Banker and Maindiratta (1992), with its
consistency proved by Sarath and Maindiratta (1997).
35

However, since the sets G1, G2, . . . are not compact, most known results on sieves do not
directly apply here.
A.2.2
Constrained Weighted Bootstrap (CWB)
A.2.2.1
Introduction
Hall and Huang (2001) proposed the monotone kernel regres-
sion method in univariate function. Du et al. (2013) generalized this model to handle mul-
tiple general shape constraints for multivariate functions, which they refer to as Constrained
Weighted Bootstrap (CWB). CWB estimator is constructed by introducing weights for each
observed data point. The weights are selected to minimize the distance to unconstrained
estimator while satisfying the shape constraints. The function is estimated as
ˆg(x|p) =
n
X
j=1
pjAj(x)yj
(A.6)
where p = (p1, . . . , pn)′, pj is the weights introduced for each observation and Aj(x) is a
local weighting matrix (e.g. local linear kernel weighting matrix). Du et al. (2013) relaxed
the restriction imposed by Hall and Huang (2001) that pj is non-negative and propose to
calculate p by minimizing its distance to unrestricted weights, pu = (1/n, . . . , 1/n)′, under
derivative-based shape constraints11. The problem is formulated as follows.
min
p
D(p) =
n
X
j=1
(pj −pu)2 =
n
X
j=1
(pj −1/n)2
subject to
l(xi) ≤ˆg(s)(xi|p) ≤u(xi),
i = 1, . . . , m
(A.7)
where xi represents a set of points for evaluating constraints, the elements of s represent
the order of partial derivative, and gs(x) = [∂s1g(x) · · ·∂srg(x)]/[∂xs1
1 · · · ∂xsr
r ] for s =
(s1, s2, . . . , sr). Here the shape restrictions (e.g. concavity/convexity and monotonicity
constraints) are imposed at a set of evaluation points {xi}m
i=1 through setting appropriate
lower and upper bounds to the corresponding partial derivatives of the function. One way to
interpret the CWB estimator is as a two-step process: 1) estimate an unconstrained kernel
11The use of the equality constraint P
j pj = 1 in Du et al. (2013) is a typo, and this condition is not
used by them. In fact, it may harm the estimation procedure. Our empirical results show that this equality
constraint only makes diﬀerence in very few cases and the diﬀerence is typically small.
36

estimator; 2) ﬁnd the shape constrained function that is as close as possible (as measured
by the Euclidean distance in p-space) to the unconstrained kernel estimator. Based on
our experience, CWB tends to suﬀer from computational diﬃculties and occasionally poor
estimates in small samples. We suggest changing the objective function to minimize the
distance from the estimated function to the observed data. This modiﬁcation seems to
improve the estimates empirically as shown in Appendix E.
A.2.2.2
CWB estimator that minimize the distance from the observed data
We propose an extension of the CWB estimator by converting the objective function from
p-space to y-space. Instead of minimizing the distance between the unconstrained estimator
and the shape restricted functional estimate by minimizing the distance between the two
functions in p-space, we propose to minimize the distance between the observed vector of
y and the shape restricted functional estimates in y-space. The estimator, which we shall
refer to as CWB in y-space, is formulated as follows:
min
p
Dy(p) =
n
X
j=1
(yj −ˆg(Xj|p))2
subject to
l(xi) ≤ˆg(s)(xi|p) ≤u(xi),
i = 1, . . . , m,
n
X
j=1
pj = 1.
(A.8)
Since the objective function is not necessarily convex in p, this problem is a general
nonlinear optimization problem which is harder to solve.
A.2.2.3
Calculating the ﬁrst partial derivative of ˆg(x|p) for CWB
Du et al.
(2013) proposed the CWB estimator which requires estimating the ﬁrst partial derivatives
of unconstrained functional estimates, ˆg(1)(x|p). Here, we test two diﬀerent methods of
calculating the partial derivatives. The ﬁrst method is to calculate the numerical derivative,
ˆg(1)(x|p) = ˆg(x+∆|p)−ˆg(x|p)
∆
, to obtain the approximated derivative estimate. Racine (2016)
shows that the numerical derivative is very close to the analytic derivative. The second
method is to use the slope estimates of local linear estimator directly as a proxy for the
ﬁrst partial derivative. We evaluate the performance of CWB in p-space estimator with
37

these two diﬀerent methods. Table A.1 and Table A.2 summarize the RMSE performance
against the true function on the observed points and the evaluation points respectively.
The experimental setting is based on Experiment 1 in Section 5.
Table A.1. RMSE on observation points for diﬀerent methods to obtain ˆg(1)(x|p).
Average RMSE on the observation points
Number of observations
100
200
300
400
500
2-input
Numerical derivative
0.260
0.163
0.143
0.153
0.164
Slope estimates of LL
0.421
0.357
0.284
0.306
0.293
3-input
Numerical derivative
0.236
0.256
0.208
0.246
0.240
Slope estimates of LL
0.356
0.427
0.336
0.294
0.279
4-input
Numerical derivative
0.259
0.226
0.222
0.216
0.210
Slope estimates of LL
0.388
0.397
0.276
0.261
0.259
Table A.2. RMSE on evaluation points for diﬀerent methods to obtain ˆg(1)(x|p).
Average RMSE on the evaluation points
Number of observations
100
200
300
400
500
2-input
Numerical derivative
0.284
0.188
0.157
0.176
0.193
Slope estimates of LL
0.445
0.387
0.321
0.334
0.323
3-input
Numerical derivative
0.309
0.355
0.272
0.331
0.271
Slope estimates of LL
0.438
0.507
0.403
0.371
0.363
4-input
Numerical derivative
0.408
0.381
0.354
0.333
0.308
Slope estimates of LL
0.530
0.535
0.396
0.387
0.368
The results show that CWB using the numerical derivative performs better than CWB
using the slope estimates from the local linear kernel estimator particularly when the sample
size is small.
A.3
A comparison between SCKLS, CNLS and CWB
Figure A.2 is meant to be illustrative of the relationship between the SCKLS, CNLS and
CWB estimators in a two-dimensional estimated ǫ-space where there are more than two
observations, but for the rest of the n−2 observations, their estimated ǫjs are held ﬁx. The
gray area indicates the cone of concave and monotonic functions. CNLS estimates a mono-
tonic and concave function while minimizing the sum of squared errors, that is, minimizing
the distance from the origin to the cone in the estimated ǫ-space. CWB estimates a mono-
tonic and concave function by ﬁnding the closest point, measured in p-space, on the cone
of concave and monotonic functions to unconstrained kernel estimate. SCKLS minimizes
38

 ǫi
 ǫl
Cone of Concave and
Monotonic Functions
  True Function
  SCKLS Estimate
  Unconstrained Kernel Estimate
  CNLS Estimate
   CWB Estimate
(0, 0)
Figure A.2. Comparison of diﬀerent estimators in the estimated-ǫ-space.
a weighted function of estimated errors, and therefore avoids overﬁtting the observed data.
However, as shown in B.2, SCKLS can be interpreted as minimizing the weighted distance
from the unconstrained local linear kernel estimator to the cone of concave and monotonic
functions.
A.3.1
CNLS as a Special Cases of SCKLS
Let ˆgn and ˆgCNLS
n
denote the SCKLS estimator and the CNLS estimator respectively. We
will next examine the relationship between them.
Assumption A.1. The set of evaluation points is equal to the set of sample input vectors,
i.e. m = n and xi = Xi for i = 1, . . . , n.
Proposition A.1. Suppose that Assumption A.1 holds. Then, for any n, when the vector
of bandwidth goes to zero, i.e. ∥h∥→0 (where h = (h1, . . . , hd)′), the SCKLS estimator
ˆgn converges to the CNLS estimator ˆgCNLS
n
pointwise at X1, . . . , Xn.
Proposition A.1 essentially says that CNLS can be viewed as a special case of SCKLS.
Note that in comparison to the CNLS estimator, our SCKLS estimator has tuning pa-
rameters, which to some extent control the bias–variance tradeoﬀ(in a non-trivial way
39

given the shape restrictions). For reasonable values of these tuning parameters, SCKLS
estimator performs better than CNLS. See also Section 5 of the main manuscript. This
is especially true for the estimates close to the boundary of the input space, where im-
posing the shape constraint alone could lead to severe overﬁtting of the data, and thus
biased estimates. Indeed, in view of Theorem 3 (from the main manuscript), we have that
supS
ˆgn(x) −g0(x)
 = op(1), while on the other hand, supS
ˆgCNLS
n
(x) −g0(x)
 does not
converge to zero in probability.
Additional equivalence results can also be shown. Proposition A.2 shows the equivalence
of linear regression subject to monotonicity constraints and the SCKLS estimator when the
bandwidth vector approaches inﬁnity.
Proposition A.2. Given Assumption 1(v). For any given n, when the bandwidth vector goes
to inﬁnity (i.e. mink=1,...,d hk →∞), the SCKLS estimator converges to the least squares
estimator of the linear regression model subject to monotonicity constraints.
A.3.2
CWB in y-space as a Special Cases of SCKLS
Let ˆgn and ˆgCW BY
n
denote the SCKLS estimator and the CWB y-space estimator respec-
tively. We will next examine the relationship between them.
Proposition A.3. Suppose that Assumption A.1 holds. Then, for any n, when the vector of
bandwidth goes to zero for both the SCKLS estimator and the CWB in y-space estimator,
i.e. ∥h∥→0 (where h = (h1, . . . , hd)′), the SCKLS estimator ˆgn converges to the CWB in
y-space estimator ˆgCW BY
n
pointwise at X1, . . . , Xn.
Proposition A.3 states that SCKLS and CWB in y-space estimators converge to the
same estimates as ∥h∥→0. Combining with Proposition A.1, CNLS can be viewed as a
special case of SCKLS and CWB in y-space.
40

A.3.3
The relationship between CWB in p-space and SCKLS
Again start from the SCKLS estimator, and in view of Assumption 1 (v), for any suﬃciently
small h, we have
K
Xj −xi
h

=





0
if xi ̸= Xj,
K(0)
if xi = Xj,
for ∀i, j.
Then, the objective function of the SCKLS estimator (3) is equal to Pn
j=1(yj −aj)2K(0),
and thus
argmin
a1,b1,...,an,bn
n
X
j=1
(yj −aj)2K(0) = argmin
a1,...,an
n
X
j=1
(yj −aj)2 = argmin
a1,...,an
L(g(aj))
where L(·) = Pn
j=1(·)2 is the squared error loss function, g(aj) = yj −aj the deﬁnition
of the residual.
Alternatively now consider the objective function of CWB, speciﬁcally D(p) = Pn
j=1(pu−
pj)2 = Pn
j=1(1/n −pj)2 = L(m(g(pj))). And let L(·) continue to be deﬁned as above as
the squared error lost function and g(pj) as the deﬁnition of the residual. This implies
that m(·) =
·
yjn. Therefore, the CWB estimator can be interpreted as a projection of
a local polynomial estimator to the cone of functions which are monotonic and concave
in which the direction of projection minimizes a speciﬁc weighting of the unconstrained
local polynomial residuals in which the weights are deﬁned as
1
yjn. Therefore, even if the
vector of bandwidth goes to zero for the CWB in p-space estimator, i.e. ∥h∥→0 (where
h = (h1, . . . , hd)′), the CWB estimator and CNLS are not equivalent because the yj in the
denominator of the weights is not a function of the bandwidth.
A.3.4
On the computational aspects
We also compare the computational burden of each estimators. Table A.3 shows the size
of quadratic programming problems of each estimators: SCKLS, CNLS and CWB. The
size of a quadratic programming problem of the SCKLS estimator is fully controllable
because the number of decision variables and constraints is a function of the number of
41

evaluation points and independent of the number of observed points.
Because of this,
we can solve large-scale problems with n > 100, 000 using the SCKLS estimator while
other shape constrained nonparametric estimators might face prohibitive computational
diﬃculties without any data pre-processing.
Table A.3. The size of quadratic programming problems of each estimator.
SCKLS
CNLS
CWB
Number of decision variables
m(d + 1)
n(d + 1)
n
Number of global concavity constraints
m(m−1)
n(n −1)
m(m−1)
B
Technical proofs
B.1
Summary of the proof strategy
Theorems 1– 4 concern the consistency and convergence rate of the SCKLS estimator and
serve as the primary results in our theoretical development. As such, before presenting the
technical details, we summarize our proof strategy as follows:
1. We rewrite the SCKLS estimator, after some manipulations, as the projection of the
local linear estimator to a convex cone of monotonic and concave functions under a
certain norm. More precisely, the SCKLS estimator
ˆgn ∈argming∈G2∥g −˜gn∥2
n,m,
where ˜gn is the local linear estimator, G2 is the set that contains all the concave and
increasing functions, and ∥· ∥n,m is a norm deﬁned in detail later in Appendix B.2.
2. (Theorem 1). Let ˆgn be the SCKLS estimator and g0 ∈G2 be the truth. Using the
new formulation of SCKLS above, we see that
∥ˆgn −˜gn∥n,m ≤∥g0 −˜gn∥n,m.
42

Moreover, by the triangular inequality, we have that
∥ˆgn −g0∥n,m ≤∥ˆgn −˜gn∥n,m + ∥˜gn −g0∥n,m ≤2∥˜gn −g0∥n,m.
Using the results on the uniform consistency of the local linear estimator (e.g. Fan and Guerre
(2016), see our Lemma B.1 and Lemma B.2), we can bound the RHS of the triangle
inequality equation by Op(n−2/(4+d) log n) = op(1). Consequently, ∥ˆgn −g0∥n,m con-
verges to zero at the same rate. To complete the proof, we show that the discrete L2
distance between ˆgn and g0 is bounded above by a constant times ∥ˆgn −g0∥n,m.
3. (Theorem 2). Building upon Theorem 1, we then make use of the concavity of ˆgn
and g0 to establish uniform consistency. Loosely speaking, this relies on the fact that
the convergence in L2 for a sequence of Lipschitz (and concave) functions implies the
uniform convergence in the interior of the domain. See Lemma B.3 and Lemma B.4
below for more detail. Note that we only look at ˆgn on the a compact subset interior
of its domain, in order to make sure that ˆgn is Lipschitz there. That is also why we
do not have consistency on the boundary from the current proof strategy.
4. (Theorem 3). If we let the number of evaluation points, m, grow at a certain rate
slower than n, we can extend the uniform consistency result to the entire support
of X. The assumption on the rate of growth of m makes sure that the ﬁrst partial
derivative of SCKLS, ∂ˆgn
∂x (x), is bounded for some positive constant, so the SCKLS
is Lipschitz over the entire domain.
5. (Theorem 4). This can be viewed as a generalization of Theorem 2. The main ingre-
dient of its proof is to establish ∥ˆgn −g∗
0∥n,m = op(1). Then the uniform consistency
follows from the concavity of ˆgn and g∗
0 via Lemma B.4.
B.2
Alternative deﬁnition of SCKLS
Recall that given observations {Xj, yj}n
j=1 and evaluation points {xi}m
i=1, the (uncon-
strained) local linear estimator at xi is (˜ai, ˜bi) for i = 1, . . . , m, where (˜a1, ˜b1, . . . , ˜am, ˜bm)
43

is the (unique) minimizer of
m
X
i=1
n
X
j=1
(yj −ai −(Xj −xi)′bi)2K
Xj −xi
h

.
For simplicity, we assume that the bandwidth is equal for all input dimensions, i.e. h =
(h, . . . , h)′. Since the objective function is quadratic, for any (a1, b1, . . . , am, bm), its value
equals
nhd
m
X
i=1
 ˜ai −ai, (˜bi −bi)′h

Σi
 ˜ai −ai
(˜bi −bi)h

+ Const
where
Σi =
1
nhd
n
X
j=1
U
Xj −xi
h
n
U
Xj −xi
h
o′
K
Xj −xi
h

with U(x) being the vector (1, x′)′ and
Const =
m
X
i=1
n
X
j=1
(yj −˜ai −(Xj −xi)′˜bi)2K
Xj −xi
h

.
Therefore, SCKLS can be simply viewed as a minimizer of
m
X
i=1
 ˜ai −ai, (˜bi −bi)′h

Σi
 ˜ai −ai
(˜bi −bi)h

subject to the shape constraints imposed on (a1, b1, . . . , am, bm). More generally, ﬁxing
{X1, . . . , Xn}, {x1, . . . , xm} and h, and deﬁne a new squared distance measure between
two functions g1, g2 as
∥g1 −g2∥2
n,m = 1
m
m
X
i=1

g1(xi)−g2(xi),
 ∂g1
∂x (xi)−∂g2
∂x (xi)
′h

Σi

g1(xi) −g2(xi)
  ∂g1
∂x (xi) −∂g2
∂x (xi)
′h

,
then SCKLS belongs to12
argmin
g∈G2
∥g −˜gn∥n,m
12To be more precise technically, if g1 −g2 is not diﬀerentiable, then ∥g1 −g2∥n,m needs to be taken
as the inﬁmum among all possible sub-gradients in the previous deﬁnition. Nevertheless, since we only
consider the behavior of the functions at ﬁnitely many points, without loss of generality, here we can
restrict ourselves to diﬀerentiable functions.
44

where G2 is the set that contains all the concave and increasing functions from S to R.
Below, we list some useful results on the behaviors of Σi and (˜ai, ˜bi). These results
follow from Fan and Guerre (2016).
lemma B.1 (Lemma 5 of Fan and Guerre (2016), Page 508). Suppose that Assumption
1(i)-1(vi) hold, then with probability one, there exists C > 1 such that the eigenvalues of
Σi are in [1/C, C] for all i = 1, . . . , m for suﬃciently large n.
lemma B.2 (Proposition 7 of Fan and Guerre (2016), Page 509). Suppose that Assumption
1(i)-1(vi) hold, then as n →∞,
sup
i=1,...,m

|˜ai −g0(xi)|2,
h
n
˜bi −∂g0
∂x (xi)
o
2
= Op(n−4/(4+d) log n).
B.3
Proof of Theorems in Section 3
B.3.1
Proof of Theorem 1
Proof. With a suﬃciently large n, the uniqueness of the estimates of ˆgn(xi) and ∂ˆgn
∂x (xi)
for i = 1, . . . , m is established because our objective function corresponds to is a quadratic
programming problem with a positive deﬁnite (strictly convex) objective function with a
feasible solution. See Bertsekas (1995).
Based on our characterization of SCKLS in Appendix B.2, we note that the objective
function at the SCKLS estimate is smaller than or equal to that at the truth, and thus
∥ˆgn −˜gn∥2
n,m ≤∥g0 −˜gn∥2
n,m.
Moreover, by the triangular inequality, we have that
∥ˆgn −g0∥n,m ≤∥ˆgn −˜gn∥n,m + ∥˜gn −g0∥n,m ≤2∥˜gn −g0∥n,m.
As such,
∥ˆgn −g0∥2
n,m ≤4∥˜gn −g0∥2
n,m.
(A.9)
45

Recall that the (unconstrained) local linear estimator at xi is (˜ai, ˜bi) for i = 1, . . . , m. It
follows from Lemma B.2 that
∥˜gn−g0∥2
n,m = 1
m
m
X
i=1
 ˜ai−g0(xi),
 ˜bi−∂g0
∂x (xi)
′h

Σi

˜ai −g0(xi)
 ˜bi −∂g0
∂x (xi)

h

= Op(n−4/(4+d) log n)
In addition, from Lemma B.1, we have that
∥ˆgn −g0∥2
n,m = 1
m
m
X
i=1
 ˆgn(xi) −g0(xi),
 ∂ˆgn
∂x (xi) −∂g0
∂x (xi)
′h

Σi

ˆgn(xi) −g0(xi)
  ∂ˆgn
∂x (xi) −∂g0
∂x (xi)

h

≥
1
Cm
m
X
i=1
(ˆgn(xi) −g0(xi))2,
(A.10)
where C is the constant mentioned in the statement of Lemma B.1.
Plugging the above two equations into (A.9) yields
1
m
m
X
i=1
(ˆgn(xi) −g0(xi))2 ≤Op(n−4/(4+d) log n) = op(1).
B.3.2
Proof of Theorem 2
For the sake of clarity, we have divided the proof of Theorem 2 into several parts.
B.3.2.1
Some useful lemmas
Here we list two useful lemmas on the convergence of
convex functions.
lemma B.3. Suppose that f0, f1, f2, . . . : C′ →R are Lipschitz and convex functions, where
C′ ⊂Rd is a compact and convex set. In addition, assume that these functions all have the
same bound and Lipschitz constant. Then
lim
n→∞
Z
C′{fn(x) −f0(x)}2dx = 0
implies that
lim
n→∞sup
x∈C
|fn(x) −f0(x)| = 0
46

for any compact C in the interior of C′.
Proof. Suppose that the common Lipschitz constant is M > 0. Moreover, suppose that
sup
x∈C′ inf
y∈C ∥x −y∥=: δ.
Essentially, that means that for any x ∈C′, the ball of radius δ centered at x (denoted as
Bδ(x)) intersects with C.
Next, suppose that supx∈C |fn(x) −f0(x)| ≥ǫ for some ǫ > 0. Let
x∗∈argmaxx∈C|fn(x) −f0(x)|.
Then for any x that lies inside the ball of radius min{δ, ǫ/(4M)} centered at x∗, we have
that
|fn(x) −f0(x)| = |fn(x) −fn(x∗) + fn(x∗) −f0(x∗) + f0(x∗) −f0(x)|
≥|fn(x∗) −f0(x∗)| −|fn(x) −fn(x∗)| −|f0(x∗) −f0(x)|
≥ǫ −
ǫ
4M M −
ǫ
4M M = ǫ
2,
where we made use of the Lipschitz constant for fn and f0 in the second last line above.
Consequently,
Z
C′{fn(x) −f0(x)}2dx ≥
ǫ
2
2
Vol(Bmin{δ,ǫ/(4M)}(x∗)) = Const. × ǫd+2
for any 0 < ǫ < 4Mδ.
But since ǫ > 0 is arbitrary, lim supn→∞supx∈C |fn(x) −f0(x)| ≥ǫ for any suﬃciently
small ǫ would imply
lim sup
n→∞
Z
C′{fn(x) −f0(x)}2dx ≥Const. × ǫd+2,
violating
lim
n→∞
Z
C′{fn(x) −f0(x)}2dx = 0.
47

Our proof is thus completed by contradiction.
The following Lemma B.4 can be viewed as a small extension of Lemma B.3. This is
the version that we shall use in the proof of Theorem 2.
lemma B.4. Suppose that f0, f1, f2, . . . : C′ →R are Lipschitz and convex functions (that
could be random), where C′ ⊂Rd is a compact and convex set. In addition, assume that
these functions all have the same bound and Lipschitz constant. Furthermore, q : C′ →R
with infx∈C′ q(x) > 0. Then, for any ﬁxed compact set C in the interior of C′,
Z
C′{fn(x) −f0(x)}2q(x)dx
p→0
implies that
sup
x∈C
|fn(x) −f0(x)|
p→0
as n →∞.
Proof. Following the arguments in the proof of Lemma B.3, we see that supx∈C |fn(x) −
f0(x)| ≥ǫ would entail
Z
C′{fn(x) −f0(x)}2q(x)dx ≥
 ǫ
2
2
Vol(Bmin{δ,ǫ/(4M)}(x∗)) inf
x∈C q(x) = Const. × ǫd+2
for any suﬃciently small ǫ. Consequently,
R
C′{fn(x) −f0(x)}2q(x)dx
p→0 implies that
supx∈C |fn(x) −f0(x)|
p→0.
B.3.2.2
Lipschitz continuity of SCKLS
For the reasons that will become clearer
later, it is useful to investigate the Lipschitz continuity of SCKLS before we present our
proof of Theorem 2. Our ﬁnding is summarized in the following lemma. Its proof is similar
to that of Proposition 4 of Lim and Glynn (2012, Page 201–202), or that of Theorem 1
of Chen and Samworth (2016, online supplementary material, Page 2–6). We provide a
concise version of the proof for the sake of completeness. To better illustrate its main idea
and intuition, below we focus on the scenario of d = 1.
48

lemma B.5. Under the assumptions of the ﬁrst part of Theorem 2 (in the case where m
increases with n), for any convex and compact set C ⊂int(S) (where int(·) denotes the
interior of a set), there exists some constants B > 0 and M > 0 such that ˆgn is B-bounded
and M-Lipschitz over C with probability one as n →∞.
Proof. As explained before, here we focus on the scenario of d = 1.
Without loss of
generality, we can take S = [0, 1] and C = [δ, 1 −δ] for some δ ∈(0, 1/2).
Let B0 = sup[0,1] |g0(x)|. First, we show that the event
sup
x∈[δ,1−δ]
|ˆgn(x)| ≤2B0 + 1 =: B
happens with probability one as n →∞.
Since ˆgn is increasing, supx∈[δ,1−δ] |ˆgn(x)| = max

|ˆgn(δ)|, |ˆgn(1 −δ)|

. In addition, due
to the monotonicity of ˆgn, suppose that ˆgn(δ) ≤0, then |ˆgn(x)| ≥|ˆgn(δ)| for x ∈[0, δ];
otherwise, if ˆgn(δ) > 0, |ˆgn(x)| ≥|ˆgn(δ)| for x ∈[δ, 2δ] (actually, this statement is true
for x ∈[δ, 1]; but for our purpose, it suﬃces to only consider x ∈[δ, 2δ]).
As such,
|ˆgn(δ)| > 2B0 + 1 would imply that
1
m
m
X
i=1
(ˆgn(xi) −g0(xi))2 ≥1{ˆgn(δ)≤0}
m
m
X
i=1
(ˆgn(xi) −g0(xi))21{xi∈[0,δ]}
+ 1{ˆgn(δ)>0}
m
m
X
i=1
(ˆgn(xi) −g0(xi))21{xi∈[δ,2δ]}
≥(2B0 + 1 −B0)2
1{ˆgn(δ)≤0}
m
m
X
i=1
1{xi∈[0,δ]} + 1{ˆgn(δ)>0}
m
m
X
i=1
1{xi∈[δ,2δ]}

≥(B0 + 1)2 min
 1
m
m
X
i=1
1{xi∈[0,δ]}, 1
m
m
X
i=1
1{xi∈[δ,2δ]}

n→∞
≥
B2
0δ min
[0,1] q(x) > 0.
where q(·) is the density function with respect to what the empirical distribution of {x1, . . . , xm}
converges to (see Assumption 2(i)). Here the last line also follows from Assumption 2(i).
Note that Theorem 1 says that
1
m
Pm
i=1(ˆgn(xi) −g0(xi))2 = op(1), which would result in a
contradiction. Therefore, |ˆgn(δ)| ≤2B0 + 1.
Furthermore, we can reapply the above argument to show that |ˆgn(1 −δ)| ≤2B0 + 1.
49

Consequently,
sup
x∈[δ,1−δ]
|ˆgn(x)| ≤2B0 + 1 = B
happens with probability one as n →∞.
Second, note that the above proof works for any δ ∈(0, 1/2). Therefore, we also have
that
sup
x∈[δ/2,1−δ/2]
|ˆgn(x)| ≤2B0 + 1
with probability one as n →∞.
Finally, since ˆgn is concave, we note that the Lipschitz constant over [δ, 1−δ] is bounded
above by
max
|ˆgn(δ/2) −ˆgn(δ)|
δ/2
, |ˆgn(1 −δ/2) −ˆgn(1 −δ)|
δ/2

≤4(2B0 + 1)/δ =: M.
In other words, intuitively speaking, in terms of the Lipschitz constant, the most extreme
case for concave functions always occurs on the boundary. For general cases (i.e. d > 1),
see for instance, van der Vaart and Wellner (1996, Page 165, Problem 7).
B.3.2.3
Putting things together to prove Theorem 2
Proof.
First claim: when m increases with n.
Let C′ be a compact and convex set such that C ⊂int(C′) and C′ ⊂int(S), where
int(·) denotes the interior of a set.
By Lemma B.5, we have that ˆgn is B-bounded and M-Lipschitz over C′ with probability
one as n →∞. Therefore, {ˆgn(x) −g0(x)}21{x∈C′} belongs to the class of functions that is
bounded and equicontinuous over C′. By Theorem 3.1 of (Rao, 1962, Page 662) (which can
also be viewed as a generalization of the Uniform Law of Large Numbers; see also Chapter
2.4 of van der Vaart and Wellner (1996)), we have that

1
m
m
X
i=1
(ˆgn(xi) −g0(xi))21{xi∈C′} −
Z
C′{ˆgn(x) −g0(x)}2q(x)dx

p→0.
50

In addition, it follows from Theorem 1 that
op(1) = 1
m
m
X
i=1
(ˆgn(xi) −g0(xi))2 ≥1
m
m
X
i=1
(ˆgn(xi) −g0(xi))21{xi∈C′}.
Combining the above two equations together yields
Z
C′{ˆgn(x) −g0(x)}2q(x)dx = op(1).
It then follows immediately from Lemma B.4 that as n →∞,
sup
x∈C
|ˆgn(x) −g0(x)|
p→0.
Second claim: when m is ﬁxed.
In views of Lemma B.1 and Theorem 1,
1
C
m
X
i=1

|ˆgn(xi) −g0(xi)|2 +

∂ˆgn
∂x (xi) −∂g0
∂x (xi)

h

2
≤∥ˆgn−g0∥2
n,m = Op(n−4/(4+d) log n)
where the ﬁrst inequality is from Lemma B.1, and the last equality is from Theorem 1.
Since m is ﬁxed and h = O(n−1/(4+d)), it follows from that |ˆgn(xi)−g0(xi)| = Op(n−2/(4+d) log n)
p→
0 and ∥∂ˆgn
∂x (xi) −∂g0
∂x (xi)∥= Op(n−1/(4+d) log n)
p→0 for every i = 1, . . . , m.
B.3.3
Proof of Theorem 3
Proof. Using Equation (A.10) but focusing on the diﬀerence between the derivatives in-
stead, we have that
h2
Cm
m
X
i=1

∂ˆgn
∂x (xi) −∂g0
∂x (xi)

2
≤∥ˆgn −g0∥2
n,m = Op(n−4/(4+d) log n)
as n →∞. It then follows from h = O(n−1/(4+d)) and Assumption 3 that
m
X
i=1
∂ˆgn
∂x (xi) −∂g0
∂x (xi)

2
= Op(h−2mn−4/(4+d) log n) = op(1).
51

This implies that maxi=1,...,m
∂ˆgn
∂x (xi)

∞≤supx∈S
∂g0
∂x (x)

∞+ op(1). Now since
ˆgn(x) =
min
i∈{1,...,m}
n
ˆgn(xi) + (x −xi)′∂ˆgn
∂x (xi)
o
,
we have that with probability one,
sup
x∈S
∂ˆgn
∂x (x)

∞≤M
for some M > 0, as n →∞.
For any ǫ > 0, we can always ﬁnd a compact set Cǫ ⊂S such that supx∈S infy∈Cǫ ∥x −
y∥<
ǫ
2(M+Mg0), where Mg0 is the Lipschitz constant of g0. In view of Theorem 2, supx∈Cǫ |ˆgn(x)−
g0(x)| →0 in probability. Therefore,
sup
x∈S
|ˆgn(x) −g0(x)| ≤sup
x∈Cǫ
|ˆgn(x) −g0(x)| + (M + Mg0)
n
sup
x∈S
inf
y∈Cǫ ∥x −y∥
o
≤ǫ
as n →∞. Since ǫ is picked arbitrarily, we have shown the consistency of ˆgn over S.
52

B.4
Proof of Theorems in Section 4
B.4.1
Proof of Theorem 4
Proof. Using the deﬁnition of SCKLS in Appendix B.2 and the notation in the proofs of
Theorem 1 and Theorem 2, we have that
m
X
i=1

˜ai −g∗
0(xi),
 ˜bi −∂g∗
0
∂x (xi)
′h

Σi

˜ai −g∗
0(xi)
 ˜bi −∂g∗
0
∂x (xi)

h

≥
m
X
i=1
 ˜ai −ˆai, (˜bi −ˆbi)′h

Σi
 ˜ai −ˆai
 ˜bi −ˆbi

h

=
m
X
i=1

˜ai −g∗
0(xi),
 ˜bi −∂g∗
0
∂x (xi)
′h

Σi

˜ai −g∗
0(xi)
 ˜bi −∂g∗
0
∂x (xi)

h

+ 2
m
X
i=1
 ˜ai −g∗
0(xi), (˜bi −∂g∗
0
∂x (xi))′h

Σi

g∗
0(xi) −ˆai
( ∂g∗
0
∂x (xi) −ˆbi

h

+
m
X
i=1
 g∗
0(xi) −ˆai, (∂g∗
0
∂x (xi) −ˆbi)′h

Σi

g∗
0(xi) −ˆai
  ∂g∗
0
∂x (xi) −ˆbi

h

where we recall that ˆai and ˆbi are respectively the estimated value and its gradient from
SCKLS at evaluation point xi, i.e., ˆai = ˆgn(xi) and ˆbi = ∂ˆgn
∂x (xi).
Therefore, in view of Lemma B.2, with probability one, for suﬃciently large n,
2
m
m
X
i=1
 ˜ai −g∗
0(xi), (˜bi −∂g∗
0
∂x (xi))′h

Σi

ˆai −g∗
0(xi)
(ˆbi −∂g∗
0
∂x (xi))h

(A.11)
≥1
m
m
X
i=1
 g∗
0(xi) −ˆai, (∂g∗
0
∂x (xi) −ˆbi)′h

Σi

g∗
0(xi) −ˆai
  ∂g∗
0
∂x (xi) −ˆbi

h

≥
1
mC
m
X
i=1
(g∗
0(xi) −ˆai)2
(A.12)
Next, we show that the quantity in (A.11) converges to zero in probability as n →∞.
The proof can be divided into six steps:
1. The contribution to (A.11) from evaluation points lying outside a carefully pre-chosen
compact subset S′ of the interior of S (denoted as int(S)) can be made arbitrarily small.
53

This follows from the Cauchy–Schwarz inequality that
1
m
m
X
i=1
 ˜ai −g∗
0(xi), (˜bi −∂g∗
0
∂x (xi))′h

Σi

ˆai −g∗
0(xi)
(ˆbi −∂g∗
0
∂x (xi))h

1{x/∈S′}
≤
v
u
u
t 1
m
m
X
i=1
 ˜ai −g∗
0(xi), (˜bi −∂g∗
0
∂x (xi))′h

Σi

˜ai −g∗
0(xi)
(˜bi −∂g∗
0
∂x (xi))h

1{x/∈S′}
(A.13)
×
v
u
u
t 1
m
m
X
i=1
 ˆai −g∗
0(xi), (ˆbi −∂g∗
0
∂x (xi))′h

Σi

ˆai −g∗
0(xi)
(ˆbi −∂g∗
0
∂x (xi))h

.
(A.14)
Because of Lemma B.1 and Assumption 2(i), the quantity in (A.13) can be made arbi-
trarily small by choosing S′ suﬃciently close to S. In addition, applying the Cauchy–
Schwarz inequality to (A.11) and comparing it to (A.12) yields
2
v
u
u
t 1
m
m
X
i=1
 ˜ai −g∗
0(xi), (˜bi −∂g∗
0
∂x (xi))′h

Σi

˜ai −g∗
0(xi)
(˜bi −∂g∗
0
∂x (xi))h

×
v
u
u
t 1
m
m
X
i=1
 ˆai −g∗
0(xi), (ˆbi −∂g∗
0
∂x (xi))′h

Σi

ˆai −g∗
0(xi)
(ˆbi −∂g∗
0
∂x (xi))h

≥1
m
m
X
i=1
 g∗
0(xi) −ˆai, (∂g∗
0
∂x (xi) −ˆbi)′h

Σi

g∗
0(xi) −ˆai
  ∂g∗
0
∂x (xi) −ˆbi

h

,
so (A.14) is no greater than
2
v
u
u
t 1
m
m
X
i=1
 ˜ai −g∗
0(xi), (˜bi −∂g∗
0
∂x (xi))′h

Σi

˜ai −g∗
0(xi)
(˜bi −∂g∗
0
∂x (xi))h

→2
n Z
S
(g0(x) −g∗
0(x))2Q(dx)
o1/2
≤2
n Z
S
g2
0(x)Q(dx)
o1/2
.
Consequently, the claim in this step is proved.
2. We now investigate the contribution to (A.11) from evaluation points lying inside S′.
Using Lemma B.5, we have that ˆgn is bounded (i.e. from both below and above) and
M-Lipschitz over S′ in probability.
54

Combining this with Lemma B.1 implies that

1
m
m
X
i=1
 ˜ai −g∗
0(xi), (˜bi −∂g∗
0
∂x (xi))′h

Σi

ˆai −g∗
0(xi)
 ˆbi −∂g∗
0
∂x (xi)

h

1{x∈S′}
−1
m
m
X
i=1

(g0 −g∗
0)(xi), (∂(g0 −g∗
0)
∂x
(xi))′h

Σi

ˆai −g∗
0(xi)
 ˆbi −∂g∗
0
∂x (xi)

h

1{x∈S′}
 →0
in probability. As such, we can instead work on
1
m
m
X
i=1

(g0 −g∗
0)(xi), (∂(g0 −g∗
0)
∂x
(xi))′h

Σi

ˆai −g∗
0(xi)
 ˆbi −∂g∗
0
∂x (xi)

h

1{x∈S′}
(A.15)
3. Next, we bound (and eliminate) the inﬂuence from the parts involving partial derivatives
of g0, g∗
0 and ˆgn in (A.15). Since ˆgn is bounded and M-Lipschitz over S′ in probability,
together with Lemma B.2, we could bound (A.15) from above by
1
m
m
X
i=1
(g0(xi) −g∗
0(xi))(ˆgn(xi)) −g∗
0(xi))1{x∈S′} + O(h) + O(h2),
which is arbitrarily close to
1
m
Pm
i=1(g0(xi) −g∗
0(xi))(ˆgn(xi) −g∗
0(xi))1{x∈S′} as n →∞
(i.e. h →0). Here we also used the fact that supi=1,...,m |Σ(11)
i
−1| →0, where Σ(11)
i
is
the ﬁrst diagonal entry of the matrix Σi.
4. Now we re-expand ˆgn from S′ to S as
ˆgS′
n (x) =
min
i∈{1,...,m|xi∈S′},
n
ˆgn(xi) + (x −xi)′∂ˆgn
∂x (xi)
o
.
Three useful facts about ˆgS′
n are listed below:
• ˆgS′
n ≥ˆgn, with ˆgS′
n (xi) = ˆgn(xi) for any xi ∈S′.
• there exists some B > 0 such that supx∈S ˆgS′
n (x) ≤B in probability. Importantly,
given that there is a common compact and convex set C such that C ⊂S′ for all
the S′ to be considered, the constant B does not depend on the choice of S′. To
see this, we note that ˆgC
n = ˆgn over C, which is also B′-bounded and M′-Lipschitz
55

over C in probability via Lemma B.5. Then it follows that
ˆgS′
n ≤ˆgC
n ≤B′ + M′
sup
y1,y2∈S
∥y1 −y2∥=: B
in probability as n →∞.
• The function {(g0−g∗
0)(ˆgn −g∗
0)}(·) is bounded and Lipschitz over S′ in probability
(where the constants do not depend on n). So is {(g0 −g∗
0)(ˆgS′
n −g∗
0)}(·) over S.
This also means that {(g0 −g∗
0)(ˆgS′
n −g∗
0)}(·) is equicontinuous over S.
5. Returning to the quantity we mentioned at the end of Step 3, we note that
1
m
m
X
i=1
(g0(xi) −g∗
0(xi))(ˆai −g∗
0(xi)1{xi∈S′}
= 1
m
m
X
i=1

(g0 −g∗
0)(ˆgS′
n −g∗
0)

(xi) −1
m
m
X
i=1

(g0 −g∗
0)(ˆgS′
n −g∗
0)

(xi)1{xi /∈S′}
= 1
m
m
X
i=1

(g0 −g∗
0)(ˆgS′
n −g∗
0)

(xi) −1
m
m
X
i=1

(g0 −g∗
0)(ˆgn −g∗
0)

(xi)1{xi /∈S′}
−1
m
m
X
i=1

(g0 −g∗
0)(ˆgn −ˆgS′
n )

(xi)1{xi /∈S′}
= (I) + (II) + (III).
We deal with each of these items separately.
• By the third fact listed in the above Step 4 and Theorem 3.1 of Rao (1962), (I) in
the limit (i.e. as n →∞) is at most
sup
g∈G2
Z
S
{(g0(x) −g∗
0(x)}{g(x) −g∗
0(x)}q(x)dx ≤0.
Note that g∗
0 minimizes
G(g) :=
Z
S
(g0(x) −g(x))2q(x)dx
over all g ∈G2. The previous inequality thus follows by studying the functional
derivative for the function G(·) at g∗
0 in the direction of g −g∗
0 (N.B. g∗
0 +ǫ(g −g∗
0) ∈
56

G2 for ǫ →0) for all g ∈G2.
• Both |(II)| and |(III)| in the limit can be arbitrarily small for S′ suﬃciently close
to S. This follows from Cauchy–Schwarz inequality and an argument similar to
that in Step 1.
6. We now put things together by noting that in light of Steps 1 to 5, for any ǫ, we can ﬁnd
some S′ such that the quantity in (A.11) is no bigger than ǫ in probability as n →∞.
Since the quantity in (A.11) is also non-negative, our claim that (A.11) converges to
zero in probability is veriﬁed.
Finally, uniform consistency over any C can be shown using exactly the same approach
we demonstrated in the ﬁnal stage of proving the ﬁrst part of Theorem 2 via Lemma B.4.
B.4.2
Proof of Theorem 5
Proof. Our proof can be divided into three parts.
1. The case of g0 = 0.
Using the deﬁnition of SCKLS in Appendix B.2, it is easy to verify that Tn = ∥ˆgn −
˜gn∥n,m. For reasons that will become clear later, we denote ˆg◦
n and ˜g◦
n the SCKLS and
LL estimators based on the same covariates, evaluation points and bandwidth used in
calculating Tn, but with the response vector (ǫ1, . . . , ǫn)′ (instead of yn) and set T ◦
n =
∥˜g◦
n −ˆg◦
n∥n,m. Obviously, when g0 = 0 (which is the case here), ˆg◦
n = ˆgn, ˜g◦
n = ˜gn and
T ◦
n = Tn.
Now, for k = 1, . . . , B, Tnk = ∥ˆgnk −˜gnk∥n,m, where ˆgnk and ˜gnk are respectively the
SCKLS and LL estimators based on the same covariates, evaluation points and bandwidth
used in calculating Tn, but with the response vector (u1k˜ǫ1, . . . , unk˜ǫn)′. Further, we deﬁne
a slightly modiﬁed bootstrap version of the test statistic as T ◦
nk = ∥ˆg◦
nk −˜g◦
nk∥n,m, where
ˆg◦
nk and ˜g◦
nk are the SCKLS and LL estimators based on the same covariates, evaluation
points and bandwidth used in calculating Tn, but with the response (u1kǫ1, . . . , unkǫn)′.
Let e = (|ǫ1|, . . . , |ǫn|)′ and denote p◦
n =
1
B
PB
i=1 1{T ◦
n≤T ◦
nk}.
Then, it follows from the
symmetry of the error distribution that conditioning on the values of the absolute errors
57

(i.e. (|ǫ1|, . . . , |ǫn|)′ = e), the quantities
T ◦
n, T ◦
n1, . . . , T ◦
nB
are exchangeable. Consequently, as B →∞,
P(p◦
n ≤α) = E
n
P

p◦
n ≤α
(|ǫ1|, . . . , |ǫn|)′ = e
o
≤⌊Bα⌋+ 1
1 + B
→α.
Back to the elements in the quantity pn, our aim is to show that 1{Tn≤T ◦
nk} ≤1{Tn≤Tnk+∆n}
for large n. Note that
Tnk−T ◦
nk = ∥˜gnk−ˆgnk∥n,m−∥˜g◦
nk−ˆg◦
nk∥n,m ≤∥˜gnk−ˆg◦
nk∥n,m−∥˜g◦
nk−ˆg◦
nk∥n,m ≤∥˜gnk−˜g◦
nk∥n,m
Because we estimated the error vector in Step 1 using LL (without any shape restrictions), it
follows from Proposition 7 of Fan and Guerre (2016) that supj |˜ǫj−ǫj| ≤Op(n−2/(4+d) log1/2 n).
By the linearity of the LL estimator (w.r.t. the response vector), we have that supk ∥˜gnk −
˜g◦
nk∥2
n,m = Op(n−4/(4+d) log n). Consequently, with arbitrarily high probability,
inf
k=1,...,B(Tnk + ∆n −T ◦
nk) > 0
for suﬃciently large n. This yields 1{T ◦
n≤T ◦
nk} ≤1{Tn≤Tnk+∆n} and thus pn ≥p◦
n. As a result,
P(pn ≤α) ≤P(p◦
n ≤α) ≤α, as required.
2. The general case of g0 ∈G2.
To relate Tn to what we investigated before (i.e. g0 = 0), we recall the deﬁnitions of ˆg◦
n
and ˜g◦
n from the previous case, and deﬁne an additional quantity ˜g†
n to be the LL estimator
in exactly the same setting, but is obtained using the response vector (g0(X1), . . . , g0(Xn))′.
By the linearity of the LL, ˜gn = ˜g◦
n + ˜g†
n. Since g0 is continuously twice-diﬀerentiable, we
have that
Tn = ∥˜gn −ˆgn∥n,m ≤∥˜g◦
n + ˜g†
n −ˆg◦
n −g0∥n,m ≤∥˜g◦
n −ˆg◦
n∥n,m + ∥˜g†
n −g0∥n,m = T ◦
n + Op(h2).
58

As a result, with arbitrarily high probability, for every k = 1, . . . , B,
Tnk + ∆n −Tn = T ◦
nk −T ◦
n + (Tnk −T ◦
nk) −(Tn −T ◦
n) + ∆n ≥T ◦
nk −T ◦
n
for suﬃciently large n. This also leads to 1{T ◦
n≤T ◦
nk} ≤1{Tn≤Tnk+∆n}. We could then directly
apply the argument from the previous case to conclude that P(pn ≤α) ≤α.
3. The case of g0 /∈G2
Here g0 is assumed to be ﬁxed and continuously twice-diﬀerentiable.
First, two situations are considered.
• Under Assumption 2(i), we recall that
g∗
0 := argmin
g∈G2
Z
S
{g(x) −g0(x)}2Q(dx).
Since g0 /∈G2, there must exists some compact set S′ ⊂int(S) such that Q(S′) > 0
and
inf
x∈S′ |g∗
0(x) −g0(x)| > δ.
Note that
T 2
n = ∥ˆgn−˜gn∥2
n,m ≥1
m
m
X
i=1

ˆgn(xi)−˜gn(xi),
 ∂(g1 −g2)
∂x
(xi)
′h

Σi
ˆgn(xi) −˜gn(xi)
∂(ˆgn−˜gn)
∂x
(xi)h

1{xi∈S′}.
Here we have that ˜gn →g0 by Fan and Guerre (2016) and ˆgn →g∗
0 over S′ by our
Theorem 4. Since ˜gn −ˆgn is Lipschitz over S′, it is easy to verify (see also Step 3
of the proof of Theorem 4) that the righthand side of the above display equation
is bounded below by δ2Q(S′) in the limit as n →∞(also h →0). Consequently,
Tn ≥c′ in probability for some c′ > 0.
• Now under Assumption 2(ii), since g0 /∈G2 and the evaluation points are reasonably
well spread across S (i.e. Assumption 2(ii)), for suﬃciently large and ﬁxed m, we can
always ﬁnd some evaluation points where the imposed shape constraint is violated.
59

This means that
inf
g∈G2 ∥g −g0∥n,m ≥c
in probability for some c > 0. So we still have that
Tn = ∥ˆgn −˜gn∥n,m ≥∥ˆgn −g0∥n,m −∥˜gn −g0∥n,m ≥inf
g∈G2 ∥g −g0∥n,m −op(1) ≥c′
in probability for some c′ > 0.
Second, it follows from the proof for the case of g0 = 0 that
Tnk = T ◦
nk + Tnk −T ◦
nk ≤∥˜g◦
nk∥n,m + ∥˜gnk −˜g◦
nk∥n,m = op(1).
Finally, write Wnk = 1{Tnk+∆n>c′/2}.
We note that Wn1, . . . , WnB are exchangeable.
Thus, for any α ∈(0, 1), as n →∞,
P(Do not reject H0) = P
 
1
B
B
X
k=1
1{Tn≤Tnk+∆n} ≥α
!
≤P(Tn ≤c′/2) + P
 
Tn > c′/2, 1
B
B
X
k=1
1{Tn≤Tnk+∆n} ≥α
!
≤P(Tn ≤c′/2) + P
 
1
B
B
X
k=1
Wnk ≥α
!
≤P(Tn ≤c′/2) + E(Wn1)
α
→0,
where we used Markov’s inequality in the ﬁnal line above. So the Type II error at the
alternative indeed converges to 0.
60

B.5
Proof of Propositions in Appendix A.3
B.5.1
Proof of Proposition A.1
Proof. In view of Assumption 1 (v), for any suﬃciently small h, we have
K
Xj −xi
h

=





0
if xi ̸= Xj,
K(0)
if xi = Xj,
for ∀i, j.
Then, the objective function of (3) is equal to Pn
j=1(yj −aj)2K(0), and thus
argmin
a1,b1,...,an,bn
n
X
j=1
(yj −aj)2K(0) = argmin
a1,...,an
n
X
j=1
(yj −aj)2
Writing aj = αj + β′
jXj and bj = βj for j = 1, . . . , n by deﬁnition. Then, quadratic
programming problem (3) can be rewritten as follows:
min
α,β
n
X
j=1
(yj −(αj + β′
jXj))2
subject to
αj + β′
jXj ≤αl + β′
lXj,
j, l = 1, . . . , n
βj ≥0,
j = 1, . . . , n
which is equivalent to the formulation of the CNLS estimator (A.5).
B.5.2
Proof of Proposition A.2
Proof. When mink=1,...,d hk →∞, we have
K
Xj −xi
h

= K(0)
for ∀i, j.
(A.16)
By substituting (A.16) into the objective function of (3) converges to
m
X
i=1
n
X
j=1
(yj −ai −(Xj −xi)′bi)2K(0).
61

Next, we derive the minimum of the objective function in the limit. Let’s consider
argmin
a1,b1,...,am,bm
m
X
i=1
n
X
j=1
(yj −ai −(Xj −xi)′bi)2
(A.17)
subject to constraints.
Rewrite ai + (Xj −xi)′bi = αi + β′
iXj for i = 1, . . . , m and
j = 1, . . . , n. Then the objective function of (3) can be rewritten as follows with (A.17).
min
α1,β1,...,αm,βm
m
X
i=1
n
X
j=1
(yj −(αi + β′
iXj))2
subject to
αi + β′
ixi ≤αl + β′
lxi
i, l = 1, . . . , m
βi ≥0
i = 1, . . . , m
Here, since we do not impose any weight on the objective function, it is easy to see that
α1 = · · · = αm and β1 = · · · = βm.
Then the Afriat constraints become redundant,
resulting in
min
α,β
n
X
j=1
(yj −(α + β′Xj))2
subject to
β ≥0.
B.5.3
Proof of Proposition A.3
Proof. In view of Assumption 1 (v), for any suﬃciently small h, we have
K
Xj −xi
h

=





0
if xi ̸= Xj,
K(0)
if xi = Xj,
for ∀i, j.
Then, the objective function of the SCKLS estimator (3) is equal to Pn
j=1(yj −aj)2K(0),
and thus
argmin
a1,b1,...,an,bn
n
X
j=1
(yj −aj)2K(0) = argmin
a1,...,an
n
X
j=1
(yj −aj)2
Also consider Assumption A1 (i) from Du et al. (2013), we can say something similar
for CWB in y-space. For any suﬃciently small h, we have
62

Aj(xi) =





0
if xi ̸= Xj,
n
if xi = Xj,
for ∀i, j.
and thus
ˆg(xi|p) =
n
X
j=1
pjAj(Xi)yj = npiyi ∀i = 1, . . . , n.
(A.18)
Then we can rewrite the CWB in y-space estimator as follows:
min
p
Dy(p) =
n
X
i=1
(yi −npiyi)2
subject to
l(xi) ≤ˆg(s)(xi|p) ≤u(xi),
i = 1, . . . , n.
(A.19)
Recognize that if ˆgn = npiyi is true, then SCKLS and CWB in y-space are equivalent.
Take ˆgn as the solution to SCKLS estimator and let pi be a set of decision variables, we
see ˆgn = npiyi is simply a system of n equations and n unknowns.
C
Testing for aﬃnity using SCKLS
C.1
The procedure
To further illustrate the usefulness of SCKLS for testing other shapes, we study the problem
of testing
H0 :
g0 : S →R is aﬃne
against
H1 :
g0 : S →R is not aﬃne.
The main idea of our test is motivated by Sen and Meyer (2017). The critical value of the
test can be easily computed using Monte Carlo or bootstrap methods.
To start of with, we deﬁne ˆgV
n , the SCKLS estimator with only a set of convexity
63

constraints as
min
ai,bi
m
X
i=1
n
X
j=1
(yj −ai −(Xj −xi)′bi)2K
Xj −xi
h

subject to
ai −al ≤b′
i(xi −xl),
i, l = 1, . . . , m
Furthermore, ˆgΛ
n, the SCKLS estimator using only a set of concavity constraints is deﬁned
as
min
ai,bi
m
X
i=1
n
X
j=1
(yj −ai −(Xj −xi)′bi)2K
Xj −xi
h

subject to
ai −al ≥b′
i(xi −xl),
i, l = 1, . . . , m
We now describe our testing procedure as follows.
1. First, we run linear regression on the response against the covariates and call the
least squares ﬁt gL
n. Next, we ﬁt the data using SCKLS (with evaluation points at
x1, . . . , xm and bandwidth hn). The resulting estimators are denoted by ˆgV
n and ˆgΛ
n,
where ˆgV
n is the SCKLS estimator using only a set of convexity constraints, while
ˆgΛ
n is the SCKLS estimator using only a set of concavity constraints, all based on
{Xj, yj}n
j=1.We then deﬁne the test statistics to be
Tn = max
 1
m
m
X
i=1
{ˆgV
n (xi) −gL
n(xi)}2, 1
m
m
X
i=1
{ˆgΛ
n(xi) −gL
n(xi)}2

.
2. We simulate the distributional behavior of the test statistics B times under H0.
For k = 1, . . . , B, we set the observations to be {Xj, yjk}n
j=1 (i.e. no change in the
values of the covariates), where ynk = (y1k, . . . , ynk)′ is drawn using the wild bootstrap
procedure as described in Section 4.2 (or the ordinary bootstrap procedure if we know
that the errors are homogeneous). Then we run linear regression on ynk against the
covariates and denote the least squares ﬁt by gL
nk. Fitting the data using SCKLS
(with the same set of evaluation points and the same bandwidth as before) leads to
the resulting estimators ˆgV
nk and ˆgΛ
nk, where ˆgV
nk is the SCKLS estimator using only
the convexity constraint, while ˆgΛ
nk is the SCKLS estimator using only the concavity
64

constraint, all based on {Xj, yjk}n
j=1. So
Tnk = max
 1
m
m
X
i=1
{ˆgV
nk(xi) −gL
nk(xi)}2, 1
m
m
X
i=1
{ˆgΛ
nk(xi) −gL
nk(xi)}2

.
3. The Monte Carlo p-value is deﬁned as
pn = 1
B
B
X
k=1
1{Tn≤Tnk}.
For a test of size α ∈(0, 1), we reject H0 if pn < α.
The intuition of the test is as follows. First, an aﬃne function is both convex and
concave. Therefore under H0, both SCKLS estimates, ˆgV
n and ˆgΛ
n, should be close to the
linear ﬁt gL
n, so the value of Tn should be small. Second, a function is both convex and
concave only if it is aﬃne.
So given enough observations, we should be able to reject
the null hypothesis under H1. Third, we used the fact that Tn based on {Xj, yj}n
j=1 and
{Xj, ǫj}n
j=1 are exactly the same under H0 when simulating the distributional behavior of
Tn.
Finally, we remark that in case we know that g0 is monotonically increasing a priori,
we could test H′
0 : g0 is monotonically increasing and aﬃne using essentially the same
procedure with only minor modiﬁcations described in the following: we instead run linear
regression with signed constraints in both Step 1 and Step 2, replace ˆgV
n by the SCKLS
with both the convexity and monotonicity constraints, and replace ˆgΛ
n by the SCKLS with
both the concavity and monotonicity constraints.
C.2
A simulation study
We now examine the ﬁnite-sample performance of the aﬃnity test using data generated
from the following DGP:
g0(x) = 1
d
d
X
k=1
xp
k
(A.20)
where x = (x1, . . . , xd)′. With n observations, for each pair (Xj, yj), each component of the
input, Xjk, is randomly and independently drawn from uniform distribution unif[0, 1], and
65

the additive noise, ǫj, is randomly and independently sampled from a normal distribution,
N(0, 0.1).
We considered diﬀerent sample sizes n ∈{100, 300, 500} and vary the number of inputs
d ∈{1, 2}, and perform 100 simulations to compute the rejection rate for each scenario.
We used the ordinary bootstrap method with B = 500.
In the scenarios we considered g0 is aﬃne if p = 1.0, and is non-linear if p ∈{0.2, 0.5, 2, 5}.
Table A.4 show the rejection rate for each scenario with one-input and two-input at
α = 0.05. We conclude that the proposed test works well with a moderate sample size.
Table A.4. Rejection rate of the aﬃnity test using SCKLS at α = 0.05
Sample size (n)
Shape Parameter (p)
Power of the Test
d = 1
d = 2
100
0.2
0.99
0.74
0.5
0.97
0.79
1.0
0.05
0.02
2.0
1.00
1.00
5.0
1.00
1.00
300
0.2
1.00
1.00
0.5
1.00
0.99
1.0
0.05
0.01
2.0
1.00
1.00
5.0
1.00
1.00
500
0.2
1.00
1.00
0.5
1.00
1.00
1.0
0.08
0.01
2.0
1.00
1.00
5.0
1.00
1.00
D
An algorithm for SCKLS computational performance
For a given number of evaluation points, m, SCKLS requires m(m−1) concavity constraints.
Larger values of m provide a more ﬂexible functional estimate, but also increase the number
of constraints quadratically, thus, the amount of time needed to solve the quadratic program
also increases quadratically. Since one can select the number of evaluation points in SCKLS,
by selecting m the computational complexity can be potentially reduced relative to CNLS
or estimates on denser grids, i.e. with m(m −1) ≪n(n −1).
Further, Dantzig et al. (1954, 1959) proposed an iterative approach that reduces the size
of large-scale problems by relaxing a subset of the constraints and solving the relaxed model
66

with only a subset V of constraints, checking which of the excluded constraints are violated,
and iteratively adding violated constraints to the relaxed model until an optimal solution
satisﬁes all constraints. Lee et al. (2013), who applied the approach to CNLS, found a
signiﬁcant reduction in computational time. Computational performances also improves if
a subset of the constraints can be identiﬁed which are likely to be needed in the model.
Lee et al. (2013) ﬁnd the concavity constraints corresponding to pairs of observations that
are close in terms of the ℓ2 norm measured over input vectors and more likely to be binding
than those corresponding to the distant observations. We use this insight to develop a
strategy for identifying constraints to include in the initial subset V , when solving SCKLS
as described below.
Given a grid to evaluate the constraints of the SCKLS estimator, we deﬁne the initial
subset of constraints V as those constraints constructed by adjacent grid points as shown
in Figure A.3. Further, we summarize our implementation of the algorithm proposed in
Lee et al. (2013) below and label it as Algorithm 1.
x1
x2
point of evaluation
adjacent grid points
Figure A.3. Deﬁnition of adjacent grid in two-dimensional case.
67

Algorithm 1 Iterative approach for SCKLS computational speedup
t ⇐0
V ⇐{(i, l) : xi and xl are adjacent, i < l}
Solve relaxed SCKLS with V to ﬁnd initial solution {a(0)
i , b(0)
i }m
i=1
while {a(t)
i , b(t)
i }m
i=1 satisﬁes all constraints in (3) do
t ⇐t + 1
U ⇐{(i, l) : xi and xl do not satisfy constraints in (3)}
V ⇐V ∪U
Solve relaxed SCKLS with V to ﬁnd solution {a(t)
i , b(t)
i }m
i=1
end while
return {a(t)
i , b(t)
i }m
i=1
68

E
Comprehensive results of existing and additional
numerical experiments
We show the comprehensive results of experiments in Section 5 and additional experiments
to show the performance of the SCKLS estimator and its extensions. For the CWB esti-
mator, we use the convex optimization solver SeDuMi because quadprog was not able to
solve CWB13.
For CWB estimator, we use a local linear estimator to obtain the weighting matrix
Aj(x) in (A.6). The ﬁrst partial derivative of ˆg(x|p) is obtained by approximating the
derivatives through numerical diﬀerentiation ˆg(1)(x|p) = ˆg(x+∆|p)−ˆg(x|p)
∆
, where ∆is a small
positive constant14.
E.1
Uniform input – high signal-to-noise ratio (Experiment 1)
We compare the following seven estimators: SCKLS with ﬁxed bandwidth, SCKLS with
variable bandwidth, CNLS, CWB in p-space and CWB in y-space, LL, and parametric
Cobb–Douglas function estimated via ordinary least squares (OLS). Table A.5 and Ta-
ble A.6 show the RMSE of Experiment 1 on observation points and evaluation points
respectively.
Table A.7 shows the computational time of Experiment 1 for each estimator.
We also conduct simulations with diﬀerent bandwidths to analyze the sensitivity of each
estimator to bandwidths. We estimate SCKLS with ﬁxed bandwidth, CWB in p-space and
local linear with bandwidth h ∈[0, 10] with an increment by 0.01 for 1-input setting, and
we use bandwidth h ∈[0, 5] × [0, 5] with an increment by 0.25 for 2-input setting. We
perform 100 simulations for each bandwidth, and compute the optimal bandwidth with
LOOCV for each simulation. Figure 1 displays the average RMSE of each estimator. The
distribution of bandwidths selected by LOOCV are shown in the histogram. The instances
13For CWB, SeDuMi provides a better solution than quadprog, while both SeDuMi and quadprog give
exactly the same solution for SCKLS.
14Du et al. (2013) proposes to use an analytical derivative for the ﬁrst partial derivative of ˆg(x|p); how-
ever, the analytical derivative performs similarly to numerical diﬀerentiation as shown in Racine (2016).
We propose two alternative methods to compute the ﬁrst partial derivative, and compared them in Ap-
pendix A.2.2.
69

Table A.5. RMSE on observation points for Experiment 1
Average of RMSE on observation points
Number of observations
100
200
300
400
500
2-input
SCKLS ﬁxed bandwidth
0.193
0.171
0.141
0.132
0.118
SCKLS variable bandwidth
0.183
0.158
0.116
0.118
0.098
CNLS
0.229
0.163
0.137
0.138
0.116
CWB in p-space
0.189
0.167
0.158
0.140
0.129
CWB in y-space
0.205
0.136
0.173
0.141
0.120
LL
0.212
0.166
0.149
0.152
0.140
Cobb–Douglas
0.078
0.075
0.048
0.039
0.043
3-input
SCKLS ﬁxed bandwidth
0.230
0.187
0.183
0.152
0.165
SCKLS variable bandwidth
0.216
0.183
0.175
0.143
0.142
CNLS
0.294
0.202
0.189
0.173
0.168
CWB in p-space
0.228
0.221
0.210
0.183
0.172
CWB in y-space
0.209
0.362
0.218
0.154
0.160
LL
0.250
0.230
0.235
0.203
0.181
Cobb–Douglas
0.104
0.089
0.070
0.047
0.041
4-input
SCKLS ﬁxed bandwidth
0.225
0.248
0.228
0.203
0.198
SCKLS variable bandwidth
0.217
0.219
0.210
0.180
0.179
CNLS
0.315
0.294
0.246
0.235
0.214
CWB in p-space
0.238
0.262
0.231
0.234
0.198
CWB in y-space
0.222
0.240
0.248
0.303
0.332
LL
0.256
0.297
0.252
0.240
0.226
Cobb–Douglas
0.120
0.073
0.091
0.067
0.063
when SCKLS, CWB-p, and local linear provide the lowest RMSE are shown in light gray,
gray and dark gray respectively on the histogram. For one-input scenario, the SCKLS
and CWB estimator perform similar for bandwidth between 0.25 - 2.25 as shown by the
closeness of the light gray and gray curves in (a).
In contrast, for two-input scenario,
the SCKLS estimator performs better for most of the LOOCV values as shown by the
majority of the histogram colored in light gray. This indicates that LOOCV calculate for
unconstrained estimator provide bandwidths that work well for the SCKLS estimator.
70

Table A.6. RMSE on evaluation points for Experiment 1
Average of RMSE on evaluation points
Number of observations
100
200
300
400
500
2-input
SCKLS ﬁxed bandwidth
0.219
0.189
0.150
0.147
0.128
SCKLS variable bandwidth
0.212
0.176
0.125
0.132
0.103
CNLS
0.350
0.299
0.260
0.284
0.265
CWB in p-space
0.206
0.186
0.174
0.154
0.143
CWB in y-space
0.259
0.228
0.228
0.172
0.167
LL
0.247
0.182
0.167
0.171
0.156
Cobb–Douglas
0.076
0.076
0.049
0.040
0.043
3-input
SCKLS ﬁxed bandwidth
0.283
0.231
0.238
0.213
0.215
SCKLS variable bandwidth
0.292
0.237
0.235
0.196
0.187
CNLS
0.529
0.587
0.540
0.589
0.598
CWB in p-space
0.291
0.289
0.269
0.252
0.233
CWB in y-space
0.314
0.474
0.265
0.346
0.261
LL
0.336
0.340
0.360
0.326
0.264
Cobb–Douglas
0.116
0.098
0.080
0.052
0.046
4-input
SCKLS ﬁxed bandwidth
0.321
0.357
0.329
0.308
0.290
SCKLS variable bandwidth
0.378
0.348
0.363
0.320
0.301
CNLS
0.845
0.873
0.901
0.827
0.792
CWB in p-space
0.360
0.385
0.358
0.361
0.325
CWB in y-space
0.355
0.470
0.338
0.410
0.602
LL
0.482
0.527
0.483
0.495
0.445
Cobb–Douglas
0.146
0.091
0.115
0.081
0.080
(a) One-input
(b) Two-input
Figure A.4. The histogram shows the distribution of bandwidths selected by LOOCV. The
curves show the relative performance of each estimator.
71

Table A.7. Computational time for Experiment 1
Average of computational time in seconds;
(percentage of Afriat constraints included
in the ﬁnal optimization problem)
Number of observations
100
200
300
400
500
2-input
SCKLS ﬁxed bandwidth
14.1
13.3
42.2
34.7
77.4
(6.14%)
(5.28%)
(8.86%)
(7.80%)
(8.31%)
SCKLS variable bandwidth
16.4
33.9
27.6
36.0
50.6
(3.47%)
(3.44%)
(3.34%)
(3.22%)
(3.53%)
CNLS
2.0
6.1
16.5
26.5
55.3
(100%)
(100%)
(100%)
(100%)
(100%)
CWB in p-space
24.1
33.2
76.6
82.3
130
(2.39%)
(2.35%)
(2.35%)
(2.35%)
(2.35%)
CWB in y-space
39.3
92.7
111
190
233
(2.35%)
(2.35%)
(2.35%)
(2.35%)
(2.36%)
3-input
SCKLS ﬁxed bandwidth
26.9
40.4
45.5
67.3
136
(16.0%)
(16.6%)
(16.3%)
(16.4%)
(16.2%)
SCKLS variable bandwidth
20.0
42.0
37.4
47.1
58.2
(15.7%)
(15.9%)
(15.8%)
(15.8%)
(15.9%)
CNLS
3.8
16.4
37.0
82.9
161
(100%)
(100%)
(100%)
(100%)
(100%)
CWB in p-space
47.6
71.5
100
202
255
(15.5%)
(15.5%)
(15.5%)
(15.5%)
(15.5%)
CWB in y-space
120
357
443
529
424
(15.5%)
(15.5%)
(15.5%)
(15.5%)
(15.5%)
4-input
SCKLS ﬁxed bandwidth
47.5
71.6
77.4
166
235
(40.1%)
(39.9%)
(39.9%)
(40.0%)
(39.8%)
SCKLS variable bandwidth
26.8
45.6
46.8
60.5
74.8
(39.9%)
(40.0%)
(39.8%)
(39.9%)
(39.8%)
CNLS
5.8
22.4
79.1
139.8
287.8
(100%)
(100%)
(100%)
(100%)
(100%)
CWB in p-space
68.8
136
196
327
442
(39.8%)
(39.8%)
(39.8%)
(39.8%)
(39.8%)
CWB in y-space
91.3
175
195
535
545
(39.8%)
(39.8%)
(39.8%)
(39.8%)
(39.8%)
72

E.2
Uniform input – low signal-to-noise ratio
We consider a Cobb–Douglas production function with d-inputs and one-output,
g0(x1, . . . , xd) =
d
Y
k=1
x
0.8
d
k .
For each pair (Xj, yj), each component of the input, Xjk, is randomly and independently
drawn from uniform distribution unif[1, 10], and the additive noise, ǫj, is randomly and
independently sampled from a normal distribution, N(0, 1.32). We consider 15 diﬀerent
scenarios with diﬀerent numbers of observations (100, 200, 300, 400 and 500) and input
dimension (2, 3 and 4). The number of evaluation points is ﬁxed at 400, and set as a
uniform grid.
This experiment has a higher noise level in the data generation process
relative to Experiment 1.
We compare following seven estimators: SCKLS with ﬁxed bandwidth, SCKLS with
variable bandwidth, CNLS, CWB in p-space, CWB in y-space, LL, and parametric Cobb–
Douglas function estimated via ordinary least squares (OLS). Table A.8 and Table A.9 show
the RMSE of this experiment on observation points and evaluation points respectively.
73

Table A.8. RMSE on observation points for Experiment: uniform input with low signal-
to-noise ratio
Average of RMSE on observation points
Number of observations
100
200
300
400
500
2-input
SCKLS ﬁxed bandwidth
0.239
0.203
0.203
0.155
0.140
SCKLS variable bandwidth
0.240
0.185
0.168
0.139
0.119
CNLS
0.279
0.231
0.194
0.168
0.151
CWB in p-space
0.314
0.215
0.237
0.275
0.151
CWB in y-space
0.241
0.229
0.173
0.178
0.206
LL
0.287
0.244
0.230
0.214
0.161
Cobb–Douglas
0.109
0.108
0.081
0.042
0.048
3-input
SCKLS ﬁxed bandwidth
0.292
0.263
0.221
0.204
0.184
SCKLS variable bandwidth
0.281
0.242
0.198
0.180
0.175
CNLS
0.379
0.303
0.275
0.224
0.214
CWB in p-space
0.318
0.306
0.308
0.244
0.214
CWB in y-space
0.281
0.273
0.225
0.320
0.271
LL
0.333
0.306
0.288
0.259
0.214
Cobb–Douglas
0.176
0.118
0.101
0.084
0.072
4-input
SCKLS ﬁxed bandwidth
0.317
0.291
0.249
0.241
0.254
SCKLS variable bandwidth
0.290
0.254
0.236
0.222
0.215
CNLS
0.491
0.356
0.311
0.293
0.313
CWB in p-space
0.400
0.318
0.273
0.260
0.289
CWB in y-space
0.312
0.338
0.262
0.365
0.453
LL
0.335
0.342
0.257
0.274
0.283
Cobb–Douglas
0.157
0.150
0.112
0.075
0.077
Table A.9. RMSE on evaluation points for Experiment: uniform input with low signal-to-
noise ratio
Average of RMSE on evaluation points
Number of observations
100
200
300
400
500
2-input
SCKLS ﬁxed bandwidth
0.253
0.225
0.222
0.172
0.160
SCKLS variable bandwidth
0.255
0.205
0.179
0.149
0.135
CNLS
0.319
0.355
0.334
0.255
0.267
CWB in p-space
0.329
0.239
0.262
0.305
0.177
CWB in y-space
0.263
0.241
0.198
0.228
0.180
LL
0.330
0.272
0.257
0.239
0.194
Cobb–Douglas
0.112
0.112
0.083
0.044
0.049
3-input
SCKLS ﬁxed bandwidth
0.367
0.339
0.302
0.268
0.231
SCKLS variable bandwidth
0.364
0.303
0.256
0.230
0.224
CNLS
0.743
0.778
0.744
0.696
0.620
CWB in p-space
0.398
0.392
0.434
0.336
0.274
CWB in y-space
0.401
0.473
0.385
0.450
0.525
LL
0.452
0.444
0.438
0.398
0.302
Cobb–Douglas
0.202
0.130
0.110
0.093
0.079
4-input
SCKLS ﬁxed bandwidth
0.405
0.460
0.349
0.350
0.347
SCKLS variable bandwidth
0.419
0.434
0.375
0.354
0.315
CNLS
1.019
0.950
0.985
1.043
1.106
CWB in p-space
0.514
0.520
0.393
0.390
0.452
CWB in y-space
0.514
0.513
0.425
0.501
0.708
LL
0.524
0.626
0.451
0.491
0.550
Cobb–Douglas
0.187
0.194
0.134
0.092
0.091
74

E.3
Diﬀerent numbers of evaluation points (Experiment 2)
We compare following four estimators: SCKLS with ﬁxed bandwidth, SCKLS with variable
bandwidth, CWB in p-space and CWB in y-space.
Table A.10 and Table A.11 show
the RMSEs of Experiment 2 on observation points and evaluation points respectively. In
addition, Table A.12 shows the computational time of Experiment 2 for each estimator.
Table A.10. RMSE on observation points for Experiment 2
Average of RMSE on observation points
Number of evaluation points
100
300
500
2-input
SCKLS ﬁxed bandwidth
0.142
0.141
0.141
SCKLS variable bandwidth
0.113
0.112
0.112
CWB in p-space
0.149
0.151
0.156
CWB in y-space
0.225
0.122
0.129
3-input
SCKLS ﬁxed bandwidth
0.198
0.203
0.197
SCKLS variable bandwidth
0.169
0.167
0.166
CWB in p-space
0.218
0.234
0.231
CWB in y-space
0.345
0.241
0.222
4-input
SCKLS ﬁxed bandwidth
0.239
0.207
0.206
SCKLS variable bandwidth
0.195
0.192
0.191
CWB in p-space
0.219
0.227
0.296
CWB in y-space
0.466
0.290
0.292
Table A.11. RMSE on evaluation points for Experiment 2
Average of RMSE on evaluation points
Number of evaluation points
100
300
500
2-input
SCKLS ﬁxed bandwidth
0.181
0.164
0.158
SCKLS variable bandwidth
0.140
0.128
0.124
CWB in p-space
0.195
0.180
0.179
CWB in y-space
0.262
0.162
0.169
3-input
SCKLS ﬁxed bandwidth
0.304
0.267
0.257
SCKLS variable bandwidth
0.242
0.213
0.205
CWB in p-space
0.332
0.329
0.302
CWB in y-space
0.792
0.582
0.559
4-input
SCKLS ﬁxed bandwidth
0.383
0.296
0.270
SCKLS variable bandwidth
0.386
0.304
0.265
CWB in p-space
0.403
0.359
0.415
CWB in y-space
1.040
0.352
0.381
75

Table A.12. Computational time for Experiment 2
Average of computational time in seconds;
(percentage of Afriat constraints included
in the ﬁnal optimization)
Number of evaluation points
100
300
500
2-input
SCKLS ﬁxed bandwidth
26.6
28.3
34
(11.7%)
(6.6%)
(5.4%)
SCKLS variable bandwidth
21.3
21.6
24.9
(9.9%)
(4.4%)
(3.2%)
CWB in p-space
41
56.5
74.2
(8.8%)
(3.2%)
(2.0%)
CWB in y-space
52.8
103
146
(8.8%)
(3.2%)
(2.0%)
3-input
SCKLS ﬁxed bandwidth
84.8
112
134
(29.1%)
(16.7%)
(13.3%)
SCKLS variable bandwidth
21.1
37.2
59.1
(28.5%)
(15.8%)
(12.4%)
CWB in p-space
121
221
310
(28.2%)
(15.5%)
(12.2%)
CWB in y-space
181
625
948
(28.2%)
(15.5%)
(12.2%)
4-input
SCKLS ﬁxed bandwidth
149
170
597
(62.3%)
(40.0%)
(27.7%)
SCKLS variable bandwidth
24.6
52.7
468
(62.1%)
(39.9%)
(27.5%)
CWB in p-space
175
275
729
(61.9%)
(39.8%)
(27.4%)
CWB in y-space
189
288
579
(61.9%)
(39.8%)
(27.4%)
76

E.4
Non-uniform input
Experiment 4. We consider a Cobb–Douglas production function with d-inputs and one-
output,
g0(x1, . . . , xd) =
d
Y
k=1
x
0.8
d
k .
For each pair (Xj, yj), each component of the input, Xjk, is randomly and independently
drawn from a truncated exponential distribution with density function
f(x) =
3
e−3 −e−30e−3x1{x∈[1,10]},
and the additive noise, ǫj, is randomly sampled from a normal distribution, N(0, 0.72). We
consider 15 diﬀerent scenarios with diﬀerent numbers of observations (100, 200, 300, 400
and 500) and input dimension (2, 3 and 4). The number of evaluation point is ﬁxed at 400.
Note that this experiment only diﬀers from Experiment 1 in that the distribution of inputs
is skewed and thus non-uniform.
We compare following seven estimators: SCKLS with ﬁxed bandwidth with uniform/non-
uniform grid, SCKLS with variable bandwidth with uniform/non-uniform grid, CNLS,
CWB in p-space with uniform/non-uniform grid. These extension of SCKLS were presented
in detail in Appendix A.1. Table A.13 and Table A.14 show the RMSEs of Experiment 4 on
observation points and evaluation points respectively. A uniform grid is used like in Exper-
iment 1. As the dimension of input space and the number of observations increase, SCKLS
with variable bandwidth performs better than the ﬁxed bandwidth estimator. SCKLS with
non-uniform grid performs better than SCKLS with uniform grid for almost all scenarios,
largely due to the fact that the DGP has non-uniform input. Consequently, we conclude
that variable bandwidth methods, such as k-NN approach, and non-uniform grid could be
useful to handle skewed input data which is a common feature of census manufacturing
data which is the type of data we considered in the application of the main manuscript.
77

Table A.13. RMSE on observation points for Experiment: non-uniform input
Average of RMSE on observation points
Number of observations
100
200
300
400
500
2-input
SCKLS ﬁxed/uniform
0.179
0.151
0.144
0.121
0.108
SCKLS ﬁxed/non-uniform
0.185
0.153
0.159
0.123
0.107
SCKLS variable/uniform
0.183
0.156
0.142
0.125
0.104
SCKLS variable/non-uniform
0.176
0.144
0.132
0.114
0.093
CNLS
0.193
0.160
0.140
0.130
0.117
CWB p-space/uniform
0.256
0.162
0.180
0.139
0.125
CWB p-space/non-uniform
0.243
0.160
0.174
0.135
0.125
3-input
SCKLS ﬁxed/uniform
0.197
0.184
0.172
0.164
0.167
SCKLS ﬁxed/non-uniform
0.200
0.181
0.173
0.161
0.172
SCKLS variable/uniform
0.212
0.187
0.170
0.175
0.170
SCKLS variable/non-uniform
0.210
0.180
0.162
0.160
0.155
CNLS
0.303
0.246
0.201
0.185
0.166
CWB p-space/uniform
0.243
0.436
0.173
0.174
0.184
CWB p-space/non-uniform
0.233
0.194
0.176
0.165
0.173
4-input
SCKLS ﬁxed/uniform
0.219
0.211
0.196
0.209
0.187
SCKLS ﬁxed/non-uniform
0.210
0.206
0.181
0.197
0.180
SCKLS variable/uniform
0.208
0.193
0.167
0.171
0.170
SCKLS variable/non-uniform
0.206
0.193
0.164
0.169
0.168
CNLS
0.347
0.292
0.250
0.228
0.218
CWB p-space/uniform
0.219
0.205
0.205
0.184
0.218
CWB p-space/non-uniform
0.221
0.205
0.182
0.170
0.170
Table A.14. RMSE on evaluation points for Experiment: non-uniform input
Average of RMSE on evaluation points
Number of observations
100
200
300
400
500
2-input
SCKLS ﬁxed/uniform
0.262
0.220
0.244
0.157
0.196
SCKLS ﬁxed/non-uniform
0.212
0.174
0.195
0.138
0.131
SCKLS variable/uniform
0.246
0.204
0.192
0.142
0.136
SCKLS variable/non-uniform
0.193
0.160
0.145
0.120
0.100
CNLS
0.435
0.402
0.404
0.379
0.381
CWB p-space/uniform
0.422
0.287
0.376
0.246
0.264
CWB p-space/non-uniform
0.283
0.186
0.215
0.159
0.162
3-input
SCKLS ﬁxed/uniform
0.323
0.308
0.311
0.286
0.293
SCKLS ﬁxed/non-uniform
0.268
0.254
0.259
0.235
0.249
SCKLS variable/uniform
0.335
0.303
0.281
0.262
0.254
SCKLS variable/non-uniform
0.278
0.243
0.219
0.212
0.196
CNLS
0.828
0.824
0.828
0.786
0.782
CWB p-space/uniform
0.438
0.684
0.357
0.363
0.350
CWB p-space/non-uniform
0.315
0.265
0.257
0.235
0.242
4-input
SCKLS ﬁxed/uniform
0.406
0.398
0.397
0.404
0.400
SCKLS ﬁxed/non-uniform
0.339
0.343
0.333
0.371
0.331
SCKLS variable/uniform
0.417
0.423
0.368
0.364
0.356
SCKLS variable/non-uniform
0.359
0.359
0.313
0.302
0.280
CNLS
1.129
1.107
1.220
1.196
1.223
CWB p-space/uniform
0.421
0.442
0.435
0.418
0.487
CWB p-space/non-uniform
0.354
0.344
0.308
0.286
0.280
78

E.5
Estimation with a misspeciﬁed shape
We use the DGP proposed by Olesen and Ruggiero (2014) that is consistent with the
regular ultra passum law (Frisch, 1964), which appears to have an “S”-shape.
g0(x1, x2) = F(h(x1, x2))
where the scaling function is: F(w) =
15
1+e−5 log(w) , and the linear homogeneous core function
is
h(x1, x2) =

βx
σ−1
σ
1
+ (1 −β)x
σ−1
σ
2

σ
σ−1
with β = 0.45 and σ = 1.51. For j = 1, . . . , n, input, Xj = (Xj1, Xj2)′, is generated in
polar coordinates with angles η and modulus ω independently uniformly distributed on
[0.05, π/2 −0.05] and [0, 2.5], respectively. The additive noise, ǫj, is randomly sampled
from N(0, 0.72).
Note that this DGP is not concave. Here we run this experiment to assess the per-
formance of each estimator in case of shape misspeciﬁcation. Table A.15 and Table A.16
show the RMSEs of this experiment on observation points and evaluation points. Fig-
ure A.5 shows the estimation results with 1-input S-shape function from a typical run of
SCKLS. The ﬁgure shows that the SCKLS estimator results in a linear estimates for areas
where concavity is violated. Here the CWB estimator performs slightly worse when the
function is misspeciﬁed.We speculate that the main reason for this is that the optimiza-
tion problem becomes too complicated to solve since intuitively there are many binding
constraints when the data is generated by the misspeciﬁed functional form, and thus, it
becomes hard for the solver to ﬁnd a feasible solution and an improving direction.
Table A.15. RMSE on observation points for Experiment: misspeciﬁed shape
Average of RMSE on observation points
Number of observations
100
200
300
400
500
SCKLS ﬁxed bandwidth
1.424
1.435
1.405
1.392
1.421
CNLS
1.326
1.346
1.337
1.316
1.353
CWB in p-space
6.310
6.731
6.602
5.909
6.110
79

Table A.16. RMSE on evaluation points for Experiment: misspeciﬁed shape
Average of RMSE on evaluation points
Number of observations
100
200
300
400
500
SCKLS ﬁxed bandwidth
1.337
1.162
1.149
1.140
1.123
CNLS
1.375
1.424
1.404
1.403
1.385
CWB in p-space
9.100
9.483
9.599
8.435
8.719
0
0.5
1
1.5
2
2.5
X (Input)
-5
0
5
10
15
20
y (Output)
Figure A.5. A typical run of SCKLS when the truth is S-shaped.
80

F
Semiparametric partially linear model
F.1
The procedure
We develop a semiparametric partially linear model including the SCKLS estimator and a
linear function of contextual variables. The partially linear model is often used in practice.
The model estimated is represented as follows:
yj = Z′
jγ + g0(Xj) + ǫj
where Zj = (Zj1, Zj2, . . . , Zjl)′ denotes contextual variables and γ = (γ1, γ2, . . . , γl)′ is the
coeﬃcient of contextual variables, see Johnson and Kuosmanen (2011, 2012). Then, we
estimate the coeﬃcient of contextual variable:
ˆγ =
 n
X
j=1
˜Zj ˜Z′
j
!−1  n
X
j=1
˜Zj ˜yj
!
where ˜Zj = Zj −ˆE[Zj|Xj] and ˜yj = yj −ˆE[yj|Xj] respectively, and each conditional
expectation is estimated by kernel estimation method such as local linear.
Finally, we
apply the SCKLS estimator to the data {Xj, yj −Z′
j ˆγ}n
j=1. Robinson (1988) proved that ˆγ
is n1/2-consistent for γ and asymptotically normal under regularity conditions. For details
of the partially linear model, see Li and Racine (2007).
F.2
A simulation study
We show the eﬀect of adding contextual variables Zj to the estimation performance by
comparing SCKLS with and without contextual variables. We use two diﬀerent Cobb–
Douglas production functions as the true DGP:
g0(x, z) =
d
Y
k=1
x
0.8
d
k
+ zγ,
(A.21)
81

g0(x) =
d
Y
k=1
x
0.8
d
k ,
(A.22)
where for each (Xj, Zj, yj), the contextual variable Zj is a scalar value independent of
Xj drawn randomly and independently from uinf[0, 1], the coeﬃcient of the contextual
variable γ = 5, and other parameters follow DGP from Experiment 1. We apply SCKLS
with and without contextual variables to the data generated by the true production function
(A.21) and (A.22), respectively.
Table A.17 and Table A.18 show the RMSEs of this experiment on observation points
and evaluation points respectively.
The RMSE is obtained by comparing estimates of
production function and the true production function. We see that having extra contextual
variables does not deteriorate the performance of SCKLS signiﬁcantly, especially when
the input dimension is small and the number of observations is large. Our ﬁndings are
consistent with the work of Robinson (1988).
Since our application data in Section 6
has only two-input, we expect that SCKLS with Z-variables tends not to deteriorate the
estimator performance in our application.
Table A.17. RMSE on observation points for experiments with/without Z-variable
Average of RMSE on observation points
Number of observations
100
200
300
400
500
2-input
SCKLS-Z
0.224
0.212
0.239
0.160
0.146
SCKLS
0.210
0.188
0.170
0.139
0.140
3-input
SCKLS-Z
0.404
0.235
0.261
0.197
0.196
SCKLS
0.242
0.206
0.215
0.202
0.188
4-input
SCKLS-Z
0.462
0.376
0.332
0.217
0.239
SCKLS
0.247
0.231
0.202
0.202
0.198
Table A.18. RMSE on evaluation points for experiments with/without Z-variable
Average of RMSE on evaluation points
Number of observations
100
200
300
400
500
2-input
SCKLS-Z
0.245
0.234
0.256
0.172
0.166
SCKLS
0.230
0.205
0.194
0.154
0.157
3-input
SCKLS-Z
0.496
0.348
0.377
0.271
0.286
SCKLS
0.316
0.296
0.309
0.271
0.261
4-input
SCKLS-Z
0.648
0.599
0.498
0.397
0.435
SCKLS
0.385
0.381
0.341
0.350
0.336
82

0
2
4
6
8
10
Labor
×105
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Capital
×104
m=400
Observed points
Evaluation points
(a) Before deletion
0
2
4
6
8
10
Labor
×105
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Capital
×104
m=278
Observed points
Evaluation points
(b) After deletion
Figure A.6. Proposed evaluation points with Plastic industry (2520)
G
Details on the application to the Chilean manufac-
turing data
In section 6, we applied the SCKLS estimator to the Chilean manufacturing data to estimate
a production function for plastic (2520) and wood (2010) industries. Here we provide the
detailed speciﬁcation of the SCKLS estimator applied to the real data. Since the application
data is skewed as shown in Table 6, we use non-uniform grid of evaluation points and limit
evaluation points to be inside the convex hull of {Xj}n
j=1. Figure A.6 and Figure A.7
show how we set the evaluation points in our application. Originally we set the number of
evaluation points is m = 400, but after deleting ones which lie outside of the convex hull
of {Xj}n
j=1, the number is m ≈270 for both industries.
References
Ackerberg, D. A., K. Caves, and G. Frazer (2015).
Identiﬁcation properties of recent
production function estimators. Econometrica 83(6), 2411–2451.
Afriat, S. N. (1972). Eﬃciency estimation of production functions. International Economic
Review 13(3), 568–598.
83

0
2
4
6
8
10
12
14
16
Labor
×105
-1000
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
Capital
m=400
Observed points
Evaluation points
(a) Before deletion
0
2
4
6
8
10
12
14
16
Labor
×105
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
Capital
m=270
Observed points
Evaluation points
(b) After deletion
Figure A.7. Proposed evaluation points with Wood industry (2010)
Alvarez, R. and H. G¨org (2009).
Multinationals and plant exit: Evidence from chile.
International Review of Economics & Finance 18(1), 45–51.
Andrews, D. W. K. (2000). Inconsistency of the bootstrap when a parameter is on the
boundary of the parameter space. Econometrica 68(2), 399–405.
Banker, R. D. and A. Maindiratta (1992). Maximum likelihood estimation of monotone
and concave production frontiers. Journal of Productivity Analysis 3(4), 401–415.
Benavente, J. M. (2006). The role of research and innovation in promoting productivity in
chile. Economics of Innovation and New Technology 15(4-5), 301–315.
Beresteanu, A. (2005). Nonparametric analysis of cost complementarities in the telecom-
munications industry. RAND Journal of Economics 36(4), 870–889.
Beresteanu, A. (2007). Nonparametric estimation of regression functions under restrictions
on partial derivatives. Working paper.
Bernard, A. B. and J. B. Jensen (2004). Exporting and productivity in the usa. Oxford
Review of Economic Policy 20(3), 343–357.
Bertsekas, D. (1995). Nonlinear Programming. Athena Scientiﬁc.
Birke, M. and H. Dette (2007). Estimating a convex function in nonparametric regression.
Scandinavian Journal of Statistics 34(2), 384–404.
Brunk, H. D. (1955). Maximum likelihood estimates of monotone parameters. The Annals
of Mathematical Statistics 26(4), 607–616.
84

Carroll, R. J., A. Delaigle, and P. Hall (2011). Testing and estimating shape-constrained
nonparametric density and regression in the presence of measurement error. Journal of
the American Statistical Association 106(493), 191–202.
Cavaliere, G., H. Bohn Nielsen, and A. Rahbek (2017). On the consistency of bootstrap
testing for a parameter on the boundary of the parameter space. Journal of Time Series
Analysis 38, 513534.
Chen, X. and Y. J. Qiu (2016). Methods for nonparametric and semiparametric regressions
with endogeneity: a gentle guide. Cowles Foundation Discussion Papers 2032, Cowles
Foundation for Research in Economics, Yale University.
Chen, Y. and R. J. Samworth (2016). Generalized additive and index models with shape
constraints. Journal of the Royal Statistical Society Series B 78(4), 729–754.
Chen, Y. and J. A. Wellner (2016). On convex least squares estimation when the truth is
linear. Electronic Journal of Statistics 10(1), 171–209.
Cleveland, W. S. (1979). Robust locally weighted regression and smoothing scatterplots.
Journal of the American statistical association 74(368), 829–836.
Dantzig, G., R. Fulkerson, and S. Johnson (1954).
Solution of a large-scale traveling-
salesman problem. Journal of the operations research society of America 2(4), 393–410.
Dantzig, G. B., D. R. Fulkerson, and S. M. Johnson (1959). On a linear-programming,
combinatorial approach to the traveling-salesman problem. Operations Research 7(1),
58–66.
Davidson, R. and E. Flachaire (2008). The wild bootstrap, tamed at last. Journal of
Econometrics 146(1), 162 – 169.
De Loecker, J. (2007). Do exports generate higher productivity? evidence from slovenia.
Journal of International Economics 73(1), 69–98.
Du, P., C. F. Parmeter, and J. S. Racine (2013). Nonparametric kernel regression with
multiple predictors and multiple shape constraints. Statistica Sinica 23(3), 1347–1371.
Fan, Y. and E. Guerre (2016). Multivariate local polynomial estimators: Uniform boundary
properties and asymptotic linear representation. In Essays in Honor of Aman Ullah, pp.
489–537. Emerald.
Frisch, R. (1964). Theory of production. Springer.
85

Ghosal, P. and B. Sen (2016).
On univariate convex regression.
arXiv preprint
arXiv:1608.04167.
Grenander, U. (1956). On the theory of mortality measurement: part ii. Scandinavian
Actuarial Journal 1956(2), 125–153.
Grenander, U. (1981). Abstract Inference. John Wiley & Sons.
Groeneboom, P., G. Jongbloed, and J. A. Wellner (2001). Estimation of a convex function:
characterizations and asymptotic theory. The Annals of Statistics 29(6), 1653–1698.
Hall, P. and N. E. Heckman (2000). Testing for monotonicity of a regression mean by
calibrating for linear functions. The Annals of Statistics 28(1), 20–39.
Hall, P. and L.-S. Huang (2001). Nonparametric kernel regression subject to monotonicity
constraints. The Annals of Statistics 29(3), 624–647.
Hanson, D. and G. Pledger (1976). Consistency in concave regression.
The Annals of
Statistics 4(6), 1038–1050.
Henderson, D. J. and C. F. Parmeter (2015). Applied nonparametric econometrics. Cam-
bridge University Press.
Hildreth, C. (1954). Point estimates of ordinates of concave functions.
Journal of the
American Statistical Association 49(267), 598–619.
Johnson, A. L. and T. Kuosmanen (2011). One-stage estimation of the eﬀects of operational
conditions and practices on productive performance: asymptotically normal and eﬃcient,
root-n consistent stonezd method. Journal of productivity analysis 36(2), 219–230.
Johnson, A. L. and T. Kuosmanen (2012). One-stage and two-stage dea estimation of
the eﬀects of contextual variables. European Journal of Operational Research 220(2),
559–570.
Kuosmanen, T. (2008). Representation theorem for convex nonparametric least squares.
The Econometrics Journal 11(2), 308–325.
Kuosmanen, T. and M. Kortelainen (2012). Stochastic non-smooth envelopment of data:
semi-parametric frontier estimation subject to shape constraints. Journal of Productivity
Analysis 38(1), 11–28.
Lee, C.-Y., A. L. Johnson, E. Moreno-Centeno, and T. Kuosmanen (2013). A more eﬃcient
algorithm for convex nonparametric least squares.
European Journal of Operational
86

Research 227(2), 391–400.
Levinsohn, J. and A. Petrin (2003). Estimating production functions using inputs to control
for unobservables. The Review of Economic Studies 70(2), 317–341.
Li, Q. and J. S. Racine (2007). Nonparametric econometrics: theory and practice. Princeton
University Press.
Li, Z., G. Liu, and Q. Li (2016). Nonparametric knn estimation with monotone constraints.
Working paper.
Lim, E. and P. W. Glynn (2012).
Consistency of multidimensional convex regression.
Operations Research 60(1), 196–208.
Liu, R. Y. (1988, 12). Bootstrap procedures under some non-i.i.d. models. The Annals of
Statistics 16(4), 1696–1708.
Mammen, E. (1991). Nonparametric regression under qualitative smoothness assumptions.
The Annals of Statistics 19(2), 741–759.
Mammen, E. (1993). Bootstrap and wild bootstrap for high dimensional linear models.
The Annals of Statistics 21(1), 255–285.
Masry, E. (1996). Multivariate local polynomial regression for time series: Uniform strong
consistency and rates. Jounral of Time Series Analysis 17(6), 571–599.
Mazumder, R., A. Choudhury, G. Iyengar, and B. Sen (2015). A Computational Framework
for Multivariate Convex Regression and its Variants. arXiv preprint arXiv:1509.08165.
Nesterov, Y. (2005). Smooth minimization of non-smooth functions. Mathematical pro-
gramming 103(1), 127–152.
Olesen, O. B. and J. Ruggiero (2014). Maintaining the regular ultra passum law in data
envelopment analysis. European Journal of Operational Research 235(3), 798–809.
Pavcnik, N. (2002). Trade liberalization, exit, and productivity improvements: Evidence
from chilean plants. The Review of Economic Studies 69(1), 245–276.
Racine, J. and Q. Li (2004). Nonparametric estimation of regression functions with both
categorical and continuous data. Journal of Econometrics 119(1), 99–130.
Racine, J. S. (2016). Local polynomial derivative estimation: Analytic or taylor? In Essays
in Honor of Aman Ullah, pp. 617–633. Emerald.
Rao, R. R. (1962). Relations between weak and uniform convergence of measures with
87

applications. The Annals of Mathematical Statistics 33(2), 659–680.
Robinson, P. M. (1988). Root-n-consistent semiparametric regression. Econometrica 56(4),
931–954.
Sarath, B. and A. Maindiratta (1997). On the consistency of maximum likelihood estima-
tion of monotone and concave production frontiers. Journal of Productivity Analysis 8(3),
239–246.
Seijo, E. and B. Sen (2011). Nonparametric least squares estimation of a multivariate
convex regression function. The Annals of Statistics 39(3), 1633–1657.
Sen, B. and M. Meyer (2017). Testing against a linear regression model using ideas from
shape-restricted estimation.
Journal of the Royal Statistical Society Series B 2(79),
423–448.
Stone, C. J. (1977). Consistent nonparametric regression. The Annals of Statistics 5(4),
595–620.
van der Vaart, A. and J. Wellner (1996). Weak Convergence and Empirical Processes: With
Applications to Statistics. Springer Series in Statistics. Springer.
Varian, H. R. (1984). The nonparametric approach to production analysis. Economet-
rica 52(3), 579–597.
Wu, C.-F. J. (1986). Jackknife, bootstrap and other resampling methods in regression
analysis. The Annals of Statistics 14(4), 1261–1295.
88
