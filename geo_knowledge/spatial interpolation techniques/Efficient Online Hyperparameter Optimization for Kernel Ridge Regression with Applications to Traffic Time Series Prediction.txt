Eﬃcient Online Hyperparameter Optimization for Kernel Ridge
Regression with Applications to Traﬃc Time Series Prediction
Hongyuan Zhan∗1, Gabriel Gomes2, Xiaoye S. Li3,
Kamesh Madduri1, and Kesheng Wu3
1Penn State University, Computer Science and Engineering
2UC Berkeley, PATH
3Lawrence Berkeley National Laboratory, Computational Research Division
Abstract
Computational eﬃciency is an important consideration for deploying machine
learning models for time series prediction in an online setting. Machine learning al-
gorithms adjust model parameters automatically based on the data, but often require
users to set additional parameters, known as hyperparameters. Hyperparameters can
signiﬁcantly impact prediction accuracy. Traﬃc measurements, typically collected on-
line by sensors, are serially correlated. Moreover, the data distribution may change
gradually. A typical adaptation strategy is periodically re-tuning the model hyperpa-
rameters, at the cost of computational burden. In this work, we present an eﬃcient
and principled online hyperparameter optimization algorithm for Kernel Ridge regres-
sion applied to traﬃc prediction problems. In tests with real traﬃc measurement data,
our approach requires as little as one-seventh of the computation time of other tuning
methods, while achieving better or similar prediction accuracy.
1
Introduction
Modern sensors generate large amounts of timestamped measurement data. These data
sets are critical in a wide range of applications including traﬃc ﬂow prediction, transporta-
tion management, GPS navigation, and city planning. Machine learning-based prediction
algorithms typically adjust their parameters automatically based on the data, but also
require users to set additional parameters, known as hyperparameters. For example, in
a kernel-based regression model, the (ordinary) parameters are the regression weights,
whereas the hyperparameters include the kernel scales and regularization constants.
These hyperparameters have a strong inﬂuence on the prediction accuracy. Often, their
values are set based on past experience or through time-consuming grid searches. In ap-
plications where the characteristics of the data change, such as unusual traﬃc pattern due
to upcoming concert events, these hyperparameters have to be adjusted dynamically in
order to maintain prediction quality. In this paper, we use the term hyperparameter learn-
ing, hyperparameter optimization, and hyperparameter selection/tuning interchangeably,
referring to the process of conﬁguring the model speciﬁcation before model ﬁtting.
Existing hyperparameter optimization approaches [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
are designed for oﬄine applications where the data are split into training and validation
∗Email: hongyuan.zhan@gmail.com
This manuscript is an extended version of the paper in [1].
1
arXiv:1811.00620v1  [cs.LG]  1 Nov 2018

Algorithm 1 A rolling hyperparameter tuning and model ﬁtting protocol for deploying
machine learning model for time series prediction. For simplicity, we show the 1-step-ahead
prediction setting here, but multistep settings are similar.
Input: Model M, hyperparameter tuning interval n (time-steps), model ﬁtting interval
m (time-steps).
Output: Predictions ˆyt, t = 0, 1, 2, . . . , T.
1: for t = 0 to T do
2:
if ((t mod n) = 0) then
3:
Vt ←Historical Data for Model Evaluation
4:
λ ←Hyperparameter Tuning(M, Vt)
▷costly
5:
if ((t mod m) = 0) then
6:
St ←Historical Data for Model Training(t)
7:
θ∗(λ) ←Train Model(M, St, λ)
8:
ˆyt ←Predict(M, θ∗(λ))
9:
Observe yt
sets, making them unsuitable for online applications.
Therefore, we aim to construct
online hyperparameter learning strategies.
This work was motivated by online traﬃc ﬂow prediction problem. In this context
as is many others, a set of learning algorithms are used in the traﬃc stream prediction
engine, and the model hyperparameters are often reset periodically. The model re-training
is scheduled according to the operation cycle. We summarize this deployment protocol in
Algorithm 1 (for 1-step-ahead prediction due to simplicity, multi-steps-ahead are similar).
Under this protocol, operators re-select the model hyperparameters every n time-steps, and
retrain the model every m time-steps. Note that hyperparameter tuning is much more
time-consuming than model ﬁtting. For example, the widely-used grid search strategy
selects diﬀerent hyperparameter conﬁgurations based on trial-and-error over the validation
data Vt (line 4 in Algorithm 1). In each trial of hyperparameters, the model needs to be
re-trained and re-evaluated.
The implication of the high computation cost of most hyperparameter tuning methods
is that traﬃc controllers cannot aﬀord frequent adjustments on hyperparameters. Since
the distribution of traﬃc ﬂow may change gradually, keeping the hyperparameters static
may result in sub-optimal performance of the prediction model. However, traﬃc sensors
collect data at a high frequency and the data stream arrives at the control center continu-
ously, the serial correlation of measurements suggest there is a potential for optimizing the
hyperparameters in an online manner. Therefore, this work proposes an online method for
hyperparameter learning motivated by the need for eﬃcient traﬃc time series prediction.
Online optimization [15, 16, 17, 18] emerged as powerful tools to reduce the compu-
tational complexity of model ﬁtting and provide theoretical guarantees. However, when
online optimization techniques applied on streaming prediction problems, one often as-
sumes that either the learner M has no hyperparameters or the hyperparameters are
ﬁxed in advance. Despite of the advances in online convex optimization, the rolling pre-
diction scheme outlined in Algorithm 1 is still widely used in practice since almost any
learning models can be deployed in this manner. Given the justiﬁcation that cost of hyper-
parameter search dominates cost of model learning, speeding up hyperparameter selection
will be very useful in practice. Much of the existing work in online optimization are de-
signed for convex objective functions, while the relationship between hyperparameters and
prediction accuracy is generally unknown and very unlikely to be convex. Therefore, a key
challenge to address is the development of an online optimization strategy for non-convex
2

functions. The major contribution of this work is an online hyperparameter learning al-
gorithm (called OHL) for Kernel Ridge Regression. The algorithm can also be applied to
certain class of models where the objective functions satisﬁes some smoothness assump-
tions. We analyze our algorithm in non-convex regret minimization framework and prove
that it achieves the optimal local regret [18] under suitable assumptions.
We make the following contributions in this paper:
• We design a Multiple-Kernel Ridge Regression approach for short-term traﬃc time
time series prediction, which aims to learn the long-term periodicity, short-term
deviation and trending of traﬃc ﬂows simultaneously via the combination of kernels
(section 3).
• To learn the model hyperparameters eﬀectively and eﬃciently, we propose an online
hyperparameter learning (OHL) algorithm. Our strategy is to adaptively update the
model hyperparameters with streaming data (section 4.1)
• We ﬁrst provide an abstraction of the OHL algorithm for a class of models where
the objective function satisﬁes some smoothness requirements, and on which the
hyper-gradients can be computed.
We then analyze the algorithm under regret
minimization framework and show the optimality of the algorithm in terms of local
regret (section 5).
• We tested the multiple-kernel model with the proposed OHL algorithm for traﬃc
ﬂow prediction on I-210 highway, and compared the performance of Multiple-Kernel
Ridge Regression under other popular hyperparameter tuning methods. Our method
achieves similar and sometimes better prediction accuracy compared to a state-of-
art hyperparameter tuning method, while using one-seventh of the computation time
(section 6).
2
Common Hyperparameter Tuning Algorithms
2.1
Grid Search
Grid Search is the simplest and most widely used hyperparameter tuning strategy. Given
a validation set Vt and training set St from the historical data, grid search enumerates
a user-provided list of hyperparameter settings.
For each conﬁguration, the model is
ﬁtted on St and evaluated on Vt. The conﬁguration yields the best performance on Vt is
selected. Suppose there are c possible choices for each hyperparameter, the cost of grid
search is O(cd · cost(θ∗(λ))), where cost(θ∗(λ)) is the cost of obtaining θ∗(λ). Hence
the computational cost of grid search grows exponentially. When grid search is applied
periodically in every n steps (Algorithm 1), the accumulated cost of hyperparameter tuning
is
O
T
n · cd · cost(θ∗(λ))

,
(1)
where T is the total number of predictions made, d is the dimension of hyperparameters.
2.2
Random Search
Random Search has been shown to be eﬀective in high dimensions despite being intuitively
simple [6].
Given a budget of R random draws per hyperparameter selection period,
instead of enumerating a pre-deﬁned list of conﬁgurations, random search trials diﬀerent
hyperparameters.
Following the analysis in [6], let the volume of the hyperparameter
3

space be Vol(H), and let volume containing targeted hyperparameters be Vol(T ), the
probability of ﬁnding a target out of R random draws is: 1 −

1 −Vol(T )
Vol(H)
R
. Suppose
the hyperparameters oﬀering good predictions lie in a hyper-rectangle occupying 5% of
the search space [6], i.e., Vol(T )
Vol(H) = 0.05, the probability that at least one draw from 50
trials positioned inside the target hyper-rectangle is more than 90%. The accumulated
computational cost of random search is
O
T
n · R · cost(θ∗(λ))

.
(2)
2.3
Gradient-based hyperparameter optimization
Gradient-based hyperparameter optimization methods for oﬄine problems were studied in
[2, 3, 4, 7, 9, 10, 13]. In the oﬄine setting, using a training set S and a hold-out validation
set V , one may apply gradient-based algorithm with (17) by alternatively ﬁtting θ∗(λ)
on S and computing the update direction of hyperparameters on V . When the hyper-
gradient is available, [9, 4] demonstrate the superior prediction performance of gradient-
based tuning. The complexity is O

I·
 cost(∇λf)+cost (θ∗(λ))

, where I is the number
of iterations taken. In general, I = Ω
  1
ϵ

for non-strongly convex functions [19], where ϵ is
the convergence threshold. When this approach is deployed online via the rolling protocol
(Algorithm 1), the accumulated cost of hyperparameter tuning becomes
O
T
n · 1
ϵ ·
 cost(∇λf) + cost (θ∗(λ))

.
(3)
2.4
Bayesian optimization methods
Bayesian optimization [20] is also a popular hyperparameter tuning paradigm.
It has
been shown that Bayesian optimization can produce state-of-art results for tuning deep
learning models. Ironically, Bayesian optimizer itself uses kernels and involves (hyper)-
hyperparameters. Therefore, we exclude these approaches in the Experiments section due
to the complications in applying the methods.
Note that there is a common factor of T
n in equation (1), (2), and (3) due to the periodic
nature in rolling hyperparameter tuning scheduled in every n steps. In section 4.1, we
propose an online hyperparameter optimization algorithm which removes this factor. The
theoretical performance guarantees of the algorithm will be analyzed in section 5.
3
Multiple-Kernel Ridge Regression
Traﬃc ﬂow time series is dynamic and hard to predict for a number of reasons. Despite
having an approximately AM/PM and weekday/weekend periodic pattern, the short term
traﬃc variation from the mean can be signiﬁcant. This can be due to traﬃc accidents,
weather, nearby events, and other factors. In addition, traﬃc measurements can be very
noisy due to inherent uncertainties and measurement error.
We use Multiple-Kernel Ridge Regression to simultaneously capture the periodicity
pattern and short-term distortion of traﬃc data. Kernel methods provide expressive tools
to model the periodicity and the short-term nonlinear deviation. At each model learning
step τ, let y ∈RN denote a vector collecting past N data points. For each yt, let xt :=
[ys]t−1
s=t−p ∈Rp be a vector of p past ﬂow observations that are used as predictor variables
for yt. The training set Sτ consists of pairs of past-present observations {(xt, yt)}τ
t=τ−N. In
4

Kernel Ridge Regression, φλK : Rp →Rq is a feature mapping from the raw observations
to another feature space, indexed by hyperparameters λK. The Kernel Ridge Regression
problem [21, 22, 23] ﬁnds a weight vector w ∈Rq that solves
min
w
τ
X
t=τ−N

yt −φλK (xt)T w
2
+ λR ∥w∥2 ,
(4)
where λR > 0 is a regularization hyperparameter to be selected, which controls the variance
of estimation. By the Representer Theorem [22, 23], there is θ := [θj]N
j=1 ∈RN, such that
the optimal solution w∗can be written as w∗= PN
j=1 θjφλK (xτ+1−j). Hence, instead of
optimizing over w, Eqn. (4) can be equivalently solved by
argmin
θ

y −KλKθ
T 
y −KλKθ

+ λRθT KλKθ,
(5)
where KλK ∈RN×N, [KλK]ij = [φ(xi)T φ(xj)], i, j = 1, · · · , N. Therefore, instead of ex-
plicitly constructing the feature mapping φλK(·), one may work directly with suitable
kernels KλK(·, ·) : Rp × Rp →R. Roughly speaking, kernel methods express the similarity
between the N training samples with a positive semi-deﬁnite kernel matrix KλK ∈RN×N,
where λK is a vector of hyperparameters that determine the kernel.
The hyperparameters of the Kernel Ridge Regression model are denoted by λ :=
[λK, λR].
Throughout the paper, we use θ∗(λ) to denote the optimal solution of (5),
highlighting its dependence on λ. The optimal solution θ∗(λ) can be written in closed-
form:
θ∗(λ) =
 KλK + λRI
−1y.
(6)
Diﬀerent choices of kernels capture diﬀerent aspects of the data. We model the periodicity
of traﬃc ﬂows as a function of time, and model the short-term deviation from strict
periodicity by considering the memory eﬀect from recent traﬃc. With slight abuse of
notation, we also use KλK(·, ·) : Rp × Rp →R to denote a pairwise kernel function on two
data points. Let yt, yt′ be the traﬃc volume at time-stamp t, t′ respectively. The periodic
kernel proposed by Mackay [24] determines the periodicity pattern by the time diﬀerence
|t −t′| between two observations,
Kprd
ν,ω (t, t′) := exp
 −ν sin2  π|t −t′|
ω

.
(7)
In Kprd
ν,ω , ω > 0 is a hyperparameter controlling the period of recurrence, ν > 0 is another
hyperparameter deciding the scale1 of “wiggles” in traﬃc ﬂow. The short-term nonlinear
eﬀect is modelled by a squared exponential kernel using autoregressive feature xt,
Kse
ν (xt, xt′) := exp
 −ν ∥xt′ −xt∥2
2

.
(8)
The kernel scale hyperparameter ν in Kse
ν
has similar qualitative eﬀects as the one in
Kprd
ν,ω , but their values can be diﬀerent and remain to be chosen. The automatic relevance
determination (ARD) kernel is a generalization of Kse
ν
allowing each component of the
feature to have a diﬀerent length scale,
Kard
ν
(xt, xt′) := exp

−
p
X
i=1
νi
 yt−i −yt′−i

.
(9)
1most literature refer l = ν−1 as the length scale, we use the reciprocal for ease of diﬀerentiation later.
5

The number of hyperparameters in the ARD kernel increases with the number of features,
hence it is usually infeasible to optimize them with grid search. A valid kernel function
gives rise to a positive semi-deﬁnite kernel matrix, where each entry is computed from the
kernel function on two data points. Any linear combination between kernels produce a
new one. Using this property, given M diﬀerent kernels, Multiple-Kernel Ridge Regression
uses an composite kernel function:
KλK = β1K(1)
λK1 + β2K(2)
λK2 + · · · + βMK(M)
λKM ,
(10)
where PM
i=1 βi = 1, βi ≥0. We consider the coeﬃcients {βm}M
m=1 as hyperparameters,
since they determine the ﬁnal kernel matrix used in equation (5). (10) can be viewed as
an ensemble learning model from diﬀerent kernels [25, 26, 27]. The composite kernel is a
function of time and the autoregressive feature:
KλK

(t, xt), (t′, xt′)

= β1Kprd
ν,ω (t, t′) + β2Kard
ν
(xt, xt′)
(11)
After computing θ∗(λ) through equation (6), to make a prediction for time t, let the
vector of pairwise kernel mappings between (t, xt) and (t′, xt′) in the training set Sτ be
kλK :=

KλK

(t, xt), (t′, xt′)

(t′,xt′)∈Sτ ∈RN. The prediction is given by
ˆyt(θ∗(λ)) = kT
λKθ∗(λ)
(12)
To summarize, λK in Multiple-Kernel Ridge Regression includes the hyperparameters for
each kernel and the kernel combination coeﬃcients {βm}M
m=1. λK and the regularization
constant λR must be set properly to balance the eﬀects of periodicity in traﬃc ﬂow, near-
term nonlinear distortion due to unusual events, and estimation variance – resulting in a
hyperparameter optimization problem.
4
Hyperparameter Learning
4.1
Hyper-Gradient Computation for Kernels
The dimension of λ can range from tens to hundreds when an automatic relevance kernel
is used with high dimensional features. Periodic hyperparameter re-selection brings heavy
computational burden for an online operations. This motivates the development of online
methods to adaptively learn the hyperparameters. We apply the ℓ2 loss function to obtain
the prediction error for time-step t:
ℓ(yt, ˆyt(θ∗(λ))) =
 yt −kT
λKθ∗(λ)
2 := ft(λ).
(13)
Notice that given yt and the training set, the prediction error is a non-convex function
of λ. Even though the loss function is convex, the nested nature of λ in θ∗(λ) and the
kernel kλK creates non-convexity. Using the chain rule, the partial derivative of ft(λ) with
respect to the kernel hyperparameters λK is:
∂ft(λK)
∂λK
= −2

yt −ˆyt(θ∗(λ))
∂kλK
∂λK
θ∗(λ)

−2

yt −ˆyt(θ∗(λ))

kT
λK
∂θ∗(λ)
∂λK

,
(14)
6

and the partial derivative with respect to the regularization constant λR is:
∂ft(λK)
∂λR
= −2

yt −ˆyt(θ∗(λ))

kT
λK
∂θ∗(λ)
∂λR

.
(15)
Let λR ∈[L, U] ⊂R+, since KλK is positive semi-deﬁnite, A(λ) :=
 KλK + λRI

is non-
singular. Therefore, A(λ)−1 is diﬀerentiable. Let λ(i) denote the i-th hyperparameter.
Then
∂A−1(λ)
∂λ(i)
= −A−1(λ)∂A(λ)
∂λ(i) A−1(λ)
(16)
Consequently,
∂θ∗(λ)
∂λ(i) = ∂A−1(λ)
∂λ(i)
y = −A−1(λ)∂A(λ)
∂λ(i) θ∗(λ)
(17)
Equation (17) along with (14) and (15) provide the gradient w.r.t hyperparameters (hyper-
gradient) given the loss at yt. In the next subsection, we described a state-of-art gradient-
based hyperparameter optimization method [9], and our rational for improving the method.
4.2
Our Method: Online Hyperparameter Learning
We propose an online projected hyper-gradient descent algorithm to address the compu-
tational burden of applying gradient-based hyperparameter tuning algorithms. The idea
is to compute the hyperparameter gradients ∇λft(λ) on-the-ﬂy when a new datum yt is
observed, then average the historical hyper-gradients to make a smoothed update on λ
before ﬁtting θ∗(λ) (every m steps). The entire rolling hyperparameter re-selection cycle
is removed and replaced by incremental learning procedure. In addition, to speed up the
computation of hyperparameter gradients, the terms in equation (14) and (15) shared with
subsequent hyper-gradients are pre-computed and stored after an update.
The projected gradient update to the hyperparameters in every m steps is:
λnew = ΠC

λold −η
m
τ−1
X
t=τ−m
∇λft(λ)

(18)
where ΠC(·) is the orthogonal projection operator deﬁned by ΠC(u) = argmin
v∈C
∥u −v∥2
2.
The hyperparameter space for Multiple-Kernel Ridge Regression is C = [U, L] ∪∆, such
that λ(i) ∈[Ui, Li] if λ(i) is not {βj}M
j=1, and {βj}M
j=1 ∈∆:= {βT 1 = 1, β ≥0} enforces
the simplex constraints on the kernel weights.
The Online Hyperparameter Learning
(OHL) algorithm for Multiple-Kernel Ridge Regression is presented in Algorithm 2.
4.3
Complexity
Lines 4, 7, 14, and 15 in Algorithm 2 are used for adjusting hyperparameters. Line 6 for
computing θ∗(λ) is a common step for all rollingly-trained Kernel Ridge methods, and thus
not additionally introduced by Algorithm 2. The cost of computing the Jacobian matrix
for a hyperparameter in Line 7 via eqn (17) is O(N2) due to matrix-vector multiplications,
since the inverse kernel design matrix A−1(λ) has been obtained in computing θ∗(λ). Also,
this cost only occurs in every m steps. The cost of computing the partial derivative for
each hyperparameter in Line 14 via eqn (14) and (14) reduces to a O(N) inner product
operation with a column in the pre-computed Jacobian matrix. Thus, Algorithm 2 eﬃ-
ciently computes the hyperparameter gradients online. Projection onto simplex Π∆(·) for
the kernel coeﬃcients {β}M
i=1 can be computed in O(M log M) time [28], and projection
7

Algorithm 2 Online Hyperparameter Learning (OHL) and Prediction with Multiple
Kernels.
Input: Update window m, learning rate η, convex feasible set C, initial λ0 ∈C, number
of training samples N, total prediction time-steps T.
Output: Predictions ˆyt, t = 0, . . . , T −1.
1: for t = 0 : T −1 do
2:
if ((t mod m) = 0) then
3:
if t > 0 then
4:
λt = ΠC

λt−1 −η m−1gt

▷update λ
5:
St = Historical Data for Model Training(t)
6:
θ∗(λt) =
 Kλt,K + λt,RI
−1y
▷ﬁt model
7:
J = compute Jacobian matrix using Eqn. (17)
8:
gt = 0
9:
else
10:
λt = λt−1
11:
ˆyt = kλt,K(t, xt)T θ∗(λt)
▷prediction
12:
Observe yt
13:
∇λft(λt) = compute hyperparameter gradient using Eqn.
(14), Eqn.
(15) and pre-
computed J
14:
gt+1 = gt + ∇λft(λt)
▷hyper-gradient
onto box constraints is a linear time operation on the number of hyperparameters. Hence
the update step in line 4 can also be done eﬃciently. Therefore, the complexity of OHL
applied on Multiple-Kernel Ridge Regression is
O
 T
m ·
 N2d + M log M

+ TNd

(19)
5
Theoretical Analysis with Local Regret
We now present the theoretical analysis under online learning framework for non-convex
functions. Algorithm 3 is an abstraction of Algorithm 2 for general non-convex function
ft, where the subscript t represents the the time-varying nature of the hyperparameter
optimization problem due to dependence on the rolling training set St. Note that since
Algorithm 2 is a speciﬁc implementation of Algorithm 3, the result extends to our hyper-
parameter learning problem. Online learning models the iterates {zt}T−1
t=0 and the functions
{ft}T−1
t=0 as a repeated game of T rounds. At each time t, the learner selects an iterate
zt ∈C, where C ⊂Rn is a compact convex set. After zt has been chosen, a cost function
ft : C →R is revealed to the learner and the learner suﬀers a loss ft(zt). We make the
following assumptions on the cost function ft. These assumptions are satisﬁed for kernel
method hyperparameter learning problem, i.e., when ft(·) = ℓ(yt, ˆyt(θ∗(·))). Let ∥·∥to
denote the Euclidean norm throughout the rest of the paper.
A1. supz∈C |ft(z)| ≤M for all t.
A2. ft is L-Lipschitz: |ft(z) −ft(v)| ≤L ∥z −v∥.
A3. ft has Q-Lipschitz gradient:
∥∇ft(z) −∇ft(v)∥2 ≤Q ∥z −v∥.
8

Algorithm 3 Online Projected Gradient Descent with Lazy Updates.
Input: Update window m, learning rate η, convex feasible set C, initial z0 ∈C, timesteps
T.
Output: Iterates zt, t = 0, . . . , T −1.
1: for t = 0 : T −1 do
2:
if
mod (t, m) = 0 then
3:
if t > 0 then
4:
zt = ΠC

zt−1 −η m−1gt

5:
gt = 0
6:
else
7:
zt = zt−1
8:
Submit zt
9:
Observe cost function ft : C →R
10:
Compute ∇zft(zt)
11:
gt+1 = gt + ∇zft(zt)
The performance of {zt}T−1
t=0 with respect to {ft}T−1
t=0 is studied by the measure of regret.
For convex cost functions, the regret is typically deﬁned by
T−1
X
t=0
ft(zt) −min
z∈C
T−1
X
t=0
ft(z),
which is the diﬀerence between the choices {z}T−1
t=0 and the best ﬁxed decision in hindsight
[15, 17]. However, when the cost functions are non-convex, searching for global minimum
is NP-hard in general even in the oﬄine case where a static f : C →R is known in
advance. Furthermore, due to the convex constraint z ∈C, a large number of gradient
evaluations are required to discover a stationary point [18]. Thus, for oﬄine problems, a
relaxed criterion is to minimize the (C, η)-projected gradient [29, 18]:
P(z, ∇f(z), η) := 1
η

z −ΠC
 z −η∇f(z)

.
(20)
It is easy to see that P(z, ∇f(z), η) mimics the role of gradient in a projected gradient
update:
zt+1 := ΠC
 zt −η∇f(zt)

= zt −ηP(zt, ∇f(zt), η).
(21)
Since C ⊂Rn is compact and convex, and assuming f satisﬁes A1-3, then there exists a
point z∗∈C such that P(z∗, ∇f(z∗), η) = 0 [18]. Therefore, as a natural extension from
the oﬄine criterion of vanishing projected gradients, the local regret for non-convex online
learning is deﬁned as follows.
Deﬁnition 1. The local regret [18] for loss functions {ft}T−1
t=0 and sequence of iterates
{zt}T−1
t=0 is
RT =
T−1
X
t=0
∥P(zt, ∇ft(zt), η)∥2 .
(22)
Deﬁnition 1 was ﬁrst used by Hazan et al.[18].
The following theorem shows the
optimal local regret is lower bounded by Ω(T).
Theorem 1. Deﬁne C = [−1, 1]. For any T ≥1 and η ≤1, there exists a distribution D
of loss functions {ft}T−1
t=0 satisfying assumption A1-3, such that for any online algorthms,
the local regret satisﬁes
ED
 RT

≥Ω(T).
(23)
9

Proof. See Theorem 2.7 in Hazan et al. [18].
A time-smoothed follow-the-leader (FTL) algorithm was proposed in [18], achieving the
optimal local regret bound O(T) for non-convex functions. This algorithm computes the
gradients {∇ft−i(zt)}m
i=1 and updates the iterate zt in every step. For the hyperparameter
learning problem considered in this paper, when z represents hyperparameter λ, a change
from λt to λt+1 will require model reﬁtting to update θ∗(·) from θ∗(λt) to θ∗(λt+1) in every
step. Besides, the historical gradients are not reused in the time-smoothed FTL algorithm
[18], since ∇ft−i(·) is re-evaluated at latest zt in every step. Hence, the method in [18]
becomes impractical for online hyperparameter learning given a computational budget. In
contrast, Algorithm 3 accumulates the gradients and produces an update every m-steps,
which dramatically reduces the amount of gradient computation and model-reﬁtting on
θ∗(λ).
As a price paid for the speed-up, we need the following additional assumption
characterizing the variation of cost functions to achieve optimal regret bounds. Let [r]
denote [0, r] ∩Z.
A4. Assume there is a constant w ∈Z independent of T, for all m ∈[w]\{0}, there is
a constant Vm ∈R+, such that for any t, the variation of gradients from the average
within m steps is bounded:
sup
z∈C
m−1
X
i=0
∥∇ft+i(z) −∇Ft,m(z)∥2 ≤Vm,
where
Ft,m(z) := 1
m
m
X
i=0
ft+i(z).
(24)
In the convex setting, variations deﬁned similar to (24) have also been studied [30, 31,
32]. However, the variation used in [30] deﬁnes m = T, whereas in (24) m is a constant
independent of T. Note that the quadratic variation in (24) also implies
sup
z∈C
m−1
X
i=0
∥∇ft+i(z) −∇Ft,m(z)∥≤
p
mVm.
(25)
Corollary 1. There exists a distribution D of loss functions satisfying assumption A1-3,
and A4, such that for any iterates {zt}T
t=1, the lower bound ED
 RT

≥Ω(T) still applies.
Proof. See Theorem 2.7 in Hazan et al. [18], construction of D in Theorem 1 also satisﬁes
assumption A4.
We need a few properties of projected gradients before establishing the regret bound
for algorithm 3.
Lemma 1. For any z ∈C, ∇f(z), and ∇g(z),
∥P(z, ∇f(z), η) −P(z, ∇g(z), η)∥2 ≤∥∇f(z) −∇g(z)∥2
(26)
Proof. An application of Lemma 2 in Ghadimi et al. [29].
Lemma 2. For any z ∈C and ∇f(z),
⟨∇f(z), P(z, ∇f(z), η)⟩≥∥P(z, ∇f(z), η)∥2
2 .
(27)
10

Proof. See Lemma 1 in [29] and Lemma 3.2 in [18].
We now bound the local regret RT of the whole sequence by the projected gradients of
its subsequence. Recall that Algorithm 3 updates the iterate zt every m steps. Without
loss of generality, assume s = T/m ∈Z. Let τj, j = 1, · · · , s denote the steps at which an
increment will occur, i.e., 0, · · · , T −1 can be represented as
τ0, τ0 + 1, · · · , τ0 + m −1, τ1, · · · , τs, τs + 1, · · · , τs + m −1.
Moreover, zτj = zτj+i for any j ∈[s] and i ∈[m −1].
Proposition 1. Let {τj}s
j=0 denote the steps at which an increment to the iterates will
occur in Algorithm 3. Suppose m in Algorithm 3 is chosen such that m ≤w in assumption
A4, the local regret satisﬁes
RT ≤
s
X
j=0
P(zτj, ∇Fτj,m(zτj), η)
2 +
+2
p
mVm
s
X
j=0
P(zτj, ∇Fτj,m(zτj), η)
 + (s + 1)Vm.
(28)
Proof. Recall Ft,m(z) = m−1 Pm−1
i=0 ft+i(z) from equation (24),
RT =
s
X
j=0
m−1
X
i=0
P(zτj+i, ∇fτj+i(zτj+i), η)
2
=
s
X
j=0
m−1
X
i=0
P(zτj+i, ∇fτj+i(zτj+i), η) −P(zτj, ∇Fτj,m(zτj), η) + P(zτj, ∇Fτj,m(zτj), η)

2
≤
s
X
j=0
m−1
X
i=0
P(zτj, ∇fτj+i(zτj), η) −P(zτj, ∇Fτj,m(zτj), η)
 +
P(zτj, ∇Fτj,m(zτj), η)

2
(29)
where the last line follows from zτj+i = zτj for i ∈[m −1] and triangle inequality. From
Lemma 1,
RT ≤
s
X
j=0
m−1
X
i=0
 ∇fτj+i(zτj) −∇Fτj,m(zτj)
 +
P(zτj, ∇Fτj,m(zτj), η)

2
≤
s
X
j=0
m−1
X
i=0
 ∇fτj+i(zτj) −∇Fτj,m(zτj)
2 +
P(zτj, ∇Fτj,m(zτj), η)
2
+ 2
∇fτj+i(zτj) −∇Fτj,m(zτj)
 P(zτj, ∇Fτj,m(zτj), η)


(30)
Applying assumption A4 and its implication (25) yields the claim.
Proposition 1 has a very intuitive meaning: when the variation of gradients of the loss
functions are bounded, the local regret is bounded by the projected gradients of loss at
the updating steps τj, j ∈[s]. We now state the main theorem on the asymptotic growth
of local regret in Algorithm 3.
Theorem 2. Let w be the constant in assumption A4, choosing the update period m ≤w,
learning rate η ∈(0, 2
Q) in Algorithm 3, the local regret satisﬁes
RT ≤O(T).
(31)
11

Proof. From assumption A3, ∇ft(z) is Q-Lipschitz, therefore ∇Ft,m(z) is also Q-Lipschitz
for all t, m. For any τj,
Fτj,m(zτj+1) ≤Fτj,m(zτj) −η

∇Fτj,m(zτj), P
 zτj, ∇Fτj,m(zτj), η

+ Qη2
2
P(zτj, ∇Fτj,m(zτj), η)
2
2
(32)
Applying Lemma 2,
Fτj,m(zτj+1) ≤Fτj,m(zτj) −η
P(zτj, ∇Fτj,m(zτj), η)
2
2 + Qη2
2
P(zτj, ∇Fτj,m(zτj), η)
2
2
(33)
Rearrange the terms,
P(zτj, ∇Fτj,m(zτj), η)
2
2 ≤
h
Fτj,m(zτj) −Fτj,m(zτj+1)
i
(η −Qη2
2 )
(34)
From assumption A2, Fτj,m(z) is L-Lipschitz.
Since C ⊂Rn is compact, let D :=
maxu,v∈C ∥u −v∥2 denote the diameter of C. We have
P(zτj, ∇Fτj,m(zτj), η)
2
2 ≤LD(η −Qη2
2 )−1
(35)
P(zτj, ∇Fτj,m(zτj), η)

2 ≤L1/2D1/2(η −Qη2
2 )−1/2
(36)
Summing up all j = 0 to s, plugging the bounds (35) and (36) into Proposition 1, and
from assumption A4,
RT ≤(T/m)LD(η −Qη2
2 )−1 + (2T
p
Vm/√m)L1/2D1/2(η −Qη2
2 )−1/2 + TVm/m
≤O(T).
(37)
Hence the regret bound is proved as claimed.
6
Experiments
We conduct experiments to evaluate the proposed online hyperparameter learning method
on synthetic and real data. On the real data, we perform 15-minutes-ahead traﬃc ﬂow
prediction on 13 randomly-selected sensors of diﬀerent types, distributed along the I-
210 [33] highway in California. The primary goals of the experiments are to
1. Test whether our proposed Online Hyperparameter Learning (OHL) method can
adaptively learn the hyperparameters, given a misspeciﬁed starting point.
2. Examine the computational eﬃciency of OHL against other model tuning methods.
3. Compare the traﬃc ﬂow prediction accuracy of OHL with other hyperparameter
tuning strategies with multiple kernel models.
12

Figure 1: Comparing OHL and FIXED on synthetic data. Left: squared exponential kernel, Right:
Combination of periodicity and linear kernel. OHL (red line) adaptively learns towards ground
truth, even when the initial hyperparameters are mis-speciﬁed.
6.1
Synthetic data
We generate a function with periodicity and linear trends using the following scheme:
y(t) = 1 + c1AR(20) + c2 sin(t/ω)
(38)
Here, c1 = c2 = 0.5, ω = 5, the autoregressive coeﬃcients are αi/2 ∥α∥with αi = i. We
compare the ground truth with one-step-ahead predictions produced by kernel methods
under ﬁxed hyperparameters and under our OHL algorithm. In both cases, the initial
hyperparameters are the same. We set m = 10 and η = 0.001 in Algorithm 2.
We test two kernels, a squared exponential kernel and the combination of a linear kernel
and a periodic kernel. The initial kernel scale is ν = 0.1 for the square exponential kernel.
When this choice of kernel scale is ﬁxed and used for prediction, the prediction produces
a zigzagging line, indicating the kernel scale is misspeciﬁed (green line in Figure 1). With
OHL, the predictions overlap with the ﬁxed hyperparameter case initially, given the same
starting hyperparameters, but aligns much closer to ground truth after 400 time-steps.
OHL is tested for multiple kernel learning using periodicity and linear kernel. The initial
hyperparameters are βprd = 1, period ω = 5 and scale ν = 10 for the periodicity kernel,
and βlin = 0 for the linear kernel. Hence, we expect in the ﬁxed hyperparameter case,
periodicity can be reproduced but linear trend will be hard to capture. As seen in the right
chart of Figure 1, there is an equidistant gap between the ground truth and the predictions
under ﬁxed hyperparameters. With OHL, the learner soon discovers the autoregressive
drift term and the predictions are almost perfectly aligned with ground truth (right chart
of Figure 1). The synthetic experiments demonstrate that OHL can perform adaptive
learning, which allows the users to initiate the system without the costly tuning process.
6.2
I-210 Traﬃc Data
6.2.1
Data and Setup
The I-210 highway is a vital route in the San Gabriel Valley region of the Los Angeles
metropolitan area [33].
We use sensor data from thirteen randomly selected locations
covering both mainline detectors and ramp detectors. Measurements from January 1 to
May 16 of 2017 are used. The raw data were binned using a 15 minute time window.
Hence, there are 96 observations per day. 15-minutes-ahead predictions are tested.
We compare the computational eﬃciency and prediction accuracy of hyperparameter
conﬁgurations tuned by OHL versus other algorithms on multiple kernel regression. A
13

0
50
100
150
200
OHL
HOAG
RDS
Total time (minutes)
20 min
138 min
157 min
0
50
100
150
200
OHL
HOAG
RDS
Hyperparameter tuning time (minutes)
3.2 min
120 min
139 min
Figure 2: Comparing total time and hyperparameter tuning time for OHL, HOAG, and RDS. Each
bar indicates the time taken for a detector, and the labels above bars indicate average time for 13
detectors. Detectors are ordered by total time using OHL.
linear combination of the ARD kernel and periodic kernel is used. Flow data from past 20
time-steps are used as autoregressive features. The feasibility sets for the hyperparameters
are [1.5∗10−2, 1.5∗10−6] for kernel scales, [96/2, 96∗7] for periodicity in the periodic kernel,
[0.03, 3] for regularization constant. The following hyperparameter tuning algorithms are
tested:
• The Online Hyperparameter Learning (OHL) proposed in this paper.
• Hyperparameter Optimization with Approximate Gradient (HOAG) [9], a state-of-
art gradient-based hyperparameter optimization method.
• Random Search (RDS) [6].
• Grid Search and ﬁxed hyperparameters (FIXED), a baseline.
Our experimental procedure aims to mimic the application scenario of a traﬃc prediction
engine. Note that both HOAG and RDS are oﬄine tuning strategies. Therefore, to apply
HOAG and RDS in a running environment, the traﬃc operators need to re-run these
tuning strategies periodically, as described in the rolling protocol (Algorithm 1) in the
introduction.
Hence, we set the hyperparameter optimization interval for HOAG and
RDS as n = 96 ∗7, which corresponds to weekly model tuning. At each hyperparameter
tuning step, the validation set Vt consists of observations in the past one month, and these
are given to the tuning algorithm. The tuning algorithms HOAG and RDS then perform
backtesting on the validation data Vt. HOAG uses hyperparameter gradient information to
guide the search for optimal hyperparameters on Vt [9]. RDS experiments with previously
selected conﬁguration and 50 additional random conﬁgurations on the validation dataset
Vt; the one oﬀering the best backtesting accuracy is used for the next period.
RDS is simple to implement and often produces good hyperparameter tuning results.
It is thus widely used in practice[6]. After the model hyperparameters are selected for the
weekly interval, the model is trained with a training set of |St| = 2880 time-steps.
OHL is an online method, taking the streaming data and adaptively updating the hy-
perparameters. Therefore, it does not require backtesting with the validation data Vt. The
hyperparameter learning rate is set to η = 10−4 in OHL, and m is set to 96. Therefore,
OHL computes the hyperparameter gradient online and makes an adaptive update every
96 steps. We keep the training frequency and amount of training data the same for all
methods.
14

6.2.2
Computational Eﬃciency Comparisons
Figure 2 gives the total time and the hyperparameter tuning time for the methods OHL,
HOAG, and RDS, and for the 13 detectors. Overall, OHL is nearly 7× faster than HOAG
and RDS. The speedup is mainly due to the faster hyperparameter tuning in OHL com-
pared to HOAG and RDS. The average hyperparameter tuning times for OHL and HOAG
are 3.2 and 120 minutes, respectively, indicating a tuning speedup of 37.5×. The time
spent on hyperparameter selection in OHL is a small percentage of the total running time,
and hence additional computational overheads are not introduced compared to predictions
under FIXED hyperparameters. The dramatic speedup in OHL over the slow execution of
HOAG and RDS is expected: although HOAG and RDS are both good hyperparameter
tuning algorithms for I.I.D. setting, the rolling procedure for time-series prediction ap-
plies the tuning algorithms periodically according to the operation schedule (weekly in
our experiments). Each run of the hyperparameter optimization algorithms requires back-
testing on the validation set, which also in turn involves multiple parameter ﬁtting steps
corresponding to diﬀerent hyperparameters.
Even though HOAG uses gradient-based
optimization, the algorithm searches for the optimal solution of hyperparameters on Vt
during each tuning stage. In comparison, OHL extracts the hyper-gradient knowledge
adaptively from the streaming data, and making a single projected-gradient update to the
hyperparameters before re-training the model. Further, the overall times (average across
13 detectors) for OHL and HOAG are 20 and 138 minutes, indicating a 6.9× speedup.
6.2.3
Prediction Accuracy
We use Root Mean Squared Error (RMSE) to measure the traﬃc ﬂow prediction accuracy
with diﬀerent hyperparameter optimization algorithms. In order to examine the variation
of RMSE over time, we also report the RMSE as a function of time-step t:
RMSE(t) :=
v
u
u
t1
t
t
X
τ=0
(yτ −ˆyτ)2,
Thus, RMSE(t) summarizes the average prediction error from the start to time-step t.
Due to space limit, we only show the evolution of RMSE(t) over the testing period for
six detectors in Figure 3. The X axis in Figure 3 is the prediction time-step and the Y
axis corresponds to the RMSE up to that time-step. OHL, HOAG, and RDS have lower
RMSE compared to the result with FIXED hyperparameters. This is an indication of
suboptimal hyperparameters selected and then ﬁxed by grid search. The prediction accu-
racy of OHL is similar to HOAG in most cases, and sometimes better. For example, on
detector 1 and detector 2 (ﬁrst two charts in the top row of Figure 3), RDS has the lowest
RMSE in the beginning phrase of testing, but OHL gradually improves and outperforms
others over time. Meanwhile, OHL is also computationally the cheapest among the three
hyperparameter tuning strategies. The similar prediction accuracy between HOAG and
OHL in most cases are expected (Table 1), since both use gradient-based optimization
on hyperparameters. However, HOAG periodically applies the gradient-based iterations
until convergence on the validation dataset, which makes the overall computation costly.
In contrast, OHL achieves the same result with adaptive updates. On some detectors,
RMSE of the model tuned by OHL algorithm is lower than HOAG - an oﬄine gradient-
based counter part. There are two reasons that can explain why an online HO algorithm
performs better than an oﬄine one. The optimal hyperparameters on validation set under-
perform in future data, suggesting that either there is overﬁtting by hyperparameters or the
15

0
1000
2000
3000
4000
5000
6000
7000
8000
Time stamps
70
75
80
85
90
95
RMSE
Detector 1
OHL
HOAG
RDS
FIXED
0
1000
2000
3000
4000
5000
6000
7000
Time stamps
80
85
90
95
100
105
110
115
RMSE
Detector 2
OHL
HOAG
RDS
FIXED
0
1000
2000
3000
4000
5000
6000
7000
Time stamps
75
80
85
90
95
100
RMSE
Detector 10
OHL
HOAG
RDS
FIXED
0
2000
4000
6000
8000
Time stamps
85
90
95
100
105
110
115
120
RMSE
Detector 7
OHL
HOAG
RDS
FIXED
0
1000
2000
3000
4000
5000
6000
Time stamps
75
80
85
90
95
100
105
RMSE
Detector 8
OHL
HOAG
RDS
FIXED
0
2000
4000
6000
8000
Time stamps
90
95
100
105
110
115
RMSE
Detector 9
OHL
HOAG
RDS
FIXED
0
2000
4000
6000
8000
Time stamps
85
90
95
100
105
110
RMSE
Detector 11
OHL
HOAG
RDS
FIXED
0
1000
2000
3000
4000
5000
6000
7000
Time stamps
135
140
145
150
155
160
165
170
175
RMSE
Detector 3
OHL
HOAG
RDS
FIXED
0
1000
2000
3000
4000
5000
6000
7000
Time stamps
40
50
60
70
80
90
100
RMSE
Detector 12
OHL
HOAG
RDS
FIXED
0
1000
2000
3000
4000
5000
6000
Time stamps
70
75
80
85
90
95
100
105
110
RMSE
Detector 6
OHL
HOAG
RDS
FIXED
0
2000
4000
6000
8000
Time stamps
100
105
110
115
120
125
130
135
RMSE
Detector 4
OHL
HOAG
RDS
FIXED
0
2000
4000
6000
8000
Time stamps
95
100
105
110
115
120
125
130
RMSE
Detector 5
OHL
HOAG
RDS
FIXED
0
1000
2000
3000
4000
5000
6000
7000
Time stamps
65
70
75
80
85
90
RMSE
Detector 13
OHL
HOAG
RDS
FIXED
Figure 3: Prediction RMSE comparison for traﬃc ﬂow data, OHL vs other hyperparameter tuning
algorithms.
data distribution is not stationary. On the contrary, OHL enables timely hyperparameter
updates adapted to the latest observations.
7
Conclusions
Motivated by the need for hyperparameter optimization in traﬃc time series prediction,
we proposed the OHL algorithm and applied it on Multiple-Kernel Ridge Regression. The
proposed OHL algorithm achieves optimal local regret. In the traﬃc ﬂow prediction exper-
16

Table 1: RMSE percentage improvement relative to FIXED hyperparameters. Larger values are
better. OHL achieves similar accuracy to HOAG, and nearly 7× faster (see Figure 2).
RMSE Improvements
4000 time-steps
ﬁnal
ID OHL HOAG RDS
OHL HOAG RDS
1 12%
11% 12%
13%
12% 13%
2 15%
15% 15%
15%
14% 11%
3
2%
2% -11%
3%
3% -8%
4
8%
7%
4%
9%
9%
4%
5
8%
6%
3%
9%
8%
1%
6 11%
11%
5%
11%
11%
4%
7
9%
10%
9%
11%
12% 11%
8
8%
9%
7%
8%
9%
6%
9
8%
9%
2%
9%
10%
3%
10
7%
9%
9%
8%
10%
9%
11
8%
8%
4%
8%
9%
5%
12
7%
7%
6%
7%
8%
5%
13 14%
15% 14%
15%
16% 14%
iments, OHL is nearly 7× faster than other rolling hyperparameter tuning methods, while
achieving similar prediction accuracy. In addition, we observed a consistent improvement
in accuracy compared to predictions produced with static hyperparameters.
There are possible extensions to this work: eﬃcient online hyper-gradient approximation
methods for a general class of models can expand the application scope of OHL. One
direction of improvement is combining our OHL algorithm with the reverse-mode and
forward-mode computation of hyper-gradients [13].
Acknowledgements
This work is supported by the US National Science Foundation grant ACI-1253881 and a
Penn State College of Engineering seed grant.
References
[1] H. Zhan, G. Gomes, X. S. Li, K. Madduri, and K. Wu. Eﬃcient online hyperparameter
learning for traﬃc ﬂow prediction. In Intelligent Transportation Systems (ITSC), 2018
IEEE 21th International Conference on, pages 1–6. IEEE, 2018.
[2] Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computa-
tion, 12(8):1889–1900, 2000.
[3] Matthias Seeger. Cross-validation optimization for large scale hierarchical classiﬁca-
tion kernel methods. In B. Sch¨olkopf, J. C. Platt, and T. Hoﬀman, editors, Advances
in Neural Information Processing Systems 19, pages 1233–1240. MIT Press, 2007.
[4] Chuan-sheng Foo, Chuong B Do, and Andrew Y Ng.
Eﬃcient multiple hyper-
parameter learning for log-linear models. In Proc. NIPS, 2008.
17

[5] James S Bergstra, R´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for
hyper-parameter optimization. In Advances in neural information processing systems,
pages 2546–2554, 2011.
[6] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimiza-
tion. JMLR, 13(Feb):281–305, 2012.
[7] Dougal Maclaurin, David Duvenaud, and Ryan Adams.
Gradient-based hyper-
parameter optimization through reversible learning. In Proc. ICML, 2015.
[8] Kirthevasan Kandasamy, JeﬀSchneider, and Barnab´as P´oczos.
High dimensional
bayesian optimisation and bandits via additive models. In International Conference
on Machine Learning, pages 295–304, 2015.
[9] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In Proc.
ICML, 2016.
[10] Jelena Luketina, Mathias Berglund, Klaus Greﬀ, and Tapani Raiko. Scalable gradient-
based tuning of continuous regularization hyperparameters. In Maria Florina Balcan
and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference
on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages
2952–2960, New York, New York, USA, 20–22 Jun 2016. PMLR.
[11] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast
bayesian optimization of machine learning hyperparameters on large datasets. arXiv
preprint arXiv:1605.07079, 2016.
[12] Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identiﬁcation and
hyperparameter optimization. In Artiﬁcial Intelligence and Statistics, pages 240–248,
2016.
[13] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil.
For-
ward and reverse gradient-based hyperparameter optimization. In Doina Precup and
Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine
Learning, volume 70 of Proceedings of Machine Learning Research, pages 1165–1173,
International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
[14] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Tal-
walkar. Hyperband: A novel bandit-based approach to hyperparameter optimization.
The Journal of Machine Learning Research, 18(1):6765–6816, 2017.
[15] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient
ascent. In Proc. ICML, 2003.
[16] Nicolo Cesa-Bianchi and G´abor Lugosi. Prediction, learning, and games. Cambridge
university press, 2006.
[17] Elad Hazan. The convex optimization approach to regret minimization. In Suvrit
Sra, Sebastian Nowozin, and Stephen J. Wright, editors, Optimization for Machine
Learning, chapter 10, pages 287–304. MIT Press, 2012.
[18] Elad Hazan, Karan Singh, and Cyril Zhang. Eﬃcient regret minimization in non-
convex games. In Proc. ICML, 2017.
[19] Ralph Tyrell Rockafellar. Convex analysis. Princeton university press, 2015.
18

[20] J. Snoek, H. Larochelle, and R.P. Adams. Practical Bayesian optimization of machine
learning algorithms. In Proc. NIPS, 2012.
[21] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical
Learning. Springer-Verlag New York, 2009.
[22] Bernhard Sch¨olkopf, Ralf Herbrich, and Alex J Smola. A generalized representer the-
orem. In Proc. International Conference on Computational Learning Theory (COLT),
2001.
[23] B Sch¨olkopf. Learning with kernels: Support vector machines, regularization, opti-
mization, and beyond. 2002.
[24] David J. C. MacKay. Introduction to gaussian processes. http://www.inference.
org.uk/mackay/gpB.pdf, last accessed Oct 2017.
[25] David H Wolpert. Stacked generalization. Neural networks, 5(2):241–259, 1992.
[26] Zhi-Hua Zhou.
Ensemble methods:
foundations and algorithms.
Chapman and
Hall/CRC, 2012.
[27] H. Zhan, G. Gomes, X. S. Li, K. Madduri, A. Sim, and K. Wu. Consensus ensemble
system for traﬃc ﬂow prediction. IEEE Transactions on Intelligent Transportation
Systems, pages 1–12, 2018.
[28] Weiran Wang and Miguel A Carreira-Perpin´an. Projection onto the probability sim-
plex: An eﬃcient algorithm with a simple proof, and an application. arXiv preprint
arXiv:1309.1541, 2013.
[29] Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approx-
imation methods for nonconvex stochastic composite optimization.
Mathematical
Programming, 155(1-2):267–305, 2016.
[30] Elad Hazan. Extracting certainty from uncertainty: Regret bounded by variation in
costs. In Proc. COLT, 2008.
[31] Elad Hazan and Satyen Kale. On stochastic and worst-case models for investing. In
Proc. NIPS, 2009.
[32] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu,
Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In Proc.
COLT, 2012.
[33] Connected Corridors. I-210 pilot ICM project. http://ccdocs.berkeley.edu, last
accessed Oct 2017.
19
