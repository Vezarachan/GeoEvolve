arXiv:1905.08737v2  [stat.ME]  20 Sep 2019
On the marginal likelihood and cross-validation
E. Fong & C. C. Holmes
Department of Statistics, University of Oxford, OX1 3LB
edwin.fong@stats.ox.ac.uk & cholmes@stats.ox.ac.uk
September 24, 2019
Abstract
In Bayesian statistics, the marginal likelihood, also known as the evidence, is used to evaluate
model ﬁt as it quantiﬁes the joint probability of the data under the prior. In contrast, non-Bayesian
models are typically compared using cross-validation on held-out data, either through k-fold par-
titioning or leave-p-out subsampling. We show that the marginal likelihood is formally equivalent
to exhaustive leave-p-out cross-validation averaged over all values of p and all held-out test sets
when using the log posterior predictive probability as the scoring rule. Moreover, the log pos-
terior predictive is the only coherent scoring rule under data exchangeability. This offers new
insight into the marginal likelihood and cross-validation and highlights the potential sensitivity
of the marginal likelihood to the choice of the prior. We suggest an alternative approach using
cumulative cross-validation following a preparatory training phase. Our work has connections to
prequential analysis and intrinsic Bayes factors but is motivated through a different course.
1
Introduction
Probabilistic model evaluation and selection is an important task in statistics and machine learning,
particularly when multiple models are under initial consideration. In the non-Bayesian literature,
models are typically compared using out-of-sample performance criteria such as cross-validation
(Geisser and Eddy, 1979; Shao, 1993; Vehtari and Lampinen, 2002), or predictive information
(Watanabe, 2010). Computing the leave-p-out cross-validation score requires n-choose-p test set
evaluations for n data points, which in most cases is computationally unviable and hence approxi-
mations such as k-fold cross-validation are often used instead (Geisser, 1975). A survey is provided
by Arlot and Celisse (2010), and a Bayesian perspective on cross-validation by Vehtari and Ojanen
(2012); Gelman et al. (2014).
In Bayesian statistics, the marginal likelihood or model evidence is the natural measure of model
ﬁt. For a model M with likelihood function or sampling distribution {fθ(y) : θ ∈Θ} parameterized
by θ, a prior π(θ), and observations y1:n ∈Yn, the marginal likelihood or the prior predictive is
deﬁned as
pM(y1:n) =
Z
fθ(y1:n) dπ(θ) .
(1)
The marginal likelihood can be used to calculate the posterior probability of the model given the data,
p(M | y1:n) ∝pM(y1:n) p(M), as it is the probability of the data being generated under the prior
when the model is correctly speciﬁed (Robert, 2007, Chapter 7). The ratio of marginal likelihoods
between models is known as the Bayes factor that quantiﬁes the prior to posterior odds on observing
1

the data. The marginal likelihood can be difﬁcult to compute if the likelihood is peaked with respect
to the prior, although Monte Carlo solutions exist; see Robert and Wraith (2009) for a survey. Under
vague priors, the marginal likelihood may also be highly sensitive to the prior dispersion even if the
posterior is not; a well known example is Lindley’s paradox (Lindley, 1957; O’Hagan and Forster,
2004; Robert, 2014). As a result, its approximations such as the Bayesian information criterion
(Schwarz, 1978) or the deviance information criterion (Spiegelhalter et al., 2002) are widely used,
see also Gelman et al. (2014).
For our work, it is useful to note from the property of probability distributions that the log
marginal likelihood can be written as the sum of log conditionals,
log pM(y1:n) =
n
X
i=1
log pM(yi | y1:i−1)
(2)
where pM(yi | y1:i−1) =
R
fθ(yi) dπ(θ | y1:i−1) is the posterior predictive for i > 1, pM(y1 | y1:0)
=
R
fθ(y1) dπ(θ) , and this representation is true for any permutation of the data indices.
While Bayesian inference formally assumes that the model space captures the truth, in the
model misspeciﬁed or so called M-open scenario (Bernardo and Smith, 2009, Chapter 6) the log
marginal likelihood can be simply interpreted as a predictive sequential, or prequential (Dawid,
1984), scoring rule of the form S(y1:n) = P
i s(yi | y1:i−1) with score function s(yi | y1:i−1) =
log pM(yi | y1:i−1). This interpretation of the log marginal likelihood as a predictive score (Kass
and Raftery, 1995; Gneiting and Raftery, 2007; Bernardo and Smith, 2009, Chapter 6) has resulted
in alternative scoring functions for Bayesian model selection (Dawid and Musio, 2014, 2015; Wat-
son and Holmes, 2016; Shao et al., 2019), and provides insight into the relationship between the
marginal likelihood and posterior predictive methods (Vehtari and Ojanen, 2012). Key et al. (1999)
considered cross-validation from an M-open perspective and introduced a mixture utility for model
selection that trades off ﬁdelity to data with predictive power.
2
Uniqueness of the marginal likelihood under coherent scoring
To begin, we prove that under an assumption of data exchangeability, the log posterior predictive is
the only prequential scoring rule that guarantees coherent model evaluation. The coherence prop-
erty under exchangeability, where the indices of the data points carry no information, refers to the
principle that identical models on seeing the same data should be scored equally irrespective of data
ordering.
In demonstrating the uniqueness of the log posterior predictive, it is useful to introduce the notion
of a general Bayesian model (Bissiri et al., 2016), which is a framework for Bayesian updating
without the requirement of a true model. Deﬁne a parameter of interest by
θ0 = arg min
θ
Z
l(θ, y)dF0(y)
(3)
where F0(y) is the unknown true sampling distribution giving rise to the data, and l : Θ × Y →
[0, ∞) is a loss function linking an observation y to the parameter θ. Bissiri et al. (2016) argue
that after observing y1:n, a coherent update of beliefs about θ0 from a prior πG(θ) to the posterior
πG(θ | y1:n) exists and must take on the form
πG(θ | y1:n) ∝exp {−wl(θ, y1:n)} πG(θ)
(4)
where l(θ, y1:n) = P
i l(θ, yi) is an additive loss function and w > 0 is a loss scale parameter; see
Holmes and Walker (2017); Lyddon et al. (2019) on the selection of w. For w = 1 and l(θ, y) =
2

−log fθ(y), we obtain traditional Bayesian updating without assuming the model fθ(y) is true for
some value of θ. From (3), M-open Bayesian inference is simply targeting the value of θ that
minimizes the Kullback-Leibler divergence between dF0(y) and fθ(y). The form (4) is uniquely
implied by the assumptions in Theorem 1 of Bissiri et al. (2016), and we now focus on the coherence
property of the update rule. An update function ψ{l(θ, y), πG(θ)} = πG(θ | y) is coherent if, for
some inputs y1:2, it satisﬁes
ψ[l(θ, y2), ψ{l(θ, y1), πG(θ)}] = ψ{l(θ, y1) + l(θ, y2), πG(θ)}.
This coherence condition is natural under an assumption of exchangeability as we expect posterior
inferences about θ0 to be unchanged whether we observe y1:2 in any order or all at once, as it is in
traditional Bayesian updating.
We now extend this coherence condition to general Bayesian model choice, where the goal is
to evaluate the ﬁt of the observed data under the general Bayesian model class MG = {l(θ, y) :
θ ∈Θ} with a prior πG(θ). We treat w as a parameter outside of the model speciﬁcation, as there are
principled methods to select it from the model, prior and data. We deﬁne the log posterior predictive
score as
sG(˜y | y1:n) = log
Z
g{l(θ, ˜y)}dπG(θ | y1:n)
where g : [0, ∞) →[0, ∞) is a continuous monotonically decreasing scoring function that trans-
forms l(θ, y) into a predictive score for a test point ˜y. We deﬁne the cumulative prequential log score
as
SG(y1:n) =
n
X
i=1
sG(yi | y1:i−1)
where sG(y1 | y1:0) = log
R
g{l(θ, y1)}dπG(θ). The cumulative prequential log score sums the
log posterior predictive score of each consecutive data point in a prequential manner, where a large
score indicates that the model is predicting well. An intuitive choice for the scoring function might
be the negative loss g(l) = −l, but we will see that this violates coherency, as deﬁned below.
Deﬁnition 1. The model scoring function g(l) is coherent if it satisﬁes
n
X
i=1
sG(yi | y1:i−1) = log
Z
g{l(θ, y1:n)}dπG(θ)
(5)
for all Θ, π(θ) and n > 0, such that SG(y1:n) is invariant to the ordering or partitioning of the
observations.
We now present our main result on the uniqueness of the choice of g.
Proposition 1. If the model scoring function g : [0, ∞) →[0, ∞) is continuous, monotonically
decreasing and coherent, then the unique choice of scoring rule g(l) is
g(l) = exp(−wl)
where w is the loss-scale in the general Bayesian posterior.
Proof. The proof is given in the Supplementary Material.
This holds irrespective of whether the model is true or not. More importantly for us is the
corollary below.
3

Corollary 1. The marginal likelihood is the unique coherent marginal score for Bayesian inference.
Proof. Let w = 1 and l(θ, y) = −log fθ(y), and hence g{l(θ, y)} = fθ(y).
The marginal likelihood arises naturally as the unique prequential scoring rule under coherent
belief updating in the Bayesian framework. The coherence of the marginal likelihood implies an
invariance to the permutation of the observations y1:n under exchangeability, including independent
and identically distributed data, a property that is not shared by other prequential scoring rules, such
as Dawid and Musio (2014); Gr¨unwald and van Ommen (2017); Shao et al. (2019).
3
The marginal likelihood and cross-validation
3.1
Equivalence of the marginal likelihood and cumulative
cross-validation
The leave-p-out cross-validation score is deﬁned as
SCV (y1:n; p) =
1
 n
p

(
n
p)
X
t=1
1
p
p
X
j=1
s

˜y(t)
j
| y(t)
1:n−p

(6)
where ˜y(t)
1:p denotes the tth of n-choose-p possible held-out test sets, with y(t)
1:n−p the corresponding
training set, such that y1:n =

˜y(t), y(t)	
, and SCV records the average predictive score per datum.
Although leave-one-out cross-validation is a popular choice, it was shown in Shao (1993) that it
is asymptotically inconsistent for a linear model selection problem, and requires (p/n) →1 as
n →∞for consistency. We will not go into further detail here but instead refer the reader to Arlot
and Celisse (2010). Selecting a larger p has the interpretation of penalizing complexity (Vehtari and
Ojanen, 2012), as complex models will tend to over-ﬁt to a small training set. However, the number
of test set evaluations grows rapidly with p and hence k-fold cross-validation is often adopted for
computational convenience.
From a Bayesian perspective it is natural to consider the log posterior predictive as the scoring
function, s(˜y | y) = log
R
fθ(˜y)dπ(θ | y), particularly as we have now shown that it is the only
coherent scoring mechanism, which leads us to the following result.
Proposition 2. The Bayesian marginal likelihood is equivalent to the cumulative leave-p-out cross-
validation score using the log posterior predictive as the scoring rule, such that
log pM(y1:n) =
n
X
p=1
SCV (y1:n; p)
(7)
with s(˜yj | y1:n−p) = log pM(˜yj | y1:n−p) = log
R
fθ(˜yj) dπ(θ | y1:n−p).
Proof. This follows from the invariance of the marginal likelihood under arbitrary permutation of the
sequence y1:n in (2). We provide a proof and an alternative proof by induction in the Supplementary
Material.
The Bayesian marginal likelihood is simply n times the average leave-p-out cross-validation
score, n × (1/n) Pn
p=1 SCV (y1:n; p), where the scaling by n is due to (6) being a per datum score.
Bayesian models are evaluated through out-of-sample predictions on all (2n −1) possible held-
out test sets whereas cross-validation with ﬁxed p only captures a snapshot of model performance.
Evaluating the predictive performance on (2n −1) test sets would appear intractable for most appli-
cations, but we see through (7) and (1) that it is computable as a single integral.
4

3.2
Sensitivity to the prior and preparatory training
The representation of the marginal likelihood as a cumulative cross-validation score (7) provides
insight into the sensitivity to the prior. The last term in the right hand side of (7) involves no training
data, SCV (y1:n; n) = (1/n) Pn
i=1 log
R
fθ(yi) dπ(θ), which scores the model entirely on how well
the analyst is able to specify the prior. In many situations, the analyst may not want this term to
contribute to model evaluation. Moreover, there is tension between any desire to specify vague
priors to safeguard their inﬂuence and the fact that diffuse priors can lead to an arbitrarily large and
negative model score for real valued parameters from (7). It may seem inappropriate to penalize
a model based on the subjective ability to specify the prior, or to compare models using a score
that includes contributions from predictions made using only a handful of training points even with
informative priors. For example, we see that 10% of terms contributing to the marginal likelihood
come from out-of-sample predictions using, on average, less than 5% of available training data. This
is related to the start-up problem in prequential analysis (Dawid, 1992).
A natural and obvious solution is to begin evaluating the model performance after a preparatory
phase, for example using 10% or 50% of the data as preparatory training prior to testing. This leads
to a Bayesian cumulative leave-P-out cross-validation score deﬁned as
SCCV (y1:n; P) =
P
X
p=1
SCV (y1:n; p)
(8)
with a preparatory cross-validation score SP CV (y1:n; P) = Pn
p=P +1 SCV (y1:n; p), for 1 ≤P < n.
We suggest setting P to leave out 0.9n, 0.5n or max(0.9n, n −10d), where d is the total number
of model parameters, as reasonable default choices, but clearly this is situation speciﬁc. One may
be interested in reporting both SCCV and SP CV , as the latter can be regarded as an evaluation of
the prior, but we suggest that only SCCV is used for model evaluation from the arguments above.
Although full coherency is now lost, we still have coherency conditioned on a preparatory training
set, where permutation of the data within the training and test sets does not affect the score, and so
we can write (8) as
SCCV (y1:n; P) =
1
 n
P

(
n
P)
X
t=1
log pM

˜y(t)
1:P | y(t)
1:n−P

.
(9)
This equivalence is derived in the Supplementary Material in a similar fashion to Proposition 2. This
has precisely the form of the the log geometric intrinsic Bayes factor of Berger and Pericchi (1996)
but motivated by a different route. The intrinsic Bayes factor was developed in an objective Bayesian
setting (Berger and Pericchi, 2001), where improper priors cause indeterminacies in the evaluation
of the marginal likelihood. The intrinsic Bayes factor remedies this with a partition of the data into
y1:l, yl+1:n, where y1:l is the minimum training sample used to convert an improper prior π(θ) into
a proper prior π(θ | y1:l). In contrast, we set n −P to provide preparatory training and π(θ) can be
subjective. Moreover, in modern applications we often have d ≫n where intrinsic Bayes factors
cannot be applied in their original form.
5

We can approximate (9) through Monte Carlo where the training data sets y(t)
1:n−P are drawn
uniformly at random, and for non-conjugate models the inner term must also be estimated, for
example through
ˆSCCV (y1:n; P) = 1
T
T
X
t=1
log
(
1
B
B
X
b=1
fθ(t)
b

˜y(t)
1:P
)
(10)
where samples θ(t)
b
∼π

θ | y(t)
1:n−P

are obtained via T Markov chain Monte Carlo samplers. If
we assume that the number of samples B per chain is sufﬁciently large, then the variance of the
estimate ˆSCCV is approximately of the form τ 2/T . However, ﬁtting T models may be costly, but
we can run the chains in parallel. To avoid the need for T Markov chain Monte Carlo chains in (10),
we can instead take advantage of the fact that the partial posteriors for different training sets will
be similar, and utilize importance sampling (Bhattacharya and Haslett, 2007; Vehtari et al., 2017) or
sequential Monte Carlo (Bornn et al., 2010) to estimate the posterior predictives for computational
savings. We provide further details on efﬁcient computation of (10) in the Supplementary Material.
4
Illustration for the normal linear model
We illustrate the use of Bayesian cumulative cross-validation in a polynomial regression example,
where the rth polynomial model is deﬁned as
fθ(y | x, r) = N{y; θ
Tφr(x), σ2},
φr(x) =
1
x
. . .
xr−1
xrT .
We observe the data {y1:n, x1:n}, and we place a ﬁxed vague prior on the intercept term, θ0 ∼
N(θ0; 0, 1002), and θd ∼N(θd; 0, s2) for d ∈{1, . . ., r} on the remaining coefﬁcients. In our
example, we have n = 100 and the true model is r = 1, θ =
1
0.5T with known σ2 = 1. For
our prior, we vary the value of s2 ∈

10−1, 100, 104	
to investigate the impact of the prior tails. For
each prior setting, we calculate log pM(y1:n) and SCCV (y1:n; P) for models r ∈{0, 1, 2}. In this
example, log pM(y1:n) is tractable, whereas SCCV requires a Monte Carlo average over tractable
log posterior predictives. We report the mean over 10 runs of estimating SCCV with T = 106
random training/test splits. We calculate the Monte Carlo standard error over the 10 runs and report
the maximum for each setting of P.
The results are shown in Table 1, where ˆSCCV is normalized to the same scale as log pr(y1:n).
Under the strong prior s2 = 10−1 and the moderate prior s2 = 100, the marginal likelihood correctly
identiﬁes the true model, but when we increase s2 to 104 it heavily over-penalizes the more complex
models and prefers r = 0. In fact, the magnitude of the marginal likelihood and the discrepancy just
described can be made arbitrarily large by simply increasing s2, which should be guarded against
when a modeller has weak prior beliefs. This issue is not observed with ˆSCCV for the values of
P we consider. The vague prior does not impede the ability of ˆSCCV to correctly identify the true
model r = 1 and the scores are stable within each column of P.
In the Supplementary Material, we present graphical tools for exploring the cumulative cross-
validation and the effect of the choice of P on SCCV . We provide an additional example using
probit regression on the Pima Indian data set.
6

Table 1: Log marginal likelihoods and cumulative cross-validation scores for normal linear model
s2
MODEL
log pr(y1:n)
ˆSCCV (y1:n; P) × n/P
r
P = 0.9n
P = 0.5n
P = 0.1n
10−1
0
-158.82
-153.80
-153.21
-153.06
1
-155.57
-150.39
-149.55
-149.27
2
-156.12
-150.94
-149.81
-149.38
100
0
-158.82
-153.80
-153.21
-153.06
1
-156.26
-150.77
-149.66
-149.34
2
-157.80
-151.90
-150.04
-149.50
104
0
-158.82
-153.80
-153.21
-153.06
1
-160.81
-150.91
-149.68
-149.35
2
-166.93
-152.30
-150.08
-149.53
MAXIMUM STANDARD ERROR
0.002
0.008
0.023
5
Discussion
We have shown that for coherence, the unique scoring rule for Bayesian model evaluation in either
M-open or M-closed is provided by the log posterior predictive probability, and that the marginal
likelihood is equivalent to a cumulative cross-validation score over all training-test data partitions.
The coherence ﬂows from the fact that the scoring rule and the Bayesian update both use the same
information, namely the likelihood function, which is appropriate as the alternative would be to
learn and score under different criteria. If we are interested in an alternative loss function to the log
likelihood, we advocate a general Bayesian update (Bissiri et al., 2016; Lyddon et al., 2019) that
targets the parameters minimising the expected loss, with models evaluated using the corresponding
coherent cumulative cross-validation score.
Acknowledgement
The authors thank Lucian Chan, George Nicholson, the editor, an associate editor and two referees
for their helpful comments. Fong was funded by The Alan Turing Institute. Holmes was supported
by The Alan Turing Institute, the Health Data Research, U.K., the Li Ka Shing Foundation, the
Medical Research Council, and the U.K. Engineering and Physical Sciences Research Council.
References
Arlot, S. and Celisse, A. (2010). A survey of cross-validation procedures for model selection.
Statistics Surveys, 4:40–79.
Berger, J. O. and Pericchi, L. R. (1996). The intrinsic Bayes factor for model selection and predic-
tion. Journal of the American Statistical Association, 91(433):109–122.
Berger, J. O. and Pericchi, L. R. (2001). Objective Bayesian Methods for Model Selection: Introduc-
tion and Comparison, volume 38 of Lecture Notes–Monograph Series, pages 135–207. Institute
of Mathematical Statistics, Beachwood, OH.
7

Bernardo, J. and Smith, A. (2009). Bayesian Theory. Wiley Series in Probability and Statistics.
Wiley.
Bhattacharya, S. and Haslett, J. (2007). Importance re-sampling MCMC for cross-validation in
inverse problems. Bayesian Analysis, 2(2):385–407.
Bissiri, P. G., Holmes, C. C., and Walker, S. G. (2016). A general framework for updating be-
lief distributions. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
78(5):1103–1130.
Bornn, L., Doucet, A., and Gottardo, R. (2010). An efﬁcient computational approach for prior
sensitivity analysis and cross-validation. Canadian Journal of Statistics, 38(1):47–64.
Dawid, A. P. (1984). Present Position and Potential Developments: Some Personal Views: Statistical
Theory: The Prequential Approach. Journal of the Royal Statistical Society. Series A (General),
147(2):278.
Dawid, A. P. (1992). Prequential analysis, stochastic complexity and Bayesian inference. Bayesian
Statistics, 4:109–125.
Dawid, A. P. and Musio, M. (2014). Theory and applications of proper scoring rules. METRON,
72(2):169–183.
Dawid, A. P. and Musio, M. (2015).
Bayesian model selection based on proper scoring rules.
Bayesian Analysis, 10(2):479–499.
Geisser, S. (1975). The predictive sample reuse method with applications. Journal of the American
Statistical Association, 70(350):320–328.
Geisser, S. and Eddy, W. (1979). A predictive approach to model selection. Journal of the American
Statistical Association, 74:153–160.
Gelman, A., Hwang, J., and Vehtari, A. (2014). Understanding predictive information criteria for
Bayesian models. Statistics and Computing, 24:997–1016.
Gneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation.
Journal of the American Statistical Association, 102(477):359–378.
Gr¨unwald, P. and van Ommen, T. (2017). Inconsistency of Bayesian inference for misspeciﬁed
linear models, and a proposal for repairing it. Bayesian Analysis, 12(4):1069–1103.
Holmes, C. C. and Walker, S. G. (2017). Assigning a value to a power likelihood in a general
Bayesian model. Biometrika, 104(2):497–503.
Kass, R. E. and Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association,
90(430):773–795.
Key, J., Pericchi, L., and Smith, A. (1999). Bayesian model choice: What and why? (with discus-
sion). Bayesian Statistics, 5.
Lindley, D. V. (1957). A statistical paradox. Biometrika, 44(1-2):187–192.
Lyddon, S. P., Holmes, C. C., and Walker, S. G. (2019). General Bayesian updating and the loss-
likelihood bootstrap. Biometrika, 106(2):465–478.
8

Marin, J.-M. and Robert, C. P. (2010). Importance sampling methods for Bayesian discrimination
between embedded models. Frontiers of Statistical Decision Making and Bayesian Analysis,
pages 513–527.
O’Hagan, A. and Forster, J. J. (2004). Kendall’s Advanced Theory of Statistics, volume 2B: Bayesian
Inference, volume 2. Arnold.
Rischard, M., Jacob, P. E., and Pillai, N. (2018). Unbiased estimation of log normalizing constants
with applications to Bayesian cross-validation. arXiv preprint arXiv:1810.01382.
Robert, C. P. (2007). The Bayesian Choice: From Decision-Theoretic Foundations to Computational
Implementation. Springer, 2nd edition.
Robert, C. P. (2014). On the Jeffreys-Lindley paradox. Philosophy of Science, 81(2):216–232.
Robert, C. P. and Wraith, D. (2009). Computational methods for Bayesian model choice. In AIP
conference proceedings, volume 1193, pages 251–262. AIP.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6(2):461–464.
Shao, J. (1993). Linear model selection by cross-validation. Journal of the American Statistical
Association, 88(422):486–494.
Shao, S., Jacob, P. E., Ding, J., and Tarokh, V. (2019).
Bayesian model comparison with the
Hyv¨arinen score: Computation and consistency. Journal of the American Statistical Association,
pages 1–24.
Spiegelhalter, D. J., Best, N. G., Carlin, B. P., and Van Der Linde, A. (2002). Bayesian measures of
model complexity and ﬁt. Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 64(4):583–639.
Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-
out cross-validation and WAIC. Statistics and Computing, 27(5):1413–1432.
Vehtari, A. and Lampinen, J. (2002). Bayesian model assessment and comparison using cross-
validation predictive densities. Neural Computation, 14(10):2339–2468.
Vehtari, A. and Ojanen, J. (2012). A survey of Bayesian predictive methods for model assessment,
selection and comparison. Statistics Surveys, 6:142–228.
Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and widely applicable in-
formation criterion in singular learning theory. Journal of Machine Learning Research, 11:3571–
3594.
Watson, J. and Holmes, C. C. (2016). Approximate models and robust decisions. Statistical Science,
31(4):465–489.
9

A
Supplementary Material
A.1
Proof of Proposition 1
Proof. We look at the case where Θ = {0, 1}, so the prior πG(θ) is parametrized by p ∈[0, 1] with
πG(θ = 0) = p. We let n = 2, denoting the observables as y1, y2. We further denote l(0, y1) = l0
and l(1, y1) = l1, and likewise l(0, y2) = h0 and l(1, y2) = h1. We write p1 as the updated
πG(θ = 0 | y1) obtained from the general Bayesian update (4). The function g(l) must then satisfy
{g(l0)p + g(l1)(1 −p)} {g(h0)p1 + g(h1)(1 −p1)}
= {g(l0 + h0)p + g(l1 + h1)(1 −p)}
(A.1.1)
for all 0 ≤p ≤1 and for all l0, l1, h0, h1 ∈[0, ∞). If we let p = 1, then p1 = 1, so this simpliﬁes
to
g(l0)g(h0) = g(l0 + h0).
As g is continuous and monotonically decreasing, to satisfy (5) it must take on the form
g(l) = exp(−λl)
(A.1.2)
for λ ≥0. We now explicitly write out the form of p1
p1 =
exp(−wl0)p
exp(−wl0)p + exp(−wl1)(1 −p) = exp(−wl0)p
Z1
.
(A.1.3)
If we plug (A.1.2), (A.1.3) into (A.1.1), we obtain
{exp(−λl0)p + exp(−λl1)(1 −p)} {exp(−λh0) exp(−wl0)p + exp(−λh1) exp(−wl1)(1 −p)}
= Z1 [exp{−λ(l0 + h0)}p + exp{−λ(l1 + h1)}(1 −p)] .
Expanding, cancelling terms, and simplifying we obtain
exp(−λl1 −wl0){exp(−λh0) −exp(−λh1)}
= exp(−λl0 −wl1){exp(−λh0) −exp(−λh1)}
and so we must have λ = 0 or λ = w, where only the latter solution is non-trivial. We have thus
shown that for n = 2, |Θ| = 2, the unique non-trivial solution to (5) is
g(l) = exp(−wl).
(A.1.4)
The remainder of the proof involves showing that this choice of g satisﬁes (5) for all n > 0 and all
Θ and π(θ). Subbing (A.1.4) into (5), we obtain
n
Y
i=1
exp {sG(yi | y1:i−1)} =
n
Y
i=1
Z
exp{−wl(θ, yi)}
exp {−wl(θ, y1:i−1)} dπG(θ)
R
exp {−wl(θ′, y1:i−1)} dπG(θ′)
=
n
Y
i=1
R
exp {−wl(θ, y1:i)} dπG(θ)
R
exp {−wl(θ′, y1:i−1)} dπG(θ′)
=
Z
exp {−wl(θ, y1:n)} dπG(θ)
where for convenience we denote l(θ, y1:0) = 0.
10

A.2
Proof of Proposition 2
Proof. Consider the (n! × n) matrix Z with elements (Z)ti = log pM(y(t)
i
| y(t)
1:i−1), such that
the tth row of Z records the prequential sequence of log posterior predictives under the tth of n!
permutations of y1:n. By the property of conditional probabilities, we have that the row sums of Z
are equal, P
i(Z)ti = P
i(Z)t′i for all t, t′, and hence
log pM(y1:n) = 1
n!
n!
X
t=1
n
X
i=1
(Z)ti =
n
X
i=1
1
n!
n!
X
t=1
(Z)ti.
Within each column of Z, the values (Z)ti are invariant to the permutation of y1:i−1 in the
preceding i −1 columns under exchangeability. There are thus n-choose-(i −1) distinct training
sets and n−i+ 1 choices for yi given the training set. For each column i ∈{1, . . . , n}, we can then
write
1
n!
n!
X
t=1
(Z)ti =
1
  n
i−1

(
n
i−1)
X
t=1
1
n −i + 1
n−i+1
X
j=1
s

˜y(t)
j
| y(t)
1:i−1

= SCV (y1:n; n −i + 1)
where s

˜y(t)
j
| y(t)
1:i−1

= log pM

˜y(t)
j
| y(t)
1:i−1

. We have the result for p = n −i + 1.
A.3
Alternative proof of Proposition 2
To prove Proposition 2, we ﬁrst begin by showing the following proposition.
Proposition 3. For a preparatory cross-validation score, SP CV (y1:n; P), deﬁned as the sum of
cross-validation terms from leave-(P + 1)-out to leave-n-out,
SP CV (y1:n; P) =
n
X
p=P +1
SCV (y1:n; p),
we have the following equivalence relationship
SP CV (y1:n; P) =
1
 n
P

(n
P)
X
t=1
log pM

y(t)
1:n−P

(A.3.1)
which states that SP CV is the average log marginal likelihood over all choices of the training set.
Proof. To show this, we use a proof by induction. We see that (A.3.1) is trivially true for P = n−1,
as this is simply SCV (y1:n; n). Assuming (A.3.1) holds for some 1 ≤P ≤n −1, we have
SP CV (y1:n; P −1) = SP CV (y1:n; P) + SCV (y1:n; P)
=
1
 n
P

(
n
P)
X
t=1
log pM

y(t)
1:n−P

+
1
 n
P

(
n
P)
X
t=1
1
P
P
X
j=1
log pM

˜y(t)
j
| y(t)
1:n−P

=
1
P
 n
P

(n
P)
X
t=1


P log pM

y(t)
1:n−P

+
P
X
j=1
log pM

˜y(t)
j
| y(t)
1:n−P



.
11

From the properties of conditional probability, we can write
SP CV (y1:n; P −1) =
1
P
 n
P

(n
P)
X
t=1
P
X
j=1
log pM

˜y(t)
j , y(t)
1:n−P

.
(A.3.2)
Again, the marginal likelihood is invariant to the permutation of the sequence under data exchange-
ability, so we have to consider the repetitions in the partitions ˜y(t)
j , y(t)
1:n−P . For each of the n choose
(n −P + 1) unordered sequences y(t′)
1:n−P +1, there are (n −P + 1) partitions into ˜y(t)
j , y(t)
1:n−P , so
there are n −P + 1 repetitions of each unordered y(t′)
1:n−P +1 in (A.3.2). We can thus write
SP CV (y1:n; P −1) = (n −P + 1)
P
 n
P

(
n
P −1)
X
t′=1
log pM

y(t′)
1:n−P +1

=
1
 n
P −1

(
n
P −1)
X
t′=1
log pM

y(t′)
1:n−P +1

and by induction we have (A.3.1).
Proposition 2 then follows trivially by setting P = 0 in Proposition 3.
A.4
Derivation of SCCV for Bayesian models
The following corollary follows easily from Propositions 2 and 3.
Corollary 2. For the cumulative cross-validation score deﬁned as
SCCV (y1:n; P) =
P
X
p=1
SCV (y1:n; p),
(A.4.1)
we have the following equivalence relationship
SCCV (y1:n; P) =
1
 n
P

(
n
P)
X
t=1
log pM

˜y(t)
1:P | y(t)
1:n−P

.
(A.4.2)
Proof. We note that log pM(y1:n) = SCCV (y1:n; P) + SP CV (y1:n; P) from their deﬁnitions and
Proposition 2. From the permutation invariance of the marginal likelihood, we can write
log pM(y1:n) =
1
 n
P

(
n
P)
X
t=1
log pM

˜y(t)
1:P , y(t)
1:n−P

.
(A.4.3)
By subtracting (A.3.1) in Proposition 3 from (A.4.3) and regarding each term in the summation, we
have
SCCV (y1:n; P) =
1
 n
P

(n
P)
X
t=1
n
log pM

˜y(t)
1:P , y(t)
1:n−P

−log pM

y(t)
1:n−P
o
=
1
 n
P

(n
P)
X
t=1
log pM

˜y(t)
1:P | y(t)
1:n−P

12

A.5
Computing SCCV
We note that ˆSCCV in (10) is a biased estimate, and Rischard et al. (2018) provides unbiased es-
timators of log pM(˜y1:P | y1:n−P ) directly through unbiased Markov chain Monte Carlo and path
sampling methods.
The arithmetic averaging over training/test splits ˆSCCV may also be inherently unstable, as
demonstrated by the following example. Suppose that y is a binary random variable which takes on
either 0 or 1 with equal probability, and we are attempting to estimate SCCV (y1:n; n/2). For large
n, it is likely that approximately half of the values in y1:n are equal to 0 and the other half to 1.
There will thus exist a permutation of the sequence y1:n such that almost all the ﬁrst n/2 values are
equal to 0, with the remaining almost all equal to 1. The model will then be certain that y = 0 after
observing the training set, and score the remaining n/2 points very poorly, giving a large negative
log posterior predictive. This suggests that an arithmetic average may be unstable; the median or
robust trimmed mean over permutations may be stabler alternatives.
The form in (A.4.2) relies on the conditional coherency of Bayesian updating and scoring. With-
out this, SCCV still exists as deﬁned in (A.4.1), and can be directly estimated for example through
ˆSCCV (y1:n; P) = P
T
T
X
t=1
1
p(t)
p(t)
X
j=1
s

˜y(t)
j
| y(t)
1:n−p(t)

where p(t) ∼U{1, P} and the training set y(t)
1:n−p(t) is sampled uniformly at random conditioned on
p(t). This facilities alternative choices for the belief updating model and s (˜y | y).
A.6
Visualization of cumulative cross-validation
0
20
40
60
80
100
Training set size n −p
−2.0
−1.9
−1.8
−1.7
−1.6
−1.5
Individual cross-validation score SCV
Individual cross-validation score SCV
against training set size, s2 = 1
r = 0
r = 1
r = 2
0
20
40
60
80
100
Preparatory set size n −P
−155
−154
−153
−152
−151
−150
Normalized cumulative cross-validation score SCCV
Normalized cumulative cross-validation score SCCV
against preparatory set size, s2 = 1
r = 0
r = 1
r = 2
Figure A.6.1: Leave-p-out cross-validation score SCV (y1:n; p) against n −p (left) and normalized
cumulative cross-validation score SCCV (y1:n; P) × n/P against n −P (right) for s2 = 1 and
p, P ∈{1, . . ., 99} in the polynomial regression example; the maximum standard error is 0.001 for
SCV and 0.005 for ˆSCCV .
13

A visualization of the effects of the training/preparatory data size is shown in Figure A.6.1 for
s2 = 1 in the polynomial regression example. We omit SCV (y1:n; n) and SCCV (y1:n; n) for clarity
of the plot, as both are signiﬁcantly more negative than the other values. On the left we see that the
individual cross-validation term SCV (y1:n; p) prefers the simplest r = 0 model when the training
set is very small as over-ﬁtting is penalized, but as n −p increases, the true r = 1 model overtakes
it. The r = 2 model eventually overtakes the r = 0 model too, and we see the discrepancy between
r = 2 and r = 1 decrease as over-ﬁtting is penalized less and less. This latter effect is demonstrative
of how leave-one-out cross-validation under-penalizes complex models as argued in Shao (1993),
and why a value of P > 1 should be preferred. On the right, we observe a similar effect for the
cumulative cross-validation score SCCV , but the discrepancy between r = 2 and r = 1 remains
more noticeable for moderate n −P as a cumulative sum of SCV terms is being taken.
A.7
Illustration for the probit model
To demonstrate the cumulative cross-validation score in an intractable example, we carry out model
selection in the Pima Indian benchmark model with a probit model. We observe binary random
variables y1:n with associated r-dimensional covariates x1:n, and the probit model is deﬁned as
fθ(y | x) = {Φ (θ
T˜x)}y {1 −Φ (θ
T˜x)}1−y
where Φ is the standard normal cumulative distribution function and ˜x =

1
xTT. As suggested
in Marin and Robert (2010), we elicit a g-prior π(θ) = N

θ; 0r+1, g(X TX)−1	
where 0r+1 is a
r + 1 vector of 0s and X is the n by r + 1 matrix with rows ˜xT
i .
The dataset consists of n = 332 data points and we consider r = 3 covariates consisting of
glu, bp and ped, which correspond to plasma glucose concentration from an oral glucose test,
diastolic blood pressure and diabetes pedigree function respectively. We compare the full model
M0: (glu,bp,ped) with M1: (glu,bp) through log pM(y1:n) and SCCV (y1:n; P) to test for
signiﬁcance of ped. We standardize all covariates to have 0 mean and variance 1. We calculate
log pM(y1:n) using importance sampling with a Gaussian proposal with 103 samples. The proposal
mean is set to the maximum likelihood estimate of θ and proposal covariance to the estimated co-
variance matrix of the maximum likelihood estimate as suggested in Marin and Robert (2010). For
SCCV (y1:n; P), we estimate each posterior predictive in (10) with the same importance sampling
scheme where we temper the proposal such that its covariance matrix is divided by (n −P)/n. We
also use 103 proposal samples and average over T = 105 random train/test splits. We carry out 10
runs of each and report the mean and maximum standard error as before.
We see in Table A.7.1 that for g = n, the simpler model with ped omitted performs worse for
both scores, and there is thus strong evidence for ped. However, when we set g = 10n, we see
that comparing models via the marginal likelihood suggests that ped is no longer signiﬁcant, while
the cumulative cross-validation score changes little with this increased variance of the prior. As a
sanity check, we run a Gibbs sampler targeting π(θ | y1:n, x1:n) for the two prior settings within the
full model M0, and plot the marginal posterior of θped in Figure A.7.1. For reference, the posterior
means of θglu, θbp are 0.70 and 0.12 respectively. The posteriors of θped are indistinguishable for
the two prior settings, with a signiﬁcant mean for θped. This agrees well with the cumulative cross-
validation score ˆSCCV which is clearly robust to vague priors.
14

Table A.7.1: Log marginal likelihoods and cumulative cross-validation score for probit model
g
MODEL
log pM(y1:n)
ˆSCCV (y1:n; P) × n/P
P = 0.9n
n
(GLU,BP,PED)
-168.93
-165.87
(GLU,BP)
-170.00
-167.37
10n
(GLU,BP,PED)
-173.10
-166.28
(GLU,BP)
-173.05
-167.64
MAXIMUM STANDARD ERROR
0.004
0.02
−0.2
−0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
θped
0
1
2
3
4
5
Posterior kernel density estimation
Marginal posterior for ped coeﬃcient
g = n
g = 10n
Figure A.7.1: Marginal posterior density plots for θped for different prior scalings g.
15
