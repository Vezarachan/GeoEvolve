arXiv:1501.03856v8  [stat.ME]  20 Nov 2015
Cross-validation and Peeling Strategies
for Survival Bump Hunting
using Recursive Peeling Methods
Jean-Eudes Dazard ˚
Michael Choe :
Michael LeBlanc ;
J. Sunil Rao §
June 21, 2021
Abstract
We introduce a framework to build a survival/risk bump hunting model with a censored time-to-event
response. Our Survival Bump Hunting (SBH) method is based on a recursive peeling procedure that
uses a speciﬁc survival peeling criterion derived from non/semi-parametric statistics such as the hazards-
ratio, the log-rank test or the Nelson–Aalen estimator. To optimize the tuning parameter of the model
and validate it, we introduce an objective function based on survival or prediction-error statistics, such
as the log-rank test and the concordance error rate. We also describe two alternative cross-validation
techniques adapted to the joint task of decision-rule making by recursive peeling and survival estima-
tion. Numerical analyses show the importance of replicated cross-validation and the diﬀerences between
criteria and techniques in both low and high-dimensional settings. Although several non-parametric
survival models exist, none addresses the problem of directly identifying local extrema. We show how
SBH eﬃciently estimates extreme survival/risk subgroups unlike other models. This provides an in-
sight into the behavior of commonly used models and suggests alternatives to be adopted in practice.
Finally, our SBH framework was applied to a clinical dataset. In it, we identiﬁed subsets of patients
characterized by clinical and demographic covariates with a distinct extreme survival outcome, for which
tailored medical interventions could be made. An R package PRIMsrc is available on CRAN and GitHub.
Keywords:
Exploratory Survival/Risk Analysis, Survival/Risk Estimation & Prediction, Non-
Parametric Method, Cross-Validation, Bump Hunting, Rule-Induction Method.
˚Center for Proteomics and Bioinformatics, Case Western Reserve University. Cleveland, OH 44106, USA. Corresponding
author Email (JED): jxd101@case.edu
:Center for Proteomics and Bioinformatics, Case Western Reserve University. Cleveland, OH 44106, USA.
;Department of Biostatistics, School of Public Health, University of Washington, Seattle, WA 98195, USA; Public Health
Sciences, Fred Hutchinson Cancer Research Center, Seattle, WA 98109.
§Division of Biostatistics, Dept. of Epidemiology and Public Health, The University of Miami. Miami, FL 33136, USA.
1

1
Introduction
Non-Parametric Methods for Bump Hunting
The search for data structures in the form of bumps, modes, components, clusters or classes are important
as they often reveal underlying phenomena leading to scientiﬁc discoveries. It is a diﬃcult and central
problem, applicable to virtually all sort of exact and social sciences with practical applications in various
ﬁelds such as ﬁnance, marketing, physics, astronomy and biology.
It is common to treat the task of ﬁnding isolated data structures with the help of a response function as
in a regression or classiﬁcation problem or simply a probabilistic model as in a density estimation problem.
Among the non-parametric unsupervised methods, this can be done by testing modality [10, 35, 58, 61],
using nonparametric mixture models (see e.g. [7] for a review), pattern recognition or clustering. However,
beside the limitations or problems encountered by these methods in higher dimensional settings, model
ﬁtting e.g. of ﬁnite mixture models is challenged by the estimation of the true number of components
[17]. A similar situation exists for clustering procedures where the true number of clusters is unknown.
Moreover, unsupervised methods may also fail to capture true data structures simply by ignoring a response
if available [17]. Although non-parametric supervised approaches such as, for instance, decision trees and
their ensemble versions [8, 9], do not have this drawback, these classiﬁcation and regression procedures
may also perform poorly [17] since they are designed to work when the true number of classes is ﬁxed or
assumed in advance.
Exploratory supervised bump hunting procedures are among the few non-parametric methods that
have been proposed to address this problem. These methods seek bump supports (possibly disjoint) of
the input space of multi variables where a target function (e.g. a regression or density function) is on
average larger (or lower) than its average over the entire input space.
They cover tasks such as: (i)
Mode(s) Hunting, (ii) Local/Global Extremum(a) Finding, (iii) Subgroup(s) Identiﬁcation, (iv) Outlier(s)
Detection. One known as the Patient Rule Induction Method (PRIM) was initially introduced by Friedman
& Fisher [31] and later formalized by Polonik [59]. Essentially, the method is a recursive peeling algorithm
that explores the input space to ﬁnd rectangular solution regions where the response is expected to be
larger on average. Some interesting features common and distinct to decision trees such as Classiﬁcation
and Regression Trees (CART) [9] help describe PRIM. As a rule-induction method like CART, PRIM
generates simple decision rules describing the solution region of interest. Further, like CART, PRIM is a
non-parametric procedure, algorithmic in nature (backwards ﬁtting recursive algorithm), which makes few
statistical assumptions about the data. Although PRIM does not explicitly state a model as CART, one
can be formulated [36, 73]. Both algorithms/models have the possibility to recover complex interactions
between input variables. Basic diﬀerence between the two methods lies in their approach and goal (reviewed
in section 2.2.1).
To date, only a few extensions of the original PRIM work have been done: This includes a Bayesian
model-assisted formulation of PRIM [73], a boosted version of PRIM based on Adaboost [72], an extension
of PRIM to censored responses [47, 48] and to discrete variables [39].
Although PRIM is intrinsically
multivariate, it was uncertain from the original work how the algorithm would perform in ultra high-
dimension where collinearity [30, 31] and sparsity abound.
So, recently, an interesting body of work
studied when and why the Principal Component space can be used eﬀectively to optimize the response-
predictor relationship in bump hunting. This was ﬁrst addressed in [17], where the computational details of
such an approach were laid out for high-dimensional settings. Further, focusing on the properties of PRIM,
authors demonstrated using basic geometrical arguments how the PC rotation of the predictor space alone
can generate “improved” bump estimates [19, 20]. These developments have important implications for
general supervised learning theory and practical applications. In fact, [17] ﬁrst used a sparse PC rotation
for improving bump hunting in the context of high dimensional genomic predictors and later showed how
this approach can be used to ﬁnd additional heterogeneity in terms of survival outcomes for colon cancer
patients ([18]).
2

Model Development and Validation in Discovery-Based Research
The primary problem encountered in discovery-based research has been non-reproducible results.
For
instance, early biomarker discovery studies using modern high-throughput datasets with large number
of features have often been characterized by false or exaggerated claims and eventually disappointment
when original results could not be reproduced in an independent study [21, 23, 28, 33, 49, 54, 63, 64, 66].
Sadly, these results have been published even in high-proﬁle journals and considered to provide deﬁnitive
conclusions for both clinical care and biology. The problems of model reliability and reproducibility have
usually been characterized by issues of severe model over-ﬁtting, biased model parameter estimates and
under-estimated errors. This has been attributed to a lack of proper rules to assess the analytical validity
of studies simply because they were either under-developed or not routinely/correctly applied [57, 60]. This
problem ﬁrst received the attention of statisticians (see for instance reviews on guidelines and checklists
[6, 23, 52]) as well as editors and US regulators lately [53].
Meanwhile, considerable development work has been done in the ﬁelds of feature selection, predictive
model building and model validation to resolve the aforementioned issues. Recent developments include
strategies such as variable/feature selection, dimension reduction, coeﬃcient shrinkage and regularization.
The challenge is obviously more acute in the context of high-dimensional data where the number of variables
greatly exceeds the number of observations (so-called p " n paradigm), since usually only a small number
of variables truly enter in the model, while the large majority of them just contribute to noise. This noisy
situation is even more complicated by the multicollinearity and spurious correlation between variables as
well as the endogeneity between variables and model residual errors (see e.g. [29] for a recent review).
A common situation where model reliability and reproducibility arise is when, for instance, model
performance estimates are calculated from the same data that was used for model building, eventually re-
sulting in initially promising results, but often non-reproducible [2, 36, 64]. These so-called “resubstitution
estimates” are severely (optimistically) biased. Another problematic situation is when not all the steps of
model building (such as pre-selection, creation of the prediction rule and parameter tuning) are internal to
the cross-validation procedure, thereby creating a selection bias [2, 36, 71]. In addition, ﬁndings might not
be reproducible even when proper independent sample and validation procedures are used. Problems may
arise simply because cross-validated estimates are well-known to have large variance, a situation that is
obviously more prevalent when few independent observations or small sample size n are used [22, 24, 51].
Predictive Survival/Risk Modeling by Rule-Induction Methods
One important application of survival/risk modeling is to identify and segregate samples for predictive
diagnostic and/or prognosis purposes. Direct applications include the stratiﬁcation of patients by diagnostic
and/or prognostic groups and/or responsiveness to treatment.
Therefore, survival modeling is usually
performed to predict/classify patients into risk or responder groups (not to predict exact survival time) from
which one usually derives survival/risk functions estimates (e.g. by Kaplan–Meier estimates). However,
for the reasons mentioned above, Kaplan–Meier estimates for the risk groups computed on the same set of
data used to develop the survival model may be very biased [55, 71].
In the context of a time-to-event outcome, regression survival trees have proven to be useful. Several
developments have been made for ﬁtting decision trees to non-informative censored survival times [1,
11, 13, 32, 45, 46, 62].
Although regression survival trees are powerful techniques to understand for
instance patient outcome and for forming multiple prognostic groups, often times interest focuses only on
estimating extreme survival/risk groups. In this respect, survival bump hunting aims not at estimating
the survival/risk probability function over the entire variable space, but at searching regions where this
probability is larger (or smaller) than its average over the entire space.
Also, one possible drawback of decision trees is that the data splits at an exponential rate as the
space undergo partitioning (typically by binary splits) as opposed to a more patient rate in decision
boxes (typically by controlled quantile).
In this sense, bump hunting by recursive peeling may be a
more eﬃcient way of learning from the data.
With the exception of the work of LeBlanc et al.
on
Adaptive Risk Group Reﬁnement [48], it has not been studied whether decision boxes, obtained from box-
structured recursive peelings, would yield better estimates for constructing prognostic groups than their
3

tree-structured counterparts.
Although resampling methods are often useful in assessing the prediction accuracy of classiﬁer models,
they are not directly applicable for predictive survival modeling applications. Simon et al. have reviewed
the literature of such applications and identiﬁed serious deﬁciencies in the validation of survival risk models
[23, 65, 66]. They noted for instance that in order to utilize the cross-validation approach developed for
classiﬁcation problems, some studies have dichotomized their survival or disease-free survival data . . . .
The problem on how to cross-validate the estimation of survival distributions (e.g.
by Kaplan–Meier
curves) is not obvious [65]. In addition, beside Subramanian and Simon’s initial study on the usefulness of
resampling methods for assessing survival prediction models in high-dimensional data [67], no comparative
study has been done for rule-induction methods and speciﬁcally recursive peeling methods such as our
“Patient Recursive Survival Peeling” method (see section 2.2.7).
Contribution and Scope
Our survival/risk bump hunting model is built upon the regular bump hunting framework, which we
extended to accommodate a possibly censored time-to-event type of response. To build our survival/risk
bump hunting model, we ﬁrst describe our “Patient Recursive Survival Peeling” (PRSP) method, a non-
parametric recursive peeling procedure, derived from a rule-induction method, namely the Patient Rule
Induction Method (PRIM), which we have extended to allow for survival/risk response, possibly censored.
In the process, we describe what appropriate survival estimator and statistic may be used as a peeling
criteria to ﬁt our survival/risk bump hunting model.
One of the critiques made in the original PRIM work was the lack of validation procedure and measures
of signiﬁcance of solution regions. So, our objective was also to develop a validation procedure for the
purpose of model tuning by means of an optimization criterion of model parameters tuning and a resampling
technique amenable to the joint task of decision rule making by recursive peeling (i.e.
decision-box)
and survival estimation. Speciﬁcally, we describe here two alternative, possibly repeated, K-fold cross-
validation techniques adapted to the task, namely the “Replicated Combined CV” (RCCV) and “Replicated
Averaged CV” (RACV). Moreover, we show how to use survival end-points/prediction statistics for the
speciﬁc goal of model peeling length optimization by cross-validation.
Results support the claim that optimal survival bump hunting models may be reached using appropriate
combination of criterion and technique under certain situations, for which we provide guidelines. Finally, we
show empirical results from a real dataset application and from simulated data in low- and high-dimension,
illustrating the eﬃciency of our cross-validation and peeling strategies and the adequacy of our survival
bump hunting framework in comparison to other available non-parametric survival models.
We do not describe nor discus the speciﬁc treatment of dimension-reduction or variable selection in
high-dimensional settings for the only reason that the focus of this study is on cross-validation and peeling
strategies. Even though the issue of model unreliability is known to be more severe when there is a large
number of variables to choose from [64], it is known to persist even in low-dimensional setting [68]. So,
we posit that the framework described here is relevant and applicable to both low and high-dimensional
situations. Nevertheless, the method does include cross-validation procedures to control model size (#
covariates) in addition to model complexity (# peeling steps). It has been tested in multiple (ą 20) low
and high-dimensional situations where n ď p and even n ! p (see abstract of application article [15] and
our example datasets in our R package PRIMsrc [16]) and we show empirical analyses in high-dimensional
simulated datasets where n ă p.
2
Survival Bump Hunting for Exploratory Survival Analysis
2.1
Bump Hunting Model
2.1.1
Notation - Goal
The formal setup of bump hunting is as follows [see also 31, 59]. Let us consider a supervised problem with
a univariate output (response) random variable, denoted y P R. Further, let us consider a p-dimensional
4

random vector X P Rp of support S, also called input space, in an Euclidean space. Let us denote the p
input variables by X “ rxjsp
j“1, of joint probability density function ppXq and by fpxq “ Epy|X “ xq the
target function to be optimized (e.g. any regression function or e.g. the p.m.f or p.d.f fXpxq).
Brieﬂy, the goal in bump hunting is to ﬁnd
a sub-space or region pR Ď Sq of the input
space within which the average value ¯fR of
fpxq is expected to be signiﬁcantly larger (or
smaller) than its average value ¯fS over the
entire input space S (Figure 1). In addition,
one wishes that the corresponding support
(mass) of R, say βR, be not too small, that
is, greater than a minimal support threshold,
say 0 ă β0 ă 1.
Figure 1: Schematic representation of bump hunting in the unidi-
mensional case (p “ 1), where the target function fpxq is a regression
function of x and the estimated region R is a contiguous interval
(red segment) corresponding to larger values of fpxq on average. The
support βR of R and the average values ¯fR and ¯fS are shown.
Formally, in the continuous case of X:
¯fR “
ş
xPR fpxqppxqdx
ş
xPR ppxqdx
" ¯fS
(1)
βR “
ż
xPR
ppxqdx " β0
(2)
In supervised problems with an output variable (response) y, one would seek to characterize the condi-
tional expectation Epy|X “ xq and infer the properties of the unknown joint probability density function
ppXq, whereas in the case of unsupervised learning, one would have to directly infer the properties of ppXq,
e.g. from some density estimate, without the help of a response.
Let Sj be the support of the jth variable xj, such that the input space can be written as the (Cartesian)
outer product space S “ Śp
j“1 Sj.
Let sj Ď Sj denotes the unknown subset of values of variable xj
corresponding to the unknown support of the solution region R. Let J Ď t1, . . . , pu be the subset of indices
of selected variables in the process. The goal in bump hunting amounts to ﬁnding the value-subsets tsjujPJ
of the corresponding variables txjujPJ such that
R “
#č
jPJ
pxj P sjq : p ¯fR " ¯fSqpβR " β0q
+
(3)
2.1.2
Estimates
Since the underlying distribution is not known, the estimates of ¯fR and βR must be used.
Assume a
supervised setting, where the outcome response variable is y “ py1 . . . ynqT and the explanatory/input
variables are X “ px1 . . . xnqT , where each observation is the p-dimensional vector of covariates xi “
rxi,1 . . . xi,psT , for i P t1, . . . , nu. Plug-in estimates of the average value ¯fR of the target function fpxq and
of the support βR (eq. 2) of the region R are respectively derived as:
ˆ¯fR “
1
nˆβR
ÿ
xiP ˆR
yi “
1
nˆβR
nÿ
i“1
yiIpxi P ˆRq
(4)
ˆβR “ 1
n
ÿ
xiP ˆR
Ipxi P ˆRq “ 1
n
nÿ
i“1
Ipxi P ˆRq
(5)
5

2.1.3
Remarks
1. The goal amounts to comparing the conditional expectation of the response over the solution region
R: ¯fR “ Erfpxq|x P Rs with the unconditional one ¯fS “ Erfpxqs.
2. Larger target function average ¯fR is associated with smaller support βR of the region R (Figure 1).
So, in practice, there is a trade-oﬀbetween maximizing ¯fR and maximizing βR.
3. If the target function to be optimized is for instance the p.m.f or p.d.f fXpxq, then Prpx P Rq is the
probability mass/density of a local maximum and the task is equivalent to a mode(s) hunting.
4. In the case of real-valued inputs, the entire input space is the p-dimensional outer product space
S Ď Rp; the support Sj of each individual input variable (and of each corresponding value-subset sj)
is the usual interval of the form Sj “
”
t´
j , t`
j
ı
Ă R for j P J; the solution region R has the shape of
a |J|-dimensional hyper-rectangle in R|J|, called a box, denoted B, which can be written as the outer
product of |J| intervals of the form B “ Ś
jPJrt´
j , t`
j s.
5. In general, region R could be any smooth shape (e.g. a convex hull) possibly disjoint. Describing
or modeling such region would be diﬃcult in high dimension and especially when the number of
variables is larger than the number of observations (p " n paradigm). In general, there is a trade-oﬀ
between the goodness of ﬁt and the interpretability of the inferences that we want to make. Here, we
focus on interpretable models based on rectangular boxes in the input space of variables. Typically
these rectangular boxes are aligned to the coordinate axes, but an immediate extension is to use
linear combination rules of variables, i.e. a rotated space of input variables, such a the principal
components space. We have showed that this strategy may provide a more favorable space to learn
from the data (see for instance [17–20]).
2.1.4
Estimation by the Patient Rule Induction Method (PRIM)
The Patient Rule Induction Method (PRIM) is used to get the region estimate ˆR with corresponding
support estimate ˆβR and conditional output response mean estimate ˆ¯fR. Essentially, the method is one
of recursive peeling/pasting algorithm (a discrete version of the steepest ascent method) that explores the
input space solution region, where the response is expected to be larger on average. The method generates
a sequence of boxes that collectively cover the region estimate ˆR. The way the space is covered and the box
induction is done as well as how the patience and stopping rules are controlled is detailed in the original
article of Friedman & Fisher [31], later formalized by Polonik & Wang [59].
Covering - Coverage Stopping Rule. A sequence of boxes tBmuM
m“1 is generated from the data txi, yiun
i“1
to collectively cover the solution region R. Starting from an initial box B1 that covers all the data, the
box sequence construction algorithm is recursively applied to subsets of the data as follows. At the mth
iteration pm ą 1q, a box Bm is induced (by the top-down peeling algorithm - see next) using the data
remaining after removal of all the observations contained in the previous boxes: tpyi, xiq: xi R Ťm´1
r“1 Bru.
At the Mth iteration of the covering loop, the box sequence tBmuM
m“1 stops either (i) when the estimated
individual box support ˆβM becomes too small, say less than an arbitrary threshold 0 ă β0 ă 1 expressed
as a fraction of the entire data: ˆβM ă β0, where, or (ii) when the estimated box output mean ˆ¯yM becomes
too small, say ˆ¯yM ă ¯y where ¯y “ 1
n
řn
i“1 yi is the global mean, where
ˆβM “ 1
n
nÿ
i“1
I
˜
xi P BM & xi R
M´1
ď
m“1
Bm
¸
ˆ¯yM “
1
nˆβM
nÿ
i“1
yiI
˜
xi P BM & xi R
M´1
ď
m“1
Bm
¸
Box Induction. To induce the box Bm at the mth iteration pm ą 1q, the top-down peeling algorithm
generates a subsequence of nested boxes tBm,luL
l“1 starting from an initial box Bm,1 that covers all the
data remaining at the mth iteration of the covering loop. How L is estimated is the subject of section 3.2.
6

At the lth iteration, a sub-box is peeled oﬀ(see next) from within the current box Bm,l to produce the
next smaller box Bm,l`1. The particular sub-box b˚
m,l is chosen to yield the largest box output mean value
¯ym,l`1 within the next box Bm,l`1, such that:
¯ym,l`1 “
1
nˆβm,l`1
nÿ
i“1
yiI
˜
xi P Bm,l`1 & xi R
lď
b“1
Bm,b
¸
Bm,l`1 “ Bm,lzb˚
m,l
, where
b˚
m,l “ argmaxbm,lPCpbm,lq r¯ym,l`1 : xi P pBm,lzbm,lqs
where Cpbm,lq represents the class of potential sub-boxes bm,l eligible for removal at step or generation pm, lq
and ‘z‘ represents the set minus operator. The current box Bm,l is then updated: Bm,l`1 “ Bm,l`1zb˚
m,l
and the peeling procedure is looped until some stopping rule is met (see next). Because a top-down peeling
is a greedy search algorithm, it may cause overﬁtting, so a bottom-up pasting is applied to the minimal
candidate box to repeatedly expand along any edge until the expansion fails to increase the output response
average within the box.
Patience - Induction Stopping Rule. There are two important meta-parameters that control the box
induction algorithm: (i) the peeling fraction 0 ă α0 ă 1 that controls the degree of patience, and (ii) the
minimal box support threshold 0 ă β0 ă 1, expressed as a fraction of the whole data that is used in the
stopping criterion (see next). Only a quantile α0 of the data that is in the box Bm,l is peeled oﬀat the
lth iteration of the peeling loop as follows. Each eligible sub-box bm,l is deﬁned by a single input variable
xj. For real valued variables, there are two eligible sub-boxes b´
j,m,l P Cpbm,lq and b`
j,m,l P Cpbm,lq, which
respectively border the lower and upper boundaries of the box Bm,l on the jth input variable xj:
#
b´
j,m,l “ tx: xj ă xpα0q
j
u
b`
j,m,l “ tx: xj ą xp1´α0q
j
u
where xpα0q
j
and xp1´α0q
j
are respectively the α0th and p1 ´ α0qth quantiles of the xj values. At the
Lth iteration of the peeling loop, the box sequence tBm,luL
l“1 stops when the estimated individual support
ˆβm,L of the last box Bm,L becomes too small, say ˆβm,L ă β0, where β0 is an arbitrary minimal box support
threshold:
#ˆβm,1 “ 1
n
řn
i“1 Ipxi P Bm,1q
for
L “ 1
ˆβm,L “ 1
n
řn
i“1 Ipxi P Bm,L
&
xi R ŤL´1
l“1 Bm,lq
for
L ą 1
Note that, with our notation, the last box Bm,L of the subsequence is also the next box of the outer
box sequence tBmuM
m“1. So, Bm,L “ Bm`1, and similarly ˆ¯ym,L “ ˆ¯ym`1 and ˆβm,L “ ˆβm`1.
Decision Rules. It is desirable that the solution region R be described in an interpretable form by
logical statements involving the value-subset of each selected input variable. The above algorithm results
in simple decision rules of the input space, where each box Bm, m “ 1, . . . , M, is described by the outer
product of the value-subsets sj,m of each individual input variable xj, for j P J. The idea is to describe
the solution region R by a disjunctive rule of M conjunctive subrules of the form R “ ŤM
m“1 Rm, where
Rm “ tx P Bmu “ Ş
jPJpxj P sj,mq. In the case of real-valued input variables, each subrule becomes
Rm “ Ş
jPJpxj P rt´
j,m, t`
j,msq and the solution region R is fully described by the disjunctive rule:
ˆR “
M
ď
m“1
ˆRm “
M
ď
m“1
#č
jPJ
pxj P rt´
j,m, t`
j,msq
+
7

2.2
Survival Bump Hunting by Recursive Peeling
Assume a supervised problem, where the function of interest is a univariate survival/risk response variable
(possibly censored) in a multivariate setting of real-valued (continuous or discrete) input variables/covariates
X “ rxjsp
j“1. The goal is to characterize an extreme-survival-response support in the predictor space and
identify the corresponding box-deﬁned group of samples using a recursive peeling method derived from the
Patient Rule Induction Method (PRIM).
2.2.1
Survival-Speciﬁc Peeling Rule
As mentioned in the introduction, rule-induction methods such as decision tree-based methods have proven
to be useful to estimate relative risk in groups in the context of a time-to-event outcome. Several methods
have been proposed for ﬁtting trees to non-informative censored survival times [1, 11, 13, 32, 45, 46, 62].
Basic diﬀerences between decision-tree and decision-box methods lie in their approach and goal. In-
stead of recursively partitioning the space using speciﬁc partitioning and stopping criteria, one proceeds by
recursively peeling the space to produce box-shaped regions designed to approximate the solution region R,
using speciﬁc peeling and stopping criteria (see details in 2.1.4). In decision-trees, a recursive partitioning
method attempts to model the target function over the entire data space by generating partitions in which
the response averages will be as diﬀerent as possible, while in decision-boxes, a recursive peeling method
generates a box-shaped region in which the response average will be as extreme as possible. So, in contrast
to survival decision-trees models, survival bump hunting is not aimed at estimating the survival/risk prob-
ability function over the entire covariate space, but at ﬁnding regions where this probability is larger than
its average over the entire space. Numerical analysis below (4) show comparisons of relative risk estimates
obtained from decision-boxes versus those obtained from decision-trees. Other interesting diﬀerences lie in
the weaknesses and strengths of the outputs and their applications, which we left for discussion (6).
In this section, we describe the use of several candidate survival-speciﬁc peeling criteria and discuss
their merits or strengths. Most of these criteria are borrowed from the survival splitting rules used to
grow regression survival trees [1, 45, 46, 62, 69] or from their ensemble versions [40]. Here, survival-speciﬁc
peeling criteria are to be used to decide which covariate will be selected to give the best peel between two
boxes from two consecutive generations (parent-child descendance) of the box induction/peeling loop in a
recursive peeling algorithm (see next section 2.2.7).
To account for censoring, we simply supervise by proxy for extreme time-to-event outcome, turning the
censored outcome y into an uncensored “surrogate” outcome z. Using previous notation (section 2.1.4),
a peeling at step pm, lq of the box induction/peeling sequence produces a partition of the survival data
from the parent box Bm,l´1 into two partitions, for a given set of covariates: the child box Bm,l and its
complement. The focus is on selecting a sub-box bm,l at step pm, lq of the box induction/peeling sequence
that is to be peeled oﬀfrom the parent box Bm,l´1 along one of its faces (i.e. direction of peeling := axis
of dimension j) to induce the next child box Bm,l and its complement. This is done by maximizing the
“surrogate” outcome rate of increase between two consecutive generations of boxes Bm,l´1 and Bm,l of
the box induction/peeling sequence. Denote by zpm, lq the box “surrogate” outcome at step or generation
pm, lq of the box induction/peeling sequence (Algorithm 1). The rate of increase in zpm, lq at step or
generation pm, lq between two consecutive generations of boxes Bm,l´1 and Bm,l is deﬁned as:
rpm, lq “ zpm, lq ´ zpm, l ´ 1q
βm,l´1 ´ βm,l
(6)
Finally, the particular sub-box b˚
m,l that is chosen to yield the largest box increase rate rpm, lq between
box Bm,l´1 and the next one Bm,l is such that
Bm,l “ Bm,l´1zb˚
m,l
, where
b˚
m,l “ argmax
bm,lPCpbm,lq
rrpm, lqs ,
(7)
where Cpbm,lq represents the class of potential sub-boxes bm,l eligible for removal at step or generation
pm, lq.
8

2.2.2
Survival Notation and Deﬁnitions
Let’s denote the two child boxes described above by tBg,m,lu2
g“1, where, by convention, let’s decide that
subscript g “ 1 stands for the “in-box” Bm,l and g “ 2 for its complement or “out-of-box”. Dropping
further step subscripts pm, lq for simplicity, assume that there are n individuals in parent box Bm,l´1 and
ng in a given child box Bg,m,l for ﬁxed g P t1, 2u such that n “ ř2
g“1 ng. Also, we let γipgq “ I pxi P Bg,m,lq
be the indicator function of individual subject i within a given child box Bg,m,l at step pm, lq for ﬁxed
g P t1, 2u.
The response variable being subject to censoring, we use the general random censoring model. We
focus on a univariate right-censored survival outcome under the assumptions of independent observations,
non-competitive risks and random (type-I or -II) non-informative censoring. Denote the true survival time
(or lifetime/failure time) by the random variable T and the observed censoring time by the random variable
C, then the observed survival time is the random variable Y “ minpT, Cq. Also, under our assumptions, C
is assumed to be independent of T conditionally on covariates X. Let the observed event indicator random
variable be ∆“ IpT ď Cq.
For each observation i P t1, . . . , nu in parent box Bm,l´1, the true survival time, observed censoring time,
observed survival time and observed indicator event are the realizations denoted by Ti, Ci, Yi “ minpTi, Ciq
and δi “ IpTi ď Ciq, respectively. Also, denote by tp1q ă tp2q ă . . . ă tpNq for N ď n the distinct ordered
event times of death (not counting censoring times) in parent box Bm,l´1. Note that intervals between
events tphq for h P t1, . . . , Nu are not necessarily uniform. Finally, the observed data in parent box Bm,l´1
consists of pYi, δi, xiq, where xi “ rxi,1 . . . xi,psT , for i P t1, . . . , nu.
Let δi,g “ γipgqIpTi ď Ciq be the observed indicator event of time point Ti for each individual i P
t1, . . . , ngu in a given child box Bg,m,l for g P t1, 2u. Also, let dh,g and nh,g be respectively the number
of events (deaths) and individuals at risk at time tphq for h P t1, . . . , Ngu in a given child box Bg,m,l
for g P t1, 2u, such that N “ ř2
g“1 Ng.
For simplicity, let’s use the same subscript h P t1, . . . , Ngu
and i P t1, . . . , ngu from parent and child boxes for indexing events and individuals, respectively. Note
that nh,g is the number of individuals in child box Bg,m,l who either have not yet had an event (or
been right-censored) just until time tphq or who had an event at time tphq. Formally, if considering all
individuals in child box Bg,m,l only, nh,g “ řng
i“1 I
`
Yi ě tphq
˘
or, if considering all individuals in parent
box Bg,m,l´1, nh,g “ řn
i“1 γipgqI
`
Yi ě tphq
˘
.
Likewise, one can write dh,g “ řng
i“1 δi,gI
`
Yi ě tphq
˘
, or
dh,g “ řn
i“1 δiγipgqI
`
Yi ě tphq
˘
. Also, denote dh “ ř2
g“1 dh,g and nh “ ř2
g“1 nh,g.
Let Sptq “ PrpT ě tq be the survival probability that an individual from the population of interest will
have a lifetime T free of the event until time t. As usual, denote by Λptq “ ´ logpSptqq the corresponding
cumulative hazard function and by λptq “ dΛptq
dt
the hazard rate function. To come up with decision-box
survival-speciﬁc peeling criteria (see next section 2.2.3), the following non/semi-parametric estimators can
be used with respect to the box-deﬁned subgroups: the Nelson–Aalen estimator, denoted by ˆHg,m,lptq,
to estimate the cumulative hazard function; and the hazard rate function estimator derived from a Cox
Proportional Hazards (CPH) regression model. By deﬁnition, these estimators are given as follows for
individuals in a given child box Bg,m,l, for ﬁxed g P t1, 2u, at step pm, lq:
ˆHg,m,lptq “
ÿ
h:tphqďt
dh,g
nh,g
As usual, the hazard rate function may be estimated by regressing the subject-speciﬁc hazard rate on
the covariates in a CPH regression model, assuming proportional hazards [12]. With the above notation,
ˆλi,g,m,lpt|xiq “ λ0ptq exp rηg,m,lpxiqs
“ λ0ptq exp rηg,m,lγipgqs
where the regression function ηg,m,lpxiq “ ηT
g,m,lxi “ řp
j“1 ηj,g,m,lxi,j with p-dimensional vectors of regres-
sion coeﬃcients ηg,m,l “ rη1,g,m,l . . . ηp,g,m,lsT and covariate xi “ rxi,1 . . . xi,psT reduce respectively to a
scalar ηg,m,l “ η1,g,m,l “ ηg,m,l times a simple box indicator variable xi “ xi,1 “ I pxi P Bg,m,lq “ γipgq.
9

2.2.3
Non-Parametric Survival Peeling Criteria
The choice of uncensored surrogate outcome zpm, lq in equation 6, that is, which estimator to choose as a
box peeling criterion at a peeling step pm, lq, is central to the PRSP algorithm (see Algorithm 1). Currently,
our Survival Bump Hunting implementation in our R package PRIMsrc [16] oﬀers three statistics derived
from the above non/semi-parametric estimators: (i) the Log-Rank Test statistic, (ii) the Nelson–Aalen
Summary statistic and (iii) the CPH-derived Log Hazard Ratio statistic (assuming proportional hazards).
• The (two-sample) log-rank test can be used at a peeling step pm, lq to compare estimates of the hazard
functions from each child box-deﬁned subgroups (“in-box” and its “out-of-box” complement). We
recently proposed to use it as a survival-speciﬁc box peeling criterion [14]. Using the (two-sample)
log-rank test statistic is actually a natural candidate for survival decision-box, having been a well-
established concept for splitting trees in survival decision-trees [1, 45, 46, 62, 69] and for being robust
in non-proportional hazard settings [46]. The approximate log-rank test introduced by LeBlanc and
Crowley can be used instead to greatly reduce computations [46]. Formally, one can derive the Log-
Rank Test (LRT) statistic, denoted ˆχLRT pm, lq, for the individuals in a given child box Bg,m,l, for
ﬁxed g P t1, 2u (e.g. g “ 1), at step pm, lq as follows:
ˆχLRT pm, lq “
řN
h“1
´
dh,1 ´ nh,1
dh
nh
¯
cřN
h“1 nh,1
dh
nh
´
1 ´ nh,1
nh
¯ ´
nh´dh
nh´1
¯
(8)
• If the Nelson–Aalen estimator is used, one can derive an overall summary statistic across all observed
time points Yi for i P t1, . . . , ngu of the individuals in a given child box Bg,m,l, for ﬁxed g P t1, 2u
(e.g. g “ 1), at step pm, lq. This is done by adding the Nelson–Aalen estimators over all these time
points to obtain a so-called Cumulative Hazard Summary (CHS), denoted ˆΛCHSpm, lq:
ˆΛCHSpm, lq “
n1
ÿ
i“1
ˆH1,m,lpYiq
“
n1
ÿ
i“1
˜
ÿ
h:thďYi
dh,1
nh,1
¸
“
n1
ÿ
i“1
˜
ÿ
h:thďYi
řn1
i“1 δi,1I
`
Yi ě tphq
˘
řn1
i“1 I
`
Yi ě tphq
˘
¸
“
n1
ÿ
i“1
ˆřn1
i“1 δi,1
n1
˙
“
n1
ÿ
i“1
δi,1
(9)
• Alternatively, the use of a Hazard Ratio or Relative Risk was originally proposed by LeBlanc et al
[47, 48]. If the Cox-PH hazard rate estimate is used, then one derives the Log-Hazard Ratio (LHR)
statistic, denoted ˆλLHRpm, lq, for the individuals in both child boxes Bg,m,l, for g P t1, 2u, at step
pm, lq:
ˆλLHRpm, lq “ log
"λ0ptq exp rη1,m,lγip1qs
λ0ptq exp rη2,m,lγip2qs
*
“ log
"exppη1,m,lq
expp0q
*
“ η1,m,l
(10)
where γip1q “ 1 and γip2q “ 0 by convention.
10

Finally, all the above three peeling criteria statistics can be used to maximize the diﬀerences in survival
outcomes between two consecutive boxes ˆBm,l´1 and ˆBm,l of the box induction/peeling sequence. This
leads to the derivation of the corresponding box rate of increase estimate ˆrpm, lq, at step pm, lq, according
to equation 6:
ˆrLRT pm, lq “ ˆχLRT pm, lq ´ ˆχLRT pm, l ´ 1q
ˆβm,l´1 ´ ˆβm,l
(11)
ˆrCHSpm, lq “
ˆΛCHSpm, lq ´ ˆΛCHSpm, l ´ 1q
ˆβm,l´1 ´ ˆβm,l
“
řn1,m,l
i“1
δi,1,m,l ´ řn1,m,l
i“1
δi,1,m,l´1
ˆβm,l´1 ´ ˆβm,l
(12)
ˆrLHRpm, lq “
ˆλLHRpm, lq ´ ˆλLHRpm, l ´ 1q
ˆβm,l´1 ´ ˆβm,l
“ η1,m,l ´ η1,m,l´1
ˆβm,l´1 ´ ˆβm,l
(13)
2.2.4
Comments
An alternative estimator is to consider the conditional probability Pg,m,lpt|xiq “ PrpTi ď t|xi P Bg,m,lq,
which amounts to computing the Nelson-Aalen estimator ˆHg,m,lptq conditioning on the data in a given
child box Bg,m,l. Although this probability is interpretable and estimable, it is by deﬁnition a function of
an observed event (death) at time t (in a given child box Bg,m,l). So, one would need to ﬁx a meaningful
survival time t. One could think, for instance, of the box median survival time (as is commonly done)
or the box maximal event time. In addition, this would induce a likely loss of “power” in contrast to an
estimator based on the global survival distribution. As a result, for a given choice of t, this probability
may be easy to estimate in some boxes but not estimable in other boxes after suﬃcient peeling.
Note that the Nelson–Aalen estimator is known to imply conservation of events [56], that is in this
case, the total number of deaths is conserved in each child box Bg,m,l. In fact, that is what the Cumulative
Hazard Summary (CHS) statistic amounts to (see eq: 9).
A modiﬁed summary statistic derived from the Nelson–Aalen is also possible by normalizing ˆΛCHSpm, lq
to the total box sample size ng. This would have the advantage of hedging against large versus small box
bias. In our experience, this could be important in the case of discrete covariates.
In addition to the above assumption on the censoring mechanism, the CPH-derived Log-Hazard Ratio
or Relative Risk statistic to be used in equation 6 assumes proportional hazards, which may not be realistic.
For this reason, this survival peeling criterion is referred to the reader as not preferred and left as a means
of comparison to potentially better alternative survival peeling criteria described above (section 2.2.3).
2.2.5
Alternative Survival Peeling Criteria
Further discussion of the use of the above estimators is found in our comparisons of numerical results
(section 4.3) and in the discussion (6). Additionally, we mention below a few more alternative survival
peeling criteria, although none of these is preferred nor implemented in our R package.
1. It is common to estimate the hazard rate of the simplest parametric survival model (exponential
survival model) by the parametric Maximum Likelihood Estimator (MLE). Using above notation
and dropping further step subscripts pm, lq for simplicity, if we let Ti „ Exppλq for i P t1, . . . , ngu
then the parametric MLE is: ˆλgptq “
řng
i“1 δiγipgq
řng
i“1 Yiγipgq. The use of this simple parametric estimator of
hazard rate for a box was originally proposed by LeBlanc et al. [47, 48]. Since it is always estimable,
it could be used to maximize the box rate of increase rpm, lq (eq. 6) at each step pm, lq. It also does
come with likely the least variance. However, the underlying assumption of constant hazard rate over
the duration of time makes it potentially unrealistic and therefore not preferred.
2. The Log-Rank Score Test statistic for splitting in trees (see [37]) or their ensemble versions [40] is
another potential criterion available. Note that if there are no tied event times, the Log-Rank Test
and the Log-Rank Score Test statistics are identical and, unless there are a large number of tied
times, will give very similar results.
11

Although some may argue that residuals are counter intuitive to use as a peeling criterion, others
have used them. For instance, the use of Martingale residuals is strongly recommended by Kehl et al.
[43]. Their claim is that they perform better than the deviance residuals, which are a transformation of
the Martingale residuals correcting for long tails of the residual distribution. However, others have found
that the deviance residuals lead to better rule induction results for bump hunting (Steve Horvath et al.’s
personal communication and [50]). Therneau et al [69] have also found that using the deviance residuals
as a splitting criterion in regression trees leads to better results than the Martingale residuals.
1. Martingale Residuals Mi “ δi ´ ˆΛpYi, xiq “ δi ´ ˆΛ0pYiq exppηT xiq, for the ith observation, result from
ﬁtting an intercept-only Cox regression to the censored survival times. The idea is to use these as
new (uncensored) outcomes in the model instead of time [69], where δi is the event indicator and
ˆΛ0pYiq is a non-parametric estimate of the baseline cumulative hazard function for the entire sample.
2. Deviance Residuals Di “ signp ˆ
Miq
c
2
”
δi log
´
δi
ˆ
Λ0pYiq
¯
´ ˆ
Mi
ı
, for the ith observation, have a less
symmetric distribution than Martingale residuals [69]. LeBlanc and Crowley [45] also demonstrated
that (i) using deviance residuals in regression trees is similar to the survival tree methods presented
by Segal [62] and Ciampi et al. [11], and that (ii) using deviance residuals is more eﬃcient than using
Martingale residuals with regression trees.
2.2.6
Box End-Point Statistics
Below is a summary of box end-point statistics of interest one can derive in our Survival Bump Hunting
method. Each is deﬁned for each step pm, lq and all are implemented in our R package PRIMsrc [16]:
1. Log Hazards Ratios (LHR), denoted λpm, lq between the highest-risk group/box and lower-risk
groups/boxes of the same generation.
2. Log-Rank Test statistic (LRT), denoted χpm, lq between the highest-risk group/box and lower-risk
groups/boxes of the same generation.
3. Concordance Error Rate (CER), denoted θpm, lq in the highest-risk group/box, that is a prediction
performance metric taking censoring into account. For each step pm, lq, θpm, lq “ 1 ´ Cpm, lq, where
C is Harrel’s Concordance Index for censored data [34], a rank correlation U-statistic, to estimate
the probability of concordance between predicted and observed survival times.
4. Event-Free Probability (EFP), denoted P0pm, lq or probability of non-event until a certain time
Tpm, lq in the highest-risk group/box (Figure 2 left). For instance, the Probability of Event-Free
Survival (PEFS) or the Survival Rate (SR) are frequently used. However, P0pm, lq may not always
be reached for a speciﬁed time Tpm, lq. In this case, we determine the limit end-point P 1
0pm, lq or
Minimal Event-Free Probability (MEFP) and corresponding maximal time T 1pm, lq, which are always
observable (see Figure 2 left).
5. Event-Free Time (EFT), denoted T0pm, lq or time to reach a certain end-point probability Ppm, lq
in the highest-risk group/box (Figure 2 right). For instance, the Median Survival (MS) is frequently
used to indicate the period of time where 50% of subjects have reached survival. However, T0pm, lq
may not always be reached for a certain probability Ppm, lq. In this case, we determine the limit
end-point T 1
0pm, lq or Maximal Event-Free Time (MEFT) and corresponding minimal probability
P 1pm, lq, which are always observable (see Figure 2 right).
6. Box characteristics:
• 2p box edges
”
t´
j pm, lq, t`
j pm, lq
ıp
j“1,
• box support (mass) βpm, lq
• box membership indicator γpm, lq
7. Traces of Covariate Usage V Upm, lq and Covariate Importance V Ipm, lq
8. Kaplan–Meir curves of survival probability values with log-rank test permutation p-values ppm, lq
12

Figure 2: Survival end points statistics used in Survival Bump Hunting at each step pm, lq of the box generation. Left: Event-
Free Probability P0 and Minimal Event-Free Probability (MEFP) P 1
0. Right: Event-Free Time T0 and Maximal Event-Free
Time (MEFT) T 1
0. Subscripts pm, lq are dropped for simpliﬁcation but understood.
2.2.7
Estimation by Patient Recursive Survival Peeling
The strategy employed here is a recursive peeling algorithm for survival bump hunting. Our “Patient
Recursive Survival Peeling” method proceeds similarly ato which it is done in PRIM except for the box
induction peeling/pasting criteria and the induction stopping rule (see section 2.1.4):
Algorithm 1 Patient Recursive Survival Peeling (annotated below w.l.o.g for a maximization problem).
• Start with the training data Lp1q and a maximal box ˆB1 containing it
• For m P t1, . . . , Mu:
1: Generate a box ˆBm using the remaining training data Lpmq
2: For l P t1, . . . , Lu:
– Top-down peeling: Generate a box ˆBm,l by conducting a stepwise covariate selection/usage:
shrink the box by compressing one face (peeling), so as to peel oﬀa quantile α0 of observations
of a covariate xj for j P t1, . . . , pu. Choose the direction of peeling j that yields the largest box
increase rate ˆrpm, lq of the statistic used as peeling criterion between box ˆBm,l´1 and Bm,l in
the next generation: Log-Rank Test ˆχLRT pm, lq, Cumulative Hazard Summary ˆΛCHSpm, lq,
Log Hazards Ratio ˆλLHRpm, lq. The current box ˆBm,l´1 is then updated: ˆBm,l “ ˆBm,l´1zˆb˚
m,l,
where ˆb˚
m,l “ argmax
ˆbm,lPCpbm,lq
rˆrpm, lqs
– Bottom-up pasting: Expand the box along any face (pasting) as long as the resulting box
increase rate ˆrpm, lq ą 0
– Stop the peeling loop until a minimal box support ˆβm,L of ˆBm,L is such that it reached a
minimal box support 0 ď β0 ď 1, expressed as a fraction of the data: ˆβm,L ď β0
– l Ð l ` 1
3: Step #2 give a sequence of nested boxes t ˆBm,luL
l“1, where L is the estimated number of
peeling/pasting steps with diﬀerent numbers of observations in each box.
Call the next box
ˆBm`1 “ ˆBm,L. Remove the data in box ˆBm from the training data: Lpm`1q “ Lpmqz ˆBm
4: Stop the covering loop when running out of data or when a minimal number of observations
remains within the last box ˆBM, say ˆβM ď β0
5: m Ð m ` 1
• Steps #1 – #5 produce a sequence of (not necessarily nested) boxes t ˆBmuM
m“1, where M is the
estimated total number of boxes covering Lp1q
• Collect the decision rules of all boxes t ˆBmuM
m“1 into a simple ﬁnal decision rule ˆR of the solution
region ˆR of the form: ˆR “ ŤM
m“1 ˆRm, where ˆRm “ Ş
jPJpxj P rt´
j,m, t`
j,msq giving a full description of
the estimated bumps in the entire input space
13

3
Cross-Validation for Recursive Peeling Methods and a Survival/Risk
Outcome
3.1
Split-Sample-Validation
3.1.1
Setup
We previously tested the possibility of ﬁnding survival bumps in a small dataset, namely the Veteran’s
Administration lung cancer trial data from Kalbﬂeisch and Prentice [42]. We could unravel interesting
subgroups of patients with a poor survival time that could be characterized by a set of descriptive rules
on the predictors including treatment intervention.
Typically, this was indicative that an alternative
intervention therapy could be required for these non-responders. While this approach showed promising
results, it remained naive in that possible issues of bias and overﬁtting were not kept in check by model
validation.
Assessment of model performance (e.g. prediction accuracy) requires the use of separation of the whole
data L between a “training set” L∖t used to build a model and an independent “testing set” Lt used
to assess model performance. To do so, the Split-Sample Validation technique (a.k.a Full Validation) is
possible. Using this approach, a model is entirely developed on the training set L∖t. Then, samples in
the independent testing set Lt are used to determine the error rates. The samples in the testing set are
never to be used for any aspect of model development such as variable selection and calibration and can
therefore be used to check model performance [55, 71].
Here, cross-validation of box estimates should include all steps of the box generation sequence tBmuM
m“1
i.e. for the (outer) coverage loop of our “Patient Recursive Survival Peeling” method (Algorithm 1), each
step of which involves a peeling sequence tBm,luL
l“1 of the (inner) box peeling/induction loop. However,
for simplicity, cross-validation designs of box estimates and resulting decision rule ˆRm are shown below for
ﬁxed m P t1, . . . , Mu of the complete box sequence tt ˆBm,luL
l“1uM
m“1, so that subscript m is further dropped.
Without loss of generality, ﬁx m “ 1 (ﬁrst coverage box).
3.1.2
Estimated Box Quantities of Interest
Using previous notation, if we let ˆBl be the lth trained box and ˆβl be its estimated box support for
l P t1, . . . , Lu of a box peeling sequence t ˆBluL
l“1, then the test-set mean estimate of a box quantity of
interest q for the lth peeling step is indexed by the lth test box support ˆβt
l as follows:
qpˆβt
l q “
1
nt ˆβt
l
nt
ÿ
i“1
ˆqt
iI
´
xt
i P ˆBl
¯
(14)
where qp¨q is the functional corresponding to the quantity q, ˆβt
l “ 1
nt
řnt
i“1 I
´
xt
i P ˆBt
l
¯
and ˆqt
i, xt
i, nt are
test-set quantities. Useful test-set quantities for the highest-risk box are box end-point statistics mentioned
in section 2.2.6.
3.2
K-fold Cross-Validation
3.2.1
Resampling Design - Notation
Although using a fully independent test set for evaluating a predictive bump hunting model is always
advisable, the sample size n in discovery-based studies is often too small to eﬀectively split the data into
training and testing sets and provide accurate estimates [6, 22, 64]. In such cases, resampling techniques
such as K-fold Cross-Validation (CV) are required [2, 55].
In resampling based on full K-fold cross-validation, the whole data L is randomly partitioned into
K approximately equal parts of test samples or test-sets pL1, . . . , Lk, . . . , LKq. For each test-set Lk, for
k P t1, . . . , Ku, a training set Lpkq is formed from the union of the remaining K ´ 1 subsets: Lpkq “
L ∖Lk. The process is repeated K times, so that K test-sets Lk are formed of about equal size and K
corresponding training subsets Lpkq, for k P t1, . . . , Ku. Typically, K P t3, . . . , 10u. The training samples
are approximately of size « npK ´ 1q{K and the test samples are of size nt « n{K.
14

3.2.2
Cross-Validation Techniques
Recently, we described a cross-validation technique for recursive peeling methods in a survival/risk setting
[14]. The subject of this section is to give a more in-depth development of this strategy and compare it to
standard cross-validation techniques.
There are issues when dealing with K-fold CV: ﬁrst, how to cross-validate a simple peeling trajectory
t ˆBluL
l“1 and related statistics is not straightforward; second, how to cross-validate survival curve estimates
and related statistics is also not intuitive (see also [65]); third, the data splitting in the cross-validation
step should balance the class distributions of the outcome (i.e. here the censoring rate) within the cross-
validation splits, which we call “stratiﬁed random splitting by conservation of events”. So, regular K-fold
cross-validation is not directly applicable to the joint task of box decision rules making by recursive peeling
and survival estimation. One must design a speciﬁc cross-validation technique(s) of survival bump hunting
that is amenable to this joint task.
Hence, we propose two techniques by which K-fold cross-validation estimates can be computed:
• Averaging Technique: Estimations are ﬁrst computed for each “in-box” test subset samples, then
averaged over the cross-validation loops of random splitting to give the “Averaged Cross-Validation”
estimates (see details in section below 3.3).
• Combining Technique: All “in-box” test subset samples are ﬁrst collected from all the cross-validation
loops of random splitting to build a combined test “in-box” and corresponding combined test “in-
box” samples to compute once the ﬁnal “Combined Cross-Validation” estimates (see details in section
below 3.4).
Note that, unlike in the averaging technique, cross-validated combined estimates are computed on test
samples of size n instead of nt « n{K, which could be an advantage in the case of tiny sample size n.
In our numerical analyses, both strategies were compared with each other and with the situation of no
cross-validation (see result section 4.3).
Finally, to account for the high variability of cross-validated estimates [22, 24, 51], we iterate each cross-
validation procedure several times over some replicates B (typically, B ě 10) to average the estimates
and reduce their variance. This so-called “Replicated Cross-Validation” approach is further detailed in
section below (3.6). Also, aside the Split-Sample-Validation, mentioned in the previous section (3.1), other
resampling techniques are available, which we left for discussion (section 6).
3.2.3
Model Peeling Length Optimization Criterion
In model tuning, a trade-oﬀbetween under-ﬁtting and over-ﬁtting can be achieved by optimizing an
empirical function or objective criterion that takes censoring into account using cross-validation.
The
“optimization criterion” that we derive below is adapted to the task of of ﬁtting a survival bump hunting
model by recursive peeling with a survival outcome. Speciﬁcally, we tune a peeling model by optimizing
its complexity, that is, the ﬁnal length or number of peeling steps L of the peeling sequence. The reason
is that, for a given set of variables/covariates, the ﬁnal length L of a peeling model only depends on the
peeling meta-parameters α0 (assumed ﬁxed here) and β0 (see section 2.1.4). In fact, an upper bound on
the length of all possible peeling trajectories is given by Lα0,β0 “
Q
logpβ0q
logp1´α0q
U
(see [31] for details). So, a
cheaper cross-validation can be achieved on L only rather than on α0 and β0 simultaneously.
Assuming m ﬁxed (see step #2 of Algorithm 1) and dropping subscript m for simpliﬁcation, the process
of model building is repeated K times for k P t1, . . . , Ku as follows. First, let lpkq denote the lth peeling
step in the kth trajectory for l P t1, . . . , Lpkqu and k P t1, . . . , Ku, where Lpkq denotes the ﬁnal length of
a trained peeling model. Note that Lpkq ď Lα0,β0, for all k P t1, . . . , Ku, but, in general this inequality
is strict for large enough sample sizes. Let ˆBlpkq be the trained box of support ˆβlpkq of the box peeling
sequence t ˆBlpkquL
l“1 that is constructed from training set Lpkq, leaving out the test-set Lk during all aspects
of model building including covariate selection.
Second, once a resulting trained decision rule, abbreviated Rk, and box deﬁnition estimates are gener-
ated from each training set Lpkq, cross-validated estimates of box end-points statistics (described in 2.2.6)
15

are computed using the left-out test-set Lk. Three of these are the Log Hazard Ratio (LHR), Log-Rank
Test (LRT) and the cross-validated estimate of prediction performance, namely the Concordance Error
Rate (CER) that is obtained by calculating the test-set error rate using the left-out test-set Lk.
In the subsequent sections, we denote by superscript cv any cross-validated estimate on the test-set
Lk.
Since peeling lengths Lpkq are not necessarily equal for all k P t1, . . . , Ku, we use the following
cross-validated maximum peeling length ˆLcv
m over the K trajectories:
ˆLcv
m “
min
kPt1,...,Ku rLpkqs
(15)
After K rounds of training and testing are complete and (averaged or combined) test proﬁles of LHR,
LRT or CER estimates are determined for each step l P t1, . . . , ˆLcv
mu, model tuning is done by determining
the optimal peeling length ˆLcv of the peeling sequence. To that end, one uses the maximization of the
(averaged or combined) test proﬁles of LHR and LRT or the minimization of the (averaged or combined)
test proﬁle of CER as criterion. Formally:
ˆLcv “ argmax
lPt1,...,ˆLcv
m u
”
ˆλcvplq
ı
or
ˆLcv “ argmax
lPt1,...,ˆLcv
mu
rˆχcvplqs
or
ˆLcv “ argmin
lPt1,...,ˆLcv
mu
”
ˆθcvplq
ı
,
(16)
where ˆλcvplq is the cross-validated LHR) in the high-risk box at step l, ˆχcvplq is the cross-validated LRT
between the high vs. low-risk box at step l and ˆθcvplq is the cross-validated CER between high-risk box
predicted and observed survival times at step l.
Depending on the desired degree of conservativeness, the usual one-standard-error rule [36] may be
applied in combination with the proﬁles minimizer or maximizer to get smaller estimates corresponding to
one standard-error below the maximum of LHR and LRT or standard-error above the minimum of CER.
In the subsequent sections, we denote by superscript cv any cross-validated estimate on the test-set Lk.
3.3
K-fold Averaged Cross-Validation
In K-fold Averaged Cross-Validation, the averaged cross-validated estimate of a box quantity q at the
lpkqth step of the box peeling sequence is based on the test samples falling within the trained box ˆBlpkq.
The averaged cross-validated estimate of q at step l is simply computed by averaging the estimates obtained
from all test boxes computed over all K cross-validation loops. Speciﬁcally, each test-set Lk is used to
estimate the lpkqth test box membership indicator
ˆγt
lpkq from the model grown on the training set Lpkq.
The corresponding test box support ˆβt
lpkq is directly
derived from ˆγt
lpkq by computing the fraction of test
data falling within the trained box ˆBlpkq. The lpkqth
estimate of the box quantity q is indexed by the corre-
sponding test box support ˆβt
lpkq. For each training set
Lpkq, a trajectory curve qpxq of a box quantity q is de-
ﬁned as a piecewise constant curve, evaluated at the
lpkqth test box support ˆβt
lpkq, so that each trajectory
curve is: qpxq “ qpˆβt
lpkqq for ˆβt
lpkq`1 ď x ď ˆβt
lpkq (Fig-
ure 3), where qpˆβt
lpkqq is derived as in equation 14. The
averaged CV trajectory curve ˆqcvpxq of length ˆLcv
m is
simply the average of the K trajectory curves over the
K cross-validation loops:
ˆqcvpxq “
1
K
řK
k“1 qpˆβt
lpkqq
for ˆβt
lpkq`1 ď x ď ˆβt
lpkq.
Figure 3: Example of decreasing trajectory curve qpxq of
a box quantity q. Notice the piecewise constant curve of
qpxq for ˆβt
lpkq`1 ď x ď ˆβt
lpkq. By convention, we deﬁne
ˆβt
0 “ 1, ˆβt
Lpkq “ β0 and ˆβt
Lpkq`1 “ 0.
16

Formally, we show below how things are computed from an initial set of K trained peeling trajectories:
For
the
kth
training
set
Lpkq:
kth
training
trajectory with
box deﬁnitions
Ñ
#
kth
test
box
membership
indicators
Ñ
$
’
&
’
%
kth
test
box
supports
Ñ
#
kth test box es-
timated quanti-
ties
Ñ
#
Averaged
CV
test box quan-
tities
over
K
trajectories
Ñ
#
First peeling
step in the kth
trajectory
direction of peeling
Last peeling
step in the kth
trajectory
Ñ
1pkq
lpkq
Lpkq
ˆB1pkq
¨ ¨ ¨
ˆBlpkq
¨ ¨ ¨
ˆBLpkq
ˆγt T
1pkq “
”
ˆγt
i,1pkq
ınt
i“1
¨ ¨ ¨
ˆγt T
lpkq“
”
ˆγt
i,lpkq
ınt
i“1
¨ ¨ ¨
ˆγt T
Lpkqq “
”
ˆγt
i,Lpkq
ınt
i“1
“
”
Irxt
i P ˆB1pkqs
ınt
i“1
“
”
Irxt
i P ˆBlpkqs
ınt
i“1
“
”
Irxt
i P ˆBLpkqs
ınt
i“1
ˆβt
1pkq “ 1
nt
nt
ÿ
i“1
ˆγt
i,1pkq
¨ ¨ ¨
ˆβt
lpkq“ 1
nt
nt
ÿ
i“1
ˆγt
i,lpkq
¨ ¨ ¨
ˆβt
Lpkq “ 1
nt
nt
ÿ
i“1
ˆγt
i,Lpkq
q
´
ˆβt
1pkq
¯
¨ ¨ ¨
q
´
ˆβt
lpkq
¯
¨ ¨ ¨
q
´
ˆβt
Lpkq
¯
ˆqcvp1q“ 1
K
K
ÿ
k“1
q
´
ˆβt
1pkq
¯
¨ ¨ ¨ ˆqcvplq“ 1
K
K
ÿ
k“1
q
´
ˆβt
lpkq
¯
¨ ¨ ¨ ˆqcvpˆLcv
mq“ 1
K
K
ÿ
k“1
q
´
ˆβt
ˆLcv
m
¯
From the K test trajectories, one derives ﬁrst the “Averaged CV” optimal peeling length of the peeling
trajectory, according to the optimization criterion for model selection as in equation 16:
ˆLcv “ argmax
lPt1,...,ˆLcv
m u
”
ˆλcvplq
ı
or
ˆLcv “ argmax
lPt1,...,ˆLcv
mu
rˆχcvplqs
or
ˆLcv “ argmin
lPt1,...,ˆLcv
mu
”
ˆθcvplq
ı
Then, one derives “Averaged CV” estimates for each step l P t1, . . . , ˆLcvu as follows:
• The “Averaged CV” box deﬁnition, which can be written as the outer product of |J| intervals as in
equation 3, is formed by taking the rectangular box where each of the 2|J| edge is averaged over the
K cross-validation loops:
ˆBcvplq “
ą
jPJ
rˆt´
j,l, ˆt`
j,ls
where for each j P J,
$
’
&
’
%
ˆt´
j,l “
ave
kPt1,...,Ku rˆt´
j,lpkqs
ˆt`
j,l “
ave
kPt1,...,Ku rˆt`
j,lpkqs
• The “Averaged CV” box membership indicator is formed by counting the data within the “Averaged
CV” box:
ˆγcv T plq “
”
Irxi P ˆBcvplqs
ın
i“1
• The “Averaged CV” box support is computed as the fraction of data within the “Averaged CV” box:
ˆβcvplq “ 1
n
nÿ
i“1
Irxi P ˆBcvplqs
17

• The “Averaged CV” box end-point quantity q is taken as the averaged CV trajectory curve evaluated
at the lpkqth test box support ˆβt
lpkq:
ˆqcvplq “ 1
K
K
ÿ
k“1
q
´
ˆβt
lpkq
¯
, where
q
´
ˆβt
lpkq
¯
“
1
nt ˆβt
lpkq
nt
ÿ
i“1
ˆqt
iI
´
xt
i P ˆBlpkq
¯
as in equation 14.
The latter is done for the “Averaged CV” box estimates of: (i) The Log Hazard Ratio (LHR) in
the high-risk box: ˆλcvplq “
1
K
řK
k“1 λ
´
ˆβt
lpkq
¯
; (ii) The Log-Rank Test (LRT) between the high vs.
low-risk box: ˆχcvplq “ 1
K
řK
k“1 χ
´
ˆβt
lpkq
¯
; (iii) The Concordance Error Rate (CER) between high-risk
box predicted and observed survival times: ˆθcvplq “ 1
K
řK
k“1 θ
´
ˆβt
lpkq
¯
; (iv) The Minimal Event-Free
Probability (MEFP): x
P 1
0
cvplq “
1
K
řK
k“1 P 1
0
´
ˆβt
lpkq
¯
; (v) The Minimal Event-Free Time (MEFT):
x
T 1
0
cvplq “ 1
K
řK
k“1 T 1
0
´
ˆβt
lpkq
¯
.
3.4
K-fold Combined Cross-Validation
In K-fold Combined Cross-Validation, for each loop, samples from the training set are used to train a
peeling model of a certain length, then samples from the test-set are used to determine the “in-box” test-
set samples falling into the trained box. Eventually, all “in-box” test samples are combined together and
all “out-of-box” test samples are combined together as well. So, in K-fold Combined CV, estimate of a box
quantity q is computed once on the collective test-set “in-box” samples, formed over the K cross-validation
loops. This allows the estimation of box quantities and box survival distribution curves for both “in-box”
and “out-of-box” samples.
Speciﬁcally, each test-set Lk is used to estimate the test box membership indicator ˜γt
lpkq from the
model grown on the kth training set Lpkq. The lth combined cross-validated test box membership indicator
˜γcvplq is formed once by taking the vector concatenation of all the cross-validated test box membership
indicators t˜γt
lpkquK
k“1 over the K cross-validation loops. The corresponding lth combined cross-validated
test box support ˜βcvplq is then directly derived from ˜γcvplq.
The combined cross-validated estimate of a box quantity q at the lth step of the peeling trajectory is
then computed once from the combined cross-validated test box membership indicator ˜γplqcv and indexed
by the corresponding test box support ˜βplqcv. Here, the combined cross-validated trajectory curve ˜qkpxq is
deﬁned as the piecewise constant curve of length ˜Lcv
m, evaluated at the lth combined cross-validated test
box membership indicator ˜γcvplq.
Formally, we show below how things are computed from an initial set of K trained peeling trajectories
(where f denotes the concatenation operator).
18

For
the
kth
training
set
Lpkq:
kth
training
trajectory with
box deﬁnitions
Ñ
#
kth
test
box
membership
indicators
Ñ
#
Combined
CV
test box mem-
bership indica-
tors over K tra-
jectories
Ñ
$
’
’
’
’
&
’
’
’
’
%
Combined
CV
test
box
sup-
ports
over
K
trajectories
Ñ
#
Combined
CV
test box quanti-
ties over K tra-
jectories
Ñ
#
First peeling
step in the kth
trajectory
direction of peeling
Last peeling
step in the kth
trajectory
Ñ
1pkq
lpkq
Lpkq
˜B1pkq
¨ ¨ ¨
˜Blpkq
¨ ¨ ¨
˜BLpkq
˜γt T
1pkq“
”
Irxt
i P ˜B1pkqs
ınt
i“1 ¨ ¨ ¨ ˜γt T
lpkq“
”
Irxt
i P ˜Blpkqs
ınt
i“1 ¨ ¨ ¨ ˜γt T
Lpkq“
”
Irxt
i P ˜BLpkqs
ınt
i“1
˜γcvp1q“r˜γcv
i p1qsn
i“1
¨ ¨ ¨
˜γcvplq“r˜γcv
i plqsn
i“1
¨ ¨ ¨
˜γcvp˜Lcv
mq“
”
˜γcv
i p˜Lcv
mq
ın
i“1
“
Kn
k“1
˜γt
1pkq
“
Kn
k“1
˜γt
lpkq
“
Kn
k“1
˜γt
˜Lcv
m
˜βcvp1q“ 1
n
n
ÿ
i“1
˜γcv
i p1q ¨ ¨ ¨ ˜βcvplq“ 1
n
nÿ
i“1
˜γcv
i plq ¨ ¨ ¨ ˜βcvp˜Lcv
mq“ 1
n
nÿ
i“1
˜γcv
i p˜Lcv
mq
˜qcvp1q“q
”
˜β
cvp1q
ı
¨ ¨ ¨
˜qcvplq“q
”
˜β
cvplq
ı
¨ ¨ ¨
˜qcvp˜Lcv
mq“q
”
˜β
cvp˜Lcv
mq
ı
From the K test trajectories, one derives ﬁrst the “Combined CV” optimal peeling length of the peeling
trajectory, according to the optimization criterion as in equation 16:
˜Lcv “ argmax
lPt1,...,˜Lcv
m u
”
˜λcvplq
ı
or
˜Lcv “ argmax
lPt1,...,˜Lcv
mu
r˜χcvplqs
or
˜Lcv “ argmin
lPt1,...,˜Lcv
mu
”
˜θcvplq
ı
Likewise, from the K test trajectories, one derives “Combined CV” estimates for each step l P
t1, . . . , ˜Lcvu as follows:
• The “Combined CV” box membership indicator (Boolean n-vector) is formed by vector-concatenation
of all the test box membership indicators over the K cross-validation loops:
˜γcv T plq “ r˜γcv
i plqsn
i“1 “
Kn
k“1
˜γt
lpkq “
Kn
k“1
”
Irxt
i P ˜Blpkqs
ınt
i“1
• The “Combined CV” box deﬁnition, which can be written as the outer product of |J| intervals as in
equation 3, is formed by taking the rectangular box (2|J| edges) circumscribing all the “in-box” test
samples over the K cross-validation loops:
˜Bcvplq “
ą
jPJ
r˜t´
j,l, ˜t`
j,ls
where for each j P J,
$
’
&
’
%
˜t´
j,l “
min
kPt1,...,Kurxt
i,j, i P t1, . . . , ntu: xt
i,j P ˜Blpkqs
˜t`
j,l “
max
kPt1,...,Kurxt
i,j, i P t1, . . . , ntu: xt
i,j P ˜Blpkqs
19

• The “Combined CV” box support is computed as the fraction of data within the “Combined CV”
box:
˜βcvplq “ 1
n
nÿ
i“1
˜γcv
i plq
• The “Combined CV” box end-point quantity q is taken as the result of the functional qp¨q evaluated
at the lth “Combined CV” test box support ˜βcvplq:
˜qcvplq “ q
”
˜β
cvplq
ı
The latter is done for the “Combined CV” box estimates of: (i) The Log Hazard Ratio (LHR) in
the high-risk box: ˜λcvplq “ λ
”
˜β
cvplq
ı
; (ii) The Log-Rank Test (LRT) between the high vs. low-risk
box: ˜χcvplq “ χ
”
˜β
cvplq
ı
; (ii) The Concordance Error Rate (CER) between high-risk box predicted
and observed survival times: ˜θcvplq “ θ
”
˜β
cvplq
ı
; (iv) The Minimal Event-Free Probability (MEFP):
Ă
P 1
0
cvplq “ P 1
0
”
˜β
cvplq
ı
; (v) The Minimal Event-Free Time (MEFT): Ă
T 1
0
cvplq “ T 1
0
”
˜β
cvplq
ı
.
3.5
K-fold Cross-Validation of P-Values
The log-rank test statistic (e.g. χ2
1 for a two group comparison) is a classical measure to evaluate the
statistical signiﬁcance of separation between survival curves. However, the null distribution of the log-rank
test is not valid for cross-validated curves because the observations used to cross-validate the curves are
not independent anymore.
For each step l P t1, . . . , ˜Lrcvu, we generate the null distribution of the cross-validated log-rank statistic
˜χcvpaqplq for a P t1, . . . , Au by randomly permuting the correspondence of survival times and censoring
indicators of the data and by computing the corresponding cross-validated survival curves and cross-
validated log-rank statistic for that permutation. By repeating A times the entire K-fold cross-validation
process for many random permutations (typically A “ 1000), one generates a null distribution of the
permuted log-rank statistics (annotated below w.l.o.g. for the case of “Combined CV”):
t˜χcvpaqplquA
a“1
The proportion of replicates with log-rank statistic greater than or equal to the observed statistic ˜χcvplq
for the un-permuted data is the statistical signiﬁcance level for the test. Log-rank test permutation p-values
are then calculated for each step l P t1, . . . , ˜Lrcvu as:
˜pcvplq “ 1
A
A
ÿ
a“1
I
”
˜χcvpaqplq ě ˜χcvplq
ı
(17)
These p-values may be discrete: the precision depends on the number A of random permutations and
the lower bound 1{A may be reached in practise.
3.6
Replicated K-fold Cross-Validation
Typically, K-fold cross-validation is repeated B “ 10 ´ 100 times and resulting replicated cross-validated
estimates are somehow “averaged” over the replicates. We denote these by the superscript rcv and each
replicate by the superscript cvpbq, for b P t1, . . . , Bu. This is done for either cross-validation technique
(shown below w.l.o.g. for the case of “Combined CV”).
Formally, one ﬁrst derives the “Replicated CV” maximal peeling length of the peeling model from the
cross-validation replicates, denoted ¯Lrcv
m . To do so, one uses the cross-validated maximum peeling length
20

ˆLcvpbq
m
of the peeling trajectory, deﬁned in equation 15, for b P t1, . . . , Bu. Formally, the “Replicated CV”
maximal peeling length ¯Lrcv
m is calculated as the ceiling-mean of the cross-validated quantities ˆLcvpbq
m
:
¯Lrcv
m “
S
1
B
B
ÿ
b“1
ˆLcvpbq
m
W
(18)
Next, depending on the optimization criterion used, one gets the “Replicated CV” optimal length of
the peeling trajectory:
¯Lrcv “ argmax
lPt1,...,¯Lrcv
m u
“¯λrcvplq
‰
or
¯Lrcv “ argmax
lPt1,...,¯Lrcv
m u
r¯χrcvplqs
or
¯Lrcv “ argmin
lPt1,...,¯Lrcv
m u
“¯θrcvplq
‰
(19)
where each optimization criterion: (i) The Log Hazard Ratio (LHR) in the high-risk box: ¯λrcvplq, (ii)
The Log-Rank Test (LRT) between the high vs. low-risk box: ¯χrcvplq, and (iii) The Concordance Error
Rate (CER) between high-risk box predicted and observed survival times: sθrcvplq is taken as the average
estimate over the B replicates for each step l P t1, . . . , ¯Lrcv
m u as follows:
¯λrcvplq “ 1
B
B
ÿ
b“1
˜λcvpbqplq
or
¯χrcvplq “ 1
B
B
ÿ
b“1
˜χcvpbqplq
or
¯θrcvplq “ 1
B
B
ÿ
b“1
˜θcvpbqplq
(20)
Using the above “Replicated CV” optimal length of the peeling trajectory, one ﬁnally derives “Repli-
cated CV” box end points from the B replicates for each step l P t1, . . . , ¯Lrcvu as follows:
• The “Replicated CV” box deﬁnition (2|J| edges) is taken as the average-box over the B replicates:
¯Brcvplq “
ave
bPt1,...,Bu
”
˜Bcvpbqplq
ı
(21)
where avep¨q denotes the averaging function by edge or dimension j for j P J.
• The “Replicated CV” box membership indicator (Boolean n-vector) is taken as the average-box mem-
bership indicator, observed to be nearly equal to the point-wise majority vote over the B replicates:
¯γrcvplq “
“
Irxi P ¯Brcvplqs
‰n
i“1 «
«
I
˜ B
ÿ
b“1
˜γcvpbq
i
plq ě
RB
2
V¸ﬀn
i“1
(22)
• The “‘Replicated CV” box support is taken as the average estimate over the B replicates:
¯βrcvplq “ 1
B
B
ÿ
b“1
˜βcvpbqplq
(23)
• Other “Replicated CV” box end-point quantities q estimates, taken as the average estimate over the
B replicates:
¯qrcvplq “ 1
B
B
ÿ
b“1
˜qcvpbqplq
(24)
This is done for: (i) The Minimal Event-Free Probability (MEFP): Ď
P 1
0
rcvplq and (ii) The Minimal
Event-Free Time (MEFT): Ď
T 1
0
rcvplq.
21

4
Numerical Analyses
4.1
Simulation Design
The p-dimensional covariates xi “ rxi,1 . . . xi,psT , for i P t1, . . . , nu, were identically and independently
drawn from either: a (i) p-multivariate normal distribution with mean vector µ and variance-covariance
matrix Σ: xi „ Nppµ, Σq; or (ii) from a p-multivariate uniform distribution on the interval ra, bs: xi „
Uppa, bq.
Simulations were carried out according to the assumptions stated in section 2.2.2. Simulated realizations
of true survival times Ti were identically and independently drawn from an exponential distribution with
rate parameter λ (and mean 1
λ): Ti „ Exppλq. Simulated realizations Ci of true censoring times were
identically and independently sampled from a uniform distribution: Ci „ Up0, vq with v ą 0, so that
approximately 100 ˆ πp%q of the simulated realizations of observed survival times Yi “ minpTi, Ciq were
censored, where π P t0.3, 0.5, 0.7u. Finally, the simulated realizations of observed event (non-censoring)
random variable indicator were δi “ IpTi ď Ciq.
To simulate survival models with various types of relationship between survival times (or hazards)
and covariates (i.e. variable informativeness) including saturated regression models and noise models, the
individual hazard rate parameter λi was simulated as an exponential regression function of individual
covariate xi: λipt|xiq “ λ0ptq exp rηpxiqs, where the regression function ηpxiq can take diﬀerent forms
depending on the simulated survival model. In summary, our simulation was done using the following
parameters (see documentation in our R package for more details [16]):
• xi „ Upp0, 1q with n “ 250 and p “ 3, or xi „ Nppr0 0 0sT , σ2Iq with n “ 100 and p “ 1000.
• by characterization of the ﬁrst coverage box B1 (i.e. for m “ 1), using constrained/directed peeling,
without pasting and with meta-parameter values pα0, β0q P tp0.10, 0.05qu.
• with censoring rate π = 0.5 and ﬁve concurrent simulated survival models, where n ą p and n ! p,
representing low- and high-dimensional situations, and where the regression function is as follows:
– In simulated models #1-4, ηpxiq “ ηT xi with regression parameters η “ rη1 . . . 0j . . . ηpsT , for
j P ∅Y t1, . . . , pu:
$
’
’
’
’
&
’
’
’
’
%
Low-dim. Saturated Model #1:
n “ 250, p “ 3
η “ r12 ´ 15 ´ 5sT
Low-dim. Un-saturated Model #2:
n “ 250, p “ 3
η “ r12 ´ 15 0sT
Low-dim. Noise Model #3:
n “ 250, p “ 3
η “ r0 0 0sT
High-dim. Un-saturated Model #4:
n “ 100, p “ 1000
η “ rη1 . . . η100 0 . . . 0sT
– In simulated model #1b, a Low-dim. Saturated Model with n “ 250 and p “ 3, where
ηpxiq “
#
ηT xi
with regression parameters η “ r12 ´ 15 ´ 5sT
for
xi P R
ui „ Up0, 1q
for
xi R R
where R “ r0.7, 1s ˆ r0, 0.2s ˆ r0, 0.4s is an arbitrary box in R3.
• using K “ 5-fold cross-validation, A “ 1024 for the permutation p-values and B “ 128 independent
replications.
4.2
Summary of Outputs
We explain below how the main diagnostic and descriptive output plots are used.
22

4.2.1
Cross-Validated Tuning Proﬁles
Cross-validated tuning proﬁles plot values of the box end-points statistics (section 2.2.6) Log Hazard Ratio
(LHR), Log-Rank Test (LRT) or Concordance Error Rate (CER), depending on the optimization criterion
chosen, as a function of peeling length or peeling steps of the peeling trajectory (model complexity). A
peeling step includes step #0 corresponding to the situation where the starting box covers the entire
test-set data Lk before peeling (Algorithm 1). These statistics are used internally or interactively to get
the “Replicated CV” optimal length of the peeling trajectory: ¯Lrcv (section 3.6). In order to successfully
determine the proﬁles minimizer or maximizer (section 3.2.3), the cross-validated tuning proﬁle should be
approximately non-monotone up to sampling variability (Figure 4). In addition, one expects an inﬂation
of variance of cross-validated point estimates towards the right-end of the cross-validated tuning proﬁle
corresponding to an increase in overﬁtting and model uncertainty for more complex models (Figure 4,
Supporting Figures 1, 2, 3).
The choice of the optimization criteria for controlling the peeling length is crucial. Two typical situa-
tions of failure of cross-validation can happen from the cross-validated tuning proﬁles of the box end-point
statistics: either an extremum cannot be reached in the proﬁle before the peeling sequence runs out of
data, or the proﬁle is essentially ﬂat due to noise or the absence of any eﬀect in the data (Figure 1). In
the former case, this results in excessive peeling steps and cross-validated values of optimal peeling lengths
¯Lrcv (eq. 19) at, or near, the maximal peeling lengths ¯Lrcv
m
(eq. 18). In the latter case, this results in
un-reliable optimal peeling lengths ¯Lrcv that can take any value between the r1, ¯Lrcv
m s boundaries. In both
cases, this leads to likely under-ﬁtted or over-ﬁtted models (see asterisk-annotated models in Table 1).
Peeling Steps
LRT Mean Profiles
0
2
4
6
8
11
14
17
20
23
26
0
50
100
150
200
Typical Successful 
 CV Tuning Profile
Peeling Steps
LHR Mean Profiles
0
2
4
6
8
11
14
17
20
23
0
5
10
15
20
Typical Failed
 CV Tuning Profile
Peeling Steps
LHR Mean Profiles
0
2
4
6
8
11
14
17
20
23
26
−2
0
2
4
6
8
10
Typical Failed 
 CV Tuning Profile
Figure 4: Illustrations of typical successful (left) and failed (center and right) cross-validated tuning proﬁles of box end-point
statistics. Left: Successful peeling stops with a “Replicated CV” optimal peeling length ¯Lrcv (see eq. 19) reached within
the r1, ¯Lrcv
m s boundaries of possible peeling lengths; Center: Failure to reach a maximum before running out of data; Right:
Failure to reach a reliable maximum because of a ﬂat proﬁle. The “Replicated CV” optimal peeling length ¯Lrcv of the peeling
trajectory is shown in each plot with the vertical black dashed line. Each colored proﬁle corresponds to one of the replications
(B “ 128). The cross-validated mean proﬁle of the statistic used in the optimization criterion is shown by the dotted black
line with standard error of the sample mean.
4.2.2
Peeling Trajectories
Cross-validated peeling trajectories are estimated by step functions of the covariates box cuts as a function
of box support/mass (Figures 5, 7). They are read from right to left as they track the top-down peeling of
the box induction process (peeling loop) of our “Patient Recursive Survival Peeling” method (Algorithm
1). These peeling trajectories are, up to sampling variability:
• Monotone (increasing or decreasing) functions for each input covariate xj, for j P t1, . . . , pu.
• Non-monotone (increasing then decreasing) functions for LHR ¯λrcvplq.
• Non-monotone (increasing then decreasing) functions for LRT ¯χrcvplq.
• Non-monotone (decreasing then increasing) functions of CER ¯θrcvplq.
23

• Monotone decreasing functions for MEFP Ď
P 1
0
rcvplq.
• Monotone decreasing functions for MEFT Ď
T 1
0
rcvplq.
4.2.3
Trace Curves
Cross-validated trace curves of covariate importance and covariate usage are estimated by piece-wise linear
and step functions, respectively, as a function of box support/mass (Figures 6, 8). Similarly to peeling
trajectories, they are read from right to left. Trace curves of covariate importance show on a single plot:
(i) the amplitude of used covariates, (ii) the order (prioritization) with which these covariates are used,
and (iii) the extent of the number of peeling steps by which each covariate is used. Covariate traces are
reminiscent of the concept of variable selection from the ﬁelds of decision tree and regularization, that is:
• In “Variable Importance”, a prediction-based statistics borrowed from the existing theory of decision
trees [9] and their ensemble version [8].
• In “Selective Shrinkage” of variable coeﬃcients/parameters from the existing theory of regularization
and variable selection (e.g. LARS [25], Lasso [70], Elastic Net [74] and Spike & Slab [41]).
4.2.4
Survival Curves
Each subplot of Figure 9, 11 and 13 corresponds to a peeling step of our Patient Recursive Survival Peeling
method for a tested model and cross-validation technique (including none). Each subplot shows cross-
validated Kaplan–Meir estimates of the survival functions, as a function of survival time, of both “in-box”
(red) and “out-of-box” (black) samples, corresponding respectively to the high-risk and low-risk groups.
Each subplot also displays the corresponding step number along with cross-validated Log Hazard Ratio
(LHR), Log-Rank Test (LRT) and log-rank permutation p-value ˜pcvplq of survival distribution separation
(see section 3.5). A single survival curve always exists at Step #0 corresponding to the situation where
the starting box covers the entire test-set data Lk before peeling (Algorithm 1). As the peeling progresses,
the survival curves of “in-box” and “out-of-box” samples further separate until the peeling stops.
4.3
Eﬀect of Model Tuning
4.3.1
Eﬀect of the Optimization Criteria
We ﬁrst compare the eﬀect of the three optimization criteria (Log Hazard Ratio LHR, Log-Rank Test
LRT or Concordance Error Rate CER - section 3.2.3) used for model tuning and selection. Evaluations
are reported on (i) the “Replicated CV” optimal peeling lengths ¯Lrcv (eq. 19) obtained from the cross-
validated tuning proﬁles of the box end-point statistics (Log Hazard Ratio LHR, Log-Rank Test LRT or
Concordance Error Rate CER), and (ii) on the cross-validated numbers of used covariates by the PRSP
algorithm (see Algorithm 1) out of the total number of pre-selected ones. Results are presented in Table
1, Supporting Table 1 and Supporting Figures 1, 2, 3 for the three peeling criteria used (Log Hazard
Ratio LHR, Log-Rank Test LRT or Cumulative Hazard Summary CHS - section 2.2.3) as well as for
our two cross-validation techniques (“Replicated Averaged CV” (RACV) vs. “Replicated Combined CV”
(RCCV)), whether in low- or high-dimensional simulated survival regression models #1, #2, #3 and #4.
From Table 1 and Supporting Figures 1, 2, 3, it results that both Log-Rank Test (LRT) and Con-
cordance Error Rate (CER) optimization criteria give satisfactory results in low-dimensional simulated
models (#1 and #2), other than the simulated noise model (#3), where the peeling length is optimally
pruned (¯Lrcv “ 9 ´ 20). This is in sharp contrast to the Log Hazard Ratio (LHR) optimization criterion
that fails to control the peeling length, regardless of the cross-validation technique or the peeling criterion
used (¯Lrcv “ 26 ´ 27).
The situation diﬀers in high-dimensional simulated models: the Concordance Error Rate (CER) appears
to be the only optimization criterion that reliably controls the peeling length in simulated model #4,
regardless of the peeling criterion or cross-validation technique used (Table 1, Supporting Figures 1, 2, 3).
Finally, note that the Concordance Error Rate CER) optimization criterion tends to yield slightly more
conservative results than the Log-Rank Test (LRT) in terms of peeling length (Table 1, Supporting Figures
24

Table 1: Eﬀect of peeling and optimization criteria as well as cross-validation techniques on the cross-validated tuning proﬁles
of the box end-point statistics (Log Hazard Ratio LHR, Log-Rank Test LRT or Concordance Error Rate CER) and the resulting
“Replicated CV” optimal peeling length ¯Lrcv (see eq. 19). Cross-validated optimal peeling lengths ¯Lrcv are reported for the
combined eﬀects of: (i) the three peeling criteria (by rows: Log Hazard Ratio (LHR), Log-Rank Test (LRT) or Cumulative
Hazard Summary (CHS)), (ii) the three optimization criteria (by columns: Log Hazard Ratio (LHR), Log-Rank Test (LRT) or
Concordance Error Rate (CER)), (iii) the two cross-validation techniques (by columns: “Replicated Averaged CV” or RACV
and “Replicated Combined CV” or RCCV), and (iv) the four tested simulation models (by rows: Model #1, #2, #3 or #4).
Asterisks denote situations where ¯Lrcv reaches either a (quasi-)minimal or (quasi-)maximal value of optimal peeling lengths,
corresponding to failed cross-validations / likely under-ﬁtted or over-ﬁtted models.
Model #1
Optimization Criterion
LHR
LRT
CER
RACV
RCCV
RACV
RCCV
RACV
RCCV
Peeling
LHR
26*
25*
20
20
10
11
Criterion
LRT
26*
25*
17
20
10
10
CHS
26*
26*
14
14
09
09
Model #2
Optimization Criterion
LHR
LRT
CER
RACV
RCCV
RACV
RCCV
RACV
RCCV
Peeling
LHR
26*
25*
17
17
12
12
Criterion
LRT
26*
24*
10
11
10
10
CHS
26*
26*
14
14
10
10
Model #3
Optimization Criterion
LHR
LRT
CER
RACV
RCCV
RACV
RCCV
RACV
RCCV
Peeling
LHR
23*
01
23*
01
05
02
Criterion
LRT
26*
02
26*
02
03
02
CHS
24*
01
23*
02
04
02
Model #4
Optimization Criterion
LHR
LRT
CER
RACV
RCCV
RACV
RCCV
RACV
RCCV
Peeling
LHR
17*
09
16*
06*
05
05
Criterion
LRT
16*
01*
16*
08*
06
08
CHS
16*
01*
16*
08*
05
05
1, 2, 3) and number of used covariates (Supporting Table 1). Also, note that the Concordance Error Rate
CER) has systematically less variance than the other Log-Rank Test (LRT) and Log Hazard Ratio (LHR)
optimization criteria (Table 1, Supporting Figures 1, 2, 3).
For the above reasons, we recommend using the Concordance Error Rate CER as optimization criterion
in every situation or the Log-Rank Test LRT in low-dimensional situation only.
4.3.2
Eﬀect of the Peeling Criteria
Overall, in all simulation models tested, the eﬀect of the peeling criteria, used for model ﬁtting of the
survival bump hunting model, is relatively marginal compared to that of the optimization criterion and/or
cross-validation technique used for model tuning.
However, for any combination of the optimization criterion and cross-validation technique used, both
Log-Rank Test LRT and Cumulative Hazard Summary CHS peeling criteria tend to induce slightly shorter
proﬁles and use less covariates than the Log Hazard Ratio LHR criterion (Table 1, Supporting Table 1,
25

Supporting Figures 1, 2, 3). Moreover, the Cumulative Hazard Summary CHS peeling criterion tends to
be slightly more conservative than the Log-Rank Test LRT (and the Log Hazard Ratio LHR) in terms of
peeling length and number of used covariates, especially in high-dimensional models (Table 1, Support-
ing Table 1, Supporting Figures 1, 2, 3).
So, we recommend using primarily the Cumulative Hazard Summary CHS and secondly the Log-Rank
Test LRT as peeling criterion (in low or high-dimensional data) to reduce the risk of over-ﬁtting (or
conversely the Log Hazard Ratio LHR to avoid excessive conservativeness).
4.3.3
Eﬀect of the Cross-Validation Technique
The diﬀerence of results between cross-validation techniques (including none) is striking in simulated
noise model #3.
Here, only the “Replicated Combined CV” (RCCV) cross-validation technique gives
satisfactory results, regardless of the optimization or peeling criterion used. As expected in this situation,
the model is extensively pruned (¯Lrcv “ 1 ´ 2) using RCCV. The same consistency is not observed for
“Replicated Averaged CV” (RACV) that fails to properly control the peeling length when the other Log-
Rank Test (LRT) or Log Hazard Ratio (LHR) optimization criteria are used (¯Lrcv « ¯Lrcv
m ) (Table 1,
Supporting Figures 1, 2, 3). Diﬀerences between cross-validation techniques are also signiﬁcant in high-
dimensional simulated models (Table 1, Supporting Figures 1, 2, 3).
Using from now on a given peeling and optimization criterion in low-dimensional simulated models,
we compare the performance of our two cross-validation techniques with each other (RACV vs. RCCV)
and with the situation of no cross-validation (NOCV). Peeling trajectory and covariate usage/importance
results are shown for model #2 where it is possible to speciﬁcally assess eﬀects of cross-validation on a
noise/random covariate (x3). Figures 5 and 6 show peeling trajectory proﬁles and covariate traces for
model #2. Table 2 gives the corresponding rules.
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
X1 covariate trajectory
Box mass
X1
NOCV.
RACV
RCCV
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
X2 covariate trajectory
Box mass
X2
NOCV.
RACV
RCCV
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
X3 covariate trajectory
Box mass
X3
NOCV.
RACV
RCCV
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Box support trajectory
Box mass
Support (β)
NOCV.
RACV
RCCV
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
MEFT trajectory
Box mass
Time
NOCV.
RACV
RCCV
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
MEFP trajectory
Box mass
Probability
NOCV.
RACV
RCCV
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
12
LHR trajectory
Box mass
Log−Hazard Ratio (λ)
NOCV.
RACV
RCCV
0.0
0.2
0.4
0.6
0.8
1.0
0
50
100
150
200
250
LRT trajectory
Box mass
Log−rank test (χ2)
NOCV.
RACV
RCCV
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
CER trajectory
Box mass
1−C (θ)
NOCV.
RACV
RCCV
Figure 5: Comparison of cross-validated peeling trajectories between situations when either cross-validation technique “Repli-
cated Combined CV” (RCCV) or “Replicated Averaged CV” (RACV) and no cross-validation (NOCV) was done. Results are
for simulated model #2 and the LRT statistic used in both peeling and optimization criteria. Compare the trajectory lengths
between either cross-validation technique and in the absence of either one. Notice also the ﬂat trajectory proﬁle of covariate
x3 in the presence of either cross-validation technique (RACV or RCCV) as opposed to the situation where no cross-validation
(NOCV) was done.
26

Clearly, both cross-validation techniques are eﬀective in terms of (i) smoothing peeling trajectories out
and (ii) pruning peeling trajectories oﬀ. Compare for instance results of simulation model #2: ¯Lrcv “ 26
without cross-validation (NOCV), ¯Lrcv “ 10 with RACV and ¯Lrcv “ 11 with RCCV (Figures 5 and 6,
Table 2). In fact, all cross-validated trajectory proﬁles in Figure 5 and covariate traces in Figure 6 stop at
¯βrcvpl “ 11q Æ 0.30 for RCCV and ¯βrcvpl “ 10q Æ 0.34 for RACV as compared to ¯βrcvpl “ 27q Æ 0.05 in
the absence of cross-validation (NOCV).
0.0
0.2
0.4
0.6
0.8
1.0
−0.4
−0.2
0.0
0.2
0.4
Covariate Importance
Box mass
(centered) Covariate Range
X1
X2
X3
X1
X2
X3
X1
X2
X3
NOCV.
RACV
RCCV
0.0
0.2
0.4
0.6
0.8
1.0
Covariate Usage
Box mass
Covariates
X1
X2
X3
NOCV.
RACV
RCCV
Figure 6: Comparison of cross-validated trace plots of covariate importance Ď
V Iplq (top) and covariate usage Ě
V Uplq (bottom)
between situations when either cross-validation technique “Replicated Combined CV” (RCCV) or “Replicated Averaged CV”
(RACV) and no cross-validation (NOCV) was done. Results are for simulated model #2 and the LRT statistic used in both
peeling and optimization criteria. Compare the trace lengths between either cross-validation technique and in the absence of
either one. Notice also the ﬂat trace of covariate x3 about 0 in the presence of either cross-validation technique (RACV or
RCCV) as opposed to the situation where no cross-validation (NOCV) was done.
Notice from the peeling trajectories and trace plots of simulation model #2 (compared to #1) how
a noise/random covariate (x3) is eﬀectively eliminated from the model after using either cross-validation
technique (RCCV or RACV), while it is not in the absence of cross-validation (NOCV). In fact, x3’s RCCV
and RACV peeling trajectories are mostly ﬂat, that is, x3 is unused in the decision rule (blue dashed curves
in Figure 5). Consistently, x3’s RCCV and RACV covariate importance trace plots are mostly ﬂat (top of
Figure 6) and x3’s RCCV and RACV covariate usage trace plots show that x3 is not used at all (bottom
of Figure 6). Similar conclusions are drawn with respect to simulated model #3 (compared to #1).
The non-monotone behavior of the LRT and the overly large LHR values obtained in the non-cross-
validated (NOCV) results of simulated model #2 (¯λrcvpl “ 26q “ 11.08) clearly reﬂect over-ﬁtting and
sub-optimal models. This is evident when comparing to the much more conservative values obtained from
the corresponding cross-validated peeling proﬁles: ¯λrcvpl “ 11q “ 3.90 for RCCV and ¯λrcvpl “ 10q “ 3.76
for RACV (Figure 5 and Table 2). This non-monotone behavior of LRT peeling proﬁle is precisely what
allows us to use it in the optimization criterion. We suggest that this could be due to a greater sensitivity
of LRT to small sample sizes at deep peeling steps.
To further compare cross-validation techniques, we generated empirical distributions of various cross-
validated estimates of box decision rules and box survival end-points/prediction statistics (section 2.2.6)
for each technique and end-point as a function of peeling steps. Distributions were obtained by generating
B “ 128 Monte-Carlo simulated datasets according to simulated model #1, where the LRT statistic was
27

Table 2: Comparison of cross-validated decision rules (upper Table) and box end points statistics of interest (lower Table)
between situations when either cross-validation technique “Replicated Combined CV” (RCCV) or “Replicated Averaged CV”
(RACV) and no cross-validation (NOCV) was done. For conciseness, only the initial and ﬁnal decision rules (¯Lrcvth step) are
shown. Values are sample mean estimates with corresponding standard errors in parenthesis (NA in the case of NOCV, where
no replication was performed - see manual of R package PRIMsrc for details [16]). Step #0 corresponds to the situation where
the starting box covers the entire test-set data Lk before peeling. Results are for simulated model #2 and the LRT statistic
used in both peeling and optimization criteria. Notice the non-usage of covariate x3 in the presence of either cross-validation
technique (RACV or RCCV) as opposed to the situation where no cross-validation (NOCV) was done.
Step l
x1
x2
x3
0
x1 ě 0.00 p0.00q
x2 ď 1.00 p0.00q
x3 ď 1.00 p0.00q
RCCV
1
x1 ě 0.03 p0.00q
x2 ď 0.88 p0.01q
x3 ď 1.00 p0.00q
...
...
...
...
11
x1 ě 0.40 p0.06q
x2 ď 0.62 p0.04q
x3 ď 0.97 p0.05q
0
x1 ě 0.00 p0.00q
x2 ď 1.00 p0.00q
x3 ď 1.00 p0.00q
RACV
1
x1 ě 0.00 p0.00q
x2 ď 0.88 p0.00q
x3 ď 1.00 p0.00q
...
...
...
...
10
x1 ě 0.42 p0.03q
x2 ď 0.61 p0.02q
x3 ď 0.96 p0.03q
0
x1 ě 0.00 pNAq
x2 ď 1.00 pNAq
x3 ď 1.00 pNAq
NOCV
1
x1 ě 0.03 pNAq
x2 ď 0.89 pNAq
x3 ď 1.00 pNAq
...
...
...
...
26
x1 ě 0.64 pNAq
x2 ď 0.34 pNAq
x3 ď 0.51 pNAq
Step l
nplq
¯βrcvplq
Ď
T 1
0
rcvplq
Ď
P 1
0
rcvplq
¯λrcvplq
¯χrcvplq
¯θrcvplq
0
250 (0.00)
1.00 (0.00)
3.00 (0.00)
0.37 (0.00)
0.00 (0.00)
0.00 (0.00)
1.00 (0.00)
RCCV
1
225 (0.00)
0.90 (0.00)
3.00 (0.00)
0.31 (0.01)
0.18 (0.02)
23.86 (1.14)
0.43 (0.00)
...
...
...
...
...
...
...
...
11
75 (2.50)
0.30 (0.01)
1.79 (0.71)
0.03 (0.02)
3.90 (0.46)
214.61 (33.56)
0.26 (0.01)
0
250 (0.00)
1.00 (0.00)
2.86 (0.04)
0.38 (0.02)
0.00 (0.00)
0.00 (0.00)
1.00 (0.00)
RACV
1
225 (2.50)
0.90 (0.01)
2.86 (0.05)
0.31 (0.02)
1.19 (0.02)
4.84 (0.27)
0.43 (0.01)
...
...
...
...
...
...
...
...
10
85 (2.50)
0.34 (0.01)
1.01 (0.34)
0.05 (0.02)
3.76 (0.41)
43.64 (6.01)
0.25 (0.02)
0
250 (NA)
1.00 (NA)
3.00 (NA)
0.37 (NA)
0.00 (NA)
0.00 (NA)
1.00 (NA)
NOCV
1
225 (NA)
0.90 (NA)
3.00 (NA)
0.31 (NA)
1.19 (NA)
23.43 (NA)
0.43 (NA)
...
...
...
...
...
...
...
...
26
12 (NA)
0.05 (NA)
0.01 (NA)
0.00 (NA)
11.08 (NA)
131.73 (NA)
0.44 (NA)
used in both peeling and optimization criteria. The replication design accounts for two folds of variability:
the one due to random splitting by cross-validation and the one due to sampling from the simulated
model.
Then, Box Coeﬃcient of Variation (BCV) of decision rules (as deﬁned in [17]) and coeﬃcient
of variation of box survival end-points/prediction statistics were computed and plotted as a function of
peeling steps. Here, cross-validated coeﬃcient of variation proﬁles do not show a consistent advantage of
one cross-validation technique over the other considering all end-point analyzed. This remains true for a
range of realistic sample sizes n P t50, 100, 200u (Supporting Figure 4).
Overall, our two cross-validation techniques, although not equivalent in design, give similar results
on most proﬁles for the sample size and simulation models tested, conﬁrming that both techniques are
appropriate to the task in most situations.
However, “Replicated Averaged CV” (RACV) appears to
be less conservative than “Replicated Combined CV” (RCCV), especially in high-dimensional settings,
which could be a problem if ones cares about reducing the risk of over-ﬁtting.
Also, RACV failed to
prune simulated model #3 (Table 1 and Supporting Figures 1, 2, 3). This indicates that our RCCV cross-
validation technique is more robust in noisy situations, possibly because RCCV uses larger test-set samples
of size n to make estimations than RACV, which uses test-set samples of size nt « n{K (section 3.2.2).
For these reasons, we recommend using RCCV preferably to RACV.
28

4.3.4
Comparison Between Simulated Survival Models
In line with the above guidelines (sections 4.3.2 and 4.3.3), we used the following criteria and techniques
in our numerical analyses for ﬁtting and tuning/selecting our survival bump hunting model: (i) The Log-
Rank Test LRT both as peeling and optimization criterion in low-dimensional settings; (ii) The Cumulative
Hazard Summary CHS as peeling criterion and the Concordance Error Rate CER as optimization criteria
in high-dimensional settings; and our “Replicated Combined Cross-Validation” (RCCV) technique. We
compared the performance of our Survival Bump Hunting procedure in terms of peeling trajectories (Figures
7, 8, Table 3, Supporting Figures 5, 6, Supporting Table 2) and survival distribution curves (Figure 9)
between all our models.
Notice the striking diﬀerences in cross-validated peeling trajectories (Figure 7) and covariate traces
(Figure 8): (i) when all covariates px1, x2, x3q are noise (model #3), or (ii) when one covariate only (x3)
is noise (model #2) or (iii) when all are informative (model #1). As expected, all peeling trajectories
related to model #3 are much shorter than in the other models, indicating an abortive PRSP procedure
with little or no covariate usage during the peeling process (Figure 7) nor involvement in the decision rule
(Table 3). Similarly, one expects little or no usage of covariate x3 in models #2 and #3, as seen in their
covariate peeling trajectories (Figure 7, Table 3).
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
X1 covariate trajectory
Box mass
X1
model #1
model #2
model #3
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
X2 covariate trajectory
Box mass
X2
model #1
model #2
model #3
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
X3 covariate trajectory
Box mass
X3
model #1
model #2
model #3
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Box support trajectory
Box mass
Support (β)
model #1
model #2
model #3
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
MEFT trajectory
Box mass
Time
model #1
model #2
model #3
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
MEFP trajectory
Box mass
Probability
model #1
model #2
model #3
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
LHR trajectory
Box mass
Log−Hazard Ratio (λ)
model #1
model #2
model #3
0.0
0.2
0.4
0.6
0.8
1.0
0
50
100
150
200
LRT trajectory
Box mass
Log−rank test (χ2)
model #1
model #2
model #3
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
CER trajectory
Box mass
1−C (θ)
model #1
model #2
model #3
Figure 7: Comparison of replicated combined cross-validated results for the peeling trajectories between simulated models #1,
#2 and #3 for the “Replicated Combined CV” (RCCV) technique and the LRT statistic used in both peeling and optimization
criteria. Notice the usage of all covariates px1, x2, x3q in model #1 as opposed to the selective usage of covariates px1, x2q in
model #2 and the abortive usage of all covariates in noise model #3.
Consistent observations can be made from the cross-validated covariate importance and covariate usage
traces of model #3. In fact, x3’s covariate importance trace stops after only ¯Lrcv “ 2 steps with box mass
¯βrcvp¯Lrcvq « 0.81 for model #3, as compared to ¯Lrcv “ 20 steps with ¯βrcvp¯Lrcvq « 0.10 for model #1 and
¯Lrcv “ 11 steps with ¯βrcvp¯Lrcvq « 0.30 for model #2 (Table 3 and Figure 8). Also notice the limited usage
of covariate x3 in the covariate usage traces of models #2 and #3 as compared to #1 and the fact that all
cross-validated peeling trajectories in model #1 extend further than in other models #2 and #3 (Figure
7, 8 and Table 3). This is consistent with our simulation design in that all covariates in regression model
29

#1 additively contribute to the hazards and to the separation of survival distributions.
0.0
0.2
0.4
0.6
0.8
1.0
−0.2
0.0
0.1
0.2
0.3
Covariate Importance
Box mass
(centered) Covariate Range
X1
X2
X3
X1
X2
X3
X1
X2
X3
model #1
model #2
model #3
0.0
0.2
0.4
0.6
0.8
1.0
Covariate Usage
Box mass
Covariates
X1
X2
X3
model #1
model #2
model #3
Figure 8: Comparison of replicated combined cross-validated trace plots of covariate importance Ď
V Iplq (top) and covariate
usage Ě
V Uplq (bottom) between simulated models #1, #2 and #3 for the “Replicated Combined CV” (RCCV) technique and
the LRT statistic used in both peeling and optimization criteria. Notice the usage of all covariates px1, x2, x3q in model #1 as
opposed to the selective usage of covariates px1, x2q in model #2 and the abortive usage of all covariates in noise model #3.
Finally, Figure 9 shows the cross-validated Kaplan–Meir survival probability curves of the highest-risk
group vs. lower-risk group in all low- and high-dimensional simulated models with their corresponding log-
rang permutation p-values of survival distribution curve separation. The separation is especially evident in
results of models #1, #2 and #4 in contrast to the overlap seen in model #3. The permutation p-values
are: ˜pcvpl “ 20q ď 9.7e ´ 5, ˜pcvpl “ 11q ď 9.7e ´ 5 and ˜pcvpl “ 8q « 0.046 for models #1, #2 and #4
respectively, and ˜pcvpl “ 2q « 0.1080 for model #3 (Figure 9).
Overall, Figures 7, 8, 9, Table 3, Supporting Figures 5, 6 and Supporting Table 2 collectively support
that our “Replicated Combined CV” (RCCV) cross-validation technique, when used with an appropriate
combination of LRT and CER statistics as peeling and/or optimization criteria, is eﬃcient at ﬁtting and
tuning a survival bump hunting model, whether in low- or high-dimensional settings. Moreover, we show
that our PRSP algorithm (Algorithm 1) successfully selects/uses a subset of (or all) the covariates that
are informative (i.e. that truly enter into the model) in the box decision rules.
30

Table 3: Comparison of cross-validated decision rules (upper Table) and box end points statistics of interest (lower Table)
between simulated models #1, #2 and #3 for the “Replicated Combined CV” (RCCV) technique and the LRT statistic used
in both peeling and optimization criteria. For conciseness, only the initial and ﬁnal decision rules (¯Lrcvth step) are shown.
Step #0 corresponds to the situation where the starting box covers the entire test-set data Lk before peeling. Values are
sample mean estimates with corresponding standard errors in parenthesis. Notice the usage of all covariates px1, x2, x3q in
model #1 as opposed to the selective usage of covariates px1, x2q in model #2 and the abortive usage of all covariates in noise
model #3.
Step l
x1
x2
x3
0
x1 ě 0.00 p0.00q
x2 ď 1.00 p0.00q
x3 ď 1.00 p0.00q
model #1
1
x1 ě 0.03 p0.00q
x2 ď 0.88 p0.00q
x3 ď 1.00 p0.00q
...
...
...
...
20
x1 ě 0.51 p0.04q
x2 ď 0.44 p0.06q
x3 ď 0.63 p0.08q
0
x1 ě 0.00 p0.00q
x2 ď 1.00 p0.00q
x3 ď 1.00 p0.00q
model #2
1
x1 ě 0.03 p0.00q
x2 ď 0.88 p0.00q
x3 ď 1.00 p0.00q
...
...
...
...
11
x1 ě 0.40 p0.06q
x2 ď 0.62 p0.04q
x3 ď 0.97 p0.05q
0
x1 ď 1.00 p0.00q
x2 ď 1.00 p0.00q
x3 ě 0.01 p0.00q
model #3
1
x1 ď 1.00 p0.00q
x2 ď 0.99 p0.01q
x3 ě 0.01 p0.01q
2
x1 ď 1.00 p0.00q
x2 ď 0.96 p0.05q
x3 ě 0.06 p0.02q
Step l
nplq
¯βrcvplq
Ď
T 1
0
rcvplq
Ď
P 1
0
rcvplq
¯λrcvplq
¯χrcvplq
¯θrcvplq
0
250 (0.00)
1.00 (0.00)
3.00 (0.00)
0.42 (0.00)
0.00 (0.00)
0.00 (0.00)
1.00 (0.00)
model #1
1
225 (0.00)
0.90 (0.00)
3.00 (0.00)
0.36 (0.00)
1.06 (0.03)
19.21 (1.13)
0.43 (0.00)
...
...
...
...
...
...
...
...
20
25 (2.50)
0.10 (0.01)
0.06 (0.05)
0.00 (0.00)
8.54 (1.29)
194.39 (30.24)
0.39 (0.01)
0
250 (0.00)
1.00 (0.00)
3.00 (0.00)
0.37 (0.00)
0.00 (0.00)
0.00 (0.00)
1.00 (0.00)
model #2
1
225 (0.00)
0.90 (0.00)
3.00 (0.00)
0.31 (0.00)
1.18 (0.02)
23.86 (1.14)
0.43 (0.00)
...
...
...
...
...
...
...
...
11
75 (2.50)
0.30 (0.01)
1.79 (0.71)
0.03 (0.71)
3.90 (0.46)
214.61 (33.56)
0.26 (0.01)
0
250 (0.00)
1.00 (0.00)
2.99 (0.00)
0.41 (0.00)
0.00 (0.00)
0.00 (0.00)
1.00 (0.00)
model #3
1
225 (2.50)
0.90 (0.01)
2.99 (0.00)
0.40 (0.01)
0.21 (0.14)
0.86 (0.79)
0.49 (0.01)
2
202 (5.00)
0.81 (0.02)
2.99 (0.00)
0.38 (0.01)
0.33 (0.11)
2.90 (1.75)
0.47 (0.01)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
outbox
inbox
p ≤0.0009765625
LHR = 8.540
LRT = 194.390
Step 20
Low−dim. model #1
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
outbox
inbox
p ≤0.0009765625
LHR = 3.900
LRT = 214.610
Step 11
Low−dim. model #2
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
outbox
inbox
p = 0.1080
LHR = 0.330
LRT = 2.900
Step 2
Low−dim. noise model #3
0.0
0.5
1.0
1.5
2.0
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
0.0
0.5
1.0
1.5
2.0
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
outbox
inbox
p = 0.0460
LHR = 0.490
LRT = 3.990
Step 8
High−dim. model #4
Figure 9: Comparison of cross-validated Kaplan–Meir survival probability curves of the high-risk (red curve “in-box”) and
low-risk (black curve “out-of-box”) groups in simulated models #1, #2, #3 and #4. Results are for the “Replicated Combined
CV” (RCCV) technique and the CHS statistic used as peeling criterion and CER used as optimization criteria. Left column:
model #1, middle column: model #2, right column: model #3. For conciseness, only the last peeling step of the peeling
sequence is shown for each model. Cross-validated LRT, LHR and permutation p-values of “in-box” samples are shown at the
bottom of the plot with the corresponding peeling step for each method. P-values ˆpcvplq ď 9.7e´5 correspond to 1/10th of the
precision limit (see section 3.5). Notice how the survival curves of “in-box” and “out-of-box” samples separates in models #1,
#2 and #4 in contrast to the overlapping situation in noise model #3 with the corresponding signiﬁcant and non-signiﬁcant
log-rank permutation p-value ˆpcvplq of survival distribution separation.
31

4.4
Comparisons Against Other Methods
4.4.1
Design and Choice of Various Non-Parametric Survival Models
Next, we compared our Survival Bump Hunting (SBH) procedure by our PRSP algorithm (1) to other
competitive non-parametric survival models or methods in terms of survival and prediction end-points
statistics. In all our performance analyses below, we used LRT in both peeling and optimization criteria
and RCCV as our cross-validation technique. Comparisons include (i) Survival Bump Hunting by our
PRSP method (Algorithm 1 and [14]), (ii) Regression Survival Trees (RST) by recursive partitioning
[1, 11, 13, 32, 45, 46, 62], (iii) Random Survival Forest (RSF) by ensemble tree-based method [40], (iv) Cox
Proportional Hazard Regression (CPHR) [12], (v) Survival Supervised PCA (SSPCA) [4], (vi) Survival
Supervised Clustering (SSC) [5].
The simulated survival model was according to simulated model #1b, that is, by generating a box-
shaped region R of the input covariate space with higher hazards than a uniform background (see 4.1).
For comparisons, B “ 128 repeated Monte-Carlo simulated datasets #1b were used to generate empirical
sampling distributions of cross-validation estimates of box statistics, survival end-points and prediction
performance metrics (section 2.2.6) and make points and conﬁdence intervals inferences. This replication
design accounts for random splitting and simulated model sampling variabilities.
For each method, an internal cross-validation was carried out for model ﬁtting/training that was done
by optimizing a speciﬁc empirical objective function of Goodness of Fit or Prediction Error measure on
the corresponding test-set, such as: (i) maximization of the Log-Rank Test statistic or minimization of a
Concordance Error Rate (SBH), maximization of the Deviance Residuals statistic (RST), maximization of
the Concordance Index (RSF), maximization of the Likelihood Ratio Statistic between the reduced vs. full
model (SSPCA), maximization of the Concordance Index (SSC). Then, cross-validation was used again to
make estimations and predictions on the combined test-sets as described before (section 3.2.2).
Whether the goal is to make estimations or predictions, one wants to classify samples into two sur-
vival/risk groups. However, unlike Survival Bump Hunting that inherently generates “in-box” and “out-of-
box” groups, all other methods do not necessarily give directly two survival/risk groups. For comparisons
purposes, one needs to come up with a calibrated way across all other methods to output two groups only.
One way, shown to work well empirically, is by using the median survival time threshold ([38]).
4.4.2
Comparison of End-Point Estimates
Speciﬁcally, the trained models generate cross-validated ﬁts from which cross-validated estimates of highest-
risk/group support and survival end-points statistics (described in section 2.2.6) are made using the left-out
test-set Lk. We report the results for all methods in Figure 10 below.
The ﬁgure shows the highest-
risk/group end-points distributions of RCCV estimates of support and survival end-points statistics com-
puted over B “ 128 repeated Monte-Carlo simulated models #1b for all competitive non-parametric
survival models under study.
In Figure 11 below, a Kaplan–Meier estimate of RCCV survival probability curve is shown for each
group and competitive non-parametric survival models under study from one replicate out of B “ 128. As
expected, it shows the extremeness of the survival distribution of the highest-risk box/group found by SBH
as compared to all other methods. The box sample sizes in the highest-risk box/group (out of n “ 250
samples) were as follows for each method (and that replicate): SBH: nSBH “ 39, RST: nRST “ 105, RSF:
nRSF “ 124, CPHR: nCP HR “ 124, SSPCA: nSSP CA “ 124, SSC: nSSC “ 170.
Overall, results from Figures 10 and 11 point out that the highest-risk box/group found by SBH is,
as expected, smaller in size (support) and more extreme in terms of survival hazards (LHR) or risks,
with consistent smaller event-free end-point times (MEFT) and probabilities (MEFP) than any other
method/model under study. This was the goal. However, we did not expect the separation of the estimated
survival distributions to be necessarily larger. In fact, the distributions of the log-rank test statistics (LRT)
are not signiﬁcantly diﬀerent between most methods (except SSC). Interestingly, the Concordance Error
Rates (CER) are slightly higher for SBH than most methods (except SSC). So, it’s possible that the task
of ﬁnding extreme survival/risks subgroups comes with some trade-oﬀbetween achieving high levels of
extremeness and high accuracy of survival time prediction.
32

SBH (LRT)
RST (LRT)
RSF (LRT)
CPHR
SSPCA
SSC
0.2
0.4
0.6
0.8
Support
SBH (LRT)
RST (LRT)
RSF (LRT)
CPHR
SSPCA
SSC
0.0
0.5
1.0
1.5
2.0
2.5
3.0
MEFT
SBH (LRT)
RST (LRT)
RSF (LRT)
CPHR
SSPCA
SSC
0.0
0.1
0.2
0.3
0.4
MEFP
SBH (LRT)
RST (LRT)
RSF (LRT)
CPHR
SSPCA
SSC
0
2
4
6
8
LHR
SBH (LRT)
RST (LRT)
RSF (LRT)
CPHR
SSPCA
SSC
0
50
100
150
200
250
300
LRT
SBH (LRT)
RST (LRT)
RSF (LRT)
CPHR
SSPCA
SSC
0.20
0.25
0.30
0.35
0.40
0.45
0.50
CER
Figure 10: Distributions of RCCV estimates of highest-risk/group end-points, computed over B “ 128 repeated Monte-Carlo
simulated models #1 and for all competitive non-parametric survival models under study. Comparisons include (i) Survival
Bump Hunting (SBH), (ii) Regression Survival Trees (RST), (iii) Random Survival Forest (RSF), (iv) Cox Proportional Hazard
Regression (CPHR), (v) Survival Supervised PCA (SSPCA), (vi) Survival Supervised Clustering (SSC). In parenthesis is shown
the criterion used for peeling or partitioning as it applies. For each SBH boxplot, the pair of horizontal dotted lines delineates
the approximate (95%) conﬁdence interval of the median. Results are for the “Replicated Combined CV” (RCCV) technique
and the LRT statistic used in the optimization criteria.
4.4.3
Comparative Prediction Performance
The simulated survival model we drew from was according to model #1b as follows: samples contained
within a box-shaped region R of the input covariate space had increased risk/hazards while samples outside
of it had a uniform background risk. The survival times were generated as in section (4.1), using the
exponential distribution with random uniform censoring. The regression function for samples within R
was as in model #1.
Due to the rule-induction nature of our “Patient Recursive Survival Peeling” method (Algorithm 1), the
box decision rule can be used as the classiﬁcation rule. The cross-validated classiﬁcation error is estimated
from the discrepancies between the true and predictive classiﬁcations of the independent observations.
Speciﬁcally, for each loop k P t1, . . . , Ku of the cross-validation, we compute a cross-validated estimate
of the error by matching the SBH test-set “in-box” prediction samples to the true ones in the high-risk
box-shaped region R of simulated model #1b using the left-out test-set Lk.
The ﬁnal cross-validated
estimate of the Miss-classiﬁcation Error Rate (MER) is given by the average of the cross-validated errors
from the K models t ˜Rkuk“K
k“1 generated from each loop of the cross-validation. This is repeated B times to
get variability estimates. Note that CER and MER, although related, are distinct: the MER evaluates
the accuracy to predict that a sample will fall into (or outside) the highest-risk box/group found by a
method/survival model, whereas the CER evaluates the accuracy to predict a sample survival time.
Prediction performances are then assessed using the usual accuracy metrics and Receiver Operating
33

0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
SBH (LRT)
Survival Time
Survival Probability
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
SBH (LRT)
Survival Time
Survival Probability
1.08
0
0
0.5
outbox
inbox
LHR = 6.063, LRT = 199.993
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
RST (LRT)
Survival Time
Survival Probability
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
RST (LRT)
Survival Time
Survival Probability
2.79
0.07
0.01
0.5
outbox
inbox
LHR = 2.506, LRT = 150.851
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
RSF (LRT)
Survival Time
Survival Probability
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
RSF (LRT)
Survival Time
Survival Probability
2.6
0.04
0.01
0.5
outbox
inbox
LHR = 2.775, LRT = 206.350
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
CPHR
Survival Time
Survival Probability
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
CPHR
Survival Time
Survival Probability
2.6
0.03
0.01
0.5
outbox
inbox
LHR = 2.906, LRT = 221.373
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
SSPCA
Survival Time
Survival Probability
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
SSPCA
Survival Time
Survival Probability
2.39
0.01
0.01
0.5
outbox
inbox
LHR = 3.144, LRT = 254.274
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
SSC
Survival Time
Survival Probability
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.2
0.4
0.6
0.8
1.0
SSC
Survival Time
Survival Probability
2.99
0.33
0.21
0.5
outbox
inbox
LHR = 0.486, LRT = 7.490
Figure 11: Kaplan–Meier plots of RCCV survival probability curves for all competitive non-parametric survival models under
study. Plots are illustrative of one replication out of B “ 128. Comparisons include (i) Survival Bump Hunting (SBH), (ii)
Regression Survival Trees (RST), (iii) Random Survival Forest (RSF), (iv) Cox Proportional Hazard Regression (CPHR), (v)
Survival Supervised PCA (SSPCA), (vi) Survival Supervised Clustering (SSC). In parenthesis is shown the criterion used for
peeling or partitioning as it applies. The “in-box” legends (red) corresponds to the highest-risk box/group. Cross-validated
LRT, LHR of “in-box” samples are shown at the top of the plot for each method (and that replicate). Results are for the
“Replicated Combined CV” (RCCV) technique and LRT statistic used in the optimization criteria.
Characteristics (ROC) for each method and compared between them. For binary classes, a common metric
for assessing the prediction performance is prediction accuracy through the use of True- and False-Positive
Rates TPR and FPR, respectively, also known as Sensitivity and 1 ´ Specificity. By deﬁnition, the
True- and False-Positive Rates are deﬁned as:
TPR “ Sensitivity “
TP
TP ` FN
FPR “ 1 ´ Specificity “
FP
FP ` TN
where TP, FP, TN, FN stands for True-Positive, False-Positive, True-Negative and False-Negative, re-
spectively. The performance of classiﬁcation is naturally assessed by measuring the accuracy of prediction,
whereas the performance of ranking is commonly measured by taking pAUCq, the Area Under the Re-
ceiver Operating Characteristics (ROC) Curve (TPR versus FPR) [44]. An AUC “ 1 corresponds to
a perfect classiﬁer, while an AUC “ 0.5 corresponds to all possible performances of a random classiﬁer.
Finally, we also report Pearson’s χ2 contingency table test p-values (after continuity correction) of inde-
pendence between the observed versus predicted counts. Table 4 reports the classiﬁcation performance
results of various survival models/methods in terms of contingency table test, area under the ROC curve
and sensitivity/speciﬁcity.
34

Table 4: Empirical χ2 contingency table test p-values p{
P.valq, Area Under the Curve p{
AUCq, Sensitivity p
{
1 ´ FPRq and
Speciﬁcity p z
T PRq for each method. Comparisons include (i) Survival Bump Hunting (SBH), (ii) Regression Survival Trees
(RST), (iii) Random Survival Forest (RSF), (iv) Cox Proportional Hazard Regression (CPHR), (v) Survival Supervised PCA
(SSPCA), (vi) Survival Supervised Clustering (SSC). Values are median estimates with standard errors of the sample mean
in parenthesis. In parenthesis, next to the method, is also shown the criterion used for peeling or partitioning as it applies.
Results are for the “Replicated Combined CV” (RCCV) technique and the LRT statistic used in the optimization criteria.
Method
SBH (LRT)
RST (LRT)
RSF (LRT)
CPHR
SSPCA
SSC
{
P.val
0.000 (0.32)
0.909 (0.42)
0.065 (0.24)
0.037 (0.29)
0.037 (0.34)
0.519 (0.33)
{
1 ´ FPR (Specificity)
0.800 (0.11)
0.947 (0.04)
0.516 (0.01)
0.516 (0.01)
0.516 (0.01)
0.373 (0.05)
z
T PR (Sensitivity)
1.000 (0.38)
0.125 (0.17)
1.000 (0.14)
1.000 (0.26)
1.000 (0.25)
0.833 (0.21)
{
AUC
0.899 (0.24)
0.533 (0.07)
0.757 (0.07)
0.758 (0.14)
0.758 (0.13)
0.591 (0.11)
Table 4 shows that SBH has a better prediction performance on all metrics than any other method/model
under study. This directly results from its better trade-oﬀof Specificity and Sensitivity. Overall, this
reﬂects the above results on comparative end-point estimates: SBH reaches out a more speciﬁc and smaller
group of samples that is more extreme in survival hazards or risks. Conversely, other methods tend to be
more sensitive, but way too un-speciﬁc. Note that this applies to Regression Survival Trees (RST) as well.
5
Real Data Analysis
Finally, we applied our Survival Bump Hunting (SBH) procedure to a publicly available real clinical dataset
from the Women’s Interagency HIV cohort Study (WIHS) [3]. It involves competing risks “AIDS/Death
(before HAART)” and “Treatment Initiation (HAART)” during HIV-1 Infection in women.
Here, for
simpliﬁcation purposes, only the ﬁrst of the two competing events (the time to AIDS/Death) was used
in our analysis. The data consists of n “ 485 complete observations on the following p “ 4 covariates in
addition to the censoring indicator and (censored) time-to-event variables (Table 5).
Table 5: Women’s Interagency HIV Study (WIHS). Clinical dataset used with covariates description.
Covariate Description
Range
AIDS/Death Diagnosis Time
T (years)
Event indicator variable
C P tAIDS/Dead “ 1, Censored “ 0u
Patient age at time of FDA approval of ﬁrst protease inhibitor
Age (years)
Injection Drug Users (IDU) history
IDU P tNo history “ 0, History “ 1u
Patients race
Race P tOther “ 0, African-American “ 1u.
CD4 count
CD4 P r0, `8s{100 cells{µl
All results in the WIHS clinical dataset were achieved using the “Replicated Combined CV” (RCCV)
technique and the LRT statistic as peeling and optimization criterion, using K “ 5, A “ 1024 and B “ 128.
We show in Figure 12 the cross-validated tuning proﬁle of LRT as a function of the number of peeling
steps. According to our optimization criterion, we determined that the resulting “Replicated Combined
CV” optimal length of the peeling trajectory is ¯Lrcv “ 5 (Figure 12).
We show in Table 6 the cross-validated decision rules and highest-risk box/group statistics at each step.
Note that the box sample size in the ﬁnal highest-risk box/group is npl “ 5q “ 262 out of a total sample
size of n “ 485. Here, the cross-validated trace of covariate usage is: CD4, Age, Age, Age (Table 6).
Finally, we show in Figure 13 the cross-validated Kaplan–Meir survival probability curves of the highest-
risk group vs. lower-risk group at each step with their corresponding permutation p-values of separation.
Notice how the curve separation increases with the peeling steps. The permutation p-values at each step
are respectively: ˜pcvpl “ 0q “ 1, ˜pcvpl “ 1 ´ 5q ď 9.7e ´ 5 (Figure 13).
35

Peeling Steps
LRT Mean Profiles
0
1
2
3
4
5
6
7
8
9
10
11
12
13
0
10
20
30
40
50
Figure 12: Cross-validated tuning proﬁle of the WIHS clinical dataset.
The “Replicated Combined CV” cross-validated
optimal peeling length (¯Lrcv “ 5) is shown with the vertical black dotted line. Each colored proﬁle corresponds to one of the
replications (B “ 128). The cross-validated mean proﬁle of the LRT statistic is shown by the solid black line with standard
error of the sample mean.
Table 6: Cross-validated decision rules (top) and highest-risk box/group statistics (bottom) of the WIHS clinical dataset.
Values are sample mean estimates with corresponding standard errors in parenthesis. The box sample size at each step is also
shown. Step #0 corresponds to the situation where the starting box covers the entire test-set data Lk before peeling.
Step l
Age
IDU
Race
CD4
0
Age ě 19.00 p0.00q
IDU ě 0.00 p0.00q
Race ď 1.00 p0.00q
CD4 ď 19.33 p0.00q
1
Age ě 19.00 p0.00q
IDU ě 0.00 p0.00q
Race ď 1.00 p0.00q
CD4 ď 8.64 p0.35q
2
Age ě 22.07 p1.94q
IDU ě 0.00 p0.00q
Race ď 1.00 p0.00q
CD4 ď 8.51 p0.35q
3
Age ě 23.30 p2.65q
IDU ě 0.00 p0.00q
Race ď 1.00 p0.00q
CD4 ď 8.46 p0.16q
4
Age ě 28.79 p0.51q
IDU ě 0.00 p0.00q
Race ď 1.00 p0.00q
CD4 ď 7.66 p0.83q
5
Age ě 29.22 p0.53q
IDU ě 0.00 p0.00q
Race ď 1.00 p0.00q
CD4 ď 6.79 p0.77q
Step l
nplq
¯βrcvplq
Ď
T 1
0
rcvplq
Ď
P 1
0
rcvplq
¯λrcvplq
¯χrcvplq
¯θrcvplq
0
485 (0.00)
1.00 (0.00)
10.8 (0.00)
0.17 (0.00)
0.00 (0.00)
0.00 (0.00)
1.00 (0.00)
1
436 (0.00)
0.90 (0.00)
10.8 (0.00)
0.17 (0.00)
0.61 (0.02)
16.90 (1.41)
0.46 (0.00)
2
398 (4.85)
0.82 (0.01)
10.8 (0.00)
0.16 (0.01)
0.54 (0.05)
18.69 (3.68)
0.45 (0.01)
3
364 (9.70)
0.75 (0.02)
10.8 (0.00)
0.14 (0.01)
0.61 (0.07)
29.28 (7.32)
0.43 (0.01)
4
325 (19.40)
0.67 (0.04)
10.8 (0.00)
0.13 (0.01)
0.61 (0.08)
32.17 (7.74)
0.42 (0.01)
5
262 (33.95)
0.54 (0.07)
10.8 (0.00)
0.11 (0.01)
0.61 (0.10)
32.51 (10.86)
0.42 (0.01)
The question of this study was whether it is possible to achieve a stratiﬁcation or prognostication of
patients for AIDS and HAART by using e.g. the ‘Injection Drug Users‘ (IDU) history. Overall, SBH
shows that it is possible to achieve a stratiﬁcation and prognostication of patients that are more likely
to be diagnosed or die of AIDS than others. The decision rule identiﬁes a subgroup of npl “ 5q “ 262
such patients that should be treated more aggressively than others, e.g. by putting them on HAART
treatment sooner. In addition, SBH reveals that these patients are characterized by a lower CD4 count of
CD4 Æ 6.79p˘0.77q{100 cells{µl with an Age Ç 29.22p˘0.53q. Moreover, SBH reveals that ‘Injection Drug
Users‘ history (IDU) was actually not the most useful covariate at this stage of determination.
6
Discussion and Conclusion
To build a survival bump hunting model, ﬁt by a recursive peeling procedure, we used two sets of criteria
with diﬀerent purposes: (i) the peeling criteria for model ﬁtting by maximization of the rate of increase in
LHR, LRT or CHS statistics (section 2.2.3), and (ii) the optimization criterion used for model tuning by
maximization of the LHR or LRT or by minimization of the CER (section 3.2.3).
36

0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
outbox
inbox
p = 1.0000
LHR = 0.000
LRT = 0.000
Step 0
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
outbox
inbox
p ≤0.0009765625
LHR = 0.610
LRT = 16.900
Step 1
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
outbox
inbox
p ≤0.0009765625
LHR = 0.540
LRT = 18.690
Step 2
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
outbox
inbox
p ≤0.0009765625
LHR = 0.610
LRT = 29.280
Step 3
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
outbox
inbox
p ≤0.0009765625
LHR = 0.610
LRT = 32.170
Step 4
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Survival Time
Survival Probability
outbox
inbox
p ≤0.0009765625
LHR = 0.610
LRT = 32.510
Step 5
Figure 13: Kaplan–Meier plots of RCCV survival probability curves of the WIHS clinical dataset. Each plot represents a
step of the peeling sequence. Step #0 corresponds to the situation where the starting box covers the entire test-set data
Lk before peeling. The “in-box” legends (red) corresponds to the highest-risk box/group. Cross-validated LRT, LHR and
permutation p-values of “in-box” samples are shown at the bottom of the plot with the corresponding peeling step for each
method. P-values ˆpcvplq ď 9.7e ´ 5 correspond to 1/10th of the precision limit (see section 3.5). Notice the single survival
curve at Step #0 before peeling and how the survival curves of “in-box” and “out-of-box” samples separates as the peeling
progresses.
Despite being a well established concept as a splitting criterion in regression survival trees (section
2.2.3), one criticism about the LRT is that it tends to favor continuous variables and causes some uneven
splits (end-cut preference). In this study, the main diﬀerence that we observed between the three peeling
criteria used so far (section 4.3.2) is in the conservativeness of the number of peeling steps or peeling
sequence length (model complexity), and to a lower extent, of the number of used covariates (model size),
regardless of the dimensionality of the data.
In summary, we recommended using as peeling criterion
primarily the Cumulative Hazard Summary CHS and secondarily the Log-Rank Test LRT to induce more
conservative estimates and reduce the risk of over-ﬁtting, especially in high-dimensional data.
In contrast, we noted that the overall eﬀect of the peeling criteria is relatively marginal compared to
that of the optimization criterion. In this study, we showed that the choice of the optimization criteria
is crucial and dependent on the dimensionality of the data.
In summary, we recommended using the
Concordance Error Rate CER as optimization criterion in every situation (low- and high-dimensional
data), or alternatively the Log-Rank Test LRT in low-dimensional situation only (section 4.3.1).
Overall, both of our replicated cross-validation techniques, namely the “Replicated Combined CV”
(RCCV) and “Replicated Averaged CV” (RACV) techniques, were found eﬀective at controlling (at least
in part) the overﬁtting and under-ﬁtting issues, conﬁrming that these techniques are appropriate to the task
of survival bump hunting modeling by a recursive peeling procedure. However, we observed diﬀerences in
the cross-validation techniques, which raised the questions whether RACV could lend to over-ﬁtting more
than RCCV, especially in high-dimensional data, and wether RACV performance could degrade faster than
RCCV in situations with small sample sizes (see section 3.2.2). For these reasons, we recommended using
RCCV preferably to RACV.
It is known that the stepwise covariate selection/usage procedure in the peeling loop of Algorithm 1
37

induces an inﬂation of variance estimates primarily because of the adaptive nature of the algorithm (each
peeling step is conditional on the previous step). Using replicated cross-validation is therefore recommended
to reduce the variability of cross-validation estimates in recursive peeling methods.
Survival estimates and model performance accuracy can be improved by the resampling technique used
and resulting bias-variance trade-oﬀ. For instance, using a “larger” number of folds in K-fold CV in the
presence of “small” number of events or samples is known to reduce bias, but also increase variance of esti-
mates. Beside our cross-validation techniques, the Leave-one-Out Cross-Validation (LOOCV - Jackknife)
or bootstrap-based resampling techniques are available, such as the ordinary bootstrap [26], the 0.632 /
Out-of-Bootstrap Cross-Validation (0.632 OOBCV - [24]) or its latest variant (0.632+ OOBCV [27]). As
per Efron, ”the ordinary bootstrap gives an estimate of error with low variability, but with a possibly large
downward bias, particularly in highly over-ﬁtted situations” [24]. Conversely, cross-validation estimators
are nearly unbiased (slightly upward), but have generally unacceptable high variability. For this reason, we
chose to use a cross-validation procedure that could be replicated. The LOOCV and 0.632 OOBCV could
be used instead of K-fold CV, but have larger variance estimators. An alternative may be the 0.632+
OOBCV estimator, which combines lower variance with a correction for bias [27].
Collectively, our peeling strategy, combined with our model tuning/selection method along with cross-
validation techniques support the claim that optimal survival bump hunting modeling can be done. Further,
results also indicate that our strategies help our “Patient Recursive Survival Peeling” (PRSP) algorithm
(1) use the most informative covariates in the decision rule. This suggests the possibility of carrying out a
joint internal variable selection with our PRSP procedure.
Some interesting diﬀerences between decision-tree and decision-box models lie in the weaknesses and
strengths of the estimated solutions and in their applications. Here are some: (i) Stratiﬁcation: if multiple
groups are of interest, an advantage of recursive partitioning is that they directly lead to multi-group
stratiﬁcations of the data, instead of just presenting a rule for a single high (low) vs. low (high) response
group; (ii) Interpretability: binary decision trees lead to an intuitive hierarchical interpretation of groups
that facilitates their interpretation unlike peeling methods that are not constrained to a tree structure; (iii)
Patience vs. Greediness: recursive partitioning methods are however notoriously “greedy” (exponential
decrease of the data as the space undergo partitioning based on typically binary split), but recursive
peeling methods can be made “patient” at will (quantile-controlled decrease of the data), eventually helping
recursive top-down peeling algorithms such as ours to better learn from the data.
7
Acknowledgments
This research was made possible with the contribution of the National Institute of Health (NIH), National
Cancer Institute (NCI). J-E. Dazard and J. Sunil Rao were supported in part by NIH grant NCI R01-
CA160593A1. Additional support came from NIH grant NCI P30-CA043703 of the Comprehensive Cancer
Center at Case Western Reserve University (J-E. D). This work made use of the High Performance Com-
puting Cluster in the Core Facility for Advanced Research Computing at Case Western Reserve University.
8
Supporting Information
• Supporting Text; Supporting Tables 1, 2; Supporting Figures 1, 2, 3, 4, 5, 6.
• Supporting Code: R package PRIMsrc, available at:
CRAN repository: https://cran.r-project.org/web/packages/PRIMsrc/index.html
GitHub repository: https://github.com/jedazard/PRIMsrc
• Supporting Datasets: R package PRIMsrc, available at:
CRAN repository: https://cran.r-project.org/web/packages/PRIMsrc/index.html
GitHub repository: https://github.com/jedazard/PRIMsrc/tree/master/data
38

9
References
[1] Ahn, H. and Loh, W. Y. (1994), “Tree-structured proportional hazards regression modeling,” Biometrics, 50, 471–85.
[2] Ambroise, C. and McLachlan, G. J. (2002), “Selection bias in gene extraction on the basis of microarray gene-expression
data,” Proc Natl Acad Sci U S A, 99, 6562–6.
[3] Bacon, M., von Wyl, V., and Alden, C. e. a. (2005), “Semi-Supervised Methods to Predict Patient Survival from Gene
Expression Data,” Clin Diagn Lab Immunol, 12, 1013–1019.
[4] Bair, E., Hastie, T., Paul, D., and Tibshirani, R. (2006), “Prediction by supervised principal components,” J Amer Stat
Assoc, 101, 119–137.
[5] Bair, E. and Tibshirani, R. (2004), “Semi-Supervised Methods to Predict Patient Survival from Gene Expression Data,”
PLoS Biol., 2, 511–522.
[6] Baker, S., Kramer, B., and Srivastava, S. (2002), “Markers for early detection of cancer: Statistical guidelines for nested
case-control studies,” BMC Medical Research Methodology, 2, 4.
[7] Bohning, D. and Seidel, W. (2003), “Editorial: recent developments in mixture models,” Comp. Stat. Data Anal., 41,
349–357.
[8] Breiman, L. (2001), “Random forests,” Mach. Learn., 45, 5–32.
[9] Breiman, L., Friedman, J., Olshen, R., and Stone, C. (1984), Classiﬁcation and Regression Trees, The Wadsworth
statistics/probability series, Boca Raton, FLorida: Chapman and Hall/CRC.
[10] Burman, P. and Polonik, W. (2009), “Multivariate Mode Hunting: Data Analytic Tools with Measures of Signiﬁcance,”
Journal of Multivariate Analysis, 100, 1198–1218.
[11] Ciampi, A., J., T., Nakache, J. P., and B., A. (1986), “Stratiﬁcation by stepwise regression, correspondence analysis and
recursive partition,” Comp. Stat. Data Anal., 4, 185–204.
[12] Cox, D. (1972), “Regression models and life-tables,” J. R. Stat. Soc., 30, 248–275.
[13] Davis, R. B. and Anderson, J. R. (1989), “Exponential survival trees,” Stat Med, 8, 947–61.
[14] Dazard, J.-E., Choe, M., LeBlanc, M., and Rao, J. (2014), “Cross-Validated Survival Bump Hunting using Recursive
Peeling Methods,” in JSM Proceedings. Section for survival methods for risk estimation/prediction, Boston, MA, USA.:
American Statistical Association, vol. IMS JSM, pp. 3366–3380.
[15] — (2016), “PRIMsrc for Identiﬁcation and Characterization of Informative Prognostic Subgroups by Survival Bump
Hunting,” (submitted).
[16] Dazard,
J.-E.,
Choe,
M.,
LeBlanc,
M.,
and
Santana,
A.
(2015),
“Contributed
R
Package
PRIM-
src:
PRIM
Survival
Regression
Classiﬁcation,”
The
Comprehensive
R
Archive
Network,
https://cran.r-
project.org/web/packages/PRIMsrc/index.html.
[17] Dazard, J.-E. and Rao, J. (2010), “Local Sparse Bump Hunting,” J. Comp. Graph. Statist., 19, 900–929.
[18] Dazard, J.-E., Rao, J., and Markowitz, S. (2012), “Local Sparse Bump Hunting Reveals Molecular Heterogeneity Of
Colon Tumors,” Statistics in Medicine, 31, 1203–1220.
[19] Diaz-Pachon, D., Dazard, J.-E., and Rao, J. (2015), “Unsupervised bump hunting using principal components,” (submit-
ted), –.
[20] Diaz-Pachon, D., Rao, J., and Dazard, J.-E. (2015), “On the explanatory power of principal components,” (submitted),
–.
[21] Dobbin, K., Beer, D., Meyerson, M., Yeatman, T., Gerald, W., Jacobson, J., Conley, B., Buetow, K., Heiskanen, M.,
Simon, R., Minna, J., Girard, L., Misek, D., Taylor, J., Hanash, S., Naoki, K., Hayes, D., Ladd-Acosta, C., Enkemann,
S., Viale, A., and Giordano, T. (2005), “Interlaboratory comparability study of cancer gene expression analysis using
oligonucleotide microarrays,” Clin Cancer Res, 11, 565–572.
[22] Dobbin, K. and Simon, R. (2007), “Sample size planning for developing classiﬁers using high dimensional DNA microarray
data,” Biostatistics, 8, 101–117.
[23] Dupuy, A. and Simon, R. (2007), “Critical Review of Published Microarray Studies for Cancer Outcome and Guidelines
on Statistical Analysis and Reporting,” J. Nat. Cancer Institute, 99, 147–157.
[24] Efron, B. (1983), “Estimating the error rate of a predication rule: Improvement on cross-validation,” J Amer Stat Assoc,
78, 316–331.
39

[25] Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004), “Least angle regression,” The Annals of Statistics, 32,
407–499.
[26] Efron, B. and Tibshirani, R. (1993), An Introduction to the Bootstrap, London: Chapman & Hall / CRC.
[27] — (1997), “Improvements on Cross-Validation: The .632+ Bootstrap Method,” J Amer Stat Assoc, 92, 548–560.
[28] Ein-Dor, L., Kela, I., Getz, G., Givol, D., and Domany, E. (2005), “Outcome signature geness in breast cancer: is there
a unique set?” Bioinformatics, 21, 171178.
[29] Fan, J., Han, F., and Liu, H. (2014), “Challenges of big data analysis,” National Science Review, 1, 293–314.
[30] Fan, J. and Lv, J. (2008), “Sure independence screening for ultrahigh dimensional feature space,” J R Statist Soc B, 70,
849–911.
[31] Friedman, J. and Fisher, N. (1999), “Bump hunting in high-dimensional data,” Statistics and Computing, 9, 123–143.
[32] Gordon, L. and Olshen, R. A. (1985), “Tree-structured survival analysis,” Cancer Treat Rep, 69, 1065–9.
[33] Haibe-Kains, B., El-Hachem, N., Birkbak, N., Jin, A., Beck, A., Aerts, H., and Quackenbush, J. (2013), “Inconsistency
in large pharmacogenomic studies,” Nature, 504, 389–394.
[34] Harrell, F. E., J., Caliﬀ, R. M., Pryor, D. B., Lee, K. L., and Rosati, R. A. (1982), “Evaluating the yield of medical
tests,” JAMA : the journal of the American Medical Association, 247, 2543–6.
[35] Hartigan, J. and Mohanty, S. (1992), “The RUNT Test for Multimodality,” Joumal of Classiﬁcation, 9, 63–70.
[36] Hastie, T., Tibshirani, R., and Friedman, J. (2009), The Elements of Statistical Learning: Data Mining, Inference, and
Prediction, New York: Springer Science.
[37] Hothorn, T. and Lausen, B. (2003), “On the exact distribution of maximally selected rank statistics,” Comput. Statist.
Data Analysis, 43, 121137.
[38] Huang, J., Ma, S., and Xie, H. (2006), “Regularized estimation in the accelerated failure time model with high dimensional
covariates,” Biometrics, 62, 813820.
[39] Il-Gyo, C. and Chi-Hyuck, J. (2008), “Flexible patient rule induction method for optimizing process variables in discrete
type,” Expert Syst. Appl., 34, 3014–3020.
[40] Ishwaran, H., Kogalur, U., Blackstone, E., and Lauer, M. (2008), “Random survival forests,” The Annals of Applied
Statistics, 2, 841–860.
[41] Ishwaran, H. and Rao, J. S. (2005), “Spike and slab variable selection: frequentist and Bayesian strategies,” Ann Statist,
33, 730–773.
[42] Kalbﬂeisch, J. and Prentice, R. (2002), The Statistical Analysis of Failure Time Data., Wiley Series in Probability and
Statistics, Hoboken, NJ, USA: Wiley, 2nd ed.
[43] Kehl, V. and Ulm, K. (2006), “Responderidentiﬁcation in clinical trials with censored data,” Comput. Statist. Data Anal.,
50, 1338–1355.
[44] Klement, W. and Flach, P. (2008), “Soft Receiver Operating Characteristics Curves,” in Proceedings of the 3rd Interna-
tional Workshop on Evaluation Methods for Machine Learning, ICML.
[45] LeBlanc, M. and Crowley, J. (1992), “Relative risk trees for censored survival data,” Biometrics, 48, 411–25.
[46] — (1993), “Survival trees by goodness of split,” J Amer Stat Assoc, 88, 457–67.
[47] LeBlanc, M., Jacobson, J., and Crowley, J. (2002), “Partitioning and peeling for constructing prognostic groups,” Stat
Methods Med Res, 11, 247–74.
[48] LeBlanc, M., Moon, J., and Crowley, J. (2005), “Adaptive Risk Group Reﬁnement,” Biometrics, 61, 370–378.
[49] Leek, J., Scharpf, R., Bravo, H., Simcha, D., Langmead, B., Johnson, W., Geman, D., Baggerly, K., and Irizarry, R.
(2010), “Tackling the widespread and critical impact of batch eﬀects in high-throughput data,” Nat Rev Genet, 11,
733–739.
[50] Liu, X., Minin, V., Huang, Y., Seligson, D., and Horvath, S. (2004), “Statistical Methods for Analyzing Tissue Microarray
Data,” J. Pharm. Stat., 14, 671–685.
[51] Markatou, M., H., T., S., B., and G., H. (2005), “Analysis of Variance of Cross-Validation Estimators of the Generalization
Error,” J. Machine Learning Research, 6, 1127–1168.
40

[52] McShane, L., Cavenagh, M., Lively, T., Eberhard, D., Bigbee, W., Williams, P., Mesirov, J., Polley, M.-Y., Kim, K.,
Tricoli, J., Taylor, J., Shuman, D., Simon, R., Doroshow, J., and Conley, B. (2013), “Criteria for the use of omics-based
predictors in clinical trials: explanation and elaboration,” BMC Medicine, 11, 220.
[53] McShane, M., Cavenagh, M., Lively, T., Eberhard, D., Bigbee, W., Mickey Williams, P., Mesirov, J., Polley, M.-Y.,
Kim, K., Tricoli, J., Taylor, J., Shuman, D., Simon, R., Doroshow, J., and Conley, B. (2013), “Criteria for the use of
omics-based predictors in clinical trials,” Nature, 502, 317–320.
[54] Michiels, S., Koscielny, S., and Hill, C. (2005), “Prediction of cancer outcome with microarrays: a multiple random
validation strategy,” Lancet, 365, 488–492.
[55] Molinaro, A., Simon, R., and Pfeiﬀer, R. (2005), “Prediction error estimation: a comparison of resampling methods,”
Bioinformatics, 21, 3301–3307.
[56] Naftel, D., E., B., and M., T. (1985), “Conservation of events,” Research report.
[57] Ntzani, E. and Ioannidis, J. (2003), “Predictive ability of {DNA} microarrays for cancer outcomes and correlates: an
empirical assessment,” The Lancet, 362, 1439 – 1444.
[58] Polonik, W. (1995), “Measuring Mass Concentration and Estimating Density Contour Clusters: an Excess Mass Ap-
proach,” The Annals of Statistics, 23, 855–881.
[59] Polonik, W. and Wang, Z. (2010), “PRIM Analysis,” Journal of Multivariate Analysis, 101, 525–540.
[60] Ransohoﬀ, D. (2004), “Rules of evidence for cancer molecular marker discovery and validation,” Nature Reviews Cancer,
4, 309–314.
[61] Rozal, G. and Hartigan, J. (1994), “The MAP Test for Multimodality,” Journal of Classiﬁcation, 11, 5–36.
[62] Segal, M. R. (1988), “Regression Trees for Censored Data,” Biometrics, 44, 35–47.
[63] Shi, L., Reid, L., Jones, W., Shippy, R., Warrington, J., Baker, S., Collins, P., de Longueville, F., Kawasaki, E., Lee,
K., Luo, Y., Sun, Y., Willey, J., Setterquist, R., Fischer, G., Tong, W., Dragan, Y., Dix, D., Frueh, F., Goodsaid, F.,
Herman, D., Jensen, R., Johnson, C., Lobenhofer, E., Puri, R., Schrf, U., Thierry-Mieg, J., Wang, C., Wilson, M., and
Consortium, M. (2006), “The MicroArray Quality Control (MAQC) project shows inter- and intraplatform reproducibility
of gene expression measurements,” Nat Biotechnol, 24, 1151–1161.
[64] Simon, R., Radmacher, M., Dobbin, K., and McShane, L. (2003), “Pitfalls in the Use of DNA Microarray Data for
Diagnostic and Prognostic Classiﬁcation,” J. Nat. Cancer Institute, 95, 14–18.
[65] Simon, R., Subramanian, J., Li, M.-C., and Menezes, S. (2011), “Using cross-validation to evaluate predictive accuracy
of survival risk classiﬁers based on high-dimensional data,” Brieﬁngs in Bioinformatics, 12, 203–214.
[66] Subramanian, J. and Simon, R. (2010), “Gene expression-based prognostic signatures in lung cancer: ready for clinical
use?” J. Natl. Cancer Inst., 102, 464474.
[67] — (2011), “An evaluation of resampling methods for assessment of survival risk prediction in high-dimensional settings,”
Stat. Med., 30, 642653.
[68] — (2013), “Overﬁtting in prediction models Is it a problem only in high dimensions?” Contemporary Clinical Trials, 36,
636641.
[69] Therneau, T., Grambsch, P., and Fleming, T. (1990), “Martingale based residuals for survival models,” Biometrika, 77,
147–160.
[70] Tibshirani, R. (1996), “Regression shrinkage and selection via the Lasso,” J R Statist Soc, 58 (Series B), 267–288.
[71] Varma, S. and Simon, R. (2006), “Bias in error estimation when using cross-validation for model selection,” BMC
bioinformatics, 7, 91–99.
[72] Wang, P., Kim, Y., Pollack, J., and Tibshirani, R. (2004), “Boosted PRIM with Application to Searching for Oncogenic
Pathway of Lung Cancer,” in Computational Systems Bioinformatics Conference, International IEEE Computer Society,
IEEE Computer Society, pp. 604–609.
[73] Wu, L. and Chipman, H. (2003), “Bayesian Model-Assisted PRIM Algorithm,” Tech. rep., Departments of Statistics and
Actuarial Science, University of Waterloo.
[74] Zou, H. and Hastie, T. (2005), “Regularization and variable selection via the elastic net,” J R Statist Soc, 67 (Series B),
301–320.
41

SUPPORTING INFORMATION
Peeling Steps
LHR Mean Profiles
model #1 (cv=LHR)
0
2
4
6
8
11
14
17
20
23
26
0
5
10
15
20
RACV
RCCV
Peeling Steps
LRT Mean Profiles
model #1 (cv=LRT)
0
2
4
6
8
11
14
17
20
23
26
0
50
100
150
200
RACV
RCCV
Peeling Steps
CER Mean Profiles
model #1 (cv=CER)
0
2
4
6
8
11
14
17
20
23
26
0.0
0.4
0.8
RACV
RCCV
Peeling Steps
LHR Mean Profiles
model #2 (cv=LHR)
0
2
4
6
8
11
14
17
20
23
26
0
5
10
15
RACV
RCCV
Peeling Steps
LRT Mean Profiles
model #2 (cv=LRT)
0
2
4
6
8
11
14
17
20
23
26
0
50
100
150
RACV
RCCV
Peeling Steps
CER Mean Profiles
model #2 (cv=CER)
0
2
4
6
8
11
14
17
20
23
26
0.0
0.4
0.8
RACV
RCCV
Peeling Steps
LHR Mean Profiles
model #3 (cv=LHR)
0
2
4
6
8
11
14
17
20
23
26
−1.0
0.0
1.0
2.0
RACV
RCCV
Peeling Steps
LRT Mean Profiles
model #3 (cv=LRT)
0
2
4
6
8
11
14
17
20
23
26
0.0
1.0
2.0
3.0
RACV
RCCV
Peeling Steps
CER Mean Profiles
model #3 (cv=CER)
0
2
4
6
8
11
14
17
20
23
26
0.0
0.4
0.8
RACV
RCCV
Peeling Steps
LHR Mean Profiles
model #4 (cv=LHR)
0
2
4
6
8
10
12
14
16
18
0
1
2
3
4
5
RACV
RCCV
Peeling Steps
LRT Mean Profiles
model #4 (cv=LRT)
0
2
4
6
8
10
12
14
16
18
0
5
10
RACV
RCCV
Peeling Steps
CER Mean Profiles
model #4 (cv=CER)
0
2
4
6
8
10
12
14
16
18
0.0
0.4
0.8
RACV
RCCV
Supporting Figure 1: Comparison of cross-validated tuning proﬁles of box end-point statistics between cross-validation tech-
niques (overlaid: “Replicated Averaged CV” or RACV (black) vs. “Replicated Combined CV” or RCCV (red)) and opti-
mization criteria (by rows: Log Hazard Ratio (LHR), Log-Rank Test (LRT) or Concordance Error Rate CER) in a given
simulation model (by columns: simulation models #1, #2, #3 or #4). Results are for the Log Hazard Ratio (LHR) peeling
criterion. The resulting “Replicated CV” optimal peeling length ¯Lrcv (see eq. 19) of the peeling trajectory is shown in each
case (vertical dashed lines). Each dotted line corresponds to a cross-validated mean proﬁle of the statistic used in the opti-
mization criterion with the corresponding standard error of the sample mean, both calculated over the replications (B “ 128).
Notice the situations of cross-validation success or failure as described in section 4.3.1 and Figure 4. Also, notice the expected
increase of variance of cross-validated point estimates towards the right-end of the proﬁles corresponding to an increase in
model uncertainty and regions of risk of overﬁtting.

Peeling Steps
LHR Mean Profiles
model #1 (cv=LHR)
0
2
4
6
8
11
14
17
20
23
26
0
5
10
15
20
RACV
RCCV
Peeling Steps
LRT Mean Profiles
model #1 (cv=LRT)
0
2
4
6
8
11
14
17
20
23
26
0
50
100
200
RACV
RCCV
Peeling Steps
CER Mean Profiles
model #1 (cv=CER)
0
2
4
6
8
11
14
17
20
23
26
0.0
0.4
0.8
RACV
RCCV
Peeling Steps
LHR Mean Profiles
model #2 (cv=LHR)
0
2
4
6
8
11
14
17
20
23
26
0
5
10
15
RACV
RCCV
Peeling Steps
LRT Mean Profiles
model #2 (cv=LRT)
0
2
4
6
8
11
14
17
20
23
26
0
50
100
200
RACV
RCCV
Peeling Steps
CER Mean Profiles
model #2 (cv=CER)
0
2
4
6
8
11
14
17
20
23
26
0.0
0.4
0.8
RACV
RCCV
Peeling Steps
LHR Mean Profiles
model #3 (cv=LHR)
0
2
4
6
8
11
14
17
20
23
26
−1
0
1
2
RACV
RCCV
Peeling Steps
LRT Mean Profiles
model #3 (cv=LRT)
0
2
4
6
8
11
14
17
20
23
26
0
1
2
3
4
RACV
RCCV
Peeling Steps
CER Mean Profiles
model #3 (cv=CER)
0
2
4
6
8
11
14
17
20
23
26
0.0
0.4
0.8
RACV
RCCV
Peeling Steps
LHR Mean Profiles
model #4 (cv=LHR)
0
2
4
6
8
10
12
14
16
18
0
1
2
3
4
5
RACV
RCCV
Peeling Steps
LRT Mean Profiles
model #4 (cv=LRT)
0
2
4
6
8
10
12
14
16
18
0
2
4
6
8
RACV
RCCV
Peeling Steps
CER Mean Profiles
model #4 (cv=CER)
0
2
4
6
8
10
12
14
16
18
0.0
0.4
0.8
RACV
RCCV
Supporting Figure 2: Comparison of cross-validated tuning proﬁles of box end-point statistics between cross-validation tech-
niques (overlaid: “Replicated Averaged CV” or RACV (black) vs. “Replicated Combined CV” or RCCV (red)) and opti-
mization criteria (by rows: Log Hazard Ratio (LHR), Log-Rank Test (LRT) or Concordance Error Rate CER) in a given
simulation model (by columns: simulation models #1, #2, #3 or #4). Results are for the Log-Rank Test (LRT) peeling
criterion. The resulting “Replicated CV” optimal peeling length ¯Lrcv (see eq. 19) of the peeling trajectory is shown in each
case (vertical dashed lines). Each dotted line corresponds to a cross-validated mean proﬁle of the statistic used in the opti-
mization criterion with the corresponding standard error of the sample mean, both calculated over the replications (B “ 128).
Notice the situations of cross-validation success or failure as described in section 4.3.1 and Figure 4. Also, notice the expected
increase of variance of cross-validated point estimates towards the right-end of the proﬁles corresponding to an increase in
model uncertainty and regions of risk of overﬁtting.

Peeling Steps
LHR Mean Profiles
model #1 (cv=LHR)
0
2
4
6
8
11
14
17
20
23
26
0
5
10
15
RACV
RCCV
Peeling Steps
LRT Mean Profiles
model #1 (cv=LRT)
0
2
4
6
8
11
14
17
20
23
26
0
50
100
150
RACV
RCCV
Peeling Steps
CER Mean Profiles
model #1 (cv=CER)
0
2
4
6
8
11
14
17
20
23
26
0.0
0.4
0.8
RACV
RCCV
Peeling Steps
LHR Mean Profiles
model #2 (cv=LHR)
0
2
4
6
8
11
14
17
20
23
26
0
5
10
15
RACV
RCCV
Peeling Steps
LRT Mean Profiles
model #2 (cv=LRT)
0
2
4
6
8
11
14
17
20
23
26
0
50
100
150
RACV
RCCV
Peeling Steps
CER Mean Profiles
model #2 (cv=CER)
0
2
4
6
8
11
14
17
20
23
26
0.0
0.4
0.8
RACV
RCCV
Peeling Steps
LHR Mean Profiles
model #3 (cv=LHR)
0
2
4
6
8
11
14
17
20
23
26
−1
0
1
2
RACV
RCCV
Peeling Steps
LRT Mean Profiles
model #3 (cv=LRT)
0
2
4
6
8
11
14
17
20
23
26
0
1
2
3
RACV
RCCV
Peeling Steps
CER Mean Profiles
model #3 (cv=CER)
0
2
4
6
8
11
14
17
20
23
26
0.0
0.4
0.8
RACV
RCCV
Peeling Steps
LHR Mean Profiles
model #4 (cv=LHR)
0
2
4
6
8
10
12
14
16
18
0
1
2
3
4
RACV
RCCV
Peeling Steps
LRT Mean Profiles
model #4 (cv=LRT)
0
2
4
6
8
10
12
14
16
18
0
2
4
6
8
10
RACV
RCCV
Peeling Steps
CER Mean Profiles
model #4 (cv=CER)
0
2
4
6
8
10
12
14
16
18
0.0
0.4
0.8
RACV
RCCV
Supporting Figure 3: Comparison of cross-validated tuning proﬁles of box end-point statistics between cross-validation tech-
niques (overlaid: “Replicated Averaged CV” or RACV (black) vs. “Replicated Combined CV” or RCCV (red)) and opti-
mization criteria (by rows: Log Hazard Ratio (LHR), Log-Rank Test (LRT) or Concordance Error Rate CER) in a given
simulation model (by columns: simulation models #1, #2, #3 or #4). Results are for the Cumulative Hazard Summary
(CHS) peeling criterion. The resulting “Replicated CV” optimal peeling length ¯Lrcv (see eq. 19) of the peeling trajectory is
shown in each case (vertical dashed lines). Each dotted line corresponds to a cross-validated mean proﬁle of the statistic used
in the optimization criterion with the corresponding standard error of the sample mean, both calculated over the replications
(B “ 128). Notice the situations of cross-validation success or failure as described in section 4.3.1 and Figure 4. Also, notice
the expected increase of variance of cross-validated point estimates towards the right-end of the proﬁles corresponding to an
increase in model uncertainty and regions of risk of overﬁtting.

5
10
15
20
0.4
0.6
0.8
1.0
1.2
1.4
1.6
BCV (n=50)
Peeling Steps
Box Coeff. Variation
RACV
RCCV
5
10
15
20
0.00
0.05
0.10
0.15
Support (n=50)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.0
0.5
1.0
1.5
2.0
2.5
3.0
MEFT (n=50)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.0
0.5
1.0
1.5
2.0
MEFP (n=50)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.1
0.2
0.3
0.4
0.5
LHR (n=50)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.2
0.3
0.4
0.5
0.6
LRT (n=50)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.00
0.05
0.10
0.15
0.20
CER (n=50)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.4
0.6
0.8
1.0
1.2
1.4
1.6
BCV (n=100)
Peeling Steps
Box Coeff. Variation
RACV
RCCV
5
10
15
20
0.00
0.05
0.10
0.15
Support (n=100)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.0
0.5
1.0
1.5
2.0
2.5
3.0
MEFT (n=100)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.0
0.5
1.0
1.5
2.0
MEFP (n=100)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.1
0.2
0.3
0.4
0.5
LHR (n=100)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.2
0.3
0.4
0.5
0.6
LRT (n=100)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.00
0.05
0.10
0.15
0.20
CER (n=100)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.4
0.6
0.8
1.0
1.2
1.4
1.6
BCV (n=200)
Peeling Steps
Box Coeff. Variation
RACV
RCCV
5
10
15
20
0.00
0.05
0.10
0.15
Support (n=200)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.0
0.5
1.0
1.5
2.0
2.5
3.0
MEFT (n=200)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.0
0.5
1.0
1.5
2.0
MEFP (n=200)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.1
0.2
0.3
0.4
0.5
LHR (n=200)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.2
0.3
0.4
0.5
0.6
LRT (n=200)
Peeling Steps
Coeff. Variation
RACV
RCCV
5
10
15
20
0.00
0.05
0.10
0.15
0.20
CER (n=200)
Peeling Steps
Coeff. Variation
RACV
RCCV
Supporting Figure 4: Proﬁles of coeﬃcient of variation of Box Coeﬃcient of Variation (BCV), survival end-points and predic-
tion performance metrics. Comparative coeﬃcient of variation proﬁles are shown for situations with decreasing sample sizes
n P t50, 100, 200u. Results are for simulated model #1 and the LRT statistic used in both peeling and optimization criteria.

Supporting Table 1: Eﬀect of peeling and optimization criteria as well as cross-validation techniques on the cross-validated
numbers of used covariates by the PRSP algorithm (see Algorithm 1) out of the total number of pre-selected ones (brackets).
Numbers are reported for the combined eﬀects of: (i) peeling criteria (by rows: Log Hazard Ratio (LHR), Log-Rank Test
(LRT) or Cumulative Hazard Summary (CHS)), (ii) optimization criteria (by columns: Log Hazard Ratio (LHR), Log-Rank
Test (LRT) or Concordance Error Rate (CER)), (iii) cross-validation techniques (by columns: “Replicated Averaged CV” or
RACV and “Replicated Combined CV” or RCCV), and (iv) the four tested simulation models (by rows: Model #1, #2, #3
or #4).
Model #1
Optimization Criterion
LHR
LRT
CER
RACV
RCCV
RACV
RCCV
RACV
RCCV
Peeling
LHR
3[ 3]
3[ 3]
3[ 3]
3[ 3]
3[ 3]
3[ 3]
Criterion
LRT
3[ 3]
3[ 3]
3[ 3]
3[ 3]
2[ 3]
2[ 3]
CHS
3[ 3]
3[ 3]
3[ 3]
3[ 3]
3[ 3]
3[ 3]
Model #2
Optimization Criterion
LHR
LRT
CER
RACV
RCCV
RACV
RCCV
RACV
RCCV
Peeling
LHR
3[ 3]
3[ 3]
3[ 3]
3[ 3]
3[ 3]
3[ 3]
Criterion
LRT
3[ 3]
3[ 3]
2[ 3]
2[ 3]
2[ 3]
2[ 3]
CHS
3[ 3]
3[ 3]
3[ 3]
3[ 3]
3[ 3]
3[ 3]
Model #3
Optimization Criterion
LHR
LRT
CER
RACV
RCCV
RACV
RCCV
RACV
RCCV
Peeling
LHR
3[ 3]
1[ 3]
3[ 3]
1[ 3]
3[ 3]
2[ 3]
Criterion
LRT
3[ 3]
2[ 3]
3[ 3]
2[ 3]
3[ 3]
2[ 3]
CHS
2[ 3]
1[ 3]
2[ 3]
2[ 3]
2[ 3]
2[ 3]
Model #4
Optimization Criterion
LHR
LRT
CER
RACV
RCCV
RACV
RCCV
RACV
RCCV
Peeling
LHR
11[352]
6[352]
10[352]
3[352]
3[352]
3[352]
Criterion
LRT
8[352]
1[352]
8[352]
5[352]
3[352]
5[352]
CHS
3[352]
1[352]
3[352]
3[352]
3[352]
3[352]

0.0
0.2
0.4
0.6
0.8
1.0
−2
−1
0
1
X6 covariate trajectory
Box mass
X6
model #4
0.0
0.2
0.4
0.6
0.8
1.0
−2
−1
0
1
2
3
X32 covariate trajectory
Box mass
X32
model #4
0.0
0.2
0.4
0.6
0.8
1.0
−2
−1
0
1
2
X34 covariate trajectory
Box mass
X34
model #4
0.0
0.2
0.4
0.6
0.8
1.0
−2
−1
0
1
2
X72 covariate trajectory
Box mass
X72
model #4
0.0
0.2
0.4
0.6
0.8
1.0
−2
−1
0
1
2
X79 covariate trajectory
Box mass
X79
model #4
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Box support trajectory
Box mass
Support (β)
model #4
0.0
0.2
0.4
0.6
0.8
1.0
1.90
1.92
1.94
1.96
1.98
MEFT trajectory
Box mass
Time
model #4
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
MEFP trajectory
Box mass
Probability
model #4
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.1
0.2
0.3
0.4
0.5
LHR trajectory
Box mass
Log−Hazard Ratio (λ)
model #4
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
LRT trajectory
Box mass
Log−rank test (χ2)
model #4
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
CER trajectory
Box mass
1−C (θ)
model #4
Supporting Figure 5: Comparison of replicated combined cross-validated results for the peeling trajectories in simulated model
#4 for the “Replicated Combined CV” (RCCV) technique and the Cumulative Hazard Summary CHS as peeling criterion and
the Concordance Error Rate CER as optimization criteria. Notice that covariates px6, x32, x34, x72, x79q were those eﬀectively
used by the PRSP algorithm (see Algorithm 1) out of p “ 1000 total covariates and p “ 100 informative ones (see simulation
design 4.1).

0.0
0.2
0.4
0.6
0.8
1.0
−0.6
−0.4
−0.2
0.0
0.2
Covariate Importance
Box mass
(centered) Covariate Range
X6
X32
X34
X72
X79
model #4
0.0
0.2
0.4
0.6
0.8
1.0
Covariate Usage
Box mass
Covariates
X6
X32
X34
X72
X79
model #4
Supporting Figure 6: Comparison of replicated combined cross-validated trace plots of covariate importance Ď
V Iplq (top)
and covariate usage Ě
V Uplq (bottom) in simulated model #4 for the “Replicated Combined CV” (RCCV) technique and the
Cumulative Hazard Summary CHS as peeling criterion and the Concordance Error Rate CER as optimization criteria. Notice
that covariates px6, x32, x34, x72, x79q were those eﬀectively used by the PRSP algorithm (see Algorithm 1) out of p “ 1000
total covariates and p “ 100 informative ones (see simulation design 4.1).
Supporting Table 2: Comparison of cross-validated decision rules (upper Supporting Table) and box end points statistics
of interest (lower Supporting Table) in simulated model #4 for the “Replicated Combined CV” (RCCV) technique and the
Cumulative Hazard Summary CHS as peeling criterion and the Concordance Error Rate CER as optimization criteria. For
conciseness, only the initial and ﬁnal decision rules (¯Lrcvth step) are shown. Step #0 corresponds to the situation where the
starting box covers the entire test-set data Lk before peeling. Values are sample mean estimates with corresponding standard
errors in parenthesis. Notice that covariates px6, x32, x34, x72, x79q were those eﬀectively used by the PRSP algorithm (see
Algorithm 1) out of p “ 1000 total covariates and p “ 100 informative ones (see simulation design 4.1).
Step l
x6
x32
x34
x72
x79
0
x6 ď 1.81 p0.00q
x32 ě ´2.07 p0.00q
x34 ě ´2.32 p0.00q
x72 ě 2.81 p0.00q
x79 ě ´2.75 p0.00q
model #4
1
x6 ď 1.80 p0.43q
x32 ě ´2.06 p0.03q
x34 ě ´2.29 p0.06q
x72 ě 2.81 p0.04q
x79 ě ´2.25 p0.30q
...
...
...
...
...
...
8
x6 ď 1.57 p0.25q
x32 ě ´1.84 p0.23q
x34 ě ´2.11 p0.20q
x72 ě 2.32 p0.50q
x79 ě ´1.83 p0.40q
Step l
nplq
¯βrcvplq
Ď
T 1
0
rcvplq
Ď
P 1
0
rcvplq
¯λrcvplq
¯χrcvplq
¯θrcvplq
0
100 (0.00)
1.00 (0.00)
1.99 (0.00)
0.46 (0.00)
0.00 (0.00)
0.00 (0.00)
1.00 (0.00)
model #4
1
89 (2.00)
0.89 (0.02)
1.98 (0.01)
0.43 (0.02)
0.50 (0.43)
2.63 (2.66)
0.47 (0.02)
...
...
...
...
...
...
...
...
8
36 (6.00)
0.36 (0.06)
1.89 (0.16)
0.37 (0.07)
0.49 (0.37)
3.99 (4.78)
0.44 (0.04)
