Semivariogram methods for modeling
Whittle-MatÂ´ern priors in Bayesian inverse problems
Richard D Brown1, Johnathan M Bardsley1 and Tiangang Cui2
1 Department of Mathematical Sciences, University of Montana, Missoula, MT
59812, United States
2 School of Mathematics, Monash University, Melbourne, Australia
E-mail: rick.brown@umontana.edu, bardsleyj@mso.umt.edu,
Tiangang.Cui@monash.edu
Abstract.
We present a new technique, based on semivariogram methodology, for
obtaining point estimates for use in prior modeling for solving Bayesian inverse
problems.
This method requires a connection between Gaussian processes with
covariance operators deï¬ned by the MatÂ´ern covariance function and Gaussian processes
with precision (inverse-covariance) operators deï¬ned by the Greenâ€™s functions of a class
of elliptic stochastic partial diï¬€erential equations (SPDEs).
We present a detailed
mathematical description of this connection. We will show that there is an equivalence
between these two Gaussian processes when the domain is inï¬nite â€“ for us, R2 â€“
which breaks down when the domain is ï¬nite due to the eï¬€ect of boundary conditions
on Greenâ€™s functions of PDEs. We show how this connection can be re-established
using extended domains. We then introduce the semivariogram method for estimating
the MatÂ´ern covariance hyperparameters, which specify the Gaussian prior needed for
stabilizing the inverse problem. Results are extended from the isotropic case to the
anisotropic case where the correlation length in one direction is larger than another.
Finally, we consider the situation where the correlation length is spatially dependent
rather than constant. We implement each method in two-dimensional image inpainting
test cases to show that it works on practical examples.
Keywords:
inverse problems, variogram, Bayesian methods, boundary conditions,
Whittle-MatÂ´ern, stochastic partial diï¬€erential equations, Gaussian ï¬eld
1. Introduction
Inverse problems are ubiquitous in science and engineering. They are characterized by
the estimation of parameters in a mathematical model from measurements and by a
high-dimensional parameter space that typically results from discretizing a function
deï¬ned on a computational domain.
For typical inverse problems, the process of
estimating model parameters from measurements is ill-posed, which motivates the use
of regularization in the deterministic setting and the choice of a prior probability density
in the Bayesian setting. In this paper, we consider linear models of the form
b = Ax + Ïµ,
Ïµ âˆ¼N(0, Î»âˆ’1IM),
(1)
arXiv:1811.09446v3  [math.NA]  9 May 2020

Semivariogram methods for inverse problems
2
where b âˆˆRM is the vector of measurements, A âˆˆRMÃ—N is the forward model
matrix, x âˆˆRN is the vector of unknown parameters, and Ïµ âˆ¼N(0, Î»âˆ’1IM) is the
observation noise that follows a zero-mean Gaussian distrubution with covariance matrix
Î»âˆ’1IM, with IM denoting the M Ã— M identity. In typical inverse problems, Ax is the
discretization of a continuous forward model Ax, where A is a linear operator and x
is a function. The components of the vector x satisfy xi = x(ui), where ui âˆˆRd is
the location of the ith element of the numerical grid. The random vector b in (1) has
conditional probability density function
p(b|x, Î») âˆexp

âˆ’Î»
2âˆ¥Ax âˆ’bâˆ¥2

,
(2)
where âˆdenotes proportionality and âˆ¥Â· âˆ¥denotes the â„“2-norm.
The maximizer of
p(b|x, Î») with respect to x is known as the maximum likelihood estimator, and we
denote it by xML. As stated above, due to ill-posedness, xML is unstable with respect
to errors in b, i.e., small changes in b result in large relative changes in xML.
There are various methods to stabilize the solution of inverse problems, but they
all involve some form of regularization. In this paper, we take the Bayesian approach
[1], which requires the deï¬nition of a prior probability density function on x. We make
the assumption that the prior is Gaussian of the form x âˆ¼N (0, (Î´P)âˆ’1), which has
probability density function
p(x|Î´) âˆexp

âˆ’Î´
2xTPx

,
(3)
where P is the precision (inverse-covariance) matrix.
Now that we have deï¬ned the prior (3) and the likelihood (2), using Bayesâ€™ law, we
multiply them together to obtain the posterior density function
p(x|b, Î», Î´) âˆp(b|x, Î»)p(x|Î´)
âˆexp

âˆ’Î»
2âˆ¥Ax âˆ’bâˆ¥2 âˆ’Î´
2xTPx

,
(4)
whose maximizer, xÎ»,Î´, is known as the maximum a posteriori (MAP) estimator. The
MAP estimator can be equivalently expressed as
xÎ»,Î´ = arg min
x
Î»
2âˆ¥Ax âˆ’bâˆ¥2 + Î´
2xTPx

.
Our primary focus in this paper is to provide formulations and hyperparameter selection
techniques for prior precision matrices that have an intuitive interpretation and can be
used to solve a wide variety of problems.
1.1. The MatÂ´ern Class of Covariance Matrices and Whittle-MatÂ´ern Priors
It remains to deï¬ne the prior covariance matrix C = Pâˆ’1.
The MatÂ´ern class of
covariance matrices has garnered much praise [2] for its ï¬‚exibility in capturing many
covariance structures and its allowance of direct control of the degree of correlation in

Semivariogram methods for inverse problems
3
the vector x [3]. The MatÂ´ern covariance matrix is deï¬ned by the MatÂ´ern covariance
function, which was ï¬rst formulated by MatÂ´ern in 1947 [4],
C(r) = Ïƒ2(r/â„“)Î½KÎ½(r/â„“)
2Î½âˆ’1Î“(Î½)
,
(5)
where r is the separation distance; KÎ½(Â·) is the modiï¬ed Bessel function of the second
kind of order Î½ [5]; Î“(Â·) is the gamma function; â„“> 0 is the range parameter; Î½ > 0
is the smoothness parameter; and Ïƒ2 is the marginal variance. Omitting Ïƒ2 gives the
MatÂ´ern correlation function. In the isotropic case, when the covariance depends only
on the distance between elements, given the covariance parameters Ïƒ2, Î½, and â„“, one
can obtain the covariance matrix C of a vector x = [x1, . . . , xN]T with spatial positions
{uT
1 , . . . , uT
N} âŠ‚Rd by letting
[C]ij = Cov(xi, xj) = C(âˆ¥ui âˆ’ujâˆ¥),
where C is deï¬ned by (5).
The parameters of the MatÂ´ern covariance function are not as straightforward to
interpret as the parameters of some other covariance functions.
When Î½ is small
(Î½ â†’0+), the spatial process is said to be rough, and when it is large (Î½ â†’âˆ),
the process is smooth [3, 6]. Figure 1 shows how the covariance function behaves with
diï¬€erent values of â„“and Î½: on the left, â„“= Ïƒ2 = 1 and Î½ varies, while on the right
Î½ = Ïƒ2 = 1 and â„“varies. Note that as Î½ increases, the behavior at small lags changes,
leading to more correlation at smaller distances and a larger practical range, which is
deï¬ned to be the distance at which the correlation is equal to 0.05. In Figure 1, this is
the distance at which the covariance function intersects the horizontal line. Meanwhile,
as â„“decreases, the decay rate of the covariance increases considerably, which decreases
the practical range.
Although â„“is known as the range parameter, the parameter Î½
also aï¬€ects the practical range. In [7], a range approximation Ï = â„“
âˆš
8Î½ is used where
C(Ï) â‰ˆ0.10.
Despite the beneï¬ts of using the MatÂ´ern class of covariance matrices, its use can
be problematic for inverse problems because computing the precision matrix P, which
is what appears in the posterior (4), requires inverting a dense N Ã— N matrix. Using
the fast Fourier transform (FFT) [8, 9, 10] to operate with P and C more eï¬ƒciently
is recommended if x is deï¬ned on a regular grid and periodic boundary conditions are
assumed. In other cases, it is useful that the MatÂ´ern covariance function has a direct
connection to a class of elliptic SPDEs [7] whose numerical discretization yields sparse
precision matrices, P, that are computationally feasible to work with even when N is
large. Connections of this type were ï¬rst shown to exist by Whittle in [11], where he
showed the connection held for a special case of the MatÂ´ern covariance class. Hence,
priors that depend on this connection are often referred to as Whittle-MatÂ´ern priors.
The connection between the general MatÂ´ern covariance function and SPDEs has been
used in a wide range of applications for deï¬ning computationally feasible priors for
high-dimensional problems [12, 13, 14]. Moreover, work has been done in establishing

Semivariogram methods for inverse problems
4
0
1
2
3
4
5
6
7
8
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
1
2
3
4
5
6
7
8
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 1. Behavior of the MatÂ´ern covariance function. The smoothness parameter,
Î½, primarily aï¬€ects the covariance at small distances whereas the range parameter, â„“,
mainly aï¬€ects the decay rate of the covariance. The horizontal line corresponds to a
covariance value of 0.05 and the practical range is the distance at which the covariance
intersects this line.
convergence theorems for, and lattice approximations of, these Whittle-MatÂ´ern priors
[15].
The remainder of the paper is organized as follows. In Section 2, we describe in
detail the connection between zero-mean Gaussian processes with the isotropic MatÂ´ern
covariance operator and those that arise as solutions of a class of elliptic SPDEs. In
Section 3, we show how to estimate the hyperparameters in the isotropic Whittle-MatÂ´ern
prior using the semivariogram method, and then we show how to use this approach to
deï¬ne the prior when solving a Bayesian inverse problem. In Section 4, we extend these
ideas to the anisotropic case and then we consider images with regions that require
diï¬€erent covariance structures in Section 5. For each section, we present numerical tests
on two-dimensional image inpainting test cases. We end with conclusions in Section 6.
2. Whittle-MatÂ´ern Class Priors via SPDEs
In this section, we will show that the Whittle-MatÂ´ern class of priors can be speciï¬ed as
the solution of the SPDE
(1 âˆ’â„“2âˆ†)Î²/2x(u) = W(u),
u âˆˆRd,
Î² = Î½ + d/2,
â„“, Î½ > 0,
(6)
where âˆ†= Pd
i=1
âˆ‚2
du2
i is the Laplacian operator in d dimensions, and W is spatial
Gaussian white noise with unit variance, which we deï¬ne below.
Although this
connection has been shown to exist [11, 7, 13], here we provide a signiï¬cantly more
detailed derivation of this result than we have seen elsewhere. Our derivation is based
on the Greenâ€™s function of the diï¬€erential operator. For other linear operators with
suï¬ƒcient smoothness, e.g., the one in the Stokes equations and the one in the heat
equation, the corresponding SPDEs can be used to deï¬ne diï¬€erent Gaussian processes
[16]. The method we employ here provides a potential way to derive the covariance
functions of the Gaussian processes induced by other linear SPDEs as well.

Semivariogram methods for inverse problems
5
2.1. Preliminary Deï¬nitions
Before deriving the solution of (6), we need some preliminary deï¬nitions.
2.1.1. Gaussian Fields
A stochastic process {x(u), u âˆˆâ„¦}, with â„¦âŠ‚Rd, is a Gaussian
ï¬eld [17] if for any k â‰¥1 and any locations u1, . . . , uk âˆˆâ„¦, [x(u1), . . . , x(uk)]T
is a normally distributed random vector with mean Âµ = [E[x(u1)], . . . , E[x(uk)]]T,
where E[Â·] denotes expected value, and covariance matrix [C]ij = Cov(x(ui), x(uj)) =
E[(x(ui) âˆ’E[x(ui)])(x(uj) âˆ’E[x(uj)])], for 1 â‰¤i, j â‰¤k. The covariance function
is deï¬ned C(ui, uj) := Cov(x(ui), x(uj)). It is necessary that the covariance function
is positive deï¬nite, i.e., for any {u1, . . . , uk}, with k â‰¥1, the covariance matrix C
deï¬ned above is positive deï¬nite. The Gaussian ï¬eld is called stationary if the mean
is constant and the covariance function satisï¬es C(u, v) = C(u âˆ’v) and isotropic if
C(u, v) = C(âˆ¥u âˆ’vâˆ¥).
2.1.2. White Noise
The term white noise [16, 18] comes from light. White light is a
homogeneous mix of wavelengths, as opposed to colored light, which is a heterogeneous
mix of wavelengths. In a similar way, white noise contains a homogeneous mix of all
the diï¬€erent basis functions. The mixing of these basis functions is determined by a
random process. When this random process is Gaussian, we have Gaussian white noise.
Consider a domain â„¦and let {Ï†j : j = 1, 2, . . .} be an orthonormal basis of L2(â„¦) where
L2(â„¦) =

f : â„¦â†’R |
R
â„¦|f(x)|2dx < âˆ
	
. Then Gaussian white noise is deï¬ned by
W(u) =
âˆ
X
j=1
Î¾jÏ†j(u),
Î¾j
iid
âˆ¼N(0, Î·2).
(7)
If we are dealing with spatial Gaussian white noise with unit variance, then u refers
to location and Î·2 = 1.
With this deï¬nition, it is clear that Gaussian white noise
has mean zero:
E[W(u)] = Pâˆ
j=1 E [Î¾j] Ï†j(u) = 0. Moreover, one can show that
Cov (W(u), W(v)) = Î·2Î´f(u âˆ’v), where Î´f(Â·) is the Dirac delta function [19], also
known as the delta distribution. We include the subscript f to diï¬€erentiate the delta
function from the Î´ hyperparameter used elsewhere in this paper. A well-known and very
important property of the Dirac delta function is that it satisï¬es the sifting property:
f(u) =
R
Rd Î´f(u âˆ’v)f(v)dv.
2.1.3. Greenâ€™s Functions
We now consider diï¬€erential equations of the form Lx(u) =
f(u), u âˆˆRd, where L is a linear, diï¬€erential operator. A Greenâ€™s function [20, 21], g,
of L is any solution of Lg(u, v) = Î´f(u âˆ’v). Using the Greenâ€™s function, the solution
of the equation Lx(u) = f(u) can be written as
x(u) =
Z
Rd g(u, v)f(v)dv.
(8)

Semivariogram methods for inverse problems
6
2.2. The Gaussian Field Solution of the SPDE (6)
In this subsection, we will prove the following theorem concerning the solution of the
SPDE (6).
Theorem 1 The solution x(u) of (6) is a Gaussian ï¬eld with mean zero and MatÂ´ern
covariance function deï¬ned by (5).
Proof. To begin, we note that the Greenâ€™s function for (6) is the solution of
(1 âˆ’â„“2âˆ†)Î²/2g(u, v) = Î´f(v âˆ’u).
(9)
Using (8), the solution to (6) is given by
x(u) =
Z
Rd g(u, v)W(v)dv,
(10)
making x(u) a Gaussian ï¬eld since it is a linear transformation of Gaussian white noise.
We now compute the mean and covariance of the Gaussian ï¬eld, x(u), deï¬ned by
(10). Since the Greenâ€™s function is a strictly-positive, symmetric, and rapidly decaying
function, we can apply Fubiniâ€™s theorem [22] to obtain the mean of x(u):
E[x(u)] = E
Z
Rd g(u, v)W(v)dv

=
Z
Rd g(u, v)E [W(v)] dv = 0.
Since x(u) has mean zero, the covariance is given by
Cov(x(u), x(uâ€²)) = E[x(u)x(uâ€²)]
=
Z
Rd
Z
Rd E[W(v)W(vâ€²)]g(u, v)dv

g(uâ€², vâ€²)dvâ€²
=
Z
Rd
Z
Rd Î´f(v âˆ’vâ€²)g(u, v)dv

g(uâ€², vâ€²)dvâ€²
=
Z
Rd g(u, vâ€²)g(uâ€², vâ€²)dvâ€².
If we deï¬ne C(u, uâ€²) := Cov(x(u), x(uâ€²)), the previous result implies that if L =
(1 âˆ’â„“2âˆ†)Î²/2, then for our linear L acting only on uâ€²,
LC(u, uâ€²) = L
Z
Rd g(u, vâ€²)g(uâ€², vâ€²)dvâ€²
=
Z
Rd
h
Lg(uâ€², vâ€²)
i
g(u, vâ€²)dvâ€²
=
Z
Rd Î´f(uâ€² âˆ’vâ€²)g(u, vâ€²)dvâ€²
= g(u, uâ€²).
(11)
To derive the Greenâ€™s function g in (11), we ï¬rst deï¬ne g(u) := g(u, 0). Then (9)
implies
(1 âˆ’â„“2âˆ†)Î²/2g(u) = Î´f(u).
(12)

Semivariogram methods for inverse problems
7
To proceed, we must take the Fourier transform [23, 24] of both sides of (12). This
yields
(1 + â„“2âˆ¥Ï‰âˆ¥2)Î²/2Ë†g(Ï‰) = 1,
where Ï‰ âˆˆCd are the coordinates in the Fourier-transformed space and the hat ( Ë†f)
notation denotes the Fourier-transform of a function f. Thus, the Fourier transform of
the Greenâ€™s function is
Ë†g(Ï‰) = (1 + â„“2âˆ¥Ï‰âˆ¥2)âˆ’Î²/2.
(13)
Next, we assume stationarity so that the covariance only depends on the relative
locations of the points, i.e., r := u âˆ’v. Then E[x(u)x(v)] = E[x(r)x(0)] = C(r, 0) :=
C(r) and (11) can be expressed LC(r) = g(r). If we take the Fourier transform of both
sides of this equation, and appeal to (13), we obtain
Ë†C(Ï‰) = (1 + â„“2âˆ¥Ï‰âˆ¥2)âˆ’Î².
Since the Laplacian, âˆ†, is invariant under rotations and translations, we have radial
symmetry, which is analogous to isotropy in the covariance. Thus we can let s = âˆ¥Ï‰âˆ¥
and r = âˆ¥râˆ¥to obtain the equivalent expression
Ë†C(s) = (1 + â„“2s2)âˆ’Î².
(14)
To transform back to the original (r) space, we use the Hankel transform [25] and its
relationship to the radially symmetric Fourier transform, i.e.,
s
dâˆ’2
2 Ë†C(s) = (2Ï€)
d
2
Z âˆ
0
J dâˆ’2
2 (sr)r
dâˆ’2
2 C(r)rdr,
where C is the original (untransformed) covariance function and JÎ½(Â·) is the Bessel
function of the ï¬rst kind of order Î½; see [26, Section 2] for a proof. Using appropriate
substitutions in the inverse Hankel transform and (14), we obtain
C(r) = (2Ï€)âˆ’d
2
r
dâˆ’1
2
Z âˆ
0
J dâˆ’2
2 (sr)s
dâˆ’1
2 (1 + â„“2s2)âˆ’Î²(sr)1/2ds.
Finally, using the integral identity [27, Eq. 20, p. 24, vol. II] and some algebra, we
obtain
C(r) =
â„“âˆ’Î²âˆ’d
2rÎ²âˆ’d
2K d
2 âˆ’Î²(r/â„“)
(2Ï€)
d
22Î²âˆ’1Î“(Î²)
.
(15)
Using the fact that KÎ½ = Kâˆ’Î½, and deï¬ning Ïƒ2 := Î“(Î½)[â„“d(4Ï€)d/2Î“(Î½ + d/2)]âˆ’1 with
Î½ := Î² âˆ’d/2, it can be shown that (15) is exactly the MatÂ´ern covariance function (5).
â–¡
2.3. The Eï¬€ect of a Finite Domain and Boundary Conditions
The proof of Theorem 1 above assumed that the domain was all of Rd, i.e. â„¦= Rd.
However, when solving inverse problems, x(u) is restricted to a ï¬nite domain â„¦âŠ‚Rd.
In such cases, boundary conditions that modify the Greenâ€™s function must be assumed,

Semivariogram methods for inverse problems
8
and thus the equivalence between the Gaussian ï¬elds deï¬ned by the SPDE (6) and those
deï¬ned by the MatÂ´ern covariance function may not hold.
To see this, consider the case where d = 2 and â„¦= [0, 1] Ã— [0, 1] with Dirichlet
(zero) boundary conditions, x(0, t) = x(1, t) = x(s, 0) = x(s, 1) = 0, where 0 â‰¤s, t â‰¤1.
Additionally, we assume Î½ = 1 so that the exponent of the diï¬€erential operator is equal
to one, making the discretization straightforward. In this case, (6) simpliï¬es to
(1 âˆ’â„“2âˆ†)x(u) = W(u),
u âˆˆR2,
â„“> 0.
Using a uniform mesh on [0, 1] Ã— [0, 1] with a step size of h = 1/n, so that N = n2,
yields the numerical discretization
(IN + (â„“/h)2L2D)x = Î´âˆ’1/2Î¾,
Î¾ âˆ¼N(0, In),
where Î´ is the scaling parameter for the prior and (1/h2)L2Dx is the standard ï¬nite-
diï¬€erence discretization of (âˆ’âˆ‚2x(u)/âˆ‚u2
1 âˆ’âˆ‚2x(u)/âˆ‚u2
2) [10].
Then the probability
density for x is given by
x|Î´, â„“âˆ¼N
 0, Î´âˆ’1(I + (â„“/h)2L2D)âˆ’2
,
or equivalently,
p(x|Î´, â„“) âˆexp

âˆ’Î´
2xT(I + (â„“/h)2L2D)2x

.
(16)
When discretizing the SPDE, there is a scaling factor needed that guarantees that the
variance scales systematically with respect to the change of the length-scaling parameter,
â„“. The exact form of this scaling factor is unimportant for our purposes since we are
ultimately only interested in a regularization parameter, Î±, as will be seen in Section
2.4. To keep notation simpler, we use Î´ as a placeholder for this term. This is also the
reason we are interested in whether the MatÂ´ern correlation rather than the covariance
is preserved when restricting our Gaussian ï¬eld to a ï¬nite domain.
We now let n = 50, so N = 502 = 2500, and generate 50 000 samples from (16)
for each of N xi values, calculate the empirical correlation between the samples, and
compare this with the theoretical correlation deï¬ned by the MatÂ´ern covariance function.
We do this for â„“= 1/4 and plot the results in the middle of Figure 2, together with the
MatÂ´ern correlation map on the left. It is clear that there is a disconnection between the
empirical correlation and the MatÂ´ern correlation.
It is crucial that the connection between the Gaussian ï¬elds deï¬ned by the SPDE
and those deï¬ned by the MatÂ´ern covariance function holds because then the parameters
in the SPDE can be estimated using the semivariogram method described in Section 3.
Fortunately, we can restore this connection by extending the computational domain. In
two dimensions, we deï¬ne â„¦= [1 âˆ’a, a] Ã— [1 âˆ’a, a], for a > 1, e.g., if a = 1.5 then
â„¦= [âˆ’0.5, 1.5] Ã— [âˆ’0.5, 1.5]. We then generate realizations for ((2a âˆ’1)n)2 = (2n)2 =
10 000 xi values on the extended domain and compute the empirical correlation only for
the xi values that correspond to the original domain, â„¦= [0, 1] Ã— [0, 1]. The results are
plotted on the right side of Figure 2, where it is clear that the empirical correlation map
is nearly indistinguishable from those obtained using the MatÂ´ern correlation function.

Semivariogram methods for inverse problems
9
Figure 2. Isotropic correlation maps. Plots of the MatÂ´ern correlation map (left),
the empirical correlation map with n = 50 computed on the domain â„¦= [0, 1] Ã— [0, 1]
(middle), and the empirical correlation map computed on the domain â„¦= [âˆ’0.5, 1.5]Ã—
[âˆ’0.5, 1.5] (right), computed from random draws from the prior (16) in 2D with Î½ = 1
and â„“= 1/4.
To determine the a value that extends the domain far enough to restore the
MatÂ´ern/SPDE connection, but not so far as to introduce unnecessary computational
cost, we look to the MatÂ´ern correlation function itself. We want to extend the domain
far enough so that all x values in [0, 1] Ã— [0, 1] have a suï¬ƒciently low correlation with
the x values at the end of the extended domain. The criterion we used to determine
if the connection was restored was based on relative error: âˆ¥Ï âˆ’Ïaâˆ¥F/âˆ¥Ïâˆ¥F < 0.05,
where Ï is the true MatÂ´ern correlation matrix, Ïa is the approximate correlation matrix
obtained by discretizing the SPDE, and âˆ¥Â· âˆ¥F denotes the Frobenius norm.
In tests, it was found that we should always extend the domain at least slightly. If
we let rc be the distance for which the MatÂ´ern correlation is approximately equal to c,
then our tests showed that setting a = 1 + r0.30 restores the connection to the MatÂ´ern
covariance for Î½ â‰¥1/2 when using zero boundary conditions and setting a = 1 + r0.20
restores the connection to the MatÂ´ern covariance for Î½ â‰¥1/2 when periodic boundary
conditions are used. For Î½ = 1 and â„“= 1/4, a should be set to 1.5 in the Dirichlet
boundary condition case, which gives a relative error in the diï¬€erence of the correlation
matrices of 0.0375, and it should be set to 1.6 when using periodic boundary conditions.
We note that since â„“is directly related to the degree of correlation in the prior, the
extension necessary to preserve the connection rises sharply as â„“increases. It is rare in
practice, however, to have â„“â‰¥1/4 when Î½ â‰¥1 since that implies the correlation persists
across the entire region. Thus, it is uncommon to have to extend beyond a domain of
[âˆ’0.5, 1.5] Ã— [âˆ’0.5, 1.5].
For the above discussion, we focused on zero boundary conditions. Similar results
hold if periodic boundary conditions are assumed, in which case L, and thus L2D, can
be diagonalized by the FFT, assuming x is deï¬ned on a regular grid. The FFT-based
diagonalization of L2D can be exploited to greatly reduce computational cost, thus when
extending the domain in two-dimensions, it is advantageous to use periodic boundary
conditions and the extended domain â„¦= [âˆ’0.5, 1.5] Ã— [âˆ’0.5, 1.5] so that L2D deï¬ned
on â„¦can be diagonalized by the FFT. A more thorough description of the eï¬€ects of

Semivariogram methods for inverse problems
10
boundary artifacts with diï¬€erent boundary conditions can be found in [28].
Finally, in our numerical experiment above, we chose a speciï¬c value of Î½, but
other values of Î½ can be chosen. The general form of the isotropic prior density in two
dimensions, with Î½ included as a hyperparameter, is
p(x|Î´, Î½, â„“) âˆexp

âˆ’Î´
2xT(I + (â„“/h)2L2D)Î½+d/2x

.
(17)
If Î½ +d/2 is a non-integer, a fractional power of I+(â„“/h)2L2D must be computed, which
is possible, generally speaking, if we have a diagonalization of I + (â„“/h)2L2D in hand,
but the resulting precision matrix is typically full and dense. Such a diagonalization is
typically computable in one-dimensional examples, even with dense matrices. In two
dimensions, however, an eï¬ƒcient diagonalization is possible only if periodic boundary
conditions are assumed. We will restrict the exponent Î½ + d/2 to be an integer in this
paper to preserve the sparsity in the precision matrix, which will be especially useful in
Section 5.
2.4. Computing MAP Estimators for Whittle-MatÂ´ern Priors
Using Bayesâ€™ law, we multiply the prior (17) by the likelihood (2) to obtain the posterior
density function
p(x|b, Î», Î´, Î½, â„“) âˆp(b|x, Î»)p(x|Î´, Î½, â„“)
âˆexp

âˆ’Î»
2âˆ¥Ax âˆ’bâˆ¥2 âˆ’Î´
2xT(I + (â„“/h)2L2D)Î½+d/2x

.
The maximizer of p(x|b, Î», Î´, Î½, â„“) is known as the MAP estimator, and it can be
computed by solving
xÎ± = arg min
x
1
2âˆ¥Ax âˆ’bâˆ¥2 + Î±
2 xT(I + (â„“/h)2L2D)Î½+d/2x

=
 ATA + Î±(I + (â„“/h)2L2D)Î½+d/2âˆ’1 ATb,
(18)
where Î± = Î´/Î». Assuming we know â„“and Î½, Î± can be estimated using one of many
regularization parameter selection methods (see, e.g.,[29, 30, 10]). One such method is
generalized cross validation (GCV):
Î± = arg min
Î·>0
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
A

ATA + Î·P
âˆ’1
ATb âˆ’b

2
tr

I âˆ’A

ATA + Î·P
âˆ’1
AT

ï£¼
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£¾
(19)
for P = (I + (â„“/h)2L2D)Î½+d/2.
In practice, Î½ is often ï¬xed [31, 12] and â„“is either estimated manually or by
using the fully Bayesian approach, which involves Markov chain Monte Carlo (MCMC)
[32] sampling.
This requires setting up hyperprior distributions and can be time
consuming, subjective and unintuitive, so we present a new method for selecting these
hyperparameters next.

Semivariogram methods for inverse problems
11
3. The Semivariogram Method for Estimating Î½ and â„“
In the inverse problem formulation above, the components of the vector x correspond
to values of an unknown function x at numerical mesh points within a spatial region
â„¦. This motivates using methods from spatial statistics to estimate the Whittle-MatÂ´ern
prior hyperparameters Î½ and â„“. One such method uses a variogram, and a corresponding
semivariogram [33], which requires the assumption of intrinsic stationarity, i.e., that the
elements of x have constant mean and the variance of the diï¬€erence between the elements
is constant throughout the region. This is a weaker assumption than is required by many
other parameter estimation tools, which is one of the reasons variograms have become
popular in spatial statistical applications [34], and it is the reason we use semivariograms
here. Although the use of semivariograms for estimating parameters to determine a
covariance structure is commonly used in spatial statistics, this is, to our knowledge,
the ï¬rst time these tools have been used to estimate prior hyperparameters for use in
inverse problems.
The semivariogram is deï¬ned by Î³(r) = 1
2Var[Z(ui) âˆ’Z(uj)], where r = ui âˆ’uj
and {Z(u) : u âˆˆâ„¦âŠ‚Rd} is a spatial process. Due to our stationarity assumption,
Var[Z(ui)] = Var[Z(uj)] = Ïƒ2, which we use to derive the following alternative
expression for Î³(r):
Î³(r) = 1
2

Var[Z(ui)] + Var[Z(uj)] âˆ’2Cov[Z(ui), Z(uj)]

= Ïƒ2 âˆ’Cov[Z(ui), Z(uj)].
Thus, the semivariogram simpliï¬es to the diï¬€erence between the variance in the
region and the covariance between two points with a diï¬€erence r. The variogram is
formally deï¬ned as 2Î³(r), hence the terms variogram and semivariogram are often
used interchangeably.
To remain consistent, we will continue to refer to Î³(r) as a
semivariogram throughout the paper.
We now need a way to estimate the semivariogram from given data. For this, we
use what is known as the sample, or empirical, semivariogram. Assuming that Z(u)
is isotropic, so that r = âˆ¥râˆ¥= âˆ¥ui âˆ’ujâˆ¥, then the empirical semivariogram can be
expressed
Ë†Î³(r) =
1
2n(r)
X
(i,j)|âˆ¥uiâˆ’ujâˆ¥=r
[z(ui) âˆ’z(uj)]2,
(20)
where z(u) is a realization of Z(u), and n(r) is the number of points that are separated
by a distance r. The Ë†Î³(r) values are often referred to as the semivariance values. In
a typical semivariogram, the semivariance values increase as r increases since points
tend to be less similar the further apart they are, which increases the variance of their
diï¬€erences.
Although the empirical semivariogram is useful in obtaining semivariance values
from data, it is not ideal for modeling data for various reasons (see [34] for details),
thus it is typical to ï¬t a semivariogram model to the empirical semivariogram. Since

Semivariogram methods for inverse problems
12
our prior distribution for x has a MatÂ´ern covariance, we will use the theoretical MatÂ´ern
semivariogram model [4, 2] given by
Î³(r, Î¸) =
ï£±
ï£´
ï£²
ï£´
ï£³
0
if r = 0
a0 + (Ïƒ2 âˆ’a0)

1 âˆ’
1
2Î½âˆ’1Î“(Î½)(r/â„“)Î½KÎ½(r/â„“)

if r > 0
(21)
where a0 â‰¥0 is the nugget, Ïƒ2 â‰¥a0 is the sill, and Î¸ = (a0, Ïƒ2, Î½, â„“). The nugget is
the term given to the semivariance value at a distance just greater than zero and the
sill is the total variance contribution or the semivariance value where the model levels
out.
The sill, Ïƒ2, is also the variance parameter in the MatÂ´ern covariance function
(5). We can estimate a0, Ïƒ2, Î½, and â„“by ï¬tting semivariogram models to the empirical
semivariogram.
There are a number of ways to ï¬t the semivariogram model to the empirical
semivariogram.
We use weighted least squares, as is commonly done [34], choosing
the Î¸ that minimizes
W(Î¸) =
X
r
n(r)
2[Î³(r, Î¸)]2[Ë†Î³(r) âˆ’Î³(r, Î¸)]2.
(22)
To minimize W(Î¸), we adapt the MATLAB codes from [35, 36].
More speciï¬cally,
we adapt [35] for computing the empirical semivariance Ë†Î³(r) and we adapt [36] for
minimizing W(Î¸). Although it is possible to optimize both Î½ and â„“continuously, we
will require Î½ + d/2 to be an integer.
Weighted least squares, in general, performs
well when ï¬nding optimal estimates for a0, Ïƒ2, and â„“for given empirical semivariogram
values when Î½ is ï¬xed, but not when Î½ is also free to vary (most software requires a ï¬xed
Î½ value). To combat this issue, and to ensure Î½ + d/2 is an integer, we cycle through
various ï¬xed values of Î½ to obtain estimates for the other parameters and their weighted
least squares value. We then choose the Î¸ with the smallest W(Î¸).
For an illustration, we generated a random ï¬eld, shown on the left side of Figure 3,
and ï¬t a semivariogram to the ï¬eld. The optimized parameters of the model are Î½ = 2
and â„“= 0.019, which corresponds to a practical range of 0.102. Thus, the values of the
ï¬eld are nearly independent a tenth of the way across the region. The sill and nugget
are estimated to be Ïƒ2 = 1.003 and a0 = 0.206, respectively. A plot of the resulting
ï¬tted MatÂ´ern semivariogram model is given on the right side of Figure 3.
The values of Î½ and â„“from Î¸ = (a0, Ïƒ2, Î½, â„“) obtained by ï¬tting the MatÂ´ern
semivariogram model to a spatial ï¬eld, as described in the previous paragraph, can
be used to deï¬ne the Whittle-MatÂ´ern prior (17). The sill, Ïƒ2, and the nugget, a0, are
not especially useful outside of ï¬tting the semivariogram model because they do not
correspond to any hyperparameter in (17). They are helpful only in determining the
best estimates for Î½ and â„“. Any contribution these parameters may have made to the
prior distribution will be accounted for in the regularization parameter, Î±. Therefore,
after ï¬tting the semivariogram models, Ïƒ2 and a0 are discarded.
With estimates for Î½ and â„“in hand, the MAP estimator, xÎ±, can then be computed
as in Section 2.4, from which we can recompute Î¸ by ï¬tting the MatÂ´ern semivariogram

Semivariogram methods for inverse problems
13
0
0.1
0.2
0.3
0.4
0.5
0.6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 3. Semivariogram. A randomly generated spatial ï¬eld is shown on the left and
the empirical semivariogram, along with the MatÂ´ern model ï¬t, is given on the right.
The ï¬tted hyperparameters are Î½ = 2 and â„“= 0.019, which corresponds to a practical
range of 0.102.
Algorithm 1 The semivariogram method for MAP estimation with Whittle-MatÂ´ern
prior.
0. Estimate Î¸ = (a0, Ïƒ2, Î½, â„“) by ï¬tting a MatÂ´ern semivariogram model to b.
1. Deï¬ne the prior (17) using Î½ and â„“, compute Î± using (19), and compute xÎ± using
(18).
2. Update Î¸ = (a0, Ïƒ2, Î½, â„“) by ï¬tting a MatÂ´ern semivariogram model to xÎ±.
3. Return to step 1 and repeat until Î½ and â„“stabilize.
model to the empirical semivariogram values of xÎ±. Repeating this process iteratively
yields Algorithm 1. Recall that b is a vector of measurements, which will usually be
noisy or have some missing values, and each element of b has a corresponding spatial
position. Since Î½ is being optimized discretely to ensure that Î² = Î½ + d/2 is an integer,
convergence will be met when Î½jâˆ’Î½jâˆ’1 = 0 where Î½j is the Î½ value ï¬t in the jth iteration.
Then â„“is said to have converged when |â„“j âˆ’â„“jâˆ’1|/â„“jâˆ’1 < Îµ with Îµ determined by the
user. In this paper, we will consider â„“to have converged when the relative diï¬€erence is
less than 0.01, which usually takes fewer than three iterations to achieve.
The semivariogram method is essentially a parametric empirical Bayes method
[37] for point estimation.
We have a distributional assumption on x, but no prior
distributions are assumed for Î½ or â„“. The hyperparameters are instead estimated by
iteratively ï¬tting semivariograms to the data.
3.1. Numerical Experiments
We now implement the semivariogram method on a two-dimensional deblurring and
inpainting example. Recall that the connection between the MatÂ´ern covariance and
the Whittle-MatÂ´ern prior depends on a stationarity assumption, which the following
example may not exhibit.
For simplicity, we will still assume stationarity and

Semivariogram methods for inverse problems
14
acknowledge that future work should be done in the case when no stationarity is
present. Additionally, the numerical examples given in this paper all use color images.
In our analysis, we will assume independence in the color bands and obtain priors and
reconstructions for each one individually.
3.1.1.
Results In this example, we assume periodic boundary conditions on the
extended domain, but due to the restriction from the extended domain â„¦to â„¦, circulant
structure is lost in the forward model matrix, and hence, linear system solves must
be done using an iterative method. As in [10, Section 3.1.3], we use preconditioned
conjugate gradient (PCG) iteration, both for computing Î± and for computing xÎ±. We
attempt to deblur and demask a 128 Ã— 128 image of Main Hall on the University of
Montana (UM) campus. To do this, we begin with a 256 Ã— 256 image, given in Figure
4, and then restrict to the center 128 Ã— 128 image. This smaller image in the middle
will be thought of as being on a domain â„¦= [0, 1] Ã— [0, 1] and the larger, full image will
then be deï¬ned on â„¦= [âˆ’0.5, 1.5] Ã— [âˆ’0.5, 1.5].
To obtain b, we ï¬rst perform a slight blurring operation on the full 256Ã—256 true
image plotted in Figure 4. Since this is a color image, the deblurring process is done
individually for the red, green, and blue intensity arrays. We then restrict to the central
128 Ã— 128 pixels (with boundaries denoted in Figure 4) and randomly remove 40% of
the pixels to obtain the masked, and moderately blurry image on the left in Figure 5.
We seek an estimate of x in the same central subregion.
Omnidirectional
semivariograms with 25 approximately equally spaced grid points in 0 < r <
âˆš
2/10
are used.
We chose
âˆš
2/10 as a cutoï¬€because it balances the need to capture the
covariance structure at short distances, which are well-known to be the most important
[34], with those at longer distances. When ï¬tting semivariograms to the masked image,
the removed entries will not be considered or else the correlation would be strongly
Figure 4.
Full 256 Ã— 256 image of Main Hall at the University of Montana with
128 Ã— 128 subimage.

Semivariogram methods for inverse problems
15
20
40
60
80
100
120
120
100
80
60
40
20
20
40
60
80
100
120
120
100
80
60
40
20
20
40
60
80
100
120
120
100
80
60
40
20
Figure 5. Two-dimensional image deblurring test case. On the left is a plot of the
blurred, masked, and noisy data; in the middle is a plot of the Tikhonov solution; and
on the right is a plot of the solution obtained using the Whittle-MatÂ´ern prior with Î½ = 1
and â„“= 0.0364, 0.0313 and 0.0543 for red, green and blue intensities, respectively.
inï¬‚uenced by those entries.
The semivariogram method is used to obtain Î½ = 1 for each color band, â„“=
0.0364, 0.0313 and 0.0543 for the red, green, and blue intensities, respectively, and
Î± = 0.0023, 0.0018 and 5.47 Ã— 10âˆ’5. Convergence was met in two iterations for each
color intensity. We also computed the Tikhonov solution, as deï¬ned in [10, Section
3.1.3], for which the prior covariance is equal to a scalar multiple of the identity matrix.
The Tikhonov Î± values for all three color bands were around 0.0004. Note that for
both of these reconstructions, the regularization parameter, Î±, was optimized using
the highest correlation between the solution and the true image rather than chosen by
GCV to ensure that any diï¬€erences in the solutions is due to the method and not a
poorly-chosen regularization parameter.
The two solutions are plotted in Figure 5. It is clear that the solution that used the
Whittle-MatÂ´ern prior is the superior reconstruction. The correlation between xÎ± and
x, the true image, is 0.950. While the Tikhonov solution is able to remove the blur, it
performs inpainting poorly since each pixel value is assumed independent of one another
due to the identity covariance matrix.
3.2. Discussion
Compared to the fully Bayesian method, the semivariogram procedure has some key
advantages. This technique produces competitive solutions and clearer interpretations
of the hyperparameters Î½ and â„“, and it can inform how far to extend the domain to
maintain a connection with the MatÂ´ern covariance. Additionally, the computation time
is only a fraction of what is needed to compute an adequate number of MCMC samples.
In our implementation of the example above, the semivariogram method was more
than 20 times faster than the fully Bayesian MCMC method. Finally, it is not trivial
to sample from a complex model such as this one without signiï¬cant autocorrelation,
whereas sampling is not needed for the semivariogram method.
The primary disadvantage is that we lose uncertainty quantiï¬cation. We also have

Semivariogram methods for inverse problems
16
to calculate Î±, the regularization parameter, using other techniques like GCV. One other
shortcoming to the semivariogram method, as described in this section, is the fact that
it requires the ï¬eld or image to be isotropic. In the next section, we extend these results
to anisotropic ï¬elds.
4. Geometric Anisotropy
The solution to (6) is an isotropic Gaussian ï¬eld, which means the correlation length is
the same in every direction. This isotropy assumption is often not satisï¬ed and so it will
be useful to have an alternate SPDE formulation for the case when correlation lengths
diï¬€er with direction. This is known as geometric anisotropy [34]. The groundwork for
constructing priors that can model anisotropy has been laid in works such as [13, 7, 38].
4.1. Anisotropic SPDE
We will derive an anisotropic SPDE that can be used in similar way that (6) was used in
the prior modeling in the isotropic case. We will only consider the two-dimensional case,
but results can be extended to d > 1 dimensions. In two dimensions, for a Gaussian
ï¬eld with correlation length â„“1 in the direction of the angle Î¸, where âˆ’Ï€/2 < Î¸ â‰¤Ï€/2 is
measured counter-clockwise from the x-axis, and correlation length â„“2 in the direction
perpendicular to Î¸, we can make the following change of variables from isotropic to
anisotropic coordinates:
w =
"
cos Î¸
âˆ’â„“2/â„“1 sin Î¸
sin Î¸
â„“2/â„“1 cos Î¸
# "
u1
u2
#
(23)
and thus
w1(u1, u2) = cos Î¸u1 âˆ’â„“2/â„“1 sin Î¸u2
w2(u1, u2) = sin Î¸u1 + â„“2/â„“1 cos Î¸u2.
We will apply the change of variables (23) to both sides of (6) to obtain the
analogous anisotropic SPDE. The Laplacian on the left-hand side can be altered using
the chain rule:
âˆ‚
âˆ‚u1
=
âˆ‚
âˆ‚w1
âˆ‚w1
âˆ‚u1
+
âˆ‚
âˆ‚w2
âˆ‚w2
âˆ‚u1
and
âˆ‚
âˆ‚u2
=
âˆ‚
âˆ‚w1
âˆ‚w1
âˆ‚u2
+
âˆ‚
âˆ‚w2
âˆ‚w2
âˆ‚u2
,
which means
âˆ‚2
âˆ‚u2
1
=
 âˆ‚2
âˆ‚w2
1
âˆ‚w1
âˆ‚u1
+
âˆ‚2
âˆ‚w1âˆ‚w2
âˆ‚w2
âˆ‚u1
 âˆ‚w1
âˆ‚u1
+

âˆ‚2
âˆ‚w1âˆ‚w2
âˆ‚w1
âˆ‚u1
+ âˆ‚2
âˆ‚w2
2
âˆ‚w2
âˆ‚u1
 âˆ‚w2
âˆ‚u1
= cos2 Î¸ âˆ‚2
âˆ‚w2
1
+ 2 sin Î¸ cos Î¸
âˆ‚2
âˆ‚w1âˆ‚w2
+ sin2 Î¸ âˆ‚2
âˆ‚w2
2

Semivariogram methods for inverse problems
17
and
âˆ‚2
âˆ‚u2
2
=
 âˆ‚2
âˆ‚w2
1
âˆ‚w1
âˆ‚u2
+
âˆ‚2
âˆ‚w2âˆ‚w1
âˆ‚w2
âˆ‚u2
 âˆ‚w1
âˆ‚u2
+

âˆ‚2
âˆ‚w1âˆ‚w2
âˆ‚w1
âˆ‚u2
+ âˆ‚2
âˆ‚w2
2
âˆ‚w2
âˆ‚u2
 âˆ‚w2
âˆ‚u2
= (â„“2/â„“1)2 sin2 Î¸ âˆ‚2
âˆ‚w2
1
âˆ’2(â„“2/â„“1)2 sin Î¸ cos Î¸
âˆ‚2
âˆ‚w1âˆ‚w2
+ (â„“2/â„“1)2 cos2 Î¸ âˆ‚2
âˆ‚w2
2
.
The right hand side of (6) is updated by changing the coordinates of the white
noise. The inverse transformation of (23) is
u =
"
cos Î¸
sin Î¸
âˆ’Ï„ sin Î¸
Ï„ cos Î¸
# "
w1
w2
#
:= f(w),
(24)
where Ï„ = â„“1/â„“2. Now, we deï¬ne the transformed white noise basis functions as
ËœÏ†j(w) = Ïˆj(f(w))| det(Jf(w))|1/2 = Ïˆj(f(w))(â„“1/â„“2)1/2,
where det(Jf(w)) denotes the determinant of the Jacobian of the transformation f(w),
which is (â„“1/â„“2)1/2 in our case. This will preserve the orthonormal properties of the basis
functions. Then, appealing to (7),
W(w) =
âˆ
X
j=1
Î¾j ËœÏ†j(w),
Î¾j
iid
âˆ¼N(0, 1)
=
âˆ
X
j=1
Î¾jÏ†j(u)(â„“1/â„“2)1/2 = (â„“1/â„“2)1/2W(u),
which means W(u) = (â„“2/â„“1)1/2W(w).
So, taking â„“= â„“1 and making the appropriate substitutions, (6) is converted to the
anisotropic SPDE:

1 âˆ’
h
(a2
Î¸ + b2
Î¸) âˆ‚2
âˆ‚w2
1 + (c2
Î¸ + d2
Î¸) âˆ‚2
âˆ‚w2
2 âˆ’2(aÎ¸cÎ¸ âˆ’bÎ¸dÎ¸)
âˆ‚2
âˆ‚w1âˆ‚w2
iÎ²/2
x(w) = (â„“2/â„“1)1/2W(w)
where aÎ¸ = â„“2 sin Î¸, bÎ¸ = â„“1 cos Î¸, cÎ¸ = â„“2 cos Î¸, and dÎ¸ = â„“1 sin Î¸. For
R =
"
â„“1 cos Î¸
â„“1 sin Î¸
âˆ’â„“2 sin Î¸
â„“2 cos Î¸
#
,
the above SPDE can be written
 1 âˆ’âˆ‡Â· RTRâˆ‡
Î²/2 x(w) = (â„“2/â„“1)1/2W(w).
(25)
Notice that if â„“1 = â„“2, this SPDE is equivalent to (6) with â„“= â„“1.
4.2. The Gaussian Field Solution of the SPDE (25)
Like in the isotropic case, we are interested in the the properties of the solution of (25),
especially its covariance function. First, we deï¬ne the anisotropic MatÂ´ern covariance
function [39] as
C(rw) = Ïƒ2(rw/Î¶)Î½KÎ½(rw/Î¶)
2Î½âˆ’1Î“(Î½)
, with Î¶ =
â„“1
p
cos2(Ïˆ âˆ’Î¸) + (â„“1/â„“2)2 sin2(Ïˆ âˆ’Î¸)
,
(26)

Semivariogram methods for inverse problems
18
where rw = âˆ¥wi âˆ’wjâˆ¥is the distance between the anisotropic coordinates, Î¶ is the new
range parameter in the direction of Ïˆ, â„“1 is the correlation length in the direction of
Î¸ and â„“2 is the correlation length in the direction perpendicular to Î¸. Notice that the
smoothness parameter, Î½, is unaï¬€ected.
The remainder of this subsection contains results used to prove the following
theorem.
Theorem 2 The solution x(w) of (25) is a Gaussian ï¬eld with mean zero and
anisotropic MatÂ´ern covariance function deï¬ned by (26).
Proof. First, we derive the Greenâ€™s function for (25), which is the solution of
 1 âˆ’âˆ‡Â· RTRâˆ‡
Î²/2 g(w, v) = Î´f(v âˆ’w).
(27)
Using (8), the solution to (25) is given by
x(w) = (â„“2/â„“1)1/2
Z
R2 g(w, v)W(v)dv,
(28)
which makes x(w) a Gaussian ï¬eld since it is a linear transformation of Gaussian white
noise. Be aware that we are still assuming stationarity in our ï¬eld. To derive the Greenâ€™s
function g in (28), we ï¬rst deï¬ne g(w) := g(w, 0). Then (27) implies
 1 âˆ’âˆ‡Â· RTRâˆ‡
Î²/2 g(w) = Î´f(w).
(29)
We would like to change from the anisotropic coordinates w to anisotropic
coordinates u in (29) so we can use the results from Section 2.2. We again use (24)
for the coordinate change and, in a similar fashion as was done earlier, we apply
the chain rule to replace âˆ‚2/âˆ‚w2
1, âˆ‚2/âˆ‚w2
2, and âˆ‚2/(âˆ‚w1âˆ‚w2) in
 1 âˆ’âˆ‡Â· RTRâˆ‡
Î²/2
with partial derivatives in terms of u.
When making this change, the coeï¬ƒcients
of âˆ‚2/âˆ‚u2
1, âˆ‚2/âˆ‚u2
2, and âˆ‚2/(âˆ‚u1âˆ‚u2) are â„“2
1, â„“2
1, and 0, respectively and so we have
(1 âˆ’âˆ‡Â· RTRâˆ‡)g(w) = (1 âˆ’â„“2
1âˆ†)g(u). Additionally, we can change variables in the
Delta function on the right side of (29) by multiplying by the determinant of the Jacobian
of (24): â„“1/â„“2. Thus, the change of variables transforms (29) into the equation
 1 âˆ’â„“2
1âˆ†
Î²/2 g(u) = (â„“1/â„“2)Î´f(u),
(30)
which is equivalent to (12) up to a constant. Hence, we can apply the results of Section
2.2.
Namely, after changing variables, the solution of (25) is a Gaussian ï¬eld with
mean zero and the isotropic MatÂ´ern covariance function deï¬ned by (5). Notice that
the constant that multiplies the Delta function on the right-hand side of (30) and the
constant that multiplies the integral in (28) will cancel when going through the process
of deriving the covariance function since the constant in (28) gets squared.
We must now make one ï¬nal change of variables back to w from u so our covariance
function will be in terms of the anisotropic coordinates rather than the isotropic
ones. Since the input to the MatÂ´ern correlation function must be a distance between
isotropic spatial locations, we need to represent an isotropic distance, ru, in terms of the

Semivariogram methods for inverse problems
19
anisotropic coordinates. Consider r := wiâˆ’wj. Then, deï¬ning rw := âˆ¥râˆ¥= âˆ¥wiâˆ’wjâˆ¥,
ru := âˆ¥ui âˆ’ujâˆ¥=

"
cos Î¸
sin Î¸
âˆ’Ï„ sin Î¸
Ï„ cos Î¸
#
(wi âˆ’wj)
 =

"
cos Î¸
sin Î¸
âˆ’Ï„ sin Î¸
Ï„ cos Î¸
#
r

=

"
cos Î¸
sin Î¸
âˆ’Ï„ sin Î¸
Ï„ cos Î¸
# "
r1
r2
# =

"
r1 cos Î¸ + r2 sin Î¸
âˆ’r1Ï„ sin Î¸ + r2Ï„ cos Î¸
# .
Now we convert to polar coordinates with r1 = rw cos Ïˆ and r2 = rw sin Ïˆ. Then
ru =

"
rw cos Ïˆ cos Î¸ + rw sin Ïˆ sin Î¸
âˆ’rwÏ„ cos Ïˆ sin Î¸ + rwÏ„ sin Ïˆ cos Î¸
# =

"
rw cos(Ïˆ âˆ’Î¸)
rwÏ„ sin(Ïˆ âˆ’Î¸)
#
= rw

cos2(Ïˆ âˆ’Î¸) + Ï„ 2 sin2(Ïˆ âˆ’Î¸)
1/2 .
Therefore, we need to adjust the distance between the vectors wi and wj by [cos2(Ïˆâˆ’Î¸)+
Ï„ 2 sin2(Ïˆâˆ’Î¸)]1/2 in order to get the distances to plug into the isotropic MatÂ´ern correlation
function. Thus, the isotropic MatÂ´ern covariance function has been generalized to the
anisotropic case using the same change of variables as in (23). Adjusting the anisotropic
distances is equivalent to deï¬ning the anisotropic MatÂ´ern covariance function as we have
in (26).
â–¡
4.3. Anisotropic Prior Modeling
To obtain a sparse representation of the precision matrix for the anisotropic MatÂ´ern
covariance, we can discretize (25) using the standard ï¬nite-diï¬€erence approximations
with appropriate boundary conditions. Taking a step size of h = 1/n on a uniform
mesh, so that N = n2 in two dimensions, yields
h
I + 1
h2(a2
Î¸ + b2
Î¸)(L âŠ—I) + 1
h2(c2
Î¸ + d2
Î¸)(I âŠ—L)
âˆ’
2
4h2(aÎ¸cÎ¸ âˆ’bÎ¸dÎ¸)(K âŠ—K)
iÎ²/2
x = Î´âˆ’1/2Î¾,
Î¾ âˆ¼N(0, IN).
where âŠ—denotes Kronecker product [10]. Note that the constant multiplying the white
noise term gets absorbed into the Î´ hyperparameter.
In the zero boundary condition case,
L =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
2
âˆ’1
0
. . .
0
âˆ’1
2
âˆ’1
. . .
0
0
âˆ’1
2
...
...
...
...
...
...
âˆ’1
0
0
. . .
âˆ’1
2
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£»
nÃ—n
and
K =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
0
1
0
. . .
0
âˆ’1
0
1
. . .
0
0
âˆ’1
0
...
...
...
...
...
...
1
0
0
. . .
âˆ’1
0
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£»
nÃ—n
,
and when using periodic boundary conditions, we let L(1, n) = L(n, 1) = K(1, n) = âˆ’1
and K(n, 1) = 1. Then
x âˆ¼N
 0, Î´âˆ’1Pâˆ’1
(31)

Semivariogram methods for inverse problems
20
where
P =

I + 1
h2(a2
Î¸ + b2
Î¸)(L âŠ—I) + 1
h2(c2
Î¸ + d2
Î¸)(I âŠ—L) âˆ’
2
4h2(aÎ¸cÎ¸ âˆ’bÎ¸dÎ¸)(K âŠ—K)
Î²
. (32)
In order to retain sparsity in P, we will again require that Î² = Î½ + d/2 be an integer.
Additionally, like in the isotropic case, an extension of the computational domain is
required to maintain a connection between (32) and (26).
Now that we have a prior covariance matrix that maintains a connection to the
anisotropic MatÂ´ern covariance, we return to the MAP estimator, which can be computed
by solving
xÎ± = arg min
x
1
2âˆ¥Ax âˆ’bâˆ¥2 + Î±
2 xTPx

=
 ATA + Î±P
âˆ’1 ATb,
(33)
where Î± = Î´/Î» and P is as in (32).
4.4. Directional Semivariograms
When ï¬tting semivariograms to a spatial ï¬eld, intrinsic stationarity and isotropy is
assumed.
In our case, we are still assuming intrinsic stationarity, but our ï¬eld is
anisotropic. Thus, a change must be made to our ï¬eld before ï¬tting a semivariogram to
obtain an estimate for â„“1 and â„“2. We again use (24), the inverse of the change used in
(23). Using the same argument that was used when transforming the Greenâ€™s function
PDE from anisotropic coordinates in (29) to isotropic coordinates in (30), it is not
diï¬ƒcult to show that (25) is transformed to
(1 âˆ’â„“2
1âˆ†)(Î½+d/2)/2x(u) = W(u),
which is equivalent to (6) with â„“= â„“1.
We can apply this same change of variables (24) to any two-dimensional spatial
ï¬eld that exhibits geometric anisotropy to achieve isotropy. For example, if we begin
with a spatial ï¬eld that exhibits its larger correlation length in the 45â—¦direction with
Ï„ = â„“1/â„“2 = 3, the change of variables will rotate the ï¬eld so the direction of maximum
correlation length is in the 0â—¦direction and will then stretch the ï¬eld along the new
y-axis to remove the geometric anisotropy and create a new, isotropic ï¬eld. This is
shown in the middle in Figure 6. Once the spatial ï¬eld has been adjusted in this way,
a semivariogram can be ï¬t to the transformed ï¬eld as in the usual, isotropic case.
In order to adjust the spatial ï¬eld to satisfy the isotropy assumptions in the way
described above, we must ascertain Î¸, the direction of maximum correlation length
measured from the x-axis, and Ï„, the ratio of the correlation length in the direction of Î¸
to the correlation length in the direction orthogonal to Î¸. Both of these parameters can
be estimated using directional empirical semivariograms. Directional semivariograms
are ï¬t in a similar way as omnidirectional semivariograms in (20), but instead of taking
all points separated by a distance r, we restrict the pairs of points to a certain angle, Ïˆ.
If we think of wi and wj as vectors, then Ïˆ is equivalent to the angle between wi âˆ’wj

Semivariogram methods for inverse problems
21
0
0.2
0.4
0.6
0
0.5
1
 = 90;  range = 0.08
0
0.2
0.4
0.6
0
0.5
1
 = 75;  range = 0.08
0
0.2
0.4
0.6
0
0.5
1
 = 60;  range = 0.14
0
0.2
0.4
0.6
0
0.5
1
 = 45;  range = 0.17
0
0.2
0.4
0.6
0
0.5
1
 = 30;  range = 0.14
0
0.2
0.4
0.6
0
0.5
1
 = 15;  range = 0.08
0
0.2
0.4
0.6
0
0.5
1
 = 0;  range = 0.08
0
0.2
0.4
0.6
0
0.5
1
 = -15;  range = 0.06
0
0.2
0.4
0.6
0
0.5
1
 = -30;  range = 0.06
0
0.2
0.4
0.6
0
0.5
1
 = -45;  range = 0.06
0
0.2
0.4
0.6
0
0.5
1
 = -60;  range = 0.06
0
0.2
0.4
0.6
0
0.5
1
 = -75;  range = 0.06
Original Field
Rotated Field
Rotated and Scaled Field
0
0.2
0.4
0.6
0
0.5
1
 = 90;  range = 0.17
0
0.2
0.4
0.6
0
0.5
1
 = 75;  range = 0.17
0
0.2
0.4
0.6
0
0.5
1
 = 60;  range = 0.17
0
0.2
0.4
0.6
0
0.5
1
 = 45;  range = 0.17
0
0.2
0.4
0.6
0
0.5
1
 = 30;  range = 0.17
0
0.2
0.4
0.6
0
0.5
1
 = 15;  range = 0.17
0
0.2
0.4
0.6
0
0.5
1
 = 0;  range = 0.17
0
0.2
0.4
0.6
0
0.5
1
 = -15;  range = 0.17
0
0.2
0.4
0.6
0
0.5
1
 = -30;  range = 0.17
0
0.2
0.4
0.6
0
0.5
1
 = -45;  range = 0.17
0
0.2
0.4
0.6
0
0.5
1
 = -60;  range = 0.17
0
0.2
0.4
0.6
0
0.5
1
 = -75;  range = 0.17
Figure 6.
Directional semivariograms.
The directional semivariograms for the
original, anisotropic ï¬eld are shown on the left.
For each of the 12 images,
semivariogram value is plotted against lag distances.
The direction of maximum
correlation is determined to be 45â—¦with a ratio of 3 since the distance required to
pass Î³crit = 0.9 was largest in that direction and that distance is 3 times greater than
the distance needed in the âˆ’45â—¦direction. The directional semivariograms for the
rotated and scaled ï¬eld are shown on the right with a ratio of 1.
and the x-axis. For example, if Ïˆ = 0, we restrict to all pairs of locations wi and wj
on the same horizontal line, i.e., with the same y-coordinate. Formally, the empirical
directional semivariogram can be deï¬ned as
Ë†Î³Ïˆ(r) =
1
2n(r, Ïˆ)
X
(i,j)|âˆ¥wiâˆ’wjâˆ¥=r ,Ï†ij=Ïˆ
[z(wi) âˆ’z(wj)]2,
(34)
where Ï†ij denotes the angle between wi âˆ’wj and the x-axis, and n(r, Ïˆ) is the number
of points that are separated by a distance r with angle of separation equal to Ïˆ. It
is common to calculate a directional semivariogram for âˆ’90â—¦< Ïˆ â‰¤90â—¦in steps of
either 15â—¦or 30â—¦. We take a step size of 15â—¦here, which will result in 12 directional
semivariograms.
Once the directional semivariograms have been calculated for each of the 12
diï¬€erent Ïˆ angles, we ï¬t a common scatterplot smoother, the loess curve [40], to the
semivariogram values in each direction to achieve continuous curves. Then, to determine
the ratio of correlation lengths, we can select a constant Î³crit value between the nugget
and sill and observe the distance required for the loess curve to surpass the height of
Î³crit. The direction of maximum correlation, Î¸, will require a larger distance to reach
Î³crit than other directions since the variance of the diï¬€erences between values in that
direction is expected to be smaller. The anisotropy ratio, Ï„, can then be computed as
the ratio between the distance in the direction of Î¸ and the distance in the direction
perpendicular to Î¸.
This process is illustrated in Figure 6. The directional semivariograms are shown
for the original, anisotropic ï¬eld on the left. We can see that the correlation length is
largest in the 45â—¦direction since the distance of 0.1684 that it takes for the curve to
pass Î³crit = 0.9 is the largest of any direction. The range distance in the âˆ’45â—¦direction
is 0.0561 and so the ratio of those ranges is Ï„ = 0.1684/0.0561 = 3.

Semivariogram methods for inverse problems
22
We can then rotate the ï¬eld clockwise by 45â—¦and stretch it in the direction of the
new y-axis by a factor of Ï„ = 3 to achieve an isotropic ï¬eld, as is done in the middle
of Figure 6. The directional semivariograms for the new ï¬eld are shown on the right in
Figure 6. It now takes a distance of 0.1684 for the variogram values to pass Î³crit for each
Ïˆ angle, which means the ratio has been reduced to one, as it should be for an isotropic
ï¬eld. It is not always the case that we can reduce the ratio of these range values down to
one, but we can reduce it enough for the ï¬eld to be considered approximately isotropic.
Once we have obtained Î¸ and Ï„ and have changed the coordinates of the ï¬eld, we
can ï¬t an isotropic omnidirectional semivariogram to estimate Î½ and â„“1. Then we let
â„“2 = â„“1/Ï„. All hyperparameters for use in (31) will have been estimated and we can
update these estimates iteratively using Algorithm 2. The convergence criteria for these
hyperparameters are as follows: Î¸j âˆ’Î¸jâˆ’1 = 0, Î½j âˆ’Î½jâˆ’1 = 0, |â„“j
1 âˆ’â„“jâˆ’1
1
|/â„“jâˆ’1
1
< 0.01, and
|â„“j
2 âˆ’â„“jâˆ’1
2
|/â„“jâˆ’1
2
< 0.01 where Î¸j, Î½j, â„“j
1 and â„“j
2 denotes the jth iteration of the respective
hyperparameter.
4.5. Numerical Experiments
We will illustrate the semivariogram method in the anisotropic case with a two-
dimensional inpainting example.
The original image, given on the left in Figure 7,
shows a rock formation in Northern Arizona known as the Wave [41] where the layers
of sandstone strata are clearly visible. We selected a subsection in the lower-middle of
the image, shown in the middle of Figure 7, to illustrate our method. This will be the
true image. We then added some noise and masked 60% of the image. This is shown
on the right in Figure 7.
Like we saw in Section 3.1.1, the prior will play a large role in the inpainting
process since much of the image is missing. We will directly compare the solution using
the anisotropic Whittle-MatÂ´ern prior to the solution using the isotropic Whittle-MatÂ´ern
prior, both of which will have hyperparameters determined using semivariograms. Like
before, the regularization parameter, Î±, will be optimized using the highest correlation
between the solution and the true image.
Algorithm 2 The Semivariogram Method for MAP Estimation with Anisotropic
Whittle-MatÂ´ern Prior.
0. Set xÎ± = b.
1. Estimate Î¸ and Ï„ by computing directional semivariograms for xÎ±.
2.
Transform the anisotropic spatial ï¬eld coordinates, w, to isotropic spatial ï¬eld
coordinates, u, using (24).
3. Estimate Î¸ = (a0, Ïƒ2, Î½, â„“1) by ï¬tting an isotropic MatÂ´ern semivariogram model to
the transformed ï¬eld. Then compute â„“2 = â„“1/Ï„.
4. Deï¬ne the prior precision matrix, P, by (32) using Î½, â„“1, â„“2, and Î¸, compute Î± using
(19), and compute xÎ± using (33).
5. Return to step 1 and repeat until Î¸, Ï„, Î½, â„“1, and â„“2 stabilize.

Semivariogram methods for inverse problems
23
20
40
60
80
100
120
120
100
80
60
40
20
20
40
60
80
100
120
120
100
80
60
40
20
Figure 7. Inpainting example. The original image showing the rock layers of the
Wave in northern Arizona is given on the left. The true image used in the inpainting
example is given in the middle. The masked image is given on the right.
20
40
60
80
100
120
120
100
80
60
40
20
20
40
60
80
100
120
120
100
80
60
40
20
20
40
60
80
100
120
120
100
80
60
40
20
Figure 8. Inpainting solutions. The true image (left) is given along with the the
isotropic solution (middle) and the anisotropic solution (right).
After calculating the directional semivariograms for the image, the direction of
maximum correlation was determined to be âˆ’75â—¦for each color intensity. For the blue
color-band, the correlation length in that direction was â„“1 = 0.1517 and the correlation
in the 15â—¦direction was â„“2 = 0.0101, which gives a ratio of Ï„ = 15. Î½ was determined
to be 1 and all of these hyperparameters converged in at most four iterations for each
color and the initial Î¸ estimate of âˆ’75â—¦given in the ï¬rst iteration remained unchanged
throughout the process. When ï¬tting an omnidirectional semivariogram to the masked
image for the isotropic case, Î½ = 2 and â„“= 0.0142.
The reconstructions are given in Figure 8. With the isotropic solution, the masking
is removed, but since the prior assigns a very small correlation between each pixel, the
reconstruction is noticeably spotty. The anisotropic solution, however, does a good job
of removing the masking completely. The reconstruction is a bit smoother than the true
image, but the original sandstone layers can be seen nicely.
Some statistics of the reconstructions are given in Table 1. Although the isotropic
solution is still competitive, the anisotropic prior gives the reconstruction that most
closely aligns with the true image. The isotropic solution has a mean absolute error
(MAE) more than 57% larger and a mean squared error (MSE) more than 180% higher

Semivariogram methods for inverse problems
24
Table 1. Statistics for inpainting MAP estimates.
True Image
Isotropic Covariance
Anisotropic Covariance
Â¯x
0.530
0.530
0.530
s
0.207
0.202
0.206
Min
0.000
âˆ’0.053
âˆ’0.065
Q1
0.357
0.364
0.360
Median
0.522
0.520
0.520
Q3
0.678
0.676
0.678
Max
1.000
1.112
1.093
ÏxÎ±,xtrue
0.944
0.981
Residual MAE
0.045
0.029
Residual MSE
0.005
0.002
than those respective measures in the anisotropic case. The anisotropic reconstruction
does fall short with the minimum value, however, which is farther from the truth than
the solution given by the isotropic prior.
4.6. Discussion
Although the reconstruction with the anisotropic prior covariance matrix is better here,
there are still some improvements that can be made. This example had a constant angle
of maximum correlation length throughout the image and the ratio between maximum
and minimum correlation was rather high, that is, greater than ï¬ve. If either of these
features fail to hold, the anisotropic prior often produces a reconstruction that performs
slightly worse or oï¬€ers no beneï¬t over using an isotropic prior. We focus on the case
when the angle of maximum anisotropy is not constant in the next section.
5. Regional Anisotropy
We have a way to deï¬ne priors for isotropic and anisotropic spatial ï¬elds as long as that
covariance structure is consistent for the entire ï¬eld. In the case where the correlation
length and angle of maximum anisotropy change throughout the image, we will want to
model each of these regions with a diï¬€erent prior.
5.1. Regional Precision Matrix
Suppose we have k diï¬€erent regions in our image, each of which has a diï¬€erent covariance
structure. We deï¬ne Di, i = 1, . . . , k, as a masking matrix such that the only non-zero
elements of Dix are those in region i. We will not allow for overlapping regions so that
Pk
i=1 Dk = I, the identity matrix. Now, to establish a prior for x in this regional case,
we take Cov(x) = Cov(D1x + . . . + Dkx) = Cov(D1x) + . . . + Cov(Dkx), since each
region is assumed independent due to not having any elements of x in common. Deï¬ne
the best Whittle-MatÂ´ern covariance structure, as chosen by a semivariogram, for region

Semivariogram methods for inverse problems
25
i as Ci with corresponding precision matrix Pi = Câˆ’1
i . Then Cov(Dix) := DiCiDi.
Thus, the prior for x in this regional case has pdf
p(x|Î´) âˆexp

âˆ’Î´
2xT(D1C1D1 + . . . + DkCkDk)âˆ’1x

,
(35)
which means our precision matrix is given by P = (D1C1D1 + . . . + DkCkDk)âˆ’1. Note
that (35) reduces to (3) with P = P1 when k = 1. In general, the Ci matrices and P
are dense, so actually constructing this precision matrix is infeasible for large problems.
Additionally, FFTs cannot be used since D1C1D1 + . . . + DkCkDk is not circulant
even if each Ci is. Thus, we seek an alternative expression such that the matrix-vector
multiplication Px is achievable.
Without loss of generality, let k = 2. Let
C1 =
"
C1A
C1B
C1C
C1D
#
= Pâˆ’1
1
=
"
P1A
P1B
P1C
P1D
#âˆ’1
and
C2 =
"
C2A
C2B
C2C
C2D
#
= Pâˆ’1
2
=
"
P2A
P2B
P2C
P2D
#âˆ’1
.
Also assume that the regions are deï¬ned in a way that divides the region vertically (an
assumption we will drop later) so that
C = Cov(x) = Cov(D1x + D2x) = Cov(D1x) + Cov(D2x) =
"
C1A
0
0
C2D
#
,
which means our precision matrix is
P = Câˆ’1 =
"
Câˆ’1
1A
0
0
Câˆ’1
2D
#
.
Using the block matrix inversion identity, it can be shown that Câˆ’1
1A = P1A âˆ’
P1BP âˆ’1
1DP1C and Câˆ’1
2D = P2D âˆ’P2CP âˆ’1
2A P2B and thus
P = Câˆ’1 =
"
Câˆ’1
1A
0
0
Câˆ’1
2D
#
=
"
P1A âˆ’P1BP âˆ’1
1DP1C
0
0
P2D âˆ’P2CP âˆ’1
2A P2B
#
,
which can be equivalently written as
P = D1P1D1 âˆ’D1P1(D2P1D2)â€ P1D1 + D2P2D2 âˆ’D2P2(D1P2D1)â€ P2D2.
In general, for k > 2,
P = Câˆ’1 = (D1C1D1 + D2C2D2 + . . . + DkCkDk)âˆ’1
=
k
X
i=1

DiPiDi âˆ’DiPi
h
(IN âˆ’Di)Pi(IN âˆ’Di)
iâ€ 
PiDi

,
(36)
which, since each Pi is sparse, involves only sparse matrices. It is straightforward to
show (36) holds even in the case where the regions do not divide the region vertically
by performing a reordering of the indices of x.

Semivariogram methods for inverse problems
26
Since we have an expression for P, we can now discuss how to perform the
multiplication Px. This will be needed to perform an iterative inverse method such as
conjugate gradient to obtain the MAP estimator. Since each Di and Pi is sparse, each
matrix vector multiplication in (36) is eï¬ƒcient except the ones involving pseudoinverses.
We can, however, take advantage of the lower-rank structure of [(IN âˆ’Di)Pi(IN âˆ’Di)]â€ ,
which has rank N âˆ’ri where ri the rank of Di. Let Pi,nz be the square matrix that
consists of all rows and columns of (IN âˆ’Di)Pi(IN âˆ’Di) that have any nonzero elements.
That is, keep row and column j of (IN âˆ’Di)Pi(IN âˆ’Di) if [IN âˆ’Di]j,j = 1. Then let
Ri = chol(Pi,nz) such that Pi,nz = RT
i Ri where chol denotes the Cholesky factorization
and Ri is upper triangular. The Cholesky decomposition is known to be eï¬ƒcient for
sparse, symmetric, positive deï¬nite matrices such as Pi,nz [42]. Then we can perform
the multiplication of DiPi[(IN âˆ’Di)Pi(IN âˆ’Di)]â€ PiDix in the following way:
1. Multiply yi = Pi(Dix).
2. Extract the N âˆ’ri elements of yi that correspond to the nonzero diagonal elements
of IN âˆ’Di: yi(ind).
3. Deï¬ne a variable zi as an N Ã— 1 vector of zeros.
4. Multiply by

(IN âˆ’Di)Pi(IN âˆ’Di)
â€ 
by taking zi(ind) = Ri\(RT
i \yi(ind)).
5. Complete the multiplication Di(Pizi).
6. Repeat for 1 â‰¤i â‰¤k, so Px =
k
X
i=1
Di(Pizi).
Step 4 is the most costly since it requires both a forward and a backward
substitution. This can be performed more eï¬ƒciently for large regions since the rank of
(IN âˆ’Di)Pi(IN âˆ’Di) is inversely related to the size of region i. Also, sparse reorderings,
such as the symmetric approximate minimum degree permutation, can be used so Ri has
fewer nonzero entries. The multiplication of Px must be performed for each iteration of
CG, but each Ri can be stored ahead of time so the Cholesky decompositions need only
be performed once. We saw some improvements in the performance of the CG algorithm
when a preconditioner was used. The total number of iterations was approximately 21%
lower, which corresponded to about a 15% overall time saving.
5.2. Numerical Experiments
We now consider an example where the angle of maximum anisotropy changes
throughout the image. We take the central portion of the Wave image from Figure
7 and again mask it so that 60% of the image is blank. Then we attempt to inpaint the
image using an isotropic prior, an anisotropic prior, and a regional anisotropic prior.
The results are shown in Figure 9. The top-right image shows the masked picture as
well as how the regions were chosen. The ï¬rst region is shown with the red overlay
while the second region is the remainder of the image. Semivariograms were ï¬t to both
regions and the top region was given a prior with an angle of maximum anisotropy of

Semivariogram methods for inverse problems
27
20
40
60
80
100
120
120
100
80
60
40
20
20
40
60
80
100
120
120
100
80
60
40
20
20
40
60
80
100
120
120
100
80
60
40
20
20
40
60
80
100
120
120
100
80
60
40
20
Figure 9. Inpainting solutions. The true image (top-left) is given along with the
masked image (top-right), the isotropic solution (bottom-left) and the anisotropic
solution (bottom-middle), and the regional solution (bottom-right).
Table 2. Statistics for regional inpainting MAP estimates.
True Image
Isotropic Covariance
Anisotropic Covariance
Regional Covariance
Â¯x
0.567
0.565
0.566
0.566
s
0.207
0.200
0.202
0.207
Min
0.000
âˆ’0.014
âˆ’0.082
âˆ’0.032
Q1
0.400
0.402
0.402
0.398
Median
0.565
0.564
0.564
0.562
Q3
0.722
0.717
0.718
0.720
Max
1.000
1.085
1.077
1.160
ÏxÎ±,xtrue
0.954
0.954
0.969
Residual MAE
0.042
0.041
0.035
Residual MSE
0.004
0.004
0.003
âˆ’30â—¦while Î¸ = âˆ’75â—¦for the bottom region. In the anisotropic solution given in the
bottom-middle of the ï¬gure, Î¸ = âˆ’75â—¦throughout the image. Qualitatively, the regional
solution in the bottom-right of the ï¬gure looks best.
Turning to Table 2, we can see the statistics comparing the diï¬€erent reconstructions.
The isotropic and anisotropic solutions were similar in terms of the correlation and mean
errors, but the regional solution is better in both of those categories and is similar in
the others.

Semivariogram methods for inverse problems
28
5.3. Discussion
The regional covariance solution performed better in this example, but it does have
some shortcomings. Firstly, it is best used when the distinction between regions is high.
This is because the transition between regions when using this prior is abrupt, rather
than smooth. Smoothing the transition between regions is something we leave to future
work. Additionally, since multiplying P by x requires inverting a matrix, this method
can be slow when that matrix is large, which corresponds to a small region. Therefore,
we suggest using small regions only when necessary. Alternatively, it is possible to solve
a diï¬€erent inverse problem for each region independently and then combine the results.
This will allow FFTs to be used since the precision matrix for each inverse problem will
be in the form of (32).
6. Conclusion
In this paper, we introduced a method for selecting hyperparameters for use in the
prior distribution of x based on semivariogram modeling.
We think of the noisy
data as a spatial ï¬eld and ï¬t semivariograms to the noisy data and then iteratively
to the MAP estimates to obtain point estimates for the prior hyperparameters. This
method relies on the fact that the solution of the SPDE (6) is a Gaussian process with
zero mean and MatÂ´ern covariance operator, which we have shown in detal. However,
this connection requires an inï¬nite domain, for us R2. For a ï¬nite domain, which is
typically required for computations, the connection is broken, i.e., the SPDE solution
is a zero mean Gaussian process without a MatÂ´ern covariance operator. Fortunately,
the connection can be restored by extending the ï¬nite computational domain.
We
showed how to systematically choose the extended domain using the MatÂ´ern parameters.
The semivariogram method has the beneï¬ts of giving point estimates with a more
intuitive interpretation while providing an objective way to choose an extension of the
computational domain that is adequate for restoring the SPDE/MatÂ´ern connection.
We then applied the semivariogram method to an isotropic inpainting and deblurring
example in two dimensions.
We generalized the isotropic results to the anisotropic case and showed the
semivariogram method can be applied as well by using directional semivariograms and
the anisotropic SPDE (25). An inpainting example comparing reconstructions using
isotropic and anisotropic priors was presented.
Finally, we discussed an even more
general case when the image has regions with diï¬€ering correlation lengths and angles of
maximum correlation, which requires a sparse precision matrix that can be obtained via
a discretized SPDE for each region. One more example was shown that yielded good
solutions.

Semivariogram methods for inverse problems
29
Acknowledgments
J. Bardsley acknowledges support from the Gordon Preston Fellowship oï¬€ered by the
School of Mathematics at Monash University. T. Cui acknowledges support from the
Australian Research Council, under grant number CE140100049 (ACEMS). We would
also like to acknowledge the assistance of Dr. Jon Graham at the University of Montana
with the semivariogram methodology.
References
[1] Jari Kaipio and Erkki Somersalo. Statistical and Computational Methods for Inverse Problems.
Springer, 2005.
[2] Michael L Stein.
Interpolation of Spatial Data: Some Theory for Kriging.
Springer Science &
Business Media, 2012.
[3] Peter Guttorp and Tilmann Gneiting. Studies in the history of probability and statistics XLIX
On the MatÂ´ern correlation family. Biometrika, 93(4):989â€“995, 12 2006.
[4] Bertil MatÂ´ern. Spatial Variation, volume 36. Springer Science & Business Media, 2013.
[5] Larry C Andrews. Special Functions of Mathematics for Engineers. McGraw-Hill New York, 1992.
[6] Budiman Minasny and Alex B McBratney.
The MatÂ´ern function as a general model for soil
variograms. Geoderma, 128(3-4):192â€“207, 2005.
[7] Finn Lindgren, HËšavard Rue, and Johan LindstrÂ¨om. An explicit link between Gaussian ï¬elds and
Gaussian Markov random ï¬elds: the stochastic partial diï¬€erential equation approach. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 73(4):423â€“498, 2011.
[8] Andrew TA Wood and Grace Chan. Simulation of Stationary Gaussian Processes in [0, 1]d. Journal
of Computational and Graphical Statistics, 3(4):409â€“432, 1994.
[9] Claude R Dietrich and Garry N Newsam.
Fast and exact simulation of stationary Gaussian
processes through circulant embedding of the covariance matrix. SIAM Journal on Scientiï¬c
Computing, 18(4):1088â€“1107, 1997.
[10] Johnathan M Bardsley. Computational Uncertainty Quantiï¬cation for Inverse Problems. SIAM,
2018.
[11] Peter Whittle. On Stationary Processes in the Plane. Biometrika, 41(3/4):434â€“449, 1954.
[12] Lassi Roininen, Mark Girolami, Sari Lasanen, and Markku Markkanen. Hyperpriors for MatÂ´ern
ï¬elds with applications in Bayesian inversion. Inverse Problems & Imaging, 13, 12 2016.
[13] Lassi Roininen, Janne MJ Huttunen, and Sari Lasanen.
Whittle-MatÂ´ern priors for Bayesian
statistical inversion with applications in electrical impedance tomography.
Inverse Problems
& Imaging, 8(2):561â€“586, 2014.
[14] Karla Monterrubio-GÂ´omez, Lassi Roininen, Sara Wade, Theo Damoulas, and Mark Girolami.
Posterior Inference for Sparse Hierarchical Non-stationary Models. 04 2018.
[15] Lassi Roininen, Petteri Piiroinen, Markku Lehtinen, et al.
Constructing continuous stationary
covariances as limits of the second-order stochastic diï¬€erence equations.
Inverse Problems &
Imaging, 7(2):611â€“647, 2013.
[16] Gabriel J Lord, Catherine E Powell, and Tony Shardlow.
An Introduction to Computational
Stochastic PDEs. Number 50 in Cambridge texts in applied mathematics. Cambridge University
Press, 2014.
[17] Havard Rue and Leonhard Held.
Gaussian Markov Random Fields: Theory and Applications.
CRC press, 2005.
[18] John B Walsh.
An introduction to stochastic partial diï¬€erential equations.
In Â´Ecole dâ€™Â´EtÂ´e de
ProbabilitÂ´es de Saint Flour XIV - 1984, pages 265â€“439. Springer Berlin Heidelberg, 1986.
[19] Sadri Hassani. Dirac Delta Function. In Mathematical Methods, pages 289â€“319. Springer, 2000.

Semivariogram methods for inverse problems
30
[20] Mark S Gockenbach. Partial Diï¬€erential Equations: Analytical and Numerical Methods, volume
122. SIAM, 2005.
[21] Ivar Stakgold and Michael J Holst. Greenâ€™s Functions and Boundary Value Problems, volume 99.
John Wiley & Sons, 2011.
[22] Stanis law Saks. Theory of the integral. Hafner Publishing Company, 1937.
[23] Ian Naismith Sneddon. Fourier Transforms. Courier Corporation, 1995.
[24] Mateusz KwaÂ´snicki.
Ten equivalent deï¬nitions of the fractional Laplace operator.
Fractional
Calculus and Applied Analysis, 20(1):7â€“51, 2017.
[25] Robert Piessens.
The Hankel Transform.
In Alexander D Poularikas, editor, Transforms and
Applications Handbook, chapter 9. CRC Press, Boca Raton, FL, 2000.
[26] Loukas Grafakos and Gerald Teschl. On Fourier transforms of radial functions and distributions.
Journal of Fourier Analysis and Applications, 19(1):167â€“179, 2013.
[27] Harry Bateman. Tables of Integral Transforms [Volumes I & II]. McGraw-Hill, 1954.
[28] U Khristenko, L Scarabosio, P Swierczynski, E Ullmann, and B Wohlmuth. Analysis of Boundary
Eï¬€ects on PDE-Based Sampling of Whittleâ€“MatÂ´ern Random Fields.
SIAM/ASA Journal on
Uncertainty Quantiï¬cation, 7(3):948â€“974, 2019.
[29] Curtis R Vogel. Computational Methods for Inverse Problems. Siam, 2002.
[30] Per Christian Hansen.
Rank-Deï¬cient and Discrete Ill-Posed Problems: Numerical Aspects of
Linear Inversion, volume 4. Siam, 2005.
[31] Majid Jafari Khaledi and Firoozeh Rivaz. Empirical Bayes spatial prediction using a Monte Carlo
EM algorithm. Statistical Methods and Applications, 18(1):35â€“47, 2009.
[32] Christian Robert and George Casella.
Monte Carlo Statistical Methods.
Springer Science &
Business Media, 2013.
[33] Oliver Schabenberger and Carol A Gotway. Statistical Methods for Spatial Data Analysis. CRC
press, 2017.
[34] Noel Cressie. Statistics for Spatial Data. John Wiley & Sons, 2015.
[35] W. Schwanghart. Experimental (Semi-) Variogram, 09 Jan 2013. MATLAB Central File Exchange.
Retrieved 21 May 2018.
[36] W. Schwanghart. variogramï¬t, 14 Oct 2010. MATLAB Central File Exchange. Retrieved 21 May
2018.
[37] George Casella. An Introduction to Empirical Bayes Data Analysis. The American Statistician,
39(2):83â€“87, 1985.
[38] Dave Hale.
Implementing an anisotropic and spatially varying MatÂ´ern model covariance with
smoothing ï¬lters. 2013.
[39] Kathryn Anne Haskard. An anisotropic MatÂ´ern spatial covariance model: REML estimation and
properties. PhD thesis, University of Adelaide, 2007.
[40] William G Jacoby.
Loess: a nonparametric, graphical tool for depicting relationships between
variables. Electoral Studies, 19(4):577â€“613, 2000.
[41] Gb11111. Arizona â€“ the wave. Flickr. Retrieved 22 May 2019.
[42] David S Watkins. Fundamentals of Matrix Computations, volume 64. John Wiley & Sons, 2004.
